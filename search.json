[
  {
    "objectID": "static/templates/the_data.html",
    "href": "static/templates/the_data.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('{{date}}')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load({{year}}, week = {{week}})\n\n{{#datasets}}\n{{{dataset_name}}} &lt;- tuesdata${{{dataset_name}}}\n{{/datasets}}\n\n# Option 2: Read directly from GitHub\n\n{{#datasets}}\n{{{dataset_name}}} &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/{{year}}/{{date}}/{{dataset_file}}')\n{{/datasets}}\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('{{date}}')\n\n# Option 2: Read directly from GitHub and assign to an object\n\n{{#datasets}}\n{{{dataset_name}}} = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/{{year}}/{{date}}/{{dataset_file}}')\n{{/datasets}}\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download datasets for the week, and load them as a NamedTuple of DataFrames\ndata = tt_load(\"{{date}}\")\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\n{{#datasets}}\n{{{dataset_name}}} = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/{{year}}/{{date}}/{{dataset_file}}\")\n{{/datasets}}\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\n{{#datasets}}\n{{{dataset_name}}} = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/{{year}}/{{date}}/{{dataset_file}}\", DataFrame)\n{{/datasets}}"
  },
  {
    "objectID": "static/templates/the_data.html#the-data",
    "href": "static/templates/the_data.html#the-data",
    "title": "TidyTuesday",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('{{date}}')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load({{year}}, week = {{week}})\n\n{{#datasets}}\n{{{dataset_name}}} &lt;- tuesdata${{{dataset_name}}}\n{{/datasets}}\n\n# Option 2: Read directly from GitHub\n\n{{#datasets}}\n{{{dataset_name}}} &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/{{year}}/{{date}}/{{dataset_file}}')\n{{/datasets}}\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('{{date}}')\n\n# Option 2: Read directly from GitHub and assign to an object\n\n{{#datasets}}\n{{{dataset_name}}} = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/{{year}}/{{date}}/{{dataset_file}}')\n{{/datasets}}\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download datasets for the week, and load them as a NamedTuple of DataFrames\ndata = tt_load(\"{{date}}\")\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\n{{#datasets}}\n{{{dataset_name}}} = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/{{year}}/{{date}}/{{dataset_file}}\")\n{{/datasets}}\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\n{{#datasets}}\n{{{dataset_name}}} = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/{{year}}/{{date}}/{{dataset_file}}\", DataFrame)\n{{/datasets}}"
  },
  {
    "objectID": "sharing.html",
    "href": "sharing.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "We welcome all newcomers, enthusiasts, and experts to participate, but be mindful of these things:\n\nThe data set comes from the source article or the source that the article credits. Be mindful that the data is what it is! You are welcome to explore beyond the provided dataset, but the data is provided as a “toy” dataset to practice techniques on.\n\nThis is NOT about criticizing the original article or graph. Real people made the graphs, collected or acquired the data! Focus on the provided dataset, learning, and improving your techniques in R.\n\nThis is NOT about criticizing or tearing down your fellow data practitioners or their code! Be supportive and kind to each other! Like others’ posts and help promote the data community!\n\nUse the hashtag #TidyTuesday on social media. You might also wish to add a language-specific hashtag: R: #RStats; Python: #PyData, #PydyTuesday; Julia: #JuliaLang, #TidierTuesday. On BlueSky, add the #DataBS (the BlueSky Data community) hashtag.\nInclude a picture of the visualization when you post to social media.\n\nInclude alt text for any visualizations shared, so that everyone can participate! We have an article with tips and tricks to help you with this aspect of data communication. This is important for any images you share on social media, and is part of learning to be an effective data science communicator!\nInclude a copy of the code used to create your visualization when you post to social media.\nFocus on improving your craft, even if you end up with something simple!\n\nGive credit to the original data source whenever possible.\n\nFor R, the bskyr package is helpful for posting to Bluesky, and the rtoot package is helpful for posting to Mastodon."
  },
  {
    "objectID": "sharing.html#sharing-your-output",
    "href": "sharing.html#sharing-your-output",
    "title": "TidyTuesday",
    "section": "",
    "text": "We welcome all newcomers, enthusiasts, and experts to participate, but be mindful of these things:\n\nThe data set comes from the source article or the source that the article credits. Be mindful that the data is what it is! You are welcome to explore beyond the provided dataset, but the data is provided as a “toy” dataset to practice techniques on.\n\nThis is NOT about criticizing the original article or graph. Real people made the graphs, collected or acquired the data! Focus on the provided dataset, learning, and improving your techniques in R.\n\nThis is NOT about criticizing or tearing down your fellow data practitioners or their code! Be supportive and kind to each other! Like others’ posts and help promote the data community!\n\nUse the hashtag #TidyTuesday on social media. You might also wish to add a language-specific hashtag: R: #RStats; Python: #PyData, #PydyTuesday; Julia: #JuliaLang, #TidierTuesday. On BlueSky, add the #DataBS (the BlueSky Data community) hashtag.\nInclude a picture of the visualization when you post to social media.\n\nInclude alt text for any visualizations shared, so that everyone can participate! We have an article with tips and tricks to help you with this aspect of data communication. This is important for any images you share on social media, and is part of learning to be an effective data science communicator!\nInclude a copy of the code used to create your visualization when you post to social media.\nFocus on improving your craft, even if you end up with something simple!\n\nGive credit to the original data source whenever possible.\n\nFor R, the bskyr package is helpful for posting to Bluesky, and the rtoot package is helpful for posting to Mastodon."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "Here we have gathered links relevant to TidyTuesday.\n\n\n\n\n\nLink\nDescription\n\n\n\n\nLink\nThe Data Science Learning Community Website\n\n\nLink\nThe R for Data Science textbook\n\n\nLink\nCarbon for sharing beautiful code pics\n\n\nLink\nPost gist to Carbon from RStudio\n\n\nLink\nPost to Carbon from RStudio\n\n\nLink\nJoin GitHub!\n\n\nLink\nBasics of GitHub\n\n\nLink\nLearn how to use GitHub with R\n\n\nLink\nSave high-rez ggplot2 images",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "links.html#getting-started",
    "href": "links.html#getting-started",
    "title": "Useful links",
    "section": "",
    "text": "Link\nDescription\n\n\n\n\nLink\nThe Data Science Learning Community Website\n\n\nLink\nThe R for Data Science textbook\n\n\nLink\nCarbon for sharing beautiful code pics\n\n\nLink\nPost gist to Carbon from RStudio\n\n\nLink\nPost to Carbon from RStudio\n\n\nLink\nJoin GitHub!\n\n\nLink\nBasics of GitHub\n\n\nLink\nLearn how to use GitHub with R\n\n\nLink\nSave high-rez ggplot2 images",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "dataset_idea.html",
    "href": "dataset_idea.html",
    "title": "Submitting a dataset idea",
    "section": "",
    "text": "Submitting a dataset idea\nPlease consider submitting the dataset through a pull request instead! This is the best way to help us.\nLook through existing dataset suggestions to see if the dataset you have in mind has already been suggested. You may be able to provide more information to an existing suggestion.\nSee our guide to reviewing submitted datasets for the checklist we will need to complete for the submitted dataset. The more you can do that in the initial issue, the better!\nIf you find yourself filling out all of the information, we strongly suggest submitting a pull request instead! We’ll help you through the process if you’ve never submitted a pull request before!\nYou can submit a dataset here."
  },
  {
    "objectID": "data/curated/template/intro.html",
    "href": "data/curated/template/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "PasteQuoteHere"
  },
  {
    "objectID": "data/2026/readme.html",
    "href": "data/2026/readme.html",
    "title": "2026 Data",
    "section": "",
    "text": "2026 Data\nArchive of datasets and articles from the 2026 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2026-01-06\nBring your own data from 2025!\nNA\nNA\n\n\n2\n2026-01-13\nThe Languages of Africa\nLanguages of Africa\nLanguages of Africa\n\n\n3\n2026-01-20\nAstronomy Picture of the Day (APOD) Archive\nNASA API\nAstronomy Picture of the Day\n\n\n4\n2026-01-27\nBrazilian Companies\nOpen data CNPJ - December 2025\nWikipedia’s List of largest Brazilian companies"
  },
  {
    "objectID": "data/2026/2026-01-27/readme.html",
    "href": "data/2026/2026-01-27/readme.html",
    "title": "Brazilian Companies",
    "section": "",
    "text": "This week we’re exploring Brazilian Companies, curated from Brazil’s open CNPJ (Cadastro Nacional da Pessoa Jurídica) records published by the Brazilian Ministry of Finance / Receita Federal on the national open-data portal (dados.gov.br).\n\nThe CNPJ open data is a large-scale public registry of Brazilian legal entities. For this dataset, the raw company records were cleaned and enriched with lookup tables (legal nature, owner qualification, and company size), then filtered to retain firms above a share-capital threshold so the analysis focuses on meaningful variation in capital stock.\n\n\nWhich legal nature categories concentrate the highest total and average capital stock?\nHow does company size relate to capital stock (and how skewed is it)?\nDo specific owner qualification groups dominate high-capital companies?\nWhat patterns emerge when comparing the top capital-stock tail across categories (legal nature, size, qualification)?\n\nThank you to Marcelo Silva for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2026-01-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 4)\n\ncompanies &lt;- tuesdata$companies\nlegal_nature &lt;- tuesdata$legal_nature\nqualifications &lt;- tuesdata$qualifications\nsize &lt;- tuesdata$size\n\n# Option 2: Read directly from GitHub\n\ncompanies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv')\nlegal_nature &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv')\nqualifications &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv')\nsize &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2026-01-27')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncompanies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv')\nlegal_nature = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv')\nqualifications = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv')\nsize = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download datasets for the week, and load them as a NamedTuple of DataFrames\ndata = tt_load(\"2026-01-27\")\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncompanies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv\")\nlegal_nature = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv\")\nqualifications = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv\")\nsize = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncompanies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv\", DataFrame)\nlegal_nature = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv\", DataFrame)\nqualifications = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv\", DataFrame)\nsize = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncompany_id\ninteger\nCompany identifier (8-digit ID used as the primary key in this dataset).\n\n\ncompany_name\ncharacter\nCompany legal name (as provided in the source registry).\n\n\nlegal_nature\ncharacter\nCompany legal nature (e.g., “Limited Liability Business Company (LLC)”).\n\n\nowner_qualification\ncharacter\nOwner/partner qualification label (e.g., “Managing Partner / Partner-Administrator”).\n\n\ncapital_stock\nnumeric\nDeclared share capital (BRL), numeric.\n\n\ncompany_size\ncharacter\nCompany size category (e.g.,micro-enterprise, small-enterprise, other).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nLegal nature code (source registry code).\n\n\nlegal_nature\ncharacter\nLegal nature label corresponding to id.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nOwner qualification code (source registry code).\n\n\nowner_qualification\ncharacter\nOwner qualification label corresponding to id.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nCompany size code (source registry code).\n\n\ncompany_size\ncharacter\nCompany size label corresponding to id (e.g., micro-enterprise, small-enterprise).\n\n\n\n\n\n\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\n\n# Input data\ncompanies0 = pd.read_csv(\"../raw_data/raw_companies.csv\", sep=\";\", encoding=\"cp1252\", header=None)\n\nlegal_nature = pd.read_csv('../raw_data/legal_nature.csv', sep=',')\n\nsizes = pd.read_csv(\"../raw_data/size.csv\", sep=\",\", encoding=\"cp1252\")\n\nqualifications = pd.read_csv(\"../raw_data/qualifications.csv\", sep = \",\", encoding=\"cp1252\")\n\n# Remove all private associations\ndef treat_companies_dataframe(dataframe):\n    companies_df_column_name = [\"company_id\", \"company_name\",\"legal_nature\", \"owner_qualification\",\"capital_stock\",\"company_size\",\"federal_owner\"]\n\n    dataframe.columns = companies_df_column_name\n\n    dataframe['capital_stock'] = dataframe['capital_stock'].str.replace(',', '.')\n    dataframe['capital_stock'] = pd.to_numeric(dataframe['capital_stock'], errors='coerce')\n    \n    dataframe_filtered = dataframe[dataframe['capital_stock'] &gt; 150000]\n    dataframe_filtered = dataframe_filtered.drop(columns=[\"federal_owner\"])\n    \n    return dataframe_filtered\n\ndef mapper(dataframe, dictionary: dict, column_name: str):\n    dataframe[column_name] = dataframe[column_name].map(dictionary)\n    return dataframe\n\ndef replace_info(dataframe):\n    legal_nature_dict = dict(zip(legal_nature['id'], legal_nature['legal_nature']))\n    qualification_dict = dict(zip(qualifications['id'], qualifications['owner_qualification']))\n    size_dict = dict(zip(sizes['id'], sizes['company_size']))\n\n    dataframe = mapper(dataframe, legal_nature_dict, 'legal_nature')\n    dataframe = mapper(dataframe, qualification_dict, \"owner_qualification\")\n    dataframe = mapper(dataframe, size_dict, 'company_size')\n\n    return dataframe\n\ndef merge_and_clean(df_top, df_bottom):\n\n    combined_df = pd.concat([df_top, df_bottom], ignore_index=True)\n    \n    cleaned_df = combined_df.drop_duplicates()\n    \n    return cleaned_df\n\nfiltered_df0 = treat_companies_dataframe(companies0)\n\nfiltered_df0 = replace_info(filtered_df0)\n\nwith open(\"../data/companies.csv\", mode=\"w\", newline='') as file:\n    write = csv.writer(file, delimiter=';')\n    write.writerow(filtered_df0.columns) \n    write.writerows(filtered_df0.values.tolist())"
  },
  {
    "objectID": "data/2026/2026-01-27/readme.html#the-data",
    "href": "data/2026/2026-01-27/readme.html#the-data",
    "title": "Brazilian Companies",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2026-01-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 4)\n\ncompanies &lt;- tuesdata$companies\nlegal_nature &lt;- tuesdata$legal_nature\nqualifications &lt;- tuesdata$qualifications\nsize &lt;- tuesdata$size\n\n# Option 2: Read directly from GitHub\n\ncompanies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv')\nlegal_nature &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv')\nqualifications &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv')\nsize &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2026-01-27')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncompanies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv')\nlegal_nature = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv')\nqualifications = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv')\nsize = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download datasets for the week, and load them as a NamedTuple of DataFrames\ndata = tt_load(\"2026-01-27\")\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncompanies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv\")\nlegal_nature = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv\")\nqualifications = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv\")\nsize = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncompanies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv\", DataFrame)\nlegal_nature = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv\", DataFrame)\nqualifications = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv\", DataFrame)\nsize = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv\", DataFrame)"
  },
  {
    "objectID": "data/2026/2026-01-27/readme.html#how-to-participate",
    "href": "data/2026/2026-01-27/readme.html#how-to-participate",
    "title": "Brazilian Companies",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2026/2026-01-27/readme.html#data-dictionary",
    "href": "data/2026/2026-01-27/readme.html#data-dictionary",
    "title": "Brazilian Companies",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ncompany_id\ninteger\nCompany identifier (8-digit ID used as the primary key in this dataset).\n\n\ncompany_name\ncharacter\nCompany legal name (as provided in the source registry).\n\n\nlegal_nature\ncharacter\nCompany legal nature (e.g., “Limited Liability Business Company (LLC)”).\n\n\nowner_qualification\ncharacter\nOwner/partner qualification label (e.g., “Managing Partner / Partner-Administrator”).\n\n\ncapital_stock\nnumeric\nDeclared share capital (BRL), numeric.\n\n\ncompany_size\ncharacter\nCompany size category (e.g.,micro-enterprise, small-enterprise, other).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nLegal nature code (source registry code).\n\n\nlegal_nature\ncharacter\nLegal nature label corresponding to id.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nOwner qualification code (source registry code).\n\n\nowner_qualification\ncharacter\nOwner qualification label corresponding to id.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nCompany size code (source registry code).\n\n\ncompany_size\ncharacter\nCompany size label corresponding to id (e.g., micro-enterprise, small-enterprise)."
  },
  {
    "objectID": "data/2026/2026-01-27/readme.html#cleaning-script",
    "href": "data/2026/2026-01-27/readme.html#cleaning-script",
    "title": "Brazilian Companies",
    "section": "",
    "text": "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\n\n# Input data\ncompanies0 = pd.read_csv(\"../raw_data/raw_companies.csv\", sep=\";\", encoding=\"cp1252\", header=None)\n\nlegal_nature = pd.read_csv('../raw_data/legal_nature.csv', sep=',')\n\nsizes = pd.read_csv(\"../raw_data/size.csv\", sep=\",\", encoding=\"cp1252\")\n\nqualifications = pd.read_csv(\"../raw_data/qualifications.csv\", sep = \",\", encoding=\"cp1252\")\n\n# Remove all private associations\ndef treat_companies_dataframe(dataframe):\n    companies_df_column_name = [\"company_id\", \"company_name\",\"legal_nature\", \"owner_qualification\",\"capital_stock\",\"company_size\",\"federal_owner\"]\n\n    dataframe.columns = companies_df_column_name\n\n    dataframe['capital_stock'] = dataframe['capital_stock'].str.replace(',', '.')\n    dataframe['capital_stock'] = pd.to_numeric(dataframe['capital_stock'], errors='coerce')\n    \n    dataframe_filtered = dataframe[dataframe['capital_stock'] &gt; 150000]\n    dataframe_filtered = dataframe_filtered.drop(columns=[\"federal_owner\"])\n    \n    return dataframe_filtered\n\ndef mapper(dataframe, dictionary: dict, column_name: str):\n    dataframe[column_name] = dataframe[column_name].map(dictionary)\n    return dataframe\n\ndef replace_info(dataframe):\n    legal_nature_dict = dict(zip(legal_nature['id'], legal_nature['legal_nature']))\n    qualification_dict = dict(zip(qualifications['id'], qualifications['owner_qualification']))\n    size_dict = dict(zip(sizes['id'], sizes['company_size']))\n\n    dataframe = mapper(dataframe, legal_nature_dict, 'legal_nature')\n    dataframe = mapper(dataframe, qualification_dict, \"owner_qualification\")\n    dataframe = mapper(dataframe, size_dict, 'company_size')\n\n    return dataframe\n\ndef merge_and_clean(df_top, df_bottom):\n\n    combined_df = pd.concat([df_top, df_bottom], ignore_index=True)\n    \n    cleaned_df = combined_df.drop_duplicates()\n    \n    return cleaned_df\n\nfiltered_df0 = treat_companies_dataframe(companies0)\n\nfiltered_df0 = replace_info(filtered_df0)\n\nwith open(\"../data/companies.csv\", mode=\"w\", newline='') as file:\n    write = csv.writer(file, delimiter=';')\n    write.writerow(filtered_df0.columns) \n    write.writerows(filtered_df0.values.tolist())"
  },
  {
    "objectID": "data/2026/2026-01-27/legal_nature.html",
    "href": "data/2026/2026-01-27/legal_nature.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nid\ninteger\nLegal nature code (source registry code).\n\n\nlegal_nature\ncharacter\nLegal nature label corresponding to id."
  },
  {
    "objectID": "data/2026/2026-01-27/companies.html",
    "href": "data/2026/2026-01-27/companies.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ncompany_id\ninteger\nCompany identifier (8-digit ID used as the primary key in this dataset).\n\n\ncompany_name\ncharacter\nCompany legal name (as provided in the source registry).\n\n\nlegal_nature\ncharacter\nCompany legal nature (e.g., “Limited Liability Business Company (LLC)”).\n\n\nowner_qualification\ncharacter\nOwner/partner qualification label (e.g., “Managing Partner / Partner-Administrator”).\n\n\ncapital_stock\nnumeric\nDeclared share capital (BRL), numeric.\n\n\ncompany_size\ncharacter\nCompany size category (e.g.,micro-enterprise, small-enterprise, other)."
  },
  {
    "objectID": "data/2026/2026-01-20/intro.html",
    "href": "data/2026/2026-01-20/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "This week we’re exploring the Astronomy Picture of the Day (APOD) archive. APOD is a popular NASA website featuring daily astronomy related images with a scientific explanation.\nEach day a different image or photograph of our universe is featured, along with a brief explanation. This APOD archive contains image information from the 2007 - 2025, pulled together into the {astropic} R package.\n\nWhat types of objects are most common in the archive?\nAre any images posted more than once?"
  },
  {
    "objectID": "data/2026/2026-01-13/readme.html",
    "href": "data/2026/2026-01-13/readme.html",
    "title": "The Languages of Africa",
    "section": "",
    "text": "This week we’re exploring data about popular languages spoken on the African continent. The dataset this week comes from the Languages of Africa page on Wikipedia.\n\nThe number of languages natively spoken in Africa is variously estimated (depending on the delineation of language vs. dialect) at between 1,250 and 2,100 and by some counts at over 3,000.\n\nThe dataset is rich with information on the number of languages spoken across the continent. Some of the questions that could be thought of include:\n\nWhich country in Africa has the largest number of spoken languages?\nWhich family of languages has the highest density of speakers?\nAre there any languages that cut across multiple countries?\n\nCan’t wait to see the kind of visualisations that can be created!\nThank you to Robert Muwanga, Data Enthusiast for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2026-01-13')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 2)\n\nafrica &lt;- tuesdata$africa\n\n# Option 2: Read directly from GitHub\n\nafrica &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-13/africa.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2026-01-13')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nafrica = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-13/africa.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2026-01-13')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nafrica = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-13/africa.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nafrica = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-13/africa.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlanguage\ncharacter\nName of popular African language.\n\n\nfamily\ncharacter\nGroup of languages with similar ancestry, having similar vocabulary, phonetics and grammar.\n\n\nnative_speakers\ninteger\nNumber of known native speakers of the language.\n\n\ncountry\ncharacter\nCountry where this language is spoken.\n\n\n\n\n\n\n\n#########################################################################\n# Author: Muwanga Robert                                                #\n# Date: 24 December 2025                                                #\n# License : Creative Commons (CC-BY)                                    #\n# Purpose: Scrapes data from Wikipedia on popular languages in Africa.  #\n#########################################################################\n\nrequire(tidyr)\nrequire(stringr)\nrequire(dplyr)\nrequire(purrr)\nrequire(rvest)\n\n# Let's get a list of African countries that we shall use in our data cleaning\n# process \n\nafrican_countries &lt;- \n  rvest::read_html('https://www.worldometers.info/geography/how-many-countries-in-africa/') |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(1) |&gt; \n  pull('Country') %&gt;% \n  c(., 'Ivory Coast', 'Cape Verde')\n\n# Let's extract the table of interest from Wikipedia\ndataset &lt;- \n  rvest::read_html(\"https://en.wikipedia.org/wiki/Languages_of_Africa\") |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(5) |&gt; \n  dplyr::select(\n    language = Language,\n    family = Family,\n    native_speakers = `Native speakers within Africa (L1)`, \n    country = `Official status per country`\n  ) \n\n# Clean up the family and native speaker columns, and extract the countries\n# from the country column\ndt &lt;- dataset |&gt; \n  mutate(\n    family = str_split_i(family, \" \", 1), \n    native_speakers = str_split_i(native_speakers, \" \", 1),\n    country = str_extract_all(\n      country, str_c(african_countries, collapse = \"|\"), simplify = TRUE))\n\n# From the country column, we \"expand\" the cells that have vectors, creating \n# a \"wider\" tibble\ncountries &lt;- \n  dataset |&gt; \n  select(country) |&gt; \n  map(.f = function(x) str_extract_all(\n    x, str_c(\n      african_countries, collapse = \"|\"), \n    simplify = TRUE)) |&gt; \n  as.data.frame()\n\n# We binding columns between the two datasets, make the dataset longer, and \n# remove rows that have blank entries in the country column\nafrica &lt;- bind_cols(dt, countries) |&gt; \n  select(-country) |&gt; \n  pivot_longer(\n    cols = contains(\"country\"),\n    names_to = 'country_index', values_to = 'country') |&gt; \n  mutate(\n    country = na_if(country, \"\")) |&gt; \n  drop_na()\n\n# We then clean up the native_speakers column and prepare our final dataset \n# by removing the unwanted country_index column\nafrica &lt;- africa |&gt; \n  mutate(native_speakers = str_split_i(native_speakers, pattern = '\\\\[|-', i = 1), \n         native_speakers = str_remove_all(native_speakers, pattern = ','), \n         native_speakers = as.integer(native_speakers)) |&gt; \n  filter(!is.na(native_speakers)) |&gt; \n  select(-country_index)"
  },
  {
    "objectID": "data/2026/2026-01-13/readme.html#the-data",
    "href": "data/2026/2026-01-13/readme.html#the-data",
    "title": "The Languages of Africa",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2026-01-13')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 2)\n\nafrica &lt;- tuesdata$africa\n\n# Option 2: Read directly from GitHub\n\nafrica &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-13/africa.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2026-01-13')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nafrica = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-13/africa.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2026-01-13')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nafrica = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-13/africa.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nafrica = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-13/africa.csv\", DataFrame)"
  },
  {
    "objectID": "data/2026/2026-01-13/readme.html#how-to-participate",
    "href": "data/2026/2026-01-13/readme.html#how-to-participate",
    "title": "The Languages of Africa",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2026/2026-01-13/readme.html#data-dictionary",
    "href": "data/2026/2026-01-13/readme.html#data-dictionary",
    "title": "The Languages of Africa",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nlanguage\ncharacter\nName of popular African language.\n\n\nfamily\ncharacter\nGroup of languages with similar ancestry, having similar vocabulary, phonetics and grammar.\n\n\nnative_speakers\ninteger\nNumber of known native speakers of the language.\n\n\ncountry\ncharacter\nCountry where this language is spoken."
  },
  {
    "objectID": "data/2026/2026-01-13/readme.html#cleaning-script",
    "href": "data/2026/2026-01-13/readme.html#cleaning-script",
    "title": "The Languages of Africa",
    "section": "",
    "text": "#########################################################################\n# Author: Muwanga Robert                                                #\n# Date: 24 December 2025                                                #\n# License : Creative Commons (CC-BY)                                    #\n# Purpose: Scrapes data from Wikipedia on popular languages in Africa.  #\n#########################################################################\n\nrequire(tidyr)\nrequire(stringr)\nrequire(dplyr)\nrequire(purrr)\nrequire(rvest)\n\n# Let's get a list of African countries that we shall use in our data cleaning\n# process \n\nafrican_countries &lt;- \n  rvest::read_html('https://www.worldometers.info/geography/how-many-countries-in-africa/') |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(1) |&gt; \n  pull('Country') %&gt;% \n  c(., 'Ivory Coast', 'Cape Verde')\n\n# Let's extract the table of interest from Wikipedia\ndataset &lt;- \n  rvest::read_html(\"https://en.wikipedia.org/wiki/Languages_of_Africa\") |&gt; \n  rvest::html_table() |&gt; \n  purrr::pluck(5) |&gt; \n  dplyr::select(\n    language = Language,\n    family = Family,\n    native_speakers = `Native speakers within Africa (L1)`, \n    country = `Official status per country`\n  ) \n\n# Clean up the family and native speaker columns, and extract the countries\n# from the country column\ndt &lt;- dataset |&gt; \n  mutate(\n    family = str_split_i(family, \" \", 1), \n    native_speakers = str_split_i(native_speakers, \" \", 1),\n    country = str_extract_all(\n      country, str_c(african_countries, collapse = \"|\"), simplify = TRUE))\n\n# From the country column, we \"expand\" the cells that have vectors, creating \n# a \"wider\" tibble\ncountries &lt;- \n  dataset |&gt; \n  select(country) |&gt; \n  map(.f = function(x) str_extract_all(\n    x, str_c(\n      african_countries, collapse = \"|\"), \n    simplify = TRUE)) |&gt; \n  as.data.frame()\n\n# We binding columns between the two datasets, make the dataset longer, and \n# remove rows that have blank entries in the country column\nafrica &lt;- bind_cols(dt, countries) |&gt; \n  select(-country) |&gt; \n  pivot_longer(\n    cols = contains(\"country\"),\n    names_to = 'country_index', values_to = 'country') |&gt; \n  mutate(\n    country = na_if(country, \"\")) |&gt; \n  drop_na()\n\n# We then clean up the native_speakers column and prepare our final dataset \n# by removing the unwanted country_index column\nafrica &lt;- africa |&gt; \n  mutate(native_speakers = str_split_i(native_speakers, pattern = '\\\\[|-', i = 1), \n         native_speakers = str_remove_all(native_speakers, pattern = ','), \n         native_speakers = as.integer(native_speakers)) |&gt; \n  filter(!is.na(native_speakers)) |&gt; \n  select(-country_index)"
  },
  {
    "objectID": "data/2026/2026-01-13/africa.html",
    "href": "data/2026/2026-01-13/africa.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nlanguage\ncharacter\nName of popular African language.\n\n\nfamily\ncharacter\nGroup of languages with similar ancestry, having similar vocabulary, phonetics and grammar.\n\n\nnative_speakers\ninteger\nNumber of known native speakers of the language.\n\n\ncountry\ncharacter\nCountry where this language is spoken."
  },
  {
    "objectID": "data/2025/2025-12-30/readme.html",
    "href": "data/2025/2025-12-30/readme.html",
    "title": "Christmas Novels",
    "section": "",
    "text": "This week we’re exploring “Christmas” novels from Project Gutenberg via the {gutenbergr} R package! I originally curated this dataset to serve as an “ad” of sorts for a new maintainer of that package, but Jordan Bradford has already assumed that role. Thank you for taking over stewardship of the package, Jordan! He could still use help, so, if you enjoy working with text data and R, consider stepping up to help maintain this useful package!\nYou might find Text Mining with R helpful for analyzing this data.\n\nWhich is mentioned more often in these novels: “spirit” or “santa”?\nWhat is the overall sentiment of each novel?\nHow does the text sentiment change over the course of each novel?\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 52)\n\nchristmas_novel_authors &lt;- tuesdata$christmas_novel_authors\nchristmas_novel_text &lt;- tuesdata$christmas_novel_text\nchristmas_novels &lt;- tuesdata$christmas_novels\n\n# Option 2: Read directly from GitHub\n\nchristmas_novel_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_authors.csv')\nchristmas_novel_text &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_text.csv')\nchristmas_novels &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novels.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-30')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nchristmas_novel_authors = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_authors.csv')\nchristmas_novel_text = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_text.csv')\nchristmas_novels = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novels.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-30')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nchristmas_novel_authors = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_authors.csv\")\nchristmas_novel_text = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_text.csv\")\nchristmas_novels = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novels.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nchristmas_novel_authors = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_authors.csv\", DataFrame)\nchristmas_novel_text = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_text.csv\", DataFrame)\nchristmas_novels = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novels.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_author_id\ninteger\nProject Gutenberg author ID.\n\n\nauthor\ncharacter\nThe agent_name field from the original metadata.\n\n\nbirthdate\ninteger\nYear of birth, if known.\n\n\ndeathdate\ninteger\nYear of death, if known.\n\n\nwikipedia\ncharacter\nLink to Wikipedia article on the author. If there are multiple, they are ”\n\n\naliases\ncharacter\nCharacter vector of aliases. If there are multiple, they are “/”-delimited.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nNumeric ID, used to retrieve works from Project Gutenberg\n\n\ntext\ncharacter\nA line of text from the work (NA indicates an empty line)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nNumeric ID, used to retrieve works from Project Gutenberg.\n\n\ntitle\ncharacter\nTitle of the work.\n\n\ngutenberg_author_id\ninteger\nProject Gutenberg author ID.\n\n\n\n\n\n\n\nlibrary(gutenbergr)\nlibrary(tidyverse)\n\n# I do this as a separate step so I can be sure the option has resolved before I\n# do anything in bulk.\ngutenbergr::gutenberg_get_mirror()\n\nchristmas_novels_raw &lt;- gutenbergr::gutenberg_works(\n  dplyr::if_all(dplyr::everything(), ~ !is.na(.)),\n  stringr::str_detect(.data$gutenberg_bookshelf, \"Novels\"),\n  stringr::str_detect(.data$title, \"Christmas\"),\n  stringr::str_detect(.data$gutenberg_bookshelf, \"Christmas\")\n)\n\nchristmas_novels &lt;- christmas_novels_raw |&gt;\n  dplyr::distinct(.data$gutenberg_id, .data$title, .data$gutenberg_author_id)\n\nchristmas_novel_authors &lt;- christmas_novels_raw |&gt;\n  dplyr::distinct(.data$gutenberg_author_id) |&gt;\n  dplyr::left_join(gutenbergr::gutenberg_authors, by = \"gutenberg_author_id\") |&gt;\n  # Just use the \"aliases\" column, \"alias\" is redundant.\n  dplyr::select(-\"alias\")\n\nchristmas_novel_text &lt;- gutenbergr::gutenberg_download(christmas_novels)"
  },
  {
    "objectID": "data/2025/2025-12-30/readme.html#the-data",
    "href": "data/2025/2025-12-30/readme.html#the-data",
    "title": "Christmas Novels",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 52)\n\nchristmas_novel_authors &lt;- tuesdata$christmas_novel_authors\nchristmas_novel_text &lt;- tuesdata$christmas_novel_text\nchristmas_novels &lt;- tuesdata$christmas_novels\n\n# Option 2: Read directly from GitHub\n\nchristmas_novel_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_authors.csv')\nchristmas_novel_text &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_text.csv')\nchristmas_novels &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novels.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-30')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nchristmas_novel_authors = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_authors.csv')\nchristmas_novel_text = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_text.csv')\nchristmas_novels = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novels.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-30')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nchristmas_novel_authors = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_authors.csv\")\nchristmas_novel_text = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_text.csv\")\nchristmas_novels = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novels.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nchristmas_novel_authors = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_authors.csv\", DataFrame)\nchristmas_novel_text = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novel_text.csv\", DataFrame)\nchristmas_novels = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-30/christmas_novels.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-12-30/readme.html#how-to-participate",
    "href": "data/2025/2025-12-30/readme.html#how-to-participate",
    "title": "Christmas Novels",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-12-30/readme.html#data-dictionary",
    "href": "data/2025/2025-12-30/readme.html#data-dictionary",
    "title": "Christmas Novels",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ngutenberg_author_id\ninteger\nProject Gutenberg author ID.\n\n\nauthor\ncharacter\nThe agent_name field from the original metadata.\n\n\nbirthdate\ninteger\nYear of birth, if known.\n\n\ndeathdate\ninteger\nYear of death, if known.\n\n\nwikipedia\ncharacter\nLink to Wikipedia article on the author. If there are multiple, they are ”\n\n\naliases\ncharacter\nCharacter vector of aliases. If there are multiple, they are “/”-delimited.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nNumeric ID, used to retrieve works from Project Gutenberg\n\n\ntext\ncharacter\nA line of text from the work (NA indicates an empty line)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nNumeric ID, used to retrieve works from Project Gutenberg.\n\n\ntitle\ncharacter\nTitle of the work.\n\n\ngutenberg_author_id\ninteger\nProject Gutenberg author ID."
  },
  {
    "objectID": "data/2025/2025-12-30/readme.html#cleaning-script",
    "href": "data/2025/2025-12-30/readme.html#cleaning-script",
    "title": "Christmas Novels",
    "section": "",
    "text": "library(gutenbergr)\nlibrary(tidyverse)\n\n# I do this as a separate step so I can be sure the option has resolved before I\n# do anything in bulk.\ngutenbergr::gutenberg_get_mirror()\n\nchristmas_novels_raw &lt;- gutenbergr::gutenberg_works(\n  dplyr::if_all(dplyr::everything(), ~ !is.na(.)),\n  stringr::str_detect(.data$gutenberg_bookshelf, \"Novels\"),\n  stringr::str_detect(.data$title, \"Christmas\"),\n  stringr::str_detect(.data$gutenberg_bookshelf, \"Christmas\")\n)\n\nchristmas_novels &lt;- christmas_novels_raw |&gt;\n  dplyr::distinct(.data$gutenberg_id, .data$title, .data$gutenberg_author_id)\n\nchristmas_novel_authors &lt;- christmas_novels_raw |&gt;\n  dplyr::distinct(.data$gutenberg_author_id) |&gt;\n  dplyr::left_join(gutenbergr::gutenberg_authors, by = \"gutenberg_author_id\") |&gt;\n  # Just use the \"aliases\" column, \"alias\" is redundant.\n  dplyr::select(-\"alias\")\n\nchristmas_novel_text &lt;- gutenbergr::gutenberg_download(christmas_novels)"
  },
  {
    "objectID": "data/2025/2025-12-30/christmas_novels.html",
    "href": "data/2025/2025-12-30/christmas_novels.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nNumeric ID, used to retrieve works from Project Gutenberg.\n\n\ntitle\ncharacter\nTitle of the work.\n\n\ngutenberg_author_id\ninteger\nProject Gutenberg author ID."
  },
  {
    "objectID": "data/2025/2025-12-30/christmas_novel_authors.html",
    "href": "data/2025/2025-12-30/christmas_novel_authors.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ngutenberg_author_id\ninteger\nProject Gutenberg author ID.\n\n\nauthor\ncharacter\nThe agent_name field from the original metadata.\n\n\nbirthdate\ninteger\nYear of birth, if known.\n\n\ndeathdate\ninteger\nYear of death, if known.\n\n\nwikipedia\ncharacter\nLink to Wikipedia article on the author. If there are multiple, they are ”\n\n\naliases\ncharacter\nCharacter vector of aliases. If there are multiple, they are “/”-delimited."
  },
  {
    "objectID": "data/2025/2025-12-23/languages.html",
    "href": "data/2025/2025-12-23/languages.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "Variable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language\n\n\nname\ncharacter\nLanguage name\n\n\nmacroarea\ncharacter\nGeneral geographic area in which the language is found\n\n\nlatitude\ndouble\nLatitude of language location (as point)\n\n\nlongitude\ndouble\nLongitude of language location (as point)\n\n\niso639p3code\ncharacter\nISO 639-3 identifier of language (if available)\n\n\ncountries\ncharacter\nCountries in which language is used (separated by “;”)\n\n\nis_isolate\nlogical\nWhether language is an isolate (i.e. has no known relatives)\n\n\nfamily_id\ncharacter\nUnique identifier of family that the language is part of (if not isolate)"
  },
  {
    "objectID": "data/2025/2025-12-23/families.html",
    "href": "data/2025/2025-12-23/families.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "Variable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language family\n\n\nname\ncharacter\nLanguage family name"
  },
  {
    "objectID": "data/2025/2025-12-16/roundabouts_clean.html",
    "href": "data/2025/2025-12-16/roundabouts_clean.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nname\ncharacter\nRoundabout name\n\n\naddress\ncharacter\nRoundabout address\n\n\ntown_city\ncharacter\nTown/City\n\n\ncounty_area\ncharacter\nCounty/Area\n\n\nstate_region\ncharacter\nState/Region\n\n\ncountry\ncharacter\nCountry\n\n\nlat\ndouble\nLatitude\n\n\nlong\ndouble\nLongitude\n\n\ntype\ncharacter\nType, one of “roundabout”, “traffic calming circle”, “signalized roundabout/circle”, “rotary”, “other”, “unknown”\n\n\nstatus\ncharacter\nStatus, one of “existing”, “removed”, “unknown”\n\n\nyear_completed\ninteger\nYear construction completed\n\n\napproaches\ninteger\nNumber of approaches\n\n\ndriveways\ninteger\nNumber of driveways\n\n\nlane_type\ncharacter\nLane type\n\n\nfunctional_class\ncharacter\nFunctional Class\n\n\ncontrol_type\ncharacter\nControl Type\n\n\nother_control_type\ncharacter\nOther control type\n\n\nprevious_control_type\ncharacter\nPrevious control type"
  },
  {
    "objectID": "data/2025/2025-12-16/intro.html",
    "href": "data/2025/2025-12-16/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "This week we are exploring data from the {roundabouts} package by Emil Hvitfeldt. The roundabouts package provides an R friendly way to access the roundabouts database which is compiled by Kittelson & Associates and contains information about the location, configuration, and construction of roundabout intersections around the world.\n\n“It started with an inventory of U.S. roundabouts that identified 150 sites,” Lee says. “One thing led to another, and now we’re at over 20,000 records in the database.”\n\n\nHow has roundabout construction evolved over time? Are certain regions adopting them faster than others?\nWhat types of intersections are most commonly converted to roundabouts?\nWhere are the roundabouts with the most unusual configurations (highest number of approaches/driveways)?"
  },
  {
    "objectID": "data/2025/2025-12-09/qatarcars.html",
    "href": "data/2025/2025-12-09/qatarcars.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\norigin\ncharacter\nThe country associated with the car brand.\n\n\nmake\ncharacter\nThe brand of the car, such as Toyota or Land Rover.\n\n\nmodel\ncharacter\nThe specific type of car, such as Land Cruiser or Defender.\n\n\nlength\ndouble\nLength of the car (in meters).\n\n\nwidth\ndouble\nWidth of the car (in meters).\n\n\nheight\ndouble\nHeight of the car (in meters).\n\n\nseating\ndouble\nNumber of seats in the car.\n\n\ntrunk\ndouble\nCapacity or volume of the trunk (in liters).\n\n\neconomy\ndouble\nFuel economy of the car (in liters per 100 km).\n\n\nhorsepower\ndouble\nCar horsepower.\n\n\nprice\ndouble\nPrice of the car in 2025 Qatari riyals.\n\n\nmass\ndouble\nMass of the car (in kg).\n\n\nperformance\ndouble\nTime to accelerate from 0 to 100 km/h (in seconds).\n\n\ntype\ncharacter\nThe type of the car, such as coupe, sedan, or SUV.\n\n\nenginetype\ncharacter\nThe type of engine: electric, hybrid, or petrol."
  },
  {
    "objectID": "data/2025/2025-12-02/sechselaeuten.html",
    "href": "data/2025/2025-12-02/sechselaeuten.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear of Sechselauten festival.\n\n\nduration\ndouble\nTime elapsed from ignition of Boeoeg effigy until explosion, in minutes.\n\n\ntre200m0\ndouble\nAverage air temperature 2 m above ground in degrees Celsius (monthly mean).\n\n\ntre200mn\ndouble\nMinimum air temperature 2 m above ground in degrees Celsius (absolute monthly minimum).\n\n\ntre200mx\ndouble\nMaximum air temperature 2 m above ground in degrees Celsius (absolute monthly maximum).\n\n\nsre000m0\ndouble\nTotal sunshine duration in hours (monthly total).\n\n\nsremaxmv\ndouble\nTotal sunshine duration as a percentage of the possible maximum.\n\n\nrre150m0\ndouble\nTotal precipitation in mm (monthly total).\n\n\nrecord\nlogical\nYears with average summer temperature above 19 degrees Celsius."
  },
  {
    "objectID": "data/2025/2025-12-02/intro.html",
    "href": "data/2025/2025-12-02/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "This week we’re exploring the weather prediction of Zurich’s infamous exploding snowman!\n\nThe Boeoegg is a snowman effigy made of cotton wool and stuffed with fireworks, created every year for Zurich’s “Sechselaeuten” spring festival. The saying goes that the quicker the Boeoeg’s head explodes, the finer the summer will be.\n\n\nCheck the burn duration of our snowman against the average summer temperature. Does folk science stand its ground against hard science?\nCan you find a number of successive years so that our snowman’s predictions seem more accurate?\nDoes our snowman’s forecasting ability improve if you choose climate variables other than temperature?\nWhat happened in the years for which there was no duration recorded? You can check the Wikipedia entry for “Sechselaeuten” for some funny anecdotes!"
  },
  {
    "objectID": "data/2025/2025-11-18/readme.html",
    "href": "data/2025/2025-11-18/readme.html",
    "title": "The Complete Sherlock Holmes",
    "section": "",
    "text": "This week we’re exploring the complete line-by-line text of the Sherlock Holmes stories and novels, made available through the {sherlock} R package by Emil Hvitfeldt. The dataset includes the full collection of Holmes texts, organized by book and line number, and is ideal for stylometry, sentiment analysis, and literary exploration.\n\n“The name is Sherlock Holmes and the address is 221B Baker Street.” Holmes is a consulting detective known for his keen observation, logical reasoning, and use of forensic science to solve complex cases. Created by Sir Arthur Conan Doyle, Holmes has become one of the most famous fictional detectives in literature.\n\n\nAre there patterns in how Watson narrates versus how Holmes speaks?\nHow does sentence length vary between stories?\nCan we detect shifts in tone when Watson is the narrator versus when Holmes speaks directly?\nDoes sentiment shift as the mystery unfolds?\n\nThank you to Darakhshan Nehal for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-18')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 46)\n\nholmes &lt;- tuesdata$holmes\n\n# Option 2: Read directly from GitHub\n\nholmes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-11-18')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nholmes = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-11-18')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nholmes = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nholmes = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nbook\ncharacter\nTitle of the Sherlock Holmes story or novel.\n\n\ntext\ncharacter\nLine of text extracted from the book.\n\n\nline_num\ninteger\nNarrative-preserving line index. When you load the data, blank lines (\"\") may appear as NA.\n\n\n\n\n\n\n\n# Imports\nlibrary(tidyverse)\nlibrary(devtools)\n\n#devtools::install_github(\"EmilHvitfeldt/sherlock\")\nlibrary(sherlock)\n\n# Load the dataset\nholmes &lt;- sherlock::holmes |&gt;\n  # Add line numbers to preserve narrative order\n  mutate(line_num = row_number(), .by = \"book\") |&gt;\n  # Reorder columns\n  select(\"book\", \"text\", \"line_num\")"
  },
  {
    "objectID": "data/2025/2025-11-18/readme.html#the-data",
    "href": "data/2025/2025-11-18/readme.html#the-data",
    "title": "The Complete Sherlock Holmes",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-18')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 46)\n\nholmes &lt;- tuesdata$holmes\n\n# Option 2: Read directly from GitHub\n\nholmes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-11-18')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nholmes = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-11-18')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nholmes = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nholmes = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-11-18/readme.html#how-to-participate",
    "href": "data/2025/2025-11-18/readme.html#how-to-participate",
    "title": "The Complete Sherlock Holmes",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-11-18/readme.html#data-dictionary",
    "href": "data/2025/2025-11-18/readme.html#data-dictionary",
    "title": "The Complete Sherlock Holmes",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nbook\ncharacter\nTitle of the Sherlock Holmes story or novel.\n\n\ntext\ncharacter\nLine of text extracted from the book.\n\n\nline_num\ninteger\nNarrative-preserving line index. When you load the data, blank lines (\"\") may appear as NA."
  },
  {
    "objectID": "data/2025/2025-11-18/readme.html#cleaning-script",
    "href": "data/2025/2025-11-18/readme.html#cleaning-script",
    "title": "The Complete Sherlock Holmes",
    "section": "",
    "text": "# Imports\nlibrary(tidyverse)\nlibrary(devtools)\n\n#devtools::install_github(\"EmilHvitfeldt/sherlock\")\nlibrary(sherlock)\n\n# Load the dataset\nholmes &lt;- sherlock::holmes |&gt;\n  # Add line numbers to preserve narrative order\n  mutate(line_num = row_number(), .by = \"book\") |&gt;\n  # Reorder columns\n  select(\"book\", \"text\", \"line_num\")"
  },
  {
    "objectID": "data/2025/2025-11-04/readme.html",
    "href": "data/2025/2025-11-04/readme.html",
    "title": "Lead concentration in Flint water samples in 2015",
    "section": "",
    "text": "This week we are exploring lead levels in water samples collected in Flint, Michigan in 2015. The data comes from a paper by Loux and Gibson (2018) who advocate for using this data as a teaching example in introductory statistics courses.\n\nThe Flint lead data provide a compelling example for introducing students to simple univariate descriptive statistics. In addition, they provide examples for discussion of sampling and data collection, as well as ethical data handling.\n\nThe data this week includes samples collected by the Michigan Department of Environment (MDEQ) and data from a citizen science project coordinated by Prof Marc Edwards and colleagues at Virginia Tech. Community-sourced samples were collected after concerns were raised about the MDEQ excluding samples from their data. You can read about the “murky” story behind this data here.\nThank you to @nzgwynn for submitting this dataset in #23!\n\nHow does the distribution of lead levels differ between MDEQ and Virginia Tech datasets?\nHow do key statistics (mean, median, 90th percentile) change with/without excluded samples in the MDEQ sample?\n\nThank you to Jen Richmond for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-04')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 44)\n\nflint_mdeq &lt;- tuesdata$flint_mdeq\nflint_vt &lt;- tuesdata$flint_vt\n\n# Option 2: Read directly from GitHub\n\nflint_mdeq &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv')\nflint_vt &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-11-04')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nflint_mdeq = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv')\nflint_vt = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-11-04')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nflint_mdeq = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv\")\nflint_vt = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nflint_mdeq = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv\", DataFrame)\nflint_vt = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsample\ndouble\nsample number\n\n\nlead\ndouble\nlead level in parts per billion (all samples)\n\n\nlead2\ndouble\nlead level in parts per billion (2 samples removed)\n\n\nnotes\ncharacter\ncomment about data removal\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsample\ninteger\nsample number\n\n\nlead\ndouble\nlead levels in parts per billion (ppb)\n\n\n\n\n\n\n\n# data downloaded from https://onlinelibrary.wiley.com/doi/10.1111/test.12187 \n# notes variable added to flint_mdeq to explain why samples were removed\n\n# Set the data directory. Change this if your data is in a different location.\ndata_dir &lt;- \"tt_submission\"  # Expected structure: data_dir contains test12187-supp-0001-flint.rdata\n\nload(here::here(data_dir, \"test12187-supp-0001-flint.rdata\"))\n\n# add notes\n\nflint_mdeq &lt;- flint_mdeq %&gt;% \n  mutate(notes = case_when(lead == 104 & is.na(lead2) ~ \"sample removed: house had a filter\",\n                           lead == 20 & is.na(lead2) ~ \"sample removed: business not residence\"))"
  },
  {
    "objectID": "data/2025/2025-11-04/readme.html#the-data",
    "href": "data/2025/2025-11-04/readme.html#the-data",
    "title": "Lead concentration in Flint water samples in 2015",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-04')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 44)\n\nflint_mdeq &lt;- tuesdata$flint_mdeq\nflint_vt &lt;- tuesdata$flint_vt\n\n# Option 2: Read directly from GitHub\n\nflint_mdeq &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv')\nflint_vt &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-11-04')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nflint_mdeq = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv')\nflint_vt = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-11-04')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nflint_mdeq = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv\")\nflint_vt = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nflint_mdeq = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_mdeq.csv\", DataFrame)\nflint_vt = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-04/flint_vt.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-11-04/readme.html#how-to-participate",
    "href": "data/2025/2025-11-04/readme.html#how-to-participate",
    "title": "Lead concentration in Flint water samples in 2015",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-11-04/readme.html#data-dictionary",
    "href": "data/2025/2025-11-04/readme.html#data-dictionary",
    "title": "Lead concentration in Flint water samples in 2015",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nsample\ndouble\nsample number\n\n\nlead\ndouble\nlead level in parts per billion (all samples)\n\n\nlead2\ndouble\nlead level in parts per billion (2 samples removed)\n\n\nnotes\ncharacter\ncomment about data removal\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsample\ninteger\nsample number\n\n\nlead\ndouble\nlead levels in parts per billion (ppb)"
  },
  {
    "objectID": "data/2025/2025-11-04/readme.html#cleaning-script",
    "href": "data/2025/2025-11-04/readme.html#cleaning-script",
    "title": "Lead concentration in Flint water samples in 2015",
    "section": "",
    "text": "# data downloaded from https://onlinelibrary.wiley.com/doi/10.1111/test.12187 \n# notes variable added to flint_mdeq to explain why samples were removed\n\n# Set the data directory. Change this if your data is in a different location.\ndata_dir &lt;- \"tt_submission\"  # Expected structure: data_dir contains test12187-supp-0001-flint.rdata\n\nload(here::here(data_dir, \"test12187-supp-0001-flint.rdata\"))\n\n# add notes\n\nflint_mdeq &lt;- flint_mdeq %&gt;% \n  mutate(notes = case_when(lead == 104 & is.na(lead2) ~ \"sample removed: house had a filter\",\n                           lead == 20 & is.na(lead2) ~ \"sample removed: business not residence\"))"
  },
  {
    "objectID": "data/2025/2025-10-21/readme.html",
    "href": "data/2025/2025-10-21/readme.html",
    "title": "Historic UK Meteorological & Climate Data",
    "section": "",
    "text": "This week we’re exploring historic meteorological data from the UK Met Office.\nThe UK Met Office is the United Kingdom’s national weather and climate service, providing forecasts, severe weather warnings, and climate science expertise. It helps people, businesses, and governments make informed decisions to stay safe and plan for the future. It was first established in 1854, making it one of the oldest weather services in the world.\nData has been scraped straight from the Met Office website and cleaned in a basic way. The few flags for “estimated” data and changing sunlight monitoring techniques have been removed for simplicity, but are available from the raw data. Also available is simple site metadata which includes the opening date and latitude and longitude of each station.\n\nMonthly Historical information for 37 UK Meteorological Stations. Most go back to the early 1900s, but some go back as far as 1853. Station data files are updated on a rolling monthly basis, around 10 days after the end of the month. No allowances have been made for small site changes and developments in instrumentation.\n\nYou could consider:\n\nWhich are the rainiest/sunniest/hottest regions of the UK? Has that changed over time?\nWere there any historic years that were particularly rainy/sunny/hot? Did that apply to all regions of the UK?\nHave monthly patterns in the meteorological variables changed year-on-year?\nCan you forecast future meteorology?\n\nThank you to Jack Davison for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-10-21')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 42)\n\nhistoric_station_met &lt;- tuesdata$historic_station_met\nstation_meta &lt;- tuesdata$station_meta\n\n# Option 2: Read directly from GitHub\n\nhistoric_station_met &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv')\nstation_meta &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-10-21')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nhistoric_station_met = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv')\nstation_meta = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-10-21')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nhistoric_station_met = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv\")\nstation_meta = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nhistoric_station_met = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv\", DataFrame)\nstation_meta = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstation\ncharacter\nStation ID; for binding with metadata.\n\n\nyear\ninteger\nYear.\n\n\nmonth\ninteger\nNumeric month (1-12).\n\n\ntmax\ndouble\nMean daily maximum temperature (Degrees C).\n\n\ntmin\ndouble\nMean daily minimum temperature (Degrees C).\n\n\naf\ndouble\nDays of air frost.\n\n\nrain\ndouble\nTotal rainfall (mm).\n\n\nsun\ndouble\nTotal sunshine duration (hours).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstation\ncharacter\nStation ID; for binding with measurement data.\n\n\nstation_name\ncharacter\nStation name.\n\n\nopened\ninteger\nYear in which station opened.\n\n\nlng\ndouble\nDecimal longitude.\n\n\nlat\ndouble\nDecimal latitude.\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(janitor)\n\n# list station data urls\n\nlinks &lt;-\n  rvest::read_html(\n    \"https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data\"\n  ) |&gt;\n  rvest::html_elements(\"a\") |&gt;\n  rvest::html_attr(\"href\") |&gt;\n  (\\(x) x[grepl(\"stationdata\", x)])()\n\n# function to read files from met office website\nread_met_office_fwf &lt;- function(file) {\n  # read all lines of data\n  x &lt;- readr::read_lines(file)\n\n  # get station name\n  station &lt;- basename(file) |&gt; stringr::str_remove_all(\"data.txt\")\n\n  # focus on data\n  x &lt;- x[which(grepl(\"yyyy\", x)):length(x)]\n\n  # read data\n  data &lt;-\n    readr::read_fwf(\n      I(x),\n      na = \"---\",\n      show_col_types = FALSE,\n      col_positions = readr::fwf_widths(c(7, 4, 8, 8, 8, 8, 8))\n    ) |&gt;\n    janitor::row_to_names(1) |&gt;\n    # lets not use most recent data\n    dplyr::filter(yyyy != \"2025\") |&gt;\n    # drop units row\n    dplyr::slice_tail(n = -1) |&gt;\n    # add station\n    dplyr::mutate(station = station, .before = 0) |&gt;\n    dplyr::rename(\n      year = yyyy,\n      month = mm\n    )\n\n  # return\n  return(data)\n}\n\n# read all urls\nraw_data &lt;- purrr::map(links, read_met_office_fwf, .progress = TRUE)\n\n# clean data\nhistoric_station_met &lt;-\n  raw_data |&gt;\n  dplyr::bind_rows() |&gt;\n  dplyr::filter(month != \"osed\") |&gt;\n  dplyr::mutate(\n    sun = stringr::str_remove_all(sun, \"a|C| |\\\\*|\\\\#|---|\\\\||\\\\$\"),\n    sun = ifelse(sun == \"\", NA, sun),\n    sun = as.numeric(sun)\n  ) |&gt;\n  dplyr::mutate(\n    dplyr::across(c(tmax:rain), \\(x) gsub(\"\\\\*|\\\\#\", \"\", x) |&gt; as.numeric()),\n    dplyr::across(c(year, month), as.integer)\n  )\n\n# get metadata\nstation_meta &lt;-\n  rvest::read_html(\n    \"https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data\"\n  ) |&gt;\n  rvest::html_table() |&gt;\n  purrr::pluck(1) |&gt;\n  tidyr::separate_wider_delim(\n    Location,\n    delim = \", \",\n    names = c(\"lng\", \"lat\")\n  ) |&gt;\n  dplyr::mutate(\n    dplyr::across(c(lng, lat), as.numeric),\n    station = stringr::str_remove_all(\n      links,\n      \"https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/|data.txt\"\n    )\n  ) |&gt;\n  dplyr::select(station, station_name = Name, opened = Opened, lng, lat)"
  },
  {
    "objectID": "data/2025/2025-10-21/readme.html#the-data",
    "href": "data/2025/2025-10-21/readme.html#the-data",
    "title": "Historic UK Meteorological & Climate Data",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-10-21')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 42)\n\nhistoric_station_met &lt;- tuesdata$historic_station_met\nstation_meta &lt;- tuesdata$station_meta\n\n# Option 2: Read directly from GitHub\n\nhistoric_station_met &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv')\nstation_meta &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-10-21')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nhistoric_station_met = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv')\nstation_meta = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-10-21')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nhistoric_station_met = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv\")\nstation_meta = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nhistoric_station_met = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/historic_station_met.csv\", DataFrame)\nstation_meta = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-21/station_meta.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-10-21/readme.html#how-to-participate",
    "href": "data/2025/2025-10-21/readme.html#how-to-participate",
    "title": "Historic UK Meteorological & Climate Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-10-21/readme.html#data-dictionary",
    "href": "data/2025/2025-10-21/readme.html#data-dictionary",
    "title": "Historic UK Meteorological & Climate Data",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nstation\ncharacter\nStation ID; for binding with metadata.\n\n\nyear\ninteger\nYear.\n\n\nmonth\ninteger\nNumeric month (1-12).\n\n\ntmax\ndouble\nMean daily maximum temperature (Degrees C).\n\n\ntmin\ndouble\nMean daily minimum temperature (Degrees C).\n\n\naf\ndouble\nDays of air frost.\n\n\nrain\ndouble\nTotal rainfall (mm).\n\n\nsun\ndouble\nTotal sunshine duration (hours).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstation\ncharacter\nStation ID; for binding with measurement data.\n\n\nstation_name\ncharacter\nStation name.\n\n\nopened\ninteger\nYear in which station opened.\n\n\nlng\ndouble\nDecimal longitude.\n\n\nlat\ndouble\nDecimal latitude."
  },
  {
    "objectID": "data/2025/2025-10-21/readme.html#cleaning-script",
    "href": "data/2025/2025-10-21/readme.html#cleaning-script",
    "title": "Historic UK Meteorological & Climate Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(rvest)\nlibrary(janitor)\n\n# list station data urls\n\nlinks &lt;-\n  rvest::read_html(\n    \"https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data\"\n  ) |&gt;\n  rvest::html_elements(\"a\") |&gt;\n  rvest::html_attr(\"href\") |&gt;\n  (\\(x) x[grepl(\"stationdata\", x)])()\n\n# function to read files from met office website\nread_met_office_fwf &lt;- function(file) {\n  # read all lines of data\n  x &lt;- readr::read_lines(file)\n\n  # get station name\n  station &lt;- basename(file) |&gt; stringr::str_remove_all(\"data.txt\")\n\n  # focus on data\n  x &lt;- x[which(grepl(\"yyyy\", x)):length(x)]\n\n  # read data\n  data &lt;-\n    readr::read_fwf(\n      I(x),\n      na = \"---\",\n      show_col_types = FALSE,\n      col_positions = readr::fwf_widths(c(7, 4, 8, 8, 8, 8, 8))\n    ) |&gt;\n    janitor::row_to_names(1) |&gt;\n    # lets not use most recent data\n    dplyr::filter(yyyy != \"2025\") |&gt;\n    # drop units row\n    dplyr::slice_tail(n = -1) |&gt;\n    # add station\n    dplyr::mutate(station = station, .before = 0) |&gt;\n    dplyr::rename(\n      year = yyyy,\n      month = mm\n    )\n\n  # return\n  return(data)\n}\n\n# read all urls\nraw_data &lt;- purrr::map(links, read_met_office_fwf, .progress = TRUE)\n\n# clean data\nhistoric_station_met &lt;-\n  raw_data |&gt;\n  dplyr::bind_rows() |&gt;\n  dplyr::filter(month != \"osed\") |&gt;\n  dplyr::mutate(\n    sun = stringr::str_remove_all(sun, \"a|C| |\\\\*|\\\\#|---|\\\\||\\\\$\"),\n    sun = ifelse(sun == \"\", NA, sun),\n    sun = as.numeric(sun)\n  ) |&gt;\n  dplyr::mutate(\n    dplyr::across(c(tmax:rain), \\(x) gsub(\"\\\\*|\\\\#\", \"\", x) |&gt; as.numeric()),\n    dplyr::across(c(year, month), as.integer)\n  )\n\n# get metadata\nstation_meta &lt;-\n  rvest::read_html(\n    \"https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data\"\n  ) |&gt;\n  rvest::html_table() |&gt;\n  purrr::pluck(1) |&gt;\n  tidyr::separate_wider_delim(\n    Location,\n    delim = \", \",\n    names = c(\"lng\", \"lat\")\n  ) |&gt;\n  dplyr::mutate(\n    dplyr::across(c(lng, lat), as.numeric),\n    station = stringr::str_remove_all(\n      links,\n      \"https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/|data.txt\"\n    )\n  ) |&gt;\n  dplyr::select(station, station_name = Name, opened = Opened, lng, lat)"
  },
  {
    "objectID": "data/2025/2025-10-07/readme.html",
    "href": "data/2025/2025-10-07/readme.html",
    "title": "EuroLeague Basketball",
    "section": "",
    "text": "This week we’re exploring EuroLeague Basketball, the premier men’s club basketball competition in Europe.\nThe dataset contains information on EuroLeague teams, including their country, home city, arena, seating capacity, and historical performance (Final Four appearances and titles won).\nThe dataset is curated from publicly available sources such as Wikipedia and official EuroLeague records, and was packaged in the EuroleagueBasketball R package, with documentation available at natanast.github.io/EuroleagueBasketball.\n\n“The EuroLeague is the top-tier European professional basketball club competition, widely regarded as the most prestigious competition in European basketball.” — EuroLeague\n\nSome questions you might explore with this dataset:\n- Which countries are most represented in the EuroLeague?\n- How do arena capacities compare across teams and countries? In R, the readr::parse_number() function might be helpful here. - Which clubs have been the most successful historically?\nThank you to Natasa Anastasiadou for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-10-07')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 40)\n\neuroleague_basketball &lt;- tuesdata$euroleague_basketball\n\n# Option 2: Read directly from GitHub\n\neuroleague_basketball &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-10-07')\n\n# Option 2: Read directly from GitHub and assign to an object\n\neuroleague_basketball = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-10-07')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\neuroleague_basketball = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\neuroleague_basketball = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nTeam\ncharacter\nName of the basketball team.\n\n\nHome city\ncharacter\nCity where the team is based.\n\n\nArena\ncharacter\nName of the home arena.\n\n\nCapacity\ncharacter\nSeating capacity of the arena.\n\n\nLast season\ncharacter\nRanking of team in the 2024-25 season, including teams elevated to the Euroleague from the Eurocup in that season.\n\n\nCountry\ncharacter\nCountry where the team is based.\n\n\nFinalFour_Appearances\ninteger\nNumber of times the team has reached the EuroLeague Final Four.\n\n\nTitles_Won\ninteger\nNumber of EuroLeague titles won by the team.\n\n\nYears_of_FinalFour_Appearances\ncharacter\nYears when the team reached the Final Four.\n\n\nYears_of_Titles_Won\ncharacter\nYears when the team won the EuroLeague championship.\n\n\n\n\n\n\n\n# Clean data provided by https://github.com/natanast/EuroleagueBasketball. \n# The data was already cleaned and curated in the EuroleagueBasketball R package.\n# No cleaning was necessary.\neuroleague_basketball &lt;- readr::read_csv(\"https://raw.githubusercontent.com/natanast/EuroleagueBasketball/main/data-raw/euroleague_dataset.csv\")"
  },
  {
    "objectID": "data/2025/2025-10-07/readme.html#the-data",
    "href": "data/2025/2025-10-07/readme.html#the-data",
    "title": "EuroLeague Basketball",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-10-07')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 40)\n\neuroleague_basketball &lt;- tuesdata$euroleague_basketball\n\n# Option 2: Read directly from GitHub\n\neuroleague_basketball &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-10-07')\n\n# Option 2: Read directly from GitHub and assign to an object\n\neuroleague_basketball = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-10-07')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\neuroleague_basketball = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\neuroleague_basketball = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-07/euroleague_basketball.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-10-07/readme.html#how-to-participate",
    "href": "data/2025/2025-10-07/readme.html#how-to-participate",
    "title": "EuroLeague Basketball",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-10-07/readme.html#data-dictionary",
    "href": "data/2025/2025-10-07/readme.html#data-dictionary",
    "title": "EuroLeague Basketball",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nTeam\ncharacter\nName of the basketball team.\n\n\nHome city\ncharacter\nCity where the team is based.\n\n\nArena\ncharacter\nName of the home arena.\n\n\nCapacity\ncharacter\nSeating capacity of the arena.\n\n\nLast season\ncharacter\nRanking of team in the 2024-25 season, including teams elevated to the Euroleague from the Eurocup in that season.\n\n\nCountry\ncharacter\nCountry where the team is based.\n\n\nFinalFour_Appearances\ninteger\nNumber of times the team has reached the EuroLeague Final Four.\n\n\nTitles_Won\ninteger\nNumber of EuroLeague titles won by the team.\n\n\nYears_of_FinalFour_Appearances\ncharacter\nYears when the team reached the Final Four.\n\n\nYears_of_Titles_Won\ncharacter\nYears when the team won the EuroLeague championship."
  },
  {
    "objectID": "data/2025/2025-10-07/readme.html#cleaning-script",
    "href": "data/2025/2025-10-07/readme.html#cleaning-script",
    "title": "EuroLeague Basketball",
    "section": "",
    "text": "# Clean data provided by https://github.com/natanast/EuroleagueBasketball. \n# The data was already cleaned and curated in the EuroleagueBasketball R package.\n# No cleaning was necessary.\neuroleague_basketball &lt;- readr::read_csv(\"https://raw.githubusercontent.com/natanast/EuroleagueBasketball/main/data-raw/euroleague_dataset.csv\")"
  },
  {
    "objectID": "data/2025/2025-09-23/readme.html",
    "href": "data/2025/2025-09-23/readme.html",
    "title": "FIDE Chess Player Ratings",
    "section": "",
    "text": "This week we’re exploring August and September chess player rating data from FIDE (the International Chess Federation). Monthly data files are published on the FIDE website.\nA chess rating (Elo) is an estimate of a player’s strength relative to other players. If a player performs better or worse than expected, their rating increases or decreases accordingly.\n\nThe September 2025 rating list was shaped primarily by results from the Sinquefield Cup, Quantbox Chennai Grand Masters, 61st International Akiba Rubinstein Chess Festival, and the Spanish League Honor Division 2025 – a Swiss team tournament held in Linares.\n\n\nWhich players showed the greatest improvement from August to September?\nWhich federations have the most number of titled players?\nHow did the rankings of the top male or female players change?\nWho are the top youth players?\n\nThank you to Jessica Moore for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 38)\n\nfide_ratings_august &lt;- tuesdata$fide_ratings_august\nfide_ratings_september &lt;- tuesdata$fide_ratings_september\n\n# Option 2: Read directly from GitHub\n\nfide_ratings_august &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv')\nfide_ratings_september &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-23')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nfide_ratings_august = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv')\nfide_ratings_september = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-23')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nfide_ratings_august = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv\")\nfide_ratings_september = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nfide_ratings_august = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv\", DataFrame)\nfide_ratings_september = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nIdentification number of a player within FIDE database.\n\n\nname\ncharacter\nPlayer name.\n\n\nfed\ncharacter\nFederation of player. Similar, but not identical to, IOC country codes. An unofficial list is available here\n\n\nsex\ncharacter\nSex of player (M - male, F - female)\n\n\ntitle\ncharacter\nTitle of a player (GM - Grand Master, WGM - Woman Grand Master, IM - International Master, WIM - Woman International Master, FM - FIDE Master, WFM - Woman FIDE Master, CM - Candidate Master, WCM - Woman Candidate Master)\n\n\nwtitle\ncharacter\nWoman title of a player (WGM - Woman Grand Master, WIM - Woman International Master, WFM - Woman FIDE Master, WCM - Woman Candidate Master)\n\n\notitle\ncharacter\nComma-separated list. Other titles of a player (IA - International Arbiter, FA - FIDE Arbiter, NA - National Arbiter, IO - International Organizer, FT - FIDE Trainer, FST - FIDE Senior Trainer, DI - Developmental Instructor, NI - National Instructor)\n\n\nfoa\ncharacter\nFIDE Online Arena title of a player (AGM - Arena Grand Master, AIM - Arena International Master, AFM - Arena FIDE Master, ACM - Arena Candidate Master)\n\n\nrating\ninteger\nStandard chess rating (Elo).\n\n\ngames\ninteger\nNumber of games played in the month.\n\n\nk\ninteger\nRating K factor (Value influencing how much a player’s rating changes after a game)\n\n\nbday\ninteger\nYear of birth of a player.\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nIdentification number of a player within FIDE database.\n\n\nname\ncharacter\nPlayer name.\n\n\nfed\ncharacter\nFederation of player. Similar, but not identical to, IOC country codes. An unofficial list is available here\n\n\nsex\ncharacter\nSex of player (M - male, F - female)\n\n\ntitle\ncharacter\nTitle of a player (GM - Grand Master, WGM - Woman Grand Master, IM - International Master, WIM - Woman International Master, FM - FIDE Master, WFM - Woman FIDE Master, CM - Candidate Master, WCM - Woman Candidate Master)\n\n\nwtitle\ncharacter\nWoman title of a player (WGM - Woman Grand Master, WIM - Woman International Master, WFM - Woman FIDE Master, WCM - Woman Candidate Master)\n\n\notitle\ncharacter\nComma-separated list. Other titles of a player (IA - International Arbiter, FA - FIDE Arbiter, NA - National Arbiter, IO - International Organizer, FT - FIDE Trainer, FST - FIDE Senior Trainer, DI - Developmental Instructor, NI - National Instructor)\n\n\nfoa\ncharacter\nFIDE Online Arena title of a player (AGM - Arena Grand Master, AIM - Arena International Master, AFM - Arena FIDE Master, ACM - Arena Candidate Master)\n\n\nrating\ninteger\nStandard chess rating (Elo).\n\n\ngames\ninteger\nNumber of games played in the month.\n\n\nk\ninteger\nRating K factor (Value influencing how much a player’s rating changes after a game)\n\n\nbday\ninteger\nYear of birth of a player.\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(withr)\nconflicted::conflicts_prefer(dplyr::filter)\n\n# download, unzip and read the file #\naugust_zipped &lt;- withr::local_tempfile(fileext = \".zip\")\nseptember_zipped &lt;- withr::local_tempfile(fileext = \".zip\")\n\ndownload.file(\n  \"https://ratings.fide.com/download/standard_aug25frl.zip\",\n  destfile = august_zipped,\n  mode = \"wb\"\n)\ndownload.file(\n  \"https://ratings.fide.com/download/standard_sep25frl.zip\",\n  destfile = september_zipped,\n  mode = \"wb\"\n)\n\naugust_unzipped &lt;- unzip(august_zipped, exdir = tempdir())\nseptember_unzipped &lt;- unzip(september_zipped, exdir = tempdir())\n\nfide_ratings_august &lt;- readr::read_fwf(\n  file = august_unzipped,\n  skip = 1,\n  col_types = c(\"dcccccccdddcc\"),\n  fwf_widths(\n    c(8, 68, 4, 4, 5, 5, 15, 4, 6, 4, 3, 6, 4),\n    c(\n      \"id\",\n      \"name\",\n      \"fed\",\n      \"sex\",\n      \"title\",\n      \"wtitle\",\n      \"otitle\",\n      \"foa\",\n      \"rating\",\n      \"games\",\n      \"k\",\n      \"bday\",\n      \"flag\"\n    )\n  )\n) %&gt;%\n  # cleaning up player names\n  mutate(name = str_remove(name, \"^\\\\d+\\\\s+\")) %&gt;%\n  # filter to only players who were active in the last 12 months\n  filter(flag %in% c(NA, \"w\"), bday &gt; 0) %&gt;%\n  select(-flag)\n\nfide_ratings_september &lt;- readr::read_fwf(\n  file = september_unzipped,\n  skip = 1,\n  col_types = c(\"dcccccccdddcc\"),\n  fwf_widths(\n    c(8, 68, 4, 4, 5, 5, 15, 4, 6, 4, 3, 6, 4),\n    c(\n      \"id\",\n      \"name\",\n      \"fed\",\n      \"sex\",\n      \"title\",\n      \"wtitle\",\n      \"otitle\",\n      \"foa\",\n      \"rating\",\n      \"games\",\n      \"k\",\n      \"bday\",\n      \"flag\"\n    )\n  )\n) %&gt;%\n  # cleaning up player names\n  mutate(name = str_remove(name, \"^\\\\d+\\\\s+\")) %&gt;%\n  # filter to only players who were active in the last 12 months\n  filter(flag %in% c(NA, \"w\"), bday &gt; 0) %&gt;%\n  select(-flag)"
  },
  {
    "objectID": "data/2025/2025-09-23/readme.html#the-data",
    "href": "data/2025/2025-09-23/readme.html#the-data",
    "title": "FIDE Chess Player Ratings",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 38)\n\nfide_ratings_august &lt;- tuesdata$fide_ratings_august\nfide_ratings_september &lt;- tuesdata$fide_ratings_september\n\n# Option 2: Read directly from GitHub\n\nfide_ratings_august &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv')\nfide_ratings_september &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-23')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nfide_ratings_august = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv')\nfide_ratings_september = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-23')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nfide_ratings_august = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv\")\nfide_ratings_september = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nfide_ratings_august = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_august.csv\", DataFrame)\nfide_ratings_september = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-23/fide_ratings_september.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-09-23/readme.html#how-to-participate",
    "href": "data/2025/2025-09-23/readme.html#how-to-participate",
    "title": "FIDE Chess Player Ratings",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-09-23/readme.html#data-dictionary",
    "href": "data/2025/2025-09-23/readme.html#data-dictionary",
    "title": "FIDE Chess Player Ratings",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nid\ninteger\nIdentification number of a player within FIDE database.\n\n\nname\ncharacter\nPlayer name.\n\n\nfed\ncharacter\nFederation of player. Similar, but not identical to, IOC country codes. An unofficial list is available here\n\n\nsex\ncharacter\nSex of player (M - male, F - female)\n\n\ntitle\ncharacter\nTitle of a player (GM - Grand Master, WGM - Woman Grand Master, IM - International Master, WIM - Woman International Master, FM - FIDE Master, WFM - Woman FIDE Master, CM - Candidate Master, WCM - Woman Candidate Master)\n\n\nwtitle\ncharacter\nWoman title of a player (WGM - Woman Grand Master, WIM - Woman International Master, WFM - Woman FIDE Master, WCM - Woman Candidate Master)\n\n\notitle\ncharacter\nComma-separated list. Other titles of a player (IA - International Arbiter, FA - FIDE Arbiter, NA - National Arbiter, IO - International Organizer, FT - FIDE Trainer, FST - FIDE Senior Trainer, DI - Developmental Instructor, NI - National Instructor)\n\n\nfoa\ncharacter\nFIDE Online Arena title of a player (AGM - Arena Grand Master, AIM - Arena International Master, AFM - Arena FIDE Master, ACM - Arena Candidate Master)\n\n\nrating\ninteger\nStandard chess rating (Elo).\n\n\ngames\ninteger\nNumber of games played in the month.\n\n\nk\ninteger\nRating K factor (Value influencing how much a player’s rating changes after a game)\n\n\nbday\ninteger\nYear of birth of a player.\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nIdentification number of a player within FIDE database.\n\n\nname\ncharacter\nPlayer name.\n\n\nfed\ncharacter\nFederation of player. Similar, but not identical to, IOC country codes. An unofficial list is available here\n\n\nsex\ncharacter\nSex of player (M - male, F - female)\n\n\ntitle\ncharacter\nTitle of a player (GM - Grand Master, WGM - Woman Grand Master, IM - International Master, WIM - Woman International Master, FM - FIDE Master, WFM - Woman FIDE Master, CM - Candidate Master, WCM - Woman Candidate Master)\n\n\nwtitle\ncharacter\nWoman title of a player (WGM - Woman Grand Master, WIM - Woman International Master, WFM - Woman FIDE Master, WCM - Woman Candidate Master)\n\n\notitle\ncharacter\nComma-separated list. Other titles of a player (IA - International Arbiter, FA - FIDE Arbiter, NA - National Arbiter, IO - International Organizer, FT - FIDE Trainer, FST - FIDE Senior Trainer, DI - Developmental Instructor, NI - National Instructor)\n\n\nfoa\ncharacter\nFIDE Online Arena title of a player (AGM - Arena Grand Master, AIM - Arena International Master, AFM - Arena FIDE Master, ACM - Arena Candidate Master)\n\n\nrating\ninteger\nStandard chess rating (Elo).\n\n\ngames\ninteger\nNumber of games played in the month.\n\n\nk\ninteger\nRating K factor (Value influencing how much a player’s rating changes after a game)\n\n\nbday\ninteger\nYear of birth of a player."
  },
  {
    "objectID": "data/2025/2025-09-23/readme.html#cleaning-script",
    "href": "data/2025/2025-09-23/readme.html#cleaning-script",
    "title": "FIDE Chess Player Ratings",
    "section": "",
    "text": "library(tidyverse)\nlibrary(withr)\nconflicted::conflicts_prefer(dplyr::filter)\n\n# download, unzip and read the file #\naugust_zipped &lt;- withr::local_tempfile(fileext = \".zip\")\nseptember_zipped &lt;- withr::local_tempfile(fileext = \".zip\")\n\ndownload.file(\n  \"https://ratings.fide.com/download/standard_aug25frl.zip\",\n  destfile = august_zipped,\n  mode = \"wb\"\n)\ndownload.file(\n  \"https://ratings.fide.com/download/standard_sep25frl.zip\",\n  destfile = september_zipped,\n  mode = \"wb\"\n)\n\naugust_unzipped &lt;- unzip(august_zipped, exdir = tempdir())\nseptember_unzipped &lt;- unzip(september_zipped, exdir = tempdir())\n\nfide_ratings_august &lt;- readr::read_fwf(\n  file = august_unzipped,\n  skip = 1,\n  col_types = c(\"dcccccccdddcc\"),\n  fwf_widths(\n    c(8, 68, 4, 4, 5, 5, 15, 4, 6, 4, 3, 6, 4),\n    c(\n      \"id\",\n      \"name\",\n      \"fed\",\n      \"sex\",\n      \"title\",\n      \"wtitle\",\n      \"otitle\",\n      \"foa\",\n      \"rating\",\n      \"games\",\n      \"k\",\n      \"bday\",\n      \"flag\"\n    )\n  )\n) %&gt;%\n  # cleaning up player names\n  mutate(name = str_remove(name, \"^\\\\d+\\\\s+\")) %&gt;%\n  # filter to only players who were active in the last 12 months\n  filter(flag %in% c(NA, \"w\"), bday &gt; 0) %&gt;%\n  select(-flag)\n\nfide_ratings_september &lt;- readr::read_fwf(\n  file = september_unzipped,\n  skip = 1,\n  col_types = c(\"dcccccccdddcc\"),\n  fwf_widths(\n    c(8, 68, 4, 4, 5, 5, 15, 4, 6, 4, 3, 6, 4),\n    c(\n      \"id\",\n      \"name\",\n      \"fed\",\n      \"sex\",\n      \"title\",\n      \"wtitle\",\n      \"otitle\",\n      \"foa\",\n      \"rating\",\n      \"games\",\n      \"k\",\n      \"bday\",\n      \"flag\"\n    )\n  )\n) %&gt;%\n  # cleaning up player names\n  mutate(name = str_remove(name, \"^\\\\d+\\\\s+\")) %&gt;%\n  # filter to only players who were active in the last 12 months\n  filter(flag %in% c(NA, \"w\"), bday &gt; 0) %&gt;%\n  select(-flag)"
  },
  {
    "objectID": "data/2025/2025-09-09/readme.html",
    "href": "data/2025/2025-09-09/readme.html",
    "title": "Henley Passport Index Data",
    "section": "",
    "text": "This week we are exploring data from the Henley Passport Index API. The Henley Passport Index is produced by Henley & Partners and captures the number of countries to which travelers in possession of each passport in the world may enter visa free.\n\nFor each travel destination, if no visa is required for passport holders from a country or territory, then a score with value = 1 is created for that passport. A score with value = 1 is also applied if passport holders can obtain a visa on arrival, a visitor’s permit, or an electronic travel authority (ETA) when entering the destination. These visa-types require no pre-departure government approval, because of the specific visa-waiver programs in place. Where a visa is required, or where a passport holder has to obtain a government-approved electronic visa (e-Visa) before departure, a score with value = 0 is assigned. A score with value = 0 is also assigned if passport holders need pre-departure government approval for a visa on arrival, a scenario we do not consider ‘visa-free’. The total score for each passport is equal to the number of destinations for which no visa is required (value = 1), under the conditions defined above.\n\nHenley & Partners update the Global Passport Index rankings each month and changes to the US passport rank captured media attention recently.\n\nWhich countries have made the most dramatic improvements in passport power? Which passports have lost the most visa-free access?\nDo countries with stronger trade relationships tend to offer mutual visa-free access?\nDid the COVID-19 pandemic affect passport rankings, particularly for countries that implemented strict border controls?\nHow does political instability or economic crisis impact passport strength?\n\nThank you to Brenden Smith and Jen Richmond for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 36)\n\ncountry_lists &lt;- tuesdata$country_lists\nrank_by_year &lt;- tuesdata$rank_by_year\n\n# Option 2: Read directly from GitHub\n\ncountry_lists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv')\nrank_by_year &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-09')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncountry_lists = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv')\nrank_by_year = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-09')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncountry_lists = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv\")\nrank_by_year = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncountry_lists = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv\", DataFrame)\nrank_by_year = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncode\ncharacter\nTwo letter country code.\n\n\ncountry\ncharacter\nFull country name.\n\n\nvisa_required\njson\nJSON data of code and name of the countries in which you need a traditional visa to enter these destinations, and you need to apply for it in person.\n\n\nvisa_online\njson\nJSON data of code and name of the countries in which you need a visa to enter, but you can apply for it online, and the visa you receive is electronic (pre-departure approval necessary).\n\n\nvisa_on_arrival\njson\nJSON data of code and name of the countries in which you need a visa to enter, but you can apply for and receive the visa upon arrival at the airport (no pre-departure approval necessary).\n\n\nvisa_free_access\njson\nJSON data of code and name of the countries in which you do not need a visa to enter.\n\n\nelectronic_travel_authorisation\njson\nJSON data of code and name of the countries in which you do not need a visa to enter, but you must apply for a digital electronic travel authority before arrival.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncode\ncharacter\nTwo letter country code.\n\n\ncountry\ncharacter\nFull country name.\n\n\nregion\ncharacter\nSeven category country region.\n\n\nrank\ninteger\nRanking of ‘visa_free_count’.\n\n\nvisa_free_count\ninteger\nThe number of countries that do not require a visa for the passport holder, or passport holders can obtain a visa on arrival, a visitor’s permit, or an electronic travel authority (ETA) when entering the destination.\n\n\nyear\ninteger\nYear of data.\n\n\n\n\n\n\n\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(jsonlite)\n\n# first httr request to get ranking data\n\nreq &lt;- GET(\"api.henleypassportindex.com/api/v3/countries\")\n\nparsed &lt;- req$content |&gt; \n  rawToChar() |&gt; \n  fromJSON()\n\n# Data by year was nested into a list. \n# Here we separate them out so that each year has its own row.\n\nrank_by_year &lt;- parsed$countries |&gt; \n  filter(has_data) |&gt; \n  tidyr::unnest_longer(col = data) |&gt; \n  select(code, country, region, data, year = data_id) |&gt; \n  unnest_wider(col = data)\n\n\n# The data for country_lists is pulled in with multiple httr requests.\n# We can run a query for a single country at a time, and we get back a list\n# with the country's name and code, and several data frames that show the \n# names and codes of the countries a passport owner has different visa access to. \n\n# We can define a function to transform the list into a data frame with columns that contain \n# these additional data frames. In order to meet Tidy Tuesday requirements,\n# this data was transformed into JSON to be saved into a csv file.\n\nlist_to_nested_df &lt;- function(input_list) {\n  processed_data &lt;- lapply(input_list, function(x) {\n    if(is.data.frame(x)) {\n      toJSON(I(list(x)))\n    } else {\n      x \n    }\n  })\n  \n  df &lt;- data.frame(processed_data)\n  \n  return(df)\n}\n\n\ncountry_lists &lt;- data.frame()\n\nfor (i in unique(rank_by_year$code)) {\n  print(i)\n  \n  req2 &lt;- GET(paste0(\"api.henleypassportindex.com/api/v3/visa-single/\", i))\n  parsed2 &lt;- req2$content |&gt; \n    rawToChar() |&gt; \n    fromJSON()\n  \n  add &lt;- list_to_nested_df(parsed2)\n  country_lists &lt;-  rbind(add, country_lists)\n  \n  Sys.sleep(2)\n}\n\n# After the data is loaded in, you can run the following code to \n# transform the JSON back into nested data frames or tibbles:\n#\n# country_lists |&gt;\n#   mutate(across(c(3:7),\n#                 ~map(.x, ~fromJSON(.x)[[1]] |&gt; tibble())))"
  },
  {
    "objectID": "data/2025/2025-09-09/readme.html#the-data",
    "href": "data/2025/2025-09-09/readme.html#the-data",
    "title": "Henley Passport Index Data",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 36)\n\ncountry_lists &lt;- tuesdata$country_lists\nrank_by_year &lt;- tuesdata$rank_by_year\n\n# Option 2: Read directly from GitHub\n\ncountry_lists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv')\nrank_by_year &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-09')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncountry_lists = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv')\nrank_by_year = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-09')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncountry_lists = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv\")\nrank_by_year = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncountry_lists = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv\", DataFrame)\nrank_by_year = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-09-09/readme.html#how-to-participate",
    "href": "data/2025/2025-09-09/readme.html#how-to-participate",
    "title": "Henley Passport Index Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-09-09/readme.html#data-dictionary",
    "href": "data/2025/2025-09-09/readme.html#data-dictionary",
    "title": "Henley Passport Index Data",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ncode\ncharacter\nTwo letter country code.\n\n\ncountry\ncharacter\nFull country name.\n\n\nvisa_required\njson\nJSON data of code and name of the countries in which you need a traditional visa to enter these destinations, and you need to apply for it in person.\n\n\nvisa_online\njson\nJSON data of code and name of the countries in which you need a visa to enter, but you can apply for it online, and the visa you receive is electronic (pre-departure approval necessary).\n\n\nvisa_on_arrival\njson\nJSON data of code and name of the countries in which you need a visa to enter, but you can apply for and receive the visa upon arrival at the airport (no pre-departure approval necessary).\n\n\nvisa_free_access\njson\nJSON data of code and name of the countries in which you do not need a visa to enter.\n\n\nelectronic_travel_authorisation\njson\nJSON data of code and name of the countries in which you do not need a visa to enter, but you must apply for a digital electronic travel authority before arrival.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncode\ncharacter\nTwo letter country code.\n\n\ncountry\ncharacter\nFull country name.\n\n\nregion\ncharacter\nSeven category country region.\n\n\nrank\ninteger\nRanking of ‘visa_free_count’.\n\n\nvisa_free_count\ninteger\nThe number of countries that do not require a visa for the passport holder, or passport holders can obtain a visa on arrival, a visitor’s permit, or an electronic travel authority (ETA) when entering the destination.\n\n\nyear\ninteger\nYear of data."
  },
  {
    "objectID": "data/2025/2025-09-09/readme.html#cleaning-script",
    "href": "data/2025/2025-09-09/readme.html#cleaning-script",
    "title": "Henley Passport Index Data",
    "section": "",
    "text": "library(httr)\nlibrary(tidyverse)\nlibrary(jsonlite)\n\n# first httr request to get ranking data\n\nreq &lt;- GET(\"api.henleypassportindex.com/api/v3/countries\")\n\nparsed &lt;- req$content |&gt; \n  rawToChar() |&gt; \n  fromJSON()\n\n# Data by year was nested into a list. \n# Here we separate them out so that each year has its own row.\n\nrank_by_year &lt;- parsed$countries |&gt; \n  filter(has_data) |&gt; \n  tidyr::unnest_longer(col = data) |&gt; \n  select(code, country, region, data, year = data_id) |&gt; \n  unnest_wider(col = data)\n\n\n# The data for country_lists is pulled in with multiple httr requests.\n# We can run a query for a single country at a time, and we get back a list\n# with the country's name and code, and several data frames that show the \n# names and codes of the countries a passport owner has different visa access to. \n\n# We can define a function to transform the list into a data frame with columns that contain \n# these additional data frames. In order to meet Tidy Tuesday requirements,\n# this data was transformed into JSON to be saved into a csv file.\n\nlist_to_nested_df &lt;- function(input_list) {\n  processed_data &lt;- lapply(input_list, function(x) {\n    if(is.data.frame(x)) {\n      toJSON(I(list(x)))\n    } else {\n      x \n    }\n  })\n  \n  df &lt;- data.frame(processed_data)\n  \n  return(df)\n}\n\n\ncountry_lists &lt;- data.frame()\n\nfor (i in unique(rank_by_year$code)) {\n  print(i)\n  \n  req2 &lt;- GET(paste0(\"api.henleypassportindex.com/api/v3/visa-single/\", i))\n  parsed2 &lt;- req2$content |&gt; \n    rawToChar() |&gt; \n    fromJSON()\n  \n  add &lt;- list_to_nested_df(parsed2)\n  country_lists &lt;-  rbind(add, country_lists)\n  \n  Sys.sleep(2)\n}\n\n# After the data is loaded in, you can run the following code to \n# transform the JSON back into nested data frames or tibbles:\n#\n# country_lists |&gt;\n#   mutate(across(c(3:7),\n#                 ~map(.x, ~fromJSON(.x)[[1]] |&gt; tibble())))"
  },
  {
    "objectID": "data/2025/2025-08-26/readme.html",
    "href": "data/2025/2025-08-26/readme.html",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "This week we are exploring the Billboard Hot 100 Number Ones Database. This workbook contains substantial data about every song to ever top the Billboard Hot 100 between August 4, 1958 and January 11, 2025. It was compiled by Chris Dalla Riva as he wrote the book Uncharted Territory: What Numbers Tell Us about the Biggest Hit Songs and Ourselves. It also often powers his newsletter Can’t Get Much Higher.\n\n7 years ago, I decided that I was going to listen to every number one hit. Along the way, I tracked an absurd amount of information about each song. Using that information, I wrote a data-driven history of popular music covering 1958 through today.\n\n\nHave #1 hits become shorter over time?\nDoes the relation between artist age and chart success change across time?\nWhich keys are most common in #1 hits? Do our key preferences differ by genre?\nWhat lyrical topics have dominated #1 hits across different decades?\nHow has the prevalence of explicit content changed over time?\n\nThank you to Jen Richmond (R-Ladies Sydney) for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 34)\n\nbillboard &lt;- tuesdata$billboard\ntopics &lt;- tuesdata$topics\n\n# Option 2: Read directly from GitHub\n\nbillboard &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv')\ntopics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-08-26')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nbillboard = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv')\ntopics = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-08-26')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nbillboard = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv\")\ntopics = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nbillboard = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv\", DataFrame)\ntopics = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsong\ncharacter\nSong Name\n\n\nartist\ncharacter\nArtist Name\n\n\ndate\ndate\nFirst Week to Hit Number One\n\n\nweeks_at_number_one\nnumeric\nCollective (Consecutive and Non-Consecutive) Weeks at Number One\n\n\nnon_consecutive\nnumeric\nDummy for if it were number one in non-consecutive weeks\n\n\nrating_1\nnumeric\nRating between 1 and 10, inclusive, provided by judge 1\n\n\nrating_2\nnumeric\nRating between 1 and 10, inclusive, provided by judge 2\n\n\nrating_3\nnumeric\nRating between 1 and 10, inclusive, provided by judge 3\n\n\noverall_rating\nnumeric\nSample mean of Rating 1, Rating 2, and Rating 3\n\n\ndivisiveness\nnumeric\nAverage absolute pairwise distance between all ratings, ranging from 0.0 to 6.0, inclusive\n\n\nlabel\ncharacter\nRecord Label that released the song\n\n\nparent_label\ncharacter\nThe larger label or entity that owns the label that released the song, if any\n\n\ncdr_genre\ncharacter\nGenre as assigned by Chris Dalla Riva and Vinnie Christopher.\n\n\ncdr_style\ncharacter\nSub-genre as assigned by Chris Dalla Riva and Vinnie Christopher.\n\n\ndiscogs_genre\ncharacter\nGenre as assigned by Discogs.com. See https://blog.discogs.com/en/genres-and-styles/ for more information.\n\n\ndiscogs_style\ncharacter\nStyle as assigned by Discogs.com. Style is Discogs’ equivalent of sub-genre. If there is no style listed, “None” is listed. See https://blog.discogs.com/en/genres-and-styles/ for more information.\n\n\nartist_structure\nnumeric\n0 means it is a group of three or more people. 1 means it is a solo act. 2 means it is a duo. If the number is followed by 0.5, then it means that there is at least one featured artist listed.\n\n\nfeatured_artists\ncharacter\nList of notable featured artists, along with what they did on the track, whether they were credited or not.\n\n\nmultiple_lead_vocalists\nnumeric\nDummy for if the song contains multiple people singing/rapping the lead vocal\n\n\ngroup_named_after_non_lead_singer\nnumeric\nDummy for if the group is named after someone who isn’t the lead singer (e.g., The J. Geils Band is named for the lead guitar player, not the singer)\n\n\ntalent_contestant\ncharacter\nNotes if the artist became well known through a television talent competition. If so, the talent competition is listed (e.g., American Idol)\n\n\nposthumous\nnumeric\nDummy for if the artist were dead when the song got to number one\n\n\nartist_place_of_origin\ncharacter\nThe country that the artist was born in.\n\n\nfront_person_age\nnumeric\nAge of the frontperson or bandleader on the song. If there are multiple, the average is taken. This is not necessarily the lead singer. If blank, then the age(s) could not be accurately located or age did not make sense (i.e., the band was animated).\n\n\nartist_male\nnumeric\nDummy for if the artist were a male or a group of males at the time of the release. If 0, the artist or group was all female. If 1, the artist or group was all male. If 2, the artist has a mix of males and females. If 3, the artist contains at least one non-binary person.\n\n\nartist_white\nnumeric\nDummy for if the artist was white, meaning that their ancestry was of European origin. If 0, the artist was not white. If 1, the artist was white. If 2, the artist has members that are both white and not white.\n\n\nartist_black\nnumeric\nDummy for if the artist was black, meaning that their ancestry was of African origin. If 0, the artist was not black. If 1, the artist was black. If 2, the artist has members that are both black and not black.\n\n\nsongwriters\ncharacter\nSongwriters from BMI/ASCAP Songview database. If they do not have the information, it comes from The Billboard Book of Number One Hits by Fred Bronson. If Bronson does not have the information, it comes from Spotify song credits.\n\n\nsongwriters_w_o_interpolation_sample_credits\ncharacter\nSongwriters from “Songwriters” field, excluding credits for writers who are being interpolated or sampled.\n\n\nsongwriter_male\nnumeric\nDummy for if the songwriter were a male or a group of males at the time of the release. If 0, the songwriter or group of songwriters was all female. If 1, the songwriter or group of songwriters was all male. If 2, the songwriters were a mix of males and females. If 3, the songwriters were a mix of males, females, and non-binary persons.\n\n\nsongwriter_white\nnumeric\nDummy for if the songwriteres were white, meaning that their ancestry was of European origin. If 0, the songwriters were not white. If 1, the songwriters were white. If 2, the songwriters has members that are both white and not white.\n\n\nartist_is_a_songwriter\nnumeric\nDummy for if the Artist is one of the songwriters\n\n\nartist_is_only_songwriter\nnumeric\nDummy for if the Artist is the only songwriter\n\n\nproducers\ncharacter\nProducers from Tidal’s production credits. If Tidal doesn’t have the information, it comes from Fred Bronson’s The Billboard Book of Number One Hits. If Bronson does not have the information, it comes from Spotify production credits.\n\n\nproducer_male\nnumeric\nDummy for if the producer were a male or a group of males at the time of the release. If 0, the producer or producers were all female. If 1, the producer or producers were all female. If 2, the producers were a mix of males and females. If 3, the songwriters were a mix of males, females, and non-binary persons.\n\n\nproducer_white\nnumeric\nDummy for if the producers were white, meaning that their ancestry was of European origin. If 0, the producers were not white. If 1, the producers were not white. If 2, the producers were composed of both white and not white persons.\n\n\nartist_is_a_producer\nnumeric\nDummy for if the Artist is one of the producers\n\n\nartist_is_only_producer\nnumeric\nDummy for if the artist is the only producer\n\n\nsongwriter_is_a_producer\nnumeric\nDummy for if one of the songwriters is one of the producers\n\n\ntime_signature\ncharacter\nTime Signature\n\n\nkeys\ncharacter\nKey that most captures a song. Key changes are seperated by a semi-colon. A percent sign (%) indicates that the song never returns to its original key. An ampersand (&) indicates an energy key change, or a key change that takes the song either up one or two half steps.\n\n\nsimplified_key\ncharacter\nIf the song has a single key, it is again listed here. If it has mutliple keys “Multiple Keys” is listed unless the only key change is an energy key change. In that case, the first key is listed.\n\n\nbpm\nnumeric\nBeats per minute as provided by Spotify\n\n\nenergy\nnumeric\nEnergy measure from 0 to 100 as provided by Spotify\n\n\ndanceability\nnumeric\nDanceability measure from 0 to 100 as provided by Spotify\n\n\nhappiness\nnumeric\nHappiness measure from 0 to 100 as provided by Spotify\n\n\nloudness_d_b\nnumeric\nLoudness measured in decibels as provided by Spotify\n\n\nacousticness\nnumeric\nProbability between 0 and 100 that a song is acoustic from Spotify\n\n\nvocally_based\nnumeric\nDummy for if the song is largely based around a vocal. This does not indicate if the song has a vocal, just if the arrangement is largely fleshed out by human voice, like “Don’t Worry Be Happy” by Bobby McFerrin or “Blue Moon” by The Marcels. Please note that the vocal can be a sample (e.g., “Slow Jamz” by Kanye West)\n\n\nbass_based\nnumeric\nDummy for if the song is largely based around a bass guitar or synth. This does not indicate if the song has a bass, just if the arrangement is largely fleshed out by bass.\n\n\nguitar_based\nnumeric\nDummy for if the song is largely based around a guitar. This does not indicate if the song has a guitar, just if the arrangement is largely fleshed out by guitar.\n\n\npiano_keyboard_based\nnumeric\nDummy for if the song is largely based around a piano or a keyboard. This does not indicate if the song has a piano or keyboard, just if the arrangement is largely fleshed out by one of those instruments.\n\n\norchestral_strings\nnumeric\nDummy for if the song contains a string section\n\n\nhorns_winds\nnumeric\nDummy for if the song contains a horn and/or wind section\n\n\naccordion\nnumeric\nDummy for if the song contains an accordion\n\n\nbanjo\nnumeric\nDummy for if the song contains a banjo\n\n\nbongos\nnumeric\nDummy for if the song contains bongos\n\n\nclarinet\nnumeric\nDummy for if the song contains a clarinet\n\n\ncowbell\nnumeric\nDummy for if the song contains a cowbell\n\n\nfalsetto_vocal\nnumeric\nDummy for if the song contains a falsetto vocal\n\n\nflute_piccolo\nnumeric\nDummy for if the song contains a flute or piccolo\n\n\nhandclaps_snaps\nnumeric\nDummy for if the song contains handclaps or snaps\n\n\nharmonica\nnumeric\nDummy for if the song contains a harmonica\n\n\nhuman_whistling\nnumeric\nDummy for if the song contains human whistling\n\n\nkazoo\nnumeric\nDummy for if the song contains a kazoo\n\n\nmandolin\nnumeric\nDummy for if the song contains a mandolin\n\n\npedal_lap_steel\nnumeric\nDummy for if the song contains a pedal or lap steel\n\n\nocarina\nnumeric\nDummy for if the song contains an ocarina\n\n\nsaxophone\nnumeric\nDummy for if the song contains a saxophone\n\n\nsitar\nnumeric\nDummy for if the song contains a sitar\n\n\ntrumpet\nnumeric\nDummy for if the song contains a trumpet\n\n\nukulele\nnumeric\nDummy for if the song contains a ukulele\n\n\nviolin\nnumeric\nDummy for if the song contains a violin\n\n\nsound_effects\ncharacter\nNotes if the song contains any pre-recorded sound effects and, if so, what they are.\n\n\nsong_structure\ncharacter\nCaptures the general structure of the song. ‘A1’ means only verses with no refrain. ‘A2’ means verses with a refrain at the beginning or end. ‘A3’ means a lyrical intro and then verses with a refrain at the beginning or end. ‘A4’ means a lyrical intro and then verses with no refrain. ‘C1’ means verse and chorus. ‘C2’ means a lyrical intro, verse, and chorus. ‘C3’ means verse, pre-chorus, chorus. ‘C4’ means verse, pre-chorus, chorus, post-chorus. ‘C5’ means verse, chorus, post-chorus. ‘C6’ means intro, verse, pre-chorus, chorus. ‘C7’ means intro, verse, pre-chorus, chorus, post-chorus. ‘D1’ means verse with a refrain at the beginning or end and a bridge. ‘D3’ means a lyrical intro then a verse with a refrain at the beginning or end and then a bridge. ‘E1’ means verse, chorus, and bridge. ‘E2’ means verse with a refrain at the beginning or end then a chorus and a bridge. ‘E3’ means a verse, pre-chorus, chorus, and bridge. ‘E4’ means a verse, pre-chorus, chorus, post-chorus, and bridge. ‘E5’ means a verse, chorus, post-chorus, and bridge. ‘E6’ means an Intro, Verse, Chorus, Bridge. ‘E7’ means verse with a refrain at the beginning or end then a pre-chorus, chorus, and bridge. ‘F’ means 4 or more sections. ‘I’ means it is an instrumental. If a structure has a ‘V’ at the end, it means that it is a non-rap song with a single rap verse.\n\n\nrap_verse_in_a_non_rap_song\nnumeric\nDummy for if the song contains at least one rapped verse despite not being a rap song\n\n\nlength_sec\nnumeric\nSong length in seconds\n\n\ninstrumental\nnumeric\nDummy for if it is an instrumental. Note that if a song contains little to no vocals, it is still considered an instrumental (e.g., “Harlem Shake” by Baauer)\n\n\ninstrumental_length_sec\nnumeric\nLength of time, in seconds, that does not contain a vocalist singing lyrics\n\n\nintro_length_sec\nnumeric\nLength of time, in seconds, before the first verse or chorus at the beginning of the song. Hooks are classified as introductions even if they repeat throughout.\n\n\nvocal_introduction\nnumeric\nDummy for if a song contains a vocal introduction, meaning a vocal section that appears at the beginning of the song and then not again.\n\n\nfree_time_vocal_introduction\nnumeric\nDummy for if a song contains a free time vocal introduction, meaning a vocal section with tempo rubato that appears at the beginning of the song and then not again.\n\n\nfade_out\nnumeric\nDummy for if the song fades out\n\n\nlive\nnumeric\nDummy for if the song is a live recording from a concert\n\n\ncover\nnumeric\nDummy for if the song is a cover, meaning the artist did not write the song and they were not the first person/group to record the song.\n\n\nsample\nnumeric\nDummy for if the song contains samples the recording of another song. This does not include interpolation/recording of parts of other songs. If a song is an English translation of a song in another language, it is still considered a cover.\n\n\ninterpolation\nnumeric\nDummy for if a song uses a musical or lyrical element from another existing song but re-records it themselves and is not a complete cover.\n\n\ninspired_by_a_different_song\nnumeric\nDummy for if a song is a cover or contains sampled/interpolated elements\n\n\nlyrics\ncharacter\nText of lyrics as provided by http://azlyrics.com. If http://azlyrics.com did not have the lyrics, then http://genius.com is used.\n\n\nlyrical_topic\ncharacter\nMain topic of lyrics as assigned by Author. For a list of topics, see  tab.\n\n\nlyrical_narrative\nnumeric\nDummy for if the lyrics follow a narrative, meaning a story with a loose beginning, middle, and end\n\n\nspoken_word\nnumeric\nDummy for if the song contains spoken word lyrics. Note this does not included rapped verses.\n\n\nexplicit\nnumeric\nDummy for if Spotify labels the song as explicit, if the song contains expletives, or if it is overly sexual, violent, or related to drugs for the time of its release. A word is considered an expletive if it is one of the following: ‘ass’, ‘bastard’, ‘bitch’, ‘cock’, ‘cunt’, ‘damn’, ‘dick’, ‘faggot’, ‘fuck’, ‘hell’, ‘piss’, ‘shit’, ‘twat’, all racial epithets, and all derivative words and phrases from this list.\n\n\nforeign_language\nnumeric\nDummy for if there are any non-English lyrics.\n\n\nwritten_for_a_play\nnumeric\nDummy for if the song was originally written for a play\n\n\nfeatured_in_a_then_contemporary_play\ncharacter\nNotes if the song was featured in a play around the time it topped the charts. If so, the play is listed. Please note that this does not mean the song was written for the play.\n\n\nwritten_for_a_film\nnumeric\nDummy for if the song was originally written for a film\n\n\nfeatured_in_a_then_contemporary_film\ncharacter\nNotes if the song was featured in a film around the time it topped the charts. If so, the film is listed. Please note that this does not mean the song was written for the film.\n\n\nwritten_for_a_t_v_show\nnumeric\nDummy for if the song was originally written for a T.V. show\n\n\nfeatured_in_a_then_contemporary_t_v_show\ncharacter\nNotes if the song was featured in a T.V. show around the time it topped the charts. If so, the T.V. show is listed. Please note that this does not mean the song was written for the T.V. show.\n\n\nassociated_with_dance\nnumeric\nDummy for if the song is known to have inspired people do a certain dance while it plays\n\n\ntopped_the_charts_by_multiple_artist\nnumeric\nDummy for if separate recordings of the song got to number one by different artists\n\n\ndouble_a_side\ncharacter\nNotes if the song were considered a double A-sided single. If so, the other side of the record is listed.\n\n\neurovision_entry\nnumeric\nDummy for if the song was entered into the annual Eurovision music competition\n\n\nu_s_artwork\ncharacter\nDescription of U.S. Single Artwork. “Cannot Find” means that the artwork could not be reliably located. All artwork before the year 2000 was located on https://discogs.com. Since digital music rose around that time, I begin consulting digital music stores and streaming services for artwork after 2000. If you would like to download the artwork, please follow this link.\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlyrical_topics\ncharacter\nMain topic of the lyrics\n\n\n\n\n\n\n\n# Mostly clean data by Chris Dalla Riva Billboard Hot 100 Number Ones Database\n# Google Sheet\n\nlibrary(googlesheets4)\nlibrary(janitor)\n\nbillboard &lt;- read_sheet(\n  \"https://docs.google.com/spreadsheets/d/1j1AUgtMnjpFTz54UdXgCKZ1i4bNxFjf01ImJ-BqBEt0/edit?gid=1974823090#gid=1974823090\",\n  sheet = 2,\n  na = c(\"\", \"N/A\")\n) %&gt;%\n  clean_names() %&gt;% \n  dplyr::mutate(song = unlist(song))\n\ntopics &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1j1AUgtMnjpFTz54UdXgCKZ1i4bNxFjf01ImJ-BqBEt0/edit?gid=1974823090#gid=1974823090\", sheet = 4) %&gt;%\n  clean_names()"
  },
  {
    "objectID": "data/2025/2025-08-26/readme.html#the-data",
    "href": "data/2025/2025-08-26/readme.html#the-data",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 34)\n\nbillboard &lt;- tuesdata$billboard\ntopics &lt;- tuesdata$topics\n\n# Option 2: Read directly from GitHub\n\nbillboard &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv')\ntopics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-08-26')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nbillboard = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv')\ntopics = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-08-26')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nbillboard = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv\")\ntopics = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nbillboard = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv\", DataFrame)\ntopics = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/topics.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-08-26/readme.html#how-to-participate",
    "href": "data/2025/2025-08-26/readme.html#how-to-participate",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-08-26/readme.html#data-dictionary",
    "href": "data/2025/2025-08-26/readme.html#data-dictionary",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nsong\ncharacter\nSong Name\n\n\nartist\ncharacter\nArtist Name\n\n\ndate\ndate\nFirst Week to Hit Number One\n\n\nweeks_at_number_one\nnumeric\nCollective (Consecutive and Non-Consecutive) Weeks at Number One\n\n\nnon_consecutive\nnumeric\nDummy for if it were number one in non-consecutive weeks\n\n\nrating_1\nnumeric\nRating between 1 and 10, inclusive, provided by judge 1\n\n\nrating_2\nnumeric\nRating between 1 and 10, inclusive, provided by judge 2\n\n\nrating_3\nnumeric\nRating between 1 and 10, inclusive, provided by judge 3\n\n\noverall_rating\nnumeric\nSample mean of Rating 1, Rating 2, and Rating 3\n\n\ndivisiveness\nnumeric\nAverage absolute pairwise distance between all ratings, ranging from 0.0 to 6.0, inclusive\n\n\nlabel\ncharacter\nRecord Label that released the song\n\n\nparent_label\ncharacter\nThe larger label or entity that owns the label that released the song, if any\n\n\ncdr_genre\ncharacter\nGenre as assigned by Chris Dalla Riva and Vinnie Christopher.\n\n\ncdr_style\ncharacter\nSub-genre as assigned by Chris Dalla Riva and Vinnie Christopher.\n\n\ndiscogs_genre\ncharacter\nGenre as assigned by Discogs.com. See https://blog.discogs.com/en/genres-and-styles/ for more information.\n\n\ndiscogs_style\ncharacter\nStyle as assigned by Discogs.com. Style is Discogs’ equivalent of sub-genre. If there is no style listed, “None” is listed. See https://blog.discogs.com/en/genres-and-styles/ for more information.\n\n\nartist_structure\nnumeric\n0 means it is a group of three or more people. 1 means it is a solo act. 2 means it is a duo. If the number is followed by 0.5, then it means that there is at least one featured artist listed.\n\n\nfeatured_artists\ncharacter\nList of notable featured artists, along with what they did on the track, whether they were credited or not.\n\n\nmultiple_lead_vocalists\nnumeric\nDummy for if the song contains multiple people singing/rapping the lead vocal\n\n\ngroup_named_after_non_lead_singer\nnumeric\nDummy for if the group is named after someone who isn’t the lead singer (e.g., The J. Geils Band is named for the lead guitar player, not the singer)\n\n\ntalent_contestant\ncharacter\nNotes if the artist became well known through a television talent competition. If so, the talent competition is listed (e.g., American Idol)\n\n\nposthumous\nnumeric\nDummy for if the artist were dead when the song got to number one\n\n\nartist_place_of_origin\ncharacter\nThe country that the artist was born in.\n\n\nfront_person_age\nnumeric\nAge of the frontperson or bandleader on the song. If there are multiple, the average is taken. This is not necessarily the lead singer. If blank, then the age(s) could not be accurately located or age did not make sense (i.e., the band was animated).\n\n\nartist_male\nnumeric\nDummy for if the artist were a male or a group of males at the time of the release. If 0, the artist or group was all female. If 1, the artist or group was all male. If 2, the artist has a mix of males and females. If 3, the artist contains at least one non-binary person.\n\n\nartist_white\nnumeric\nDummy for if the artist was white, meaning that their ancestry was of European origin. If 0, the artist was not white. If 1, the artist was white. If 2, the artist has members that are both white and not white.\n\n\nartist_black\nnumeric\nDummy for if the artist was black, meaning that their ancestry was of African origin. If 0, the artist was not black. If 1, the artist was black. If 2, the artist has members that are both black and not black.\n\n\nsongwriters\ncharacter\nSongwriters from BMI/ASCAP Songview database. If they do not have the information, it comes from The Billboard Book of Number One Hits by Fred Bronson. If Bronson does not have the information, it comes from Spotify song credits.\n\n\nsongwriters_w_o_interpolation_sample_credits\ncharacter\nSongwriters from “Songwriters” field, excluding credits for writers who are being interpolated or sampled.\n\n\nsongwriter_male\nnumeric\nDummy for if the songwriter were a male or a group of males at the time of the release. If 0, the songwriter or group of songwriters was all female. If 1, the songwriter or group of songwriters was all male. If 2, the songwriters were a mix of males and females. If 3, the songwriters were a mix of males, females, and non-binary persons.\n\n\nsongwriter_white\nnumeric\nDummy for if the songwriteres were white, meaning that their ancestry was of European origin. If 0, the songwriters were not white. If 1, the songwriters were white. If 2, the songwriters has members that are both white and not white.\n\n\nartist_is_a_songwriter\nnumeric\nDummy for if the Artist is one of the songwriters\n\n\nartist_is_only_songwriter\nnumeric\nDummy for if the Artist is the only songwriter\n\n\nproducers\ncharacter\nProducers from Tidal’s production credits. If Tidal doesn’t have the information, it comes from Fred Bronson’s The Billboard Book of Number One Hits. If Bronson does not have the information, it comes from Spotify production credits.\n\n\nproducer_male\nnumeric\nDummy for if the producer were a male or a group of males at the time of the release. If 0, the producer or producers were all female. If 1, the producer or producers were all female. If 2, the producers were a mix of males and females. If 3, the songwriters were a mix of males, females, and non-binary persons.\n\n\nproducer_white\nnumeric\nDummy for if the producers were white, meaning that their ancestry was of European origin. If 0, the producers were not white. If 1, the producers were not white. If 2, the producers were composed of both white and not white persons.\n\n\nartist_is_a_producer\nnumeric\nDummy for if the Artist is one of the producers\n\n\nartist_is_only_producer\nnumeric\nDummy for if the artist is the only producer\n\n\nsongwriter_is_a_producer\nnumeric\nDummy for if one of the songwriters is one of the producers\n\n\ntime_signature\ncharacter\nTime Signature\n\n\nkeys\ncharacter\nKey that most captures a song. Key changes are seperated by a semi-colon. A percent sign (%) indicates that the song never returns to its original key. An ampersand (&) indicates an energy key change, or a key change that takes the song either up one or two half steps.\n\n\nsimplified_key\ncharacter\nIf the song has a single key, it is again listed here. If it has mutliple keys “Multiple Keys” is listed unless the only key change is an energy key change. In that case, the first key is listed.\n\n\nbpm\nnumeric\nBeats per minute as provided by Spotify\n\n\nenergy\nnumeric\nEnergy measure from 0 to 100 as provided by Spotify\n\n\ndanceability\nnumeric\nDanceability measure from 0 to 100 as provided by Spotify\n\n\nhappiness\nnumeric\nHappiness measure from 0 to 100 as provided by Spotify\n\n\nloudness_d_b\nnumeric\nLoudness measured in decibels as provided by Spotify\n\n\nacousticness\nnumeric\nProbability between 0 and 100 that a song is acoustic from Spotify\n\n\nvocally_based\nnumeric\nDummy for if the song is largely based around a vocal. This does not indicate if the song has a vocal, just if the arrangement is largely fleshed out by human voice, like “Don’t Worry Be Happy” by Bobby McFerrin or “Blue Moon” by The Marcels. Please note that the vocal can be a sample (e.g., “Slow Jamz” by Kanye West)\n\n\nbass_based\nnumeric\nDummy for if the song is largely based around a bass guitar or synth. This does not indicate if the song has a bass, just if the arrangement is largely fleshed out by bass.\n\n\nguitar_based\nnumeric\nDummy for if the song is largely based around a guitar. This does not indicate if the song has a guitar, just if the arrangement is largely fleshed out by guitar.\n\n\npiano_keyboard_based\nnumeric\nDummy for if the song is largely based around a piano or a keyboard. This does not indicate if the song has a piano or keyboard, just if the arrangement is largely fleshed out by one of those instruments.\n\n\norchestral_strings\nnumeric\nDummy for if the song contains a string section\n\n\nhorns_winds\nnumeric\nDummy for if the song contains a horn and/or wind section\n\n\naccordion\nnumeric\nDummy for if the song contains an accordion\n\n\nbanjo\nnumeric\nDummy for if the song contains a banjo\n\n\nbongos\nnumeric\nDummy for if the song contains bongos\n\n\nclarinet\nnumeric\nDummy for if the song contains a clarinet\n\n\ncowbell\nnumeric\nDummy for if the song contains a cowbell\n\n\nfalsetto_vocal\nnumeric\nDummy for if the song contains a falsetto vocal\n\n\nflute_piccolo\nnumeric\nDummy for if the song contains a flute or piccolo\n\n\nhandclaps_snaps\nnumeric\nDummy for if the song contains handclaps or snaps\n\n\nharmonica\nnumeric\nDummy for if the song contains a harmonica\n\n\nhuman_whistling\nnumeric\nDummy for if the song contains human whistling\n\n\nkazoo\nnumeric\nDummy for if the song contains a kazoo\n\n\nmandolin\nnumeric\nDummy for if the song contains a mandolin\n\n\npedal_lap_steel\nnumeric\nDummy for if the song contains a pedal or lap steel\n\n\nocarina\nnumeric\nDummy for if the song contains an ocarina\n\n\nsaxophone\nnumeric\nDummy for if the song contains a saxophone\n\n\nsitar\nnumeric\nDummy for if the song contains a sitar\n\n\ntrumpet\nnumeric\nDummy for if the song contains a trumpet\n\n\nukulele\nnumeric\nDummy for if the song contains a ukulele\n\n\nviolin\nnumeric\nDummy for if the song contains a violin\n\n\nsound_effects\ncharacter\nNotes if the song contains any pre-recorded sound effects and, if so, what they are.\n\n\nsong_structure\ncharacter\nCaptures the general structure of the song. ‘A1’ means only verses with no refrain. ‘A2’ means verses with a refrain at the beginning or end. ‘A3’ means a lyrical intro and then verses with a refrain at the beginning or end. ‘A4’ means a lyrical intro and then verses with no refrain. ‘C1’ means verse and chorus. ‘C2’ means a lyrical intro, verse, and chorus. ‘C3’ means verse, pre-chorus, chorus. ‘C4’ means verse, pre-chorus, chorus, post-chorus. ‘C5’ means verse, chorus, post-chorus. ‘C6’ means intro, verse, pre-chorus, chorus. ‘C7’ means intro, verse, pre-chorus, chorus, post-chorus. ‘D1’ means verse with a refrain at the beginning or end and a bridge. ‘D3’ means a lyrical intro then a verse with a refrain at the beginning or end and then a bridge. ‘E1’ means verse, chorus, and bridge. ‘E2’ means verse with a refrain at the beginning or end then a chorus and a bridge. ‘E3’ means a verse, pre-chorus, chorus, and bridge. ‘E4’ means a verse, pre-chorus, chorus, post-chorus, and bridge. ‘E5’ means a verse, chorus, post-chorus, and bridge. ‘E6’ means an Intro, Verse, Chorus, Bridge. ‘E7’ means verse with a refrain at the beginning or end then a pre-chorus, chorus, and bridge. ‘F’ means 4 or more sections. ‘I’ means it is an instrumental. If a structure has a ‘V’ at the end, it means that it is a non-rap song with a single rap verse.\n\n\nrap_verse_in_a_non_rap_song\nnumeric\nDummy for if the song contains at least one rapped verse despite not being a rap song\n\n\nlength_sec\nnumeric\nSong length in seconds\n\n\ninstrumental\nnumeric\nDummy for if it is an instrumental. Note that if a song contains little to no vocals, it is still considered an instrumental (e.g., “Harlem Shake” by Baauer)\n\n\ninstrumental_length_sec\nnumeric\nLength of time, in seconds, that does not contain a vocalist singing lyrics\n\n\nintro_length_sec\nnumeric\nLength of time, in seconds, before the first verse or chorus at the beginning of the song. Hooks are classified as introductions even if they repeat throughout.\n\n\nvocal_introduction\nnumeric\nDummy for if a song contains a vocal introduction, meaning a vocal section that appears at the beginning of the song and then not again.\n\n\nfree_time_vocal_introduction\nnumeric\nDummy for if a song contains a free time vocal introduction, meaning a vocal section with tempo rubato that appears at the beginning of the song and then not again.\n\n\nfade_out\nnumeric\nDummy for if the song fades out\n\n\nlive\nnumeric\nDummy for if the song is a live recording from a concert\n\n\ncover\nnumeric\nDummy for if the song is a cover, meaning the artist did not write the song and they were not the first person/group to record the song.\n\n\nsample\nnumeric\nDummy for if the song contains samples the recording of another song. This does not include interpolation/recording of parts of other songs. If a song is an English translation of a song in another language, it is still considered a cover.\n\n\ninterpolation\nnumeric\nDummy for if a song uses a musical or lyrical element from another existing song but re-records it themselves and is not a complete cover.\n\n\ninspired_by_a_different_song\nnumeric\nDummy for if a song is a cover or contains sampled/interpolated elements\n\n\nlyrics\ncharacter\nText of lyrics as provided by http://azlyrics.com. If http://azlyrics.com did not have the lyrics, then http://genius.com is used.\n\n\nlyrical_topic\ncharacter\nMain topic of lyrics as assigned by Author. For a list of topics, see  tab.\n\n\nlyrical_narrative\nnumeric\nDummy for if the lyrics follow a narrative, meaning a story with a loose beginning, middle, and end\n\n\nspoken_word\nnumeric\nDummy for if the song contains spoken word lyrics. Note this does not included rapped verses.\n\n\nexplicit\nnumeric\nDummy for if Spotify labels the song as explicit, if the song contains expletives, or if it is overly sexual, violent, or related to drugs for the time of its release. A word is considered an expletive if it is one of the following: ‘ass’, ‘bastard’, ‘bitch’, ‘cock’, ‘cunt’, ‘damn’, ‘dick’, ‘faggot’, ‘fuck’, ‘hell’, ‘piss’, ‘shit’, ‘twat’, all racial epithets, and all derivative words and phrases from this list.\n\n\nforeign_language\nnumeric\nDummy for if there are any non-English lyrics.\n\n\nwritten_for_a_play\nnumeric\nDummy for if the song was originally written for a play\n\n\nfeatured_in_a_then_contemporary_play\ncharacter\nNotes if the song was featured in a play around the time it topped the charts. If so, the play is listed. Please note that this does not mean the song was written for the play.\n\n\nwritten_for_a_film\nnumeric\nDummy for if the song was originally written for a film\n\n\nfeatured_in_a_then_contemporary_film\ncharacter\nNotes if the song was featured in a film around the time it topped the charts. If so, the film is listed. Please note that this does not mean the song was written for the film.\n\n\nwritten_for_a_t_v_show\nnumeric\nDummy for if the song was originally written for a T.V. show\n\n\nfeatured_in_a_then_contemporary_t_v_show\ncharacter\nNotes if the song was featured in a T.V. show around the time it topped the charts. If so, the T.V. show is listed. Please note that this does not mean the song was written for the T.V. show.\n\n\nassociated_with_dance\nnumeric\nDummy for if the song is known to have inspired people do a certain dance while it plays\n\n\ntopped_the_charts_by_multiple_artist\nnumeric\nDummy for if separate recordings of the song got to number one by different artists\n\n\ndouble_a_side\ncharacter\nNotes if the song were considered a double A-sided single. If so, the other side of the record is listed.\n\n\neurovision_entry\nnumeric\nDummy for if the song was entered into the annual Eurovision music competition\n\n\nu_s_artwork\ncharacter\nDescription of U.S. Single Artwork. “Cannot Find” means that the artwork could not be reliably located. All artwork before the year 2000 was located on https://discogs.com. Since digital music rose around that time, I begin consulting digital music stores and streaming services for artwork after 2000. If you would like to download the artwork, please follow this link.\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlyrical_topics\ncharacter\nMain topic of the lyrics"
  },
  {
    "objectID": "data/2025/2025-08-26/readme.html#cleaning-script",
    "href": "data/2025/2025-08-26/readme.html#cleaning-script",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "# Mostly clean data by Chris Dalla Riva Billboard Hot 100 Number Ones Database\n# Google Sheet\n\nlibrary(googlesheets4)\nlibrary(janitor)\n\nbillboard &lt;- read_sheet(\n  \"https://docs.google.com/spreadsheets/d/1j1AUgtMnjpFTz54UdXgCKZ1i4bNxFjf01ImJ-BqBEt0/edit?gid=1974823090#gid=1974823090\",\n  sheet = 2,\n  na = c(\"\", \"N/A\")\n) %&gt;%\n  clean_names() %&gt;% \n  dplyr::mutate(song = unlist(song))\n\ntopics &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1j1AUgtMnjpFTz54UdXgCKZ1i4bNxFjf01ImJ-BqBEt0/edit?gid=1974823090#gid=1974823090\", sheet = 4) %&gt;%\n  clean_names()"
  },
  {
    "objectID": "data/2025/2025-08-12/readme.html",
    "href": "data/2025/2025-08-12/readme.html",
    "title": "Extreme Weather Attribution Studies",
    "section": "",
    "text": "This week we’re exploring extreme weather attribution studies. The dataset comes from Carbon Brief’s article Mapped: How climate change affects extreme weather around the world. An in-depth exploration of the evolution of extreme weather attribution science can be found in this Q & A article.\nThe data was last updated in November 2024 and single studies that cover multiple events or locations are separated out into individual entries when possible.\n\nAttribution studies calculate whether, and by how much, climate change affected the intensity, frequency or impact of extremes - from wildfires in the US and drought in South Africa through to record-breaking rainfall in Pakistan and typhoons in Taiwan.\n\nSome questions you might explore:\n\nHow do attribution study publications evolve over time? What about rapid attribution studies?\nWhat type of extreme event is more frequently the subject of an attribution study?\nIn which regions are most studies focused?\nIs there a trend in how climate change influences different types of extreme weather?\n\nThank you to Rajo for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 32)\n\nattribution_studies &lt;- tuesdata$attribution_studies\nattribution_studies_raw &lt;- tuesdata$attribution_studies_raw\n\n# Option 2: Read directly from GitHub\n\nattribution_studies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv')\nattribution_studies_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-08-12')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nattribution_studies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv')\nattribution_studies_raw = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-08-12')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nattribution_studies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv\")\nattribution_studies_raw = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nattribution_studies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv\", DataFrame)\nattribution_studies_raw = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nevent_name\ncharacter\nThe name or description of the extreme weather event studied.\n\n\nevent_period\ncharacter\nThe specific time period when the event occurred (extracted from the raw event name).\n\n\nevent_year\ncharacter\nThe year(s) or year range when the event occurred.\n\n\nstudy_focus\ncharacter\nWhether the study focused on a specific event or long-term trends.\n\n\niso_country_code\ncharacter\nThree-character ISO country code(s), with multiple countries separated by commas for multi-country studies (e.g.: “KEN, SOM”).\n\n\ncb_region\ncharacter\nThe geographic region classification used by Carbon Brief (Based on UN classification).\n\n\nevent_type\ncharacter\nThe type of extreme weather event or trend discussed in the study.\n\n\nclassification\ncharacter\nHow climate change has affected the studied event: “More severe or more likely to occur”, “No discernible human influence”, “Insufficient data/inconclusive”, “Decrease, less severe or less likely to occur”.\n\n\nsummary_statement\ncharacter\nThe authors’ key findings.\n\n\npublication_year\ndouble\nThe year when the study was published.\n\n\ncitation\ncharacter\nThe full citation for the study.\n\n\nsource\ncharacter\nThe source where the study was published.\n\n\nrapid_study\ncharacter\nWhether this was a rapid attribution study or not: analysis completed within days of the event occurring (“yes” or “no”).\n\n\nlink\ncharacter\nThe URL link to the original article.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nThe name or description of the extreme weather event studied and period when it occurred if available.\n\n\nevent_year_trend\ncharacter\nTime period when the event occurred (e.g., “2014”, “2004-05”, “mid-1990s”) or indication if the study focuses on long-term trends.\n\n\niso_country_code\ncharacter\nThree-character ISO country code(s), with multiple countries separated by commas for multi-country studies (e.g.: “KEN, SOM”).\n\n\ncb_region\ncharacter\nThe geographic region classification used by Carbon Brief (Based on UN classification).\n\n\nevent_type\ncharacter\nThe type of extreme weather event or trend discussed in the study.\n\n\nclassification\ncharacter\nHow climate change has affected the studied event: “More severe or more likely to occur”, “No discernible human influence”, “Insufficient data/inconclusive”, “Decrease, less severe or less likely to occur”.\n\n\nsummary_statement\ncharacter\nThe authors’ key findings.\n\n\npublication_year\ndouble\nThe year when the study was published.\n\n\ncitation\ncharacter\nThe full citation for the study.\n\n\nsource\ncharacter\nThe source where the study was published.\n\n\nrapid_study\ncharacter\nWhether this was a rapid attribution study or not: analysis completed within days of the event occurring (“yes” or “no”).\n\n\nlink\ncharacter\nThe URL link to the original article.\n\n\n\n\n\n\n\n# Data provided by Carbon Brief\n# Data available at https://interactive.carbonbrief.org/attribution-studies/data/papers-download.csv\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\n# Import Carbon Brief's Climate attribution studies dataset\nattribution_studies_raw &lt;- readr::read_csv(\n  \"https://interactive.carbonbrief.org/attribution-studies/data/papers-download.csv\"\n) |&gt;\n  janitor::clean_names()\n\n\n# Helper functions -------------------------------------------------------\n\n# Function to standardize year spans to consistent yyyy-yyyy format\nclean_yearspan &lt;- function(match) {\n  # Split the span using \"-\" as a delimiter\n  parts &lt;- stringr::str_split(match, \"-\")[[1]]\n  start_year &lt;- as.numeric(parts[1])\n  # Extract century from start year (e.g. 20 from 2020)\n  century &lt;- start_year %/% 100\n  # Combine the years\n  glue::glue(\"{parts[1]}-{century}{parts[2]}\")\n}\n\n# Function to clean and standardize data strings\nclean_date_string &lt;- function(col) {\n  col |&gt;\n    stringr::str_replace_all(\"–\", \"-\") |&gt;\n    # Find yyyy-yy patters and convert to yyyy-yyyy\n    stringr::str_replace_all(\"(\\\\d{4})-(\\\\d{2}$)\", \\(match) {\n      clean_yearspan(match)\n    }) |&gt;\n    stringr::str_replace_all(\" & \", \", \")\n}\n\n# Data Cleaning ----------------------------------------------------------\n\nattribution_studies &lt;- attribution_studies_raw |&gt;\n  janitor::clean_names() |&gt;\n  # Separate event names from time periods\n  # and split them into separate 'event_name' and 'event_period' columns\n  tidyr::separate_wider_regex(\n    name,\n    patterns = c(\n      event_name = \".*\",\n      \", \",\n      event_period = \".*\",\n      \"\\\\s\\\\(.*\"\n    ),\n    too_few = \"align_start\"\n  ) |&gt;\n  # Create standardized variables\n  dplyr::mutate(\n    event_year_trend = clean_date_string(event_year_trend),\n    event_period = dplyr::case_when(\n      is.na(event_period) & event_year_trend != \"Trend\" ~\n        dplyr::coalesce(event_period, event_year_trend),\n      TRUE ~ event_period\n    ) |&gt;\n      clean_date_string(),\n    event_year = dplyr::case_when(\n      event_year_trend != \"Trend\" ~ event_year_trend,\n      TRUE ~ NA_character_\n    ),\n    study_focus = dplyr::case_when(\n      event_year_trend == \"Trend\" ~ \"Trend\",\n      TRUE ~ \"Event\",\n    ),\n    .before = iso_country_code\n  ) |&gt;\n  dplyr::select(!event_year_trend)"
  },
  {
    "objectID": "data/2025/2025-08-12/readme.html#the-data",
    "href": "data/2025/2025-08-12/readme.html#the-data",
    "title": "Extreme Weather Attribution Studies",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 32)\n\nattribution_studies &lt;- tuesdata$attribution_studies\nattribution_studies_raw &lt;- tuesdata$attribution_studies_raw\n\n# Option 2: Read directly from GitHub\n\nattribution_studies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv')\nattribution_studies_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-08-12')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nattribution_studies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv')\nattribution_studies_raw = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-08-12')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nattribution_studies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv\")\nattribution_studies_raw = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nattribution_studies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies.csv\", DataFrame)\nattribution_studies_raw = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-12/attribution_studies_raw.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-08-12/readme.html#how-to-participate",
    "href": "data/2025/2025-08-12/readme.html#how-to-participate",
    "title": "Extreme Weather Attribution Studies",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-08-12/readme.html#data-dictionary",
    "href": "data/2025/2025-08-12/readme.html#data-dictionary",
    "title": "Extreme Weather Attribution Studies",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nevent_name\ncharacter\nThe name or description of the extreme weather event studied.\n\n\nevent_period\ncharacter\nThe specific time period when the event occurred (extracted from the raw event name).\n\n\nevent_year\ncharacter\nThe year(s) or year range when the event occurred.\n\n\nstudy_focus\ncharacter\nWhether the study focused on a specific event or long-term trends.\n\n\niso_country_code\ncharacter\nThree-character ISO country code(s), with multiple countries separated by commas for multi-country studies (e.g.: “KEN, SOM”).\n\n\ncb_region\ncharacter\nThe geographic region classification used by Carbon Brief (Based on UN classification).\n\n\nevent_type\ncharacter\nThe type of extreme weather event or trend discussed in the study.\n\n\nclassification\ncharacter\nHow climate change has affected the studied event: “More severe or more likely to occur”, “No discernible human influence”, “Insufficient data/inconclusive”, “Decrease, less severe or less likely to occur”.\n\n\nsummary_statement\ncharacter\nThe authors’ key findings.\n\n\npublication_year\ndouble\nThe year when the study was published.\n\n\ncitation\ncharacter\nThe full citation for the study.\n\n\nsource\ncharacter\nThe source where the study was published.\n\n\nrapid_study\ncharacter\nWhether this was a rapid attribution study or not: analysis completed within days of the event occurring (“yes” or “no”).\n\n\nlink\ncharacter\nThe URL link to the original article.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nThe name or description of the extreme weather event studied and period when it occurred if available.\n\n\nevent_year_trend\ncharacter\nTime period when the event occurred (e.g., “2014”, “2004-05”, “mid-1990s”) or indication if the study focuses on long-term trends.\n\n\niso_country_code\ncharacter\nThree-character ISO country code(s), with multiple countries separated by commas for multi-country studies (e.g.: “KEN, SOM”).\n\n\ncb_region\ncharacter\nThe geographic region classification used by Carbon Brief (Based on UN classification).\n\n\nevent_type\ncharacter\nThe type of extreme weather event or trend discussed in the study.\n\n\nclassification\ncharacter\nHow climate change has affected the studied event: “More severe or more likely to occur”, “No discernible human influence”, “Insufficient data/inconclusive”, “Decrease, less severe or less likely to occur”.\n\n\nsummary_statement\ncharacter\nThe authors’ key findings.\n\n\npublication_year\ndouble\nThe year when the study was published.\n\n\ncitation\ncharacter\nThe full citation for the study.\n\n\nsource\ncharacter\nThe source where the study was published.\n\n\nrapid_study\ncharacter\nWhether this was a rapid attribution study or not: analysis completed within days of the event occurring (“yes” or “no”).\n\n\nlink\ncharacter\nThe URL link to the original article."
  },
  {
    "objectID": "data/2025/2025-08-12/readme.html#cleaning-script",
    "href": "data/2025/2025-08-12/readme.html#cleaning-script",
    "title": "Extreme Weather Attribution Studies",
    "section": "",
    "text": "# Data provided by Carbon Brief\n# Data available at https://interactive.carbonbrief.org/attribution-studies/data/papers-download.csv\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\n# Import Carbon Brief's Climate attribution studies dataset\nattribution_studies_raw &lt;- readr::read_csv(\n  \"https://interactive.carbonbrief.org/attribution-studies/data/papers-download.csv\"\n) |&gt;\n  janitor::clean_names()\n\n\n# Helper functions -------------------------------------------------------\n\n# Function to standardize year spans to consistent yyyy-yyyy format\nclean_yearspan &lt;- function(match) {\n  # Split the span using \"-\" as a delimiter\n  parts &lt;- stringr::str_split(match, \"-\")[[1]]\n  start_year &lt;- as.numeric(parts[1])\n  # Extract century from start year (e.g. 20 from 2020)\n  century &lt;- start_year %/% 100\n  # Combine the years\n  glue::glue(\"{parts[1]}-{century}{parts[2]}\")\n}\n\n# Function to clean and standardize data strings\nclean_date_string &lt;- function(col) {\n  col |&gt;\n    stringr::str_replace_all(\"–\", \"-\") |&gt;\n    # Find yyyy-yy patters and convert to yyyy-yyyy\n    stringr::str_replace_all(\"(\\\\d{4})-(\\\\d{2}$)\", \\(match) {\n      clean_yearspan(match)\n    }) |&gt;\n    stringr::str_replace_all(\" & \", \", \")\n}\n\n# Data Cleaning ----------------------------------------------------------\n\nattribution_studies &lt;- attribution_studies_raw |&gt;\n  janitor::clean_names() |&gt;\n  # Separate event names from time periods\n  # and split them into separate 'event_name' and 'event_period' columns\n  tidyr::separate_wider_regex(\n    name,\n    patterns = c(\n      event_name = \".*\",\n      \", \",\n      event_period = \".*\",\n      \"\\\\s\\\\(.*\"\n    ),\n    too_few = \"align_start\"\n  ) |&gt;\n  # Create standardized variables\n  dplyr::mutate(\n    event_year_trend = clean_date_string(event_year_trend),\n    event_period = dplyr::case_when(\n      is.na(event_period) & event_year_trend != \"Trend\" ~\n        dplyr::coalesce(event_period, event_year_trend),\n      TRUE ~ event_period\n    ) |&gt;\n      clean_date_string(),\n    event_year = dplyr::case_when(\n      event_year_trend != \"Trend\" ~ event_year_trend,\n      TRUE ~ NA_character_\n    ),\n    study_focus = dplyr::case_when(\n      event_year_trend == \"Trend\" ~ \"Trend\",\n      TRUE ~ \"Event\",\n    ),\n    .before = iso_country_code\n  ) |&gt;\n  dplyr::select(!event_year_trend)"
  },
  {
    "objectID": "data/2025/2025-07-29/readme.html",
    "href": "data/2025/2025-07-29/readme.html",
    "title": "What have we been watching on Netflix?",
    "section": "",
    "text": "This week we are exploring TV show and movie viewing data from Netflix. Since 2023, Netflix has released regular Engagement Reports summarising the number of hours that users have spent watching each show and movie in the last 6 months.\n\nThis report, which captures ~99% of all viewing in the first half of 2025, shows that people watched a lot of Netflix — over 95B hours — spanning a wide range of genres and languages. It’s why we continue to invest in a variety of quality titles for various moods and tastes and work hard to make them great.\n\nThe dataset this week combines viewing data from late 2023 through the first half of 2025.\n\nWhat is the relation between time since release and current viewing performance? Do older shows have staying power or do newer releases dominate?\nHow does the timing of a show’s release (month/season) relate to engagement metrics?\nHow does the performance of globally available content compare to regionally restricted titles? Do global releases consistently outperform regional ones?\nHow do the top movie performers in each reporting period compare? Are the same kinds of movies consistently popular?\nHow does viewing change across seasons for shows that appear in multiple Netflix engagement reports? Do later seasons of popular shows maintain, gain, or lose audience engagement compared to their earlier seasons?\n\nThank you to Jen Richmond, RLadies-Sydney for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-29')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 30)\n\nmovies &lt;- tuesdata$movies\nshows &lt;- tuesdata$shows\n\n# Option 2: Read directly from GitHub\n\nmovies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv')\nshows &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-29')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nmovies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv')\nshows = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-29')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nmovies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv\")\nshows = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nmovies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv\", DataFrame)\nshows = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsource\ncharacter\nSource file name\n\n\nreport\ncharacter\nReport period\n\n\ntitle\ncharacter\nMovie title\n\n\navailable_globally\ncharacter\nWhether the movie is available globally or not\n\n\nrelease_date\ndate\nMovie release date\n\n\nhours_viewed\ndouble\nHours spent viewing the movie\n\n\nruntime\nPeriod\nMovie runtime\n\n\nviews\ndouble\nHours viewed / runtime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsource\ncharacter\nSource file name\n\n\nreport\ncharacter\nReport period\n\n\ntitle\ncharacter\nShow title\n\n\navailable_globally\ncharacter\nWhether the show is available globally or not\n\n\nrelease_date\ndate\nShow release date\n\n\nhours_viewed\ndouble\nHours spent viewing the show\n\n\nruntime\nPeriod\nShow runtime\n\n\nviews\ndouble\nHours viewed / runtime\n\n\n\n\n\n\n\n# Data manually downloaded from Netflix News to a `tt_submission` folder, from:\n\n# last half 2023: https://assets.ctfassets.net/4cd45et68cgf/inuAnzotdsAEgbInGLzH5/1be323ba419b2af3a96bffa29acc31a3/What_We_Watched_A_Netflix_Engagement_Report_2023Jul-Dec.xlsx\n# first half 2024: https://assets.ctfassets.net/4cd45et68cgf/2PoZlfdc46dH2gQvI8eUzI/9db5840720c47acfcf7b89ffe2402860/What_We_Watched_A_Netflix_Engagement_Report_2024Jan-Jun.xlsx\n# last half 2024: https://assets.ctfassets.net/4cd45et68cgf/6XSmoEjBjVMPRtYybT9d1E/8c0b0b2645b8712d5597b0bdbe0d64e2/What_We_Watched_A_Netflix_Engagement_Report_2024Jul-Dec.xlsx\n# first half 2025: https://assets.ctfassets.net/4cd45et68cgf/mplcXj5ulHDfbCPCr0f0I/5dbb6ec09f03df89706476e380e9b8bd/What_We_Watched_A_Netflix_Engagement_Report_2025Jan-Jun.xlsx\n\n# This script reads in the downloaded reports, separates excel sheets into separate data files, and fixes variable names\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(here)\nlibrary(janitor)\n\n# Define the folder path where your Excel files are located\nfolder_path &lt;- here(\"tt_submission\", \"data\")\n\n# Get list of Excel files\nexcel_files &lt;- list.files(folder_path, pattern = \"\\\\.xlsx$\", full.names = TRUE)\n\n# Function to read and combine data for a specific sheet type (TV or Film)\n\nread_and_combine_sheets &lt;- function(files, sheet_name) {\n  files %&gt;%\n    set_names(tools::file_path_sans_ext(basename(.))) %&gt;%\n    map_dfr(~ {\n      # Read the specific sheet from each file\n      read_excel(.x, sheet = sheet_name)\n    }, .id = \"file_id\")\n}\n\n# Read and combine TV data\n\nshows &lt;- read_and_combine_sheets(excel_files, \"TV\") %&gt;%\n  row_to_names(4) %&gt;%\n  clean_names() %&gt;%\n  rename(source = x1_what_we_watched_a_netflix_engagement_report_2025jan_jun) %&gt;%\n  mutate(report = str_sub(source, -11), .before = title) %&gt;%\n  mutate(release_date = ymd(release_date), \n         hours_viewed = as.numeric(hours_viewed),\n         runtime = hm(runtime),\n         views = as.numeric(views)) \n\n\n# Read and combine Movies data  \n\nmovies &lt;- read_and_combine_sheets(excel_files, \"Film\") %&gt;%\n  row_to_names(4) %&gt;%\n  clean_names() %&gt;%\n  rename(source = x1_what_we_watched_a_netflix_engagement_report_2025jan_jun) %&gt;%\n  mutate(report = str_sub(source, -11), .before = title) %&gt;%\n  mutate(release_date = ymd(release_date), \n         hours_viewed = as.numeric(hours_viewed),\n         runtime = hm(runtime),\n         views = as.numeric(views))"
  },
  {
    "objectID": "data/2025/2025-07-29/readme.html#the-data",
    "href": "data/2025/2025-07-29/readme.html#the-data",
    "title": "What have we been watching on Netflix?",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-29')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 30)\n\nmovies &lt;- tuesdata$movies\nshows &lt;- tuesdata$shows\n\n# Option 2: Read directly from GitHub\n\nmovies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv')\nshows &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-29')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nmovies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv')\nshows = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-29')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nmovies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv\")\nshows = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nmovies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv\", DataFrame)\nshows = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-07-29/readme.html#how-to-participate",
    "href": "data/2025/2025-07-29/readme.html#how-to-participate",
    "title": "What have we been watching on Netflix?",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-07-29/readme.html#data-dictionary",
    "href": "data/2025/2025-07-29/readme.html#data-dictionary",
    "title": "What have we been watching on Netflix?",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nsource\ncharacter\nSource file name\n\n\nreport\ncharacter\nReport period\n\n\ntitle\ncharacter\nMovie title\n\n\navailable_globally\ncharacter\nWhether the movie is available globally or not\n\n\nrelease_date\ndate\nMovie release date\n\n\nhours_viewed\ndouble\nHours spent viewing the movie\n\n\nruntime\nPeriod\nMovie runtime\n\n\nviews\ndouble\nHours viewed / runtime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsource\ncharacter\nSource file name\n\n\nreport\ncharacter\nReport period\n\n\ntitle\ncharacter\nShow title\n\n\navailable_globally\ncharacter\nWhether the show is available globally or not\n\n\nrelease_date\ndate\nShow release date\n\n\nhours_viewed\ndouble\nHours spent viewing the show\n\n\nruntime\nPeriod\nShow runtime\n\n\nviews\ndouble\nHours viewed / runtime"
  },
  {
    "objectID": "data/2025/2025-07-29/readme.html#cleaning-script",
    "href": "data/2025/2025-07-29/readme.html#cleaning-script",
    "title": "What have we been watching on Netflix?",
    "section": "",
    "text": "# Data manually downloaded from Netflix News to a `tt_submission` folder, from:\n\n# last half 2023: https://assets.ctfassets.net/4cd45et68cgf/inuAnzotdsAEgbInGLzH5/1be323ba419b2af3a96bffa29acc31a3/What_We_Watched_A_Netflix_Engagement_Report_2023Jul-Dec.xlsx\n# first half 2024: https://assets.ctfassets.net/4cd45et68cgf/2PoZlfdc46dH2gQvI8eUzI/9db5840720c47acfcf7b89ffe2402860/What_We_Watched_A_Netflix_Engagement_Report_2024Jan-Jun.xlsx\n# last half 2024: https://assets.ctfassets.net/4cd45et68cgf/6XSmoEjBjVMPRtYybT9d1E/8c0b0b2645b8712d5597b0bdbe0d64e2/What_We_Watched_A_Netflix_Engagement_Report_2024Jul-Dec.xlsx\n# first half 2025: https://assets.ctfassets.net/4cd45et68cgf/mplcXj5ulHDfbCPCr0f0I/5dbb6ec09f03df89706476e380e9b8bd/What_We_Watched_A_Netflix_Engagement_Report_2025Jan-Jun.xlsx\n\n# This script reads in the downloaded reports, separates excel sheets into separate data files, and fixes variable names\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(here)\nlibrary(janitor)\n\n# Define the folder path where your Excel files are located\nfolder_path &lt;- here(\"tt_submission\", \"data\")\n\n# Get list of Excel files\nexcel_files &lt;- list.files(folder_path, pattern = \"\\\\.xlsx$\", full.names = TRUE)\n\n# Function to read and combine data for a specific sheet type (TV or Film)\n\nread_and_combine_sheets &lt;- function(files, sheet_name) {\n  files %&gt;%\n    set_names(tools::file_path_sans_ext(basename(.))) %&gt;%\n    map_dfr(~ {\n      # Read the specific sheet from each file\n      read_excel(.x, sheet = sheet_name)\n    }, .id = \"file_id\")\n}\n\n# Read and combine TV data\n\nshows &lt;- read_and_combine_sheets(excel_files, \"TV\") %&gt;%\n  row_to_names(4) %&gt;%\n  clean_names() %&gt;%\n  rename(source = x1_what_we_watched_a_netflix_engagement_report_2025jan_jun) %&gt;%\n  mutate(report = str_sub(source, -11), .before = title) %&gt;%\n  mutate(release_date = ymd(release_date), \n         hours_viewed = as.numeric(hours_viewed),\n         runtime = hm(runtime),\n         views = as.numeric(views)) \n\n\n# Read and combine Movies data  \n\nmovies &lt;- read_and_combine_sheets(excel_files, \"Film\") %&gt;%\n  row_to_names(4) %&gt;%\n  clean_names() %&gt;%\n  rename(source = x1_what_we_watched_a_netflix_engagement_report_2025jan_jun) %&gt;%\n  mutate(report = str_sub(source, -11), .before = title) %&gt;%\n  mutate(release_date = ymd(release_date), \n         hours_viewed = as.numeric(hours_viewed),\n         runtime = hm(runtime),\n         views = as.numeric(views))"
  },
  {
    "objectID": "data/2025/2025-07-15/readme.html",
    "href": "data/2025/2025-07-15/readme.html",
    "title": "British Library Funding",
    "section": "",
    "text": "This week we’re looking into British Library funding! Thank you to Andy Jackson for compiling this data (and updating it with more information) and posting it on BlueSky! Also thanks to David Rosenthal’s 2017 blog for inspiring Andy’s efforts!\n\nI often refer back to this 2017 analysis by DSHR, which documents how the inflation-adjusted income of the British Library fell between 1999 and 2016. I referenced it in Invisible Memory Machines, but of course I was missing the data from the last eight years. Perhaps it’s all turned around since then!\n\nIt hadn’t all turned around, and he had to jump through some hoops to compile the data. Thanks for the efforts to preserve the data, Andy!\n\nWhat is the most variable source of data?\nWhat is the most consistent?\nDoes that analysis change when adjusted for inflation?\n\nSee Andy Jackson’s Google Sheet (where we got this dataset) for some sample charts.\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-15')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 28)\n\nbl_funding &lt;- tuesdata$bl_funding\n\n# Option 2: Read directly from GitHub\n\nbl_funding &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-15')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nbl_funding = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-15')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nbl_funding = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nbl_funding = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nFirst year of the annual report. Eg, 2016 is for the 2016/2017 annual report.\n\n\nnominal_gbp_millions\ndouble\nTotal reported funding in millions of Great Britain Pounds (GBP).\n\n\ngia_gbp_millions\ndouble\nReported funding from grant-in-aid (the official term for the core funding from the UK government).\n\n\nvoluntary_gbp_millions\ndouble\nReported funding covering all voluntary contributions and donations, including the valuation of donated collection items.\n\n\ninvestment_gbp_millions\ndouble\nReported funding from returns on savings and investments.\n\n\nservices_gbp_millions\ndouble\nReported funding from service delivery within the remit of being a charity. The main part of this over the years has been the document supply service, which started out as the National Lending Library for Science and Technology.\n\n\nother_gbp_millions\ndouble\nReported funding from anything that doesn’t fit into the above categories.\n\n\nyear_2000_gbp_millions\ndouble\nFunding values from the original blog post at https://blog.dshr.org/2017/08/preservation-is-not-technical-problem.html.\n\n\ninflation_adjustment\ndouble\nThe cost in each given year of 1,000,000 GBP in terms of year 2000 GBP. Figures come from the the current (2024) Bank of England inflation calculator: https://www.bankofengland.co.uk/monetary-policy/inflation/inflation-calculator.\n\n\ntotal_y2000_gbp_millions\ndouble\nTotal reported funding adjusted to Y2000 GBP.\n\n\npercentage_of_y2000_income\ndouble\ntotal_y2000_gbp_millions / total_y2000_gbp_millions for year == 2000.\n\n\ngia_y2000_gbp_millions\ndouble\nGrant-in-aid funding in Y2000 GBP.\n\n\nvoluntary_y2000_gbp_millions\ndouble\nVoluntary funding in Y2000 GBP.\n\n\ninvestment_y2000_gbp_millions\ndouble\nInvestment funding in Y2000 GBP.\n\n\nservices_y2000_gbp_millions\ndouble\nServices funding in Y2000 GBP.\n\n\nother_y2000_gbp_millions\ndouble\nOther funding in Y2000 GBP.\n\n\ngia_as_percent_of_peak_gia\ndouble\ngia_y2000_gbp_millions / max(gia_y2000_gbp_millions)\n\n\n\n\n\n\n\n# Data provided by Andy Jackson. See\n# https://anjackson.net/2024/11/27/updating-the-data-on-british-library-funding/\n# Minimal cleaning required.\n\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(janitor)\n\n# Call the auth function to make things run without prompts.\ngooglesheets4::gs4_auth(\"jonthegeek@gmail.com\")\nbl_funding &lt;- googlesheets4::read_sheet(\n  \"1uxjiuWYZrALF2mthmiYbUPieu1dEdEwv9GB8dEAizso\"\n) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::mutate(\n    year = as.integer(.data$year)\n  )"
  },
  {
    "objectID": "data/2025/2025-07-15/readme.html#the-data",
    "href": "data/2025/2025-07-15/readme.html#the-data",
    "title": "British Library Funding",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-15')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 28)\n\nbl_funding &lt;- tuesdata$bl_funding\n\n# Option 2: Read directly from GitHub\n\nbl_funding &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-15')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nbl_funding = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-15')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nbl_funding = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nbl_funding = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-15/bl_funding.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-07-15/readme.html#how-to-participate",
    "href": "data/2025/2025-07-15/readme.html#how-to-participate",
    "title": "British Library Funding",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-07-15/readme.html#data-dictionary",
    "href": "data/2025/2025-07-15/readme.html#data-dictionary",
    "title": "British Library Funding",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nyear\ninteger\nFirst year of the annual report. Eg, 2016 is for the 2016/2017 annual report.\n\n\nnominal_gbp_millions\ndouble\nTotal reported funding in millions of Great Britain Pounds (GBP).\n\n\ngia_gbp_millions\ndouble\nReported funding from grant-in-aid (the official term for the core funding from the UK government).\n\n\nvoluntary_gbp_millions\ndouble\nReported funding covering all voluntary contributions and donations, including the valuation of donated collection items.\n\n\ninvestment_gbp_millions\ndouble\nReported funding from returns on savings and investments.\n\n\nservices_gbp_millions\ndouble\nReported funding from service delivery within the remit of being a charity. The main part of this over the years has been the document supply service, which started out as the National Lending Library for Science and Technology.\n\n\nother_gbp_millions\ndouble\nReported funding from anything that doesn’t fit into the above categories.\n\n\nyear_2000_gbp_millions\ndouble\nFunding values from the original blog post at https://blog.dshr.org/2017/08/preservation-is-not-technical-problem.html.\n\n\ninflation_adjustment\ndouble\nThe cost in each given year of 1,000,000 GBP in terms of year 2000 GBP. Figures come from the the current (2024) Bank of England inflation calculator: https://www.bankofengland.co.uk/monetary-policy/inflation/inflation-calculator.\n\n\ntotal_y2000_gbp_millions\ndouble\nTotal reported funding adjusted to Y2000 GBP.\n\n\npercentage_of_y2000_income\ndouble\ntotal_y2000_gbp_millions / total_y2000_gbp_millions for year == 2000.\n\n\ngia_y2000_gbp_millions\ndouble\nGrant-in-aid funding in Y2000 GBP.\n\n\nvoluntary_y2000_gbp_millions\ndouble\nVoluntary funding in Y2000 GBP.\n\n\ninvestment_y2000_gbp_millions\ndouble\nInvestment funding in Y2000 GBP.\n\n\nservices_y2000_gbp_millions\ndouble\nServices funding in Y2000 GBP.\n\n\nother_y2000_gbp_millions\ndouble\nOther funding in Y2000 GBP.\n\n\ngia_as_percent_of_peak_gia\ndouble\ngia_y2000_gbp_millions / max(gia_y2000_gbp_millions)"
  },
  {
    "objectID": "data/2025/2025-07-15/readme.html#cleaning-script",
    "href": "data/2025/2025-07-15/readme.html#cleaning-script",
    "title": "British Library Funding",
    "section": "",
    "text": "# Data provided by Andy Jackson. See\n# https://anjackson.net/2024/11/27/updating-the-data-on-british-library-funding/\n# Minimal cleaning required.\n\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(janitor)\n\n# Call the auth function to make things run without prompts.\ngooglesheets4::gs4_auth(\"jonthegeek@gmail.com\")\nbl_funding &lt;- googlesheets4::read_sheet(\n  \"1uxjiuWYZrALF2mthmiYbUPieu1dEdEwv9GB8dEAizso\"\n) |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::mutate(\n    year = as.integer(.data$year)\n  )"
  },
  {
    "objectID": "data/2025/2025-07-01/readme.html",
    "href": "data/2025/2025-07-01/readme.html",
    "title": "Weekly US Gas Prices",
    "section": "",
    "text": "This week we’re exploring weekly US gas prices! The data comes from the U.S. Energy Information Administration (EIA), which publishes average retail gasoline and diesel prices each Monday. The original data (including additional datasets) can be found at eia.gov/petroleum/gasdiesel, and the weekly time series used here was downloaded from this XLS file.\nGas price methodology: &gt; “Every Monday, retail prices for all three grades of gasoline are collected mainly by telephone and email from a sample of approximately 1,000 retail gasoline outlets. The prices are published around 5:00 p.m. ET Monday, except on government holidays, when the data are released on Tuesday (but still represent Monday’s price). The reported price includes all taxes and is the cash pump price paid by a consumer as of 8:00 a.m. Monday. This price represents the self-serve price except in areas having only full-serve. The price data from the sample are used to calculate volume-weighted average gasoline price estimates at the national, regional, and selected city and state levels for all gasoline grades and formulations.”\nDiesel price methodology: &gt; “Every Monday, cash self-serve on-highway diesel prices (including taxes) are collected from a sample of approximately 590 retail diesel outlets in the continental United States. The sample includes a combination of truck stops and service stations that sell on-highway diesel fuel. The data represent the price of ultra low sulfur diesel (ULSD), which contains less than 15 parts-per-million sulfur. All collected prices are subjected to automated edit checks during data collection and data processing. Data flagged by the edits are verified with the respondents. Imputation is used for companies that cannot be contacted and for reported prices that are extreme outliers. The average survey response rate for 2020 was 98%. Average national and regional prices are released around 5:00 p.m. ET on Mondays, except on government holidays, in which case the data are released on Tuesday (but still represent Monday’s price).”\n\nHow did gas prices behave during major events like the 2008 recession or COVID-19 pandemic?\nAre diesel prices more or less volatile than gasoline prices?\nDo different grades or formulations of gasoline follow similar trends?\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-01')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 26)\n\nweekly_gas_prices &lt;- tuesdata$weekly_gas_prices\n\n# Option 2: Read directly from GitHub\n\nweekly_gas_prices &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-01')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nweekly_gas_prices = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-01')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nweekly_gas_prices = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nweekly_gas_prices = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nThe week-ending date for the reported fuel price.\n\n\nfuel\nfactor\nThe type of fuel reported (gasoline or diesel).\n\n\ngrade\nfactor\nThe grade or specification of the fuel (for gasoline: all, regular, midgrade, or premium; for diesel: all, ultra_low_sulfur, low_sulfur).\n\n\nformulation\nfactor\nThe formulation of the gasoline (all, conventional, or reformulated). Only applies to gasoline.\n\n\nprice\ndouble\nThe average U.S. retail price per gallon in U.S. dollars for that fuel type.\n\n\n\n\n\n\n\n# Data downloaded manually from\n# https://www.eia.gov/dnav/pet/xls/PET_PRI_GND_DCUS_NUS_W.xls to tempdir.\nfile_path &lt;- fs::path_temp(\"PET_PRI_GND_DCUS_NUS_W.xls\")\n\nweekly_gas_prices &lt;- readxl::read_xls(file_path, sheet = \"Data 1\", skip = 2) |&gt;\n  dplyr::select(\n    date = \"Date\",\n    gasoline.all.all = \"Weekly U.S. All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.all.conventional = \"Weekly U.S. All Grades Conventional Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.all.reformulated = \"Weekly U.S. All Grades Reformulated Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.regular.all = \"Weekly U.S. Regular All Formulations Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.regular.conventional = \"Weekly U.S. Regular Conventional Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.regular.reformulated = \"Weekly U.S. Regular Reformulated Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.midgrade.all = \"Weekly U.S. Midgrade All Formulations Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.midgrade.conventional = \"Weekly U.S. Midgrade Conventional Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.midgrade.reformulated = \"Weekly U.S. Midgrade Reformulated Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.premium.all = \"Weekly U.S. Premium All Formulations Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.premium.conventional = \"Weekly U.S. Premium Conventional Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.premium.reformulated = \"Weekly U.S. Premium Reformulated Retail Gasoline Prices  (Dollars per Gallon)\",\n    diesel.all = \"Weekly U.S. No 2 Diesel Retail Prices  (Dollars per Gallon)\",\n    diesel.ultra_low_sulfur = \"Weekly U.S. No 2 Diesel Ultra Low Sulfur (0-15 ppm) Retail Prices  (Dollars per Gallon)\",\n    diesel.low_sulfur = \"Weekly U.S. No 2 Diesel Low Sulfur (15-500 ppm) Retail Prices  (Dollars per Gallon)\"\n  ) |&gt;\n  dplyr::mutate(date = as.Date(date)) |&gt;\n  tidyr::pivot_longer(\n    cols = -date,\n    names_to = \"fuel_spec\",\n    values_to = \"price\"\n  ) |&gt;\n  dplyr::filter(!is.na(price)) |&gt;\n  tidyr::separate_wider_delim(\n    cols = fuel_spec,\n    delim = \".\",\n    names = c(\"fuel\", \"grade\", \"formulation\"),\n    too_few = \"align_start\"\n  ) |&gt;\n  dplyr::mutate(\n    dplyr::across(c(fuel, grade, formulation), as.factor)\n  )"
  },
  {
    "objectID": "data/2025/2025-07-01/readme.html#the-data",
    "href": "data/2025/2025-07-01/readme.html#the-data",
    "title": "Weekly US Gas Prices",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-01')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 26)\n\nweekly_gas_prices &lt;- tuesdata$weekly_gas_prices\n\n# Option 2: Read directly from GitHub\n\nweekly_gas_prices &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-01')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nweekly_gas_prices = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-01')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nweekly_gas_prices = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nweekly_gas_prices = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-07-01/readme.html#how-to-participate",
    "href": "data/2025/2025-07-01/readme.html#how-to-participate",
    "title": "Weekly US Gas Prices",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-07-01/readme.html#data-dictionary",
    "href": "data/2025/2025-07-01/readme.html#data-dictionary",
    "title": "Weekly US Gas Prices",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ndate\ndate\nThe week-ending date for the reported fuel price.\n\n\nfuel\nfactor\nThe type of fuel reported (gasoline or diesel).\n\n\ngrade\nfactor\nThe grade or specification of the fuel (for gasoline: all, regular, midgrade, or premium; for diesel: all, ultra_low_sulfur, low_sulfur).\n\n\nformulation\nfactor\nThe formulation of the gasoline (all, conventional, or reformulated). Only applies to gasoline.\n\n\nprice\ndouble\nThe average U.S. retail price per gallon in U.S. dollars for that fuel type."
  },
  {
    "objectID": "data/2025/2025-07-01/readme.html#cleaning-script",
    "href": "data/2025/2025-07-01/readme.html#cleaning-script",
    "title": "Weekly US Gas Prices",
    "section": "",
    "text": "# Data downloaded manually from\n# https://www.eia.gov/dnav/pet/xls/PET_PRI_GND_DCUS_NUS_W.xls to tempdir.\nfile_path &lt;- fs::path_temp(\"PET_PRI_GND_DCUS_NUS_W.xls\")\n\nweekly_gas_prices &lt;- readxl::read_xls(file_path, sheet = \"Data 1\", skip = 2) |&gt;\n  dplyr::select(\n    date = \"Date\",\n    gasoline.all.all = \"Weekly U.S. All Grades All Formulations Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.all.conventional = \"Weekly U.S. All Grades Conventional Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.all.reformulated = \"Weekly U.S. All Grades Reformulated Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.regular.all = \"Weekly U.S. Regular All Formulations Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.regular.conventional = \"Weekly U.S. Regular Conventional Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.regular.reformulated = \"Weekly U.S. Regular Reformulated Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.midgrade.all = \"Weekly U.S. Midgrade All Formulations Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.midgrade.conventional = \"Weekly U.S. Midgrade Conventional Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.midgrade.reformulated = \"Weekly U.S. Midgrade Reformulated Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.premium.all = \"Weekly U.S. Premium All Formulations Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.premium.conventional = \"Weekly U.S. Premium Conventional Retail Gasoline Prices  (Dollars per Gallon)\",\n    gasoline.premium.reformulated = \"Weekly U.S. Premium Reformulated Retail Gasoline Prices  (Dollars per Gallon)\",\n    diesel.all = \"Weekly U.S. No 2 Diesel Retail Prices  (Dollars per Gallon)\",\n    diesel.ultra_low_sulfur = \"Weekly U.S. No 2 Diesel Ultra Low Sulfur (0-15 ppm) Retail Prices  (Dollars per Gallon)\",\n    diesel.low_sulfur = \"Weekly U.S. No 2 Diesel Low Sulfur (15-500 ppm) Retail Prices  (Dollars per Gallon)\"\n  ) |&gt;\n  dplyr::mutate(date = as.Date(date)) |&gt;\n  tidyr::pivot_longer(\n    cols = -date,\n    names_to = \"fuel_spec\",\n    values_to = \"price\"\n  ) |&gt;\n  dplyr::filter(!is.na(price)) |&gt;\n  tidyr::separate_wider_delim(\n    cols = fuel_spec,\n    delim = \".\",\n    names = c(\"fuel\", \"grade\", \"formulation\"),\n    too_few = \"align_start\"\n  ) |&gt;\n  dplyr::mutate(\n    dplyr::across(c(fuel, grade, formulation), as.factor)\n  )"
  },
  {
    "objectID": "data/2025/2025-06-17/readme.html",
    "href": "data/2025/2025-06-17/readme.html",
    "title": "API Specs",
    "section": "",
    "text": "This week we’re exploring Web APIs! The lead volunteer for TidyTuesday (Jon Harmon) is writing a book about working with Web APIs with R as well as a series of R packages to make it easier to create API-wrapping R packages. On Thursday, 2025-06-19, Jon will present a talk on this package ecosystem at the Ghana R Conference 2025. While working on the packages and the talk, Jon explored a list of APIs from the website APIs.guru. That dataset is provided here.\n\n[APIs.guru’s] goal is to create a machine-readable Wikipedia for Web APIs in the OpenAPI Specification format.\n\n\nWhat API specs are provided by APIs.guru? Are these the same as the origin specs?\nHow many different APIs (“services”) do providers provide?\nWhat licenses do APIs use?\nAre any APIs listed more than once in the dataset?\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-06-17')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 24)\n\napi_categories &lt;- tuesdata$api_categories\napi_info &lt;- tuesdata$api_info\napi_logos &lt;- tuesdata$api_logos\napi_origins &lt;- tuesdata$api_origins\napisguru_apis &lt;- tuesdata$apisguru_apis\n\n# Option 2: Read directly from GitHub\n\napi_categories &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv')\napi_info &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv')\napi_logos &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv')\napi_origins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv')\napisguru_apis &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-06-17')\n\n# Option 2: Read directly from GitHub and assign to an object\n\napi_categories = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv')\napi_info = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv')\napi_logos = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv')\napi_origins = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv')\napisguru_apis = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-06-17')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\napi_categories = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv\")\napi_info = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv\")\napi_logos = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv\")\napi_origins = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv\")\napisguru_apis = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\napi_categories = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv\", DataFrame)\napi_info = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv\", DataFrame)\napi_logos = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv\", DataFrame)\napi_origins = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv\", DataFrame)\napisguru_apis = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\napisguru_category\ncharacter\nSorting category of this API on apis.guru. If an API is listed in multiple categories, it has more than one row in this table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\ncontact_name\ncharacter\nThe name of the person or entity responsible for this API.\n\n\ncontact_url\ncharacter\nThe url to check for information about this API.\n\n\ndescription\ncharacter\nA brief description of this API.\n\n\ntitle\ncharacter\nThe title of this API..\n\n\nprovider_name\ncharacter\nThe provider of this API. This is more meaningful if a provider has multiple APIs in the apis.guru database.\n\n\nservice_name\ncharacter\nThe service that this API covers within the provider. Pressent when a provider has multiple APIs in the apis.guru database.\n\n\nlicense_name\ncharacter\nThe name of the license associated with this API, if available.\n\n\nlicense_url\ncharacter\nThe URL of the license, if available.\n\n\nterms_of_service\ncharacter\nThe url of terms of service for this API, if available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\nbackground_color\ncharacter\nHex code (in the format #RRGGBB) intended to show behind the logo. Missing values either mean that no color is expected (the background can be transparent), or that there isn’t a valid logo available for this API.\n\n\nurl\ncharacter\nPath to the logo on apis.guru.\n\n\nalt_text\ncharacter\nText to provide for this logo visually impaired users.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\nformat\ncharacter\nThe format of the original API spec, if available. One of “apiBlueprint”, “google”, “openapi”, “postman”, “swagger”, or “wadl”.\n\n\nurl\ncharacter\nThe path to the original API spec. Note: Some of these paths are no longer valid.\n\n\nversion\ncharacter\nThe version of the format used by this API spec. Each format has its own list of possible values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\nversion\ncharacter\nVersion of the API. Data is filtered to only the “preferrred” versions.\n\n\nadded\ndatetime\nWhen the API was added to apis.guru.\n\n\nupdated\ndatetime\nWhen the API was updated on apis.guru.\n\n\nswagger_url\ncharacter\nThe path to this API spec on apis.guru.\n\n\nopenapi_ver\ncharacter\nThe version of the OpenAPI (or swagger) spec of this API on apis.guru.\n\n\nlink\ncharacter\nThe path to this information (plus some fields we don’t include here) for this API on apis.guru.\n\n\nexternal_docs_description\ncharacter\nA short description of external documentation, if provided.\n\n\nexternal_docs_url\ncharacter\nThe location of the external documentation, if provided.\n\n\n\n\n\n\n\n# This dataset was compiled using funcionality from my in-progress ecosystem of\n# packages described at https://beekeeper.api2r.org/. On Thursday, 2025-06-19, I\n# will present a talk on this ecosystem at the Ghana R Conference 2025\n# (https://ghana-rusers.org/ghana-r-conference-2025/).\n\n# I use information about APIs from https://apis.guru to test various aspects of\n# {beekeeper} and related packages. Here we use {nectar}\n# (https://nectar.api2r.org/) and {tibblify}\n# (https://mgirlich.github.io/tibblify/) to download and process the list of\n# APIs from apis.guru.\n\n# {nectar} is not available on CRAN. The rest of the packages can be installed\n# via install.packages.\n# install.packages(\"pak\")\n# pak::pak(\"jonthegeek/nectar\")\n\nlibrary(nectar)\nlibrary(tibblify)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\n\n# This section replicates functionality from the in-progress package {apisguru}\n# (https://jonthegeek.github.io/apisguru/). At the time that this dataset was\n# compiled, {apisguru} was not yet updated for the current version of {nectar},\n# so I'm not using it directly. Watch its progress as the {beekeeper} ecosystem\n# solidifies!\n\n.schema_api_spec &lt;- function() {\n  tibblify::tspec_df(\n    .tib_datetime(\"added\"),\n    tibblify::tib_chr(\"preferred\"),\n    tibblify::tib_df(\n      \"versions\",\n      .names_to = \"version\",\n      .schema_api_version_spec()\n    )\n  )\n}\n\n.schema_api_version_spec &lt;- function() {\n  tibblify::tspec_df(\n    .tib_datetime(\"added\"),\n    tibblify::tib_variant(\"info\"),\n    .tib_datetime(\"updated\"),\n    tibblify::tib_chr(\"swaggerUrl\"),\n    tibblify::tib_chr(\"swaggerYamlUrl\"),\n    tibblify::tib_chr(\"openapiVer\"),\n    tibblify::tib_chr(\"link\", required = FALSE),\n    tibblify::tib_variant(\"externalDocs\", required = FALSE)\n  )\n}\n\n.tib_datetime &lt;- function(key, ..., required = TRUE) {\n  tibblify::tib_scalar(\n    key = key,\n    ptype = vctrs::new_datetime(tzone = \"UTC\"),\n    required = required,\n    ptype_inner = character(),\n    transform = .quick_datetime,\n    ...\n  )\n}\n\n.quick_datetime &lt;- function(x, tzone = \"UTC\") {\n  as.POSIXct(gsub(\"T\", \" \", x), tz = tzone)\n}\n\nreq &lt;- nectar::req_prepare(\n  \"https://api.apis.guru/v2\",\n  path = \"/list.json\",\n  tidy_fn = nectar::resp_tidy_json,\n  tidy_args = list(\n    spec = tibblify::tspec_df(\n      .names_to = \"name\",\n      .schema_api_spec()\n    )\n  )\n)\nresp &lt;- nectar::req_perform_opinionated(req, max_reqs = Inf)\napisguru_apis &lt;- nectar::resp_tidy(resp) |&gt;\n  dplyr::select(\"name\", \"preferred\", \"versions\") |&gt;\n  tidyr::unnest(\"versions\") |&gt;\n  dplyr::filter(\n    .data$preferred == .data$version\n  ) |&gt;\n  tidyr::unnest_wider(\"externalDocs\", names_sep = \"_\") |&gt;\n  dplyr::select(-\"preferred\", -\"externalDocs_x-sha1\", -\"swaggerYamlUrl\") |&gt;\n  janitor::clean_names()\n\ndplyr::glimpse(apisguru_apis)\n\napi_info &lt;- apisguru_apis |&gt;\n  dplyr::select(\"name\", \"info\") |&gt;\n  tidyr::unnest_wider(\"info\") |&gt;\n  tidyr::unnest_wider(\"contact\", names_sep = \"_\") |&gt;\n  tidyr::unnest_wider(\"license\", names_sep = \"_\") |&gt;\n  dplyr::select(\n    \"name\",\n    \"contact_name\",\n    \"contact_url\",\n    \"description\",\n    \"title\",\n    apisguru_category = \"x-apisguru-categories\",\n    logo = \"x-logo\",\n    origin = \"x-origin\",\n    provider_name = \"x-providerName\",\n    service_name = \"x-serviceName\",\n    \"license_name\",\n    \"license_url\",\n    terms_of_service = \"termsOfService\"\n  )\napisguru_apis$info &lt;- NULL\ndplyr::glimpse(api_info)\n\napi_categories &lt;- api_info |&gt;\n  dplyr::select(\"name\", \"apisguru_category\") |&gt;\n  tidyr::unnest_longer(\"apisguru_category\")\napi_info$apisguru_category &lt;- NULL\n\napi_logos &lt;- api_info |&gt;\n  dplyr::select(\"name\", \"logo\") |&gt;\n  tidyr::unnest_wider(\"logo\") |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::select(-\"href\")\napi_info$logo &lt;- NULL\n\napi_origins &lt;- api_info |&gt;\n  dplyr::select(\"name\", \"origin\") |&gt;\n  tidyr::unnest_longer(\"origin\") |&gt;\n  tidyr::unnest_wider(\"origin\") |&gt;\n  dplyr::select(\"name\":\"version\") |&gt;\n  # Some of the entries are duplicated.\n  dplyr::distinct()\napi_info$origin &lt;- NULL"
  },
  {
    "objectID": "data/2025/2025-06-17/readme.html#the-data",
    "href": "data/2025/2025-06-17/readme.html#the-data",
    "title": "API Specs",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-06-17')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 24)\n\napi_categories &lt;- tuesdata$api_categories\napi_info &lt;- tuesdata$api_info\napi_logos &lt;- tuesdata$api_logos\napi_origins &lt;- tuesdata$api_origins\napisguru_apis &lt;- tuesdata$apisguru_apis\n\n# Option 2: Read directly from GitHub\n\napi_categories &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv')\napi_info &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv')\napi_logos &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv')\napi_origins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv')\napisguru_apis &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-06-17')\n\n# Option 2: Read directly from GitHub and assign to an object\n\napi_categories = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv')\napi_info = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv')\napi_logos = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv')\napi_origins = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv')\napisguru_apis = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-06-17')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\napi_categories = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv\")\napi_info = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv\")\napi_logos = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv\")\napi_origins = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv\")\napisguru_apis = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\napi_categories = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_categories.csv\", DataFrame)\napi_info = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_info.csv\", DataFrame)\napi_logos = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_logos.csv\", DataFrame)\napi_origins = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/api_origins.csv\", DataFrame)\napisguru_apis = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-17/apisguru_apis.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-06-17/readme.html#how-to-participate",
    "href": "data/2025/2025-06-17/readme.html#how-to-participate",
    "title": "API Specs",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-06-17/readme.html#data-dictionary",
    "href": "data/2025/2025-06-17/readme.html#data-dictionary",
    "title": "API Specs",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\napisguru_category\ncharacter\nSorting category of this API on apis.guru. If an API is listed in multiple categories, it has more than one row in this table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\ncontact_name\ncharacter\nThe name of the person or entity responsible for this API.\n\n\ncontact_url\ncharacter\nThe url to check for information about this API.\n\n\ndescription\ncharacter\nA brief description of this API.\n\n\ntitle\ncharacter\nThe title of this API..\n\n\nprovider_name\ncharacter\nThe provider of this API. This is more meaningful if a provider has multiple APIs in the apis.guru database.\n\n\nservice_name\ncharacter\nThe service that this API covers within the provider. Pressent when a provider has multiple APIs in the apis.guru database.\n\n\nlicense_name\ncharacter\nThe name of the license associated with this API, if available.\n\n\nlicense_url\ncharacter\nThe URL of the license, if available.\n\n\nterms_of_service\ncharacter\nThe url of terms of service for this API, if available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\nbackground_color\ncharacter\nHex code (in the format #RRGGBB) intended to show behind the logo. Missing values either mean that no color is expected (the background can be transparent), or that there isn’t a valid logo available for this API.\n\n\nurl\ncharacter\nPath to the logo on apis.guru.\n\n\nalt_text\ncharacter\nText to provide for this logo visually impaired users.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\nformat\ncharacter\nThe format of the original API spec, if available. One of “apiBlueprint”, “google”, “openapi”, “postman”, “swagger”, or “wadl”.\n\n\nurl\ncharacter\nThe path to the original API spec. Note: Some of these paths are no longer valid.\n\n\nversion\ncharacter\nThe version of the format used by this API spec. Each format has its own list of possible values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\napis.guru’s designation for this API.\n\n\nversion\ncharacter\nVersion of the API. Data is filtered to only the “preferrred” versions.\n\n\nadded\ndatetime\nWhen the API was added to apis.guru.\n\n\nupdated\ndatetime\nWhen the API was updated on apis.guru.\n\n\nswagger_url\ncharacter\nThe path to this API spec on apis.guru.\n\n\nopenapi_ver\ncharacter\nThe version of the OpenAPI (or swagger) spec of this API on apis.guru.\n\n\nlink\ncharacter\nThe path to this information (plus some fields we don’t include here) for this API on apis.guru.\n\n\nexternal_docs_description\ncharacter\nA short description of external documentation, if provided.\n\n\nexternal_docs_url\ncharacter\nThe location of the external documentation, if provided."
  },
  {
    "objectID": "data/2025/2025-06-17/readme.html#cleaning-script",
    "href": "data/2025/2025-06-17/readme.html#cleaning-script",
    "title": "API Specs",
    "section": "",
    "text": "# This dataset was compiled using funcionality from my in-progress ecosystem of\n# packages described at https://beekeeper.api2r.org/. On Thursday, 2025-06-19, I\n# will present a talk on this ecosystem at the Ghana R Conference 2025\n# (https://ghana-rusers.org/ghana-r-conference-2025/).\n\n# I use information about APIs from https://apis.guru to test various aspects of\n# {beekeeper} and related packages. Here we use {nectar}\n# (https://nectar.api2r.org/) and {tibblify}\n# (https://mgirlich.github.io/tibblify/) to download and process the list of\n# APIs from apis.guru.\n\n# {nectar} is not available on CRAN. The rest of the packages can be installed\n# via install.packages.\n# install.packages(\"pak\")\n# pak::pak(\"jonthegeek/nectar\")\n\nlibrary(nectar)\nlibrary(tibblify)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\n\n# This section replicates functionality from the in-progress package {apisguru}\n# (https://jonthegeek.github.io/apisguru/). At the time that this dataset was\n# compiled, {apisguru} was not yet updated for the current version of {nectar},\n# so I'm not using it directly. Watch its progress as the {beekeeper} ecosystem\n# solidifies!\n\n.schema_api_spec &lt;- function() {\n  tibblify::tspec_df(\n    .tib_datetime(\"added\"),\n    tibblify::tib_chr(\"preferred\"),\n    tibblify::tib_df(\n      \"versions\",\n      .names_to = \"version\",\n      .schema_api_version_spec()\n    )\n  )\n}\n\n.schema_api_version_spec &lt;- function() {\n  tibblify::tspec_df(\n    .tib_datetime(\"added\"),\n    tibblify::tib_variant(\"info\"),\n    .tib_datetime(\"updated\"),\n    tibblify::tib_chr(\"swaggerUrl\"),\n    tibblify::tib_chr(\"swaggerYamlUrl\"),\n    tibblify::tib_chr(\"openapiVer\"),\n    tibblify::tib_chr(\"link\", required = FALSE),\n    tibblify::tib_variant(\"externalDocs\", required = FALSE)\n  )\n}\n\n.tib_datetime &lt;- function(key, ..., required = TRUE) {\n  tibblify::tib_scalar(\n    key = key,\n    ptype = vctrs::new_datetime(tzone = \"UTC\"),\n    required = required,\n    ptype_inner = character(),\n    transform = .quick_datetime,\n    ...\n  )\n}\n\n.quick_datetime &lt;- function(x, tzone = \"UTC\") {\n  as.POSIXct(gsub(\"T\", \" \", x), tz = tzone)\n}\n\nreq &lt;- nectar::req_prepare(\n  \"https://api.apis.guru/v2\",\n  path = \"/list.json\",\n  tidy_fn = nectar::resp_tidy_json,\n  tidy_args = list(\n    spec = tibblify::tspec_df(\n      .names_to = \"name\",\n      .schema_api_spec()\n    )\n  )\n)\nresp &lt;- nectar::req_perform_opinionated(req, max_reqs = Inf)\napisguru_apis &lt;- nectar::resp_tidy(resp) |&gt;\n  dplyr::select(\"name\", \"preferred\", \"versions\") |&gt;\n  tidyr::unnest(\"versions\") |&gt;\n  dplyr::filter(\n    .data$preferred == .data$version\n  ) |&gt;\n  tidyr::unnest_wider(\"externalDocs\", names_sep = \"_\") |&gt;\n  dplyr::select(-\"preferred\", -\"externalDocs_x-sha1\", -\"swaggerYamlUrl\") |&gt;\n  janitor::clean_names()\n\ndplyr::glimpse(apisguru_apis)\n\napi_info &lt;- apisguru_apis |&gt;\n  dplyr::select(\"name\", \"info\") |&gt;\n  tidyr::unnest_wider(\"info\") |&gt;\n  tidyr::unnest_wider(\"contact\", names_sep = \"_\") |&gt;\n  tidyr::unnest_wider(\"license\", names_sep = \"_\") |&gt;\n  dplyr::select(\n    \"name\",\n    \"contact_name\",\n    \"contact_url\",\n    \"description\",\n    \"title\",\n    apisguru_category = \"x-apisguru-categories\",\n    logo = \"x-logo\",\n    origin = \"x-origin\",\n    provider_name = \"x-providerName\",\n    service_name = \"x-serviceName\",\n    \"license_name\",\n    \"license_url\",\n    terms_of_service = \"termsOfService\"\n  )\napisguru_apis$info &lt;- NULL\ndplyr::glimpse(api_info)\n\napi_categories &lt;- api_info |&gt;\n  dplyr::select(\"name\", \"apisguru_category\") |&gt;\n  tidyr::unnest_longer(\"apisguru_category\")\napi_info$apisguru_category &lt;- NULL\n\napi_logos &lt;- api_info |&gt;\n  dplyr::select(\"name\", \"logo\") |&gt;\n  tidyr::unnest_wider(\"logo\") |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::select(-\"href\")\napi_info$logo &lt;- NULL\n\napi_origins &lt;- api_info |&gt;\n  dplyr::select(\"name\", \"origin\") |&gt;\n  tidyr::unnest_longer(\"origin\") |&gt;\n  tidyr::unnest_wider(\"origin\") |&gt;\n  dplyr::select(\"name\":\"version\") |&gt;\n  # Some of the entries are duplicated.\n  dplyr::distinct()\napi_info$origin &lt;- NULL"
  },
  {
    "objectID": "data/2025/2025-06-03/readme.html",
    "href": "data/2025/2025-06-03/readme.html",
    "title": "Project Gutenberg",
    "section": "",
    "text": "This week we’re exploring books from Project Gutenberg and the {gutenbergr} R package!\n\n[{gutenbergr} allows you to] Download and process public domain works in the Project Gutenberg collection https://www.gutenberg.org/. Includes metadata for all Project Gutenberg works, so that they can be searched and retrieved.\n\n\nHow many different languages are available in the Project Gutenberg collection? How many books are available in each language?\nDo any authors appear under more than one gutenberg_author_id?\nHow might the {gutenbergr} package authors further refine the data for greater ease-of-use?\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-06-03')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 22)\n\ngutenberg_authors &lt;- tuesdata$gutenberg_authors\ngutenberg_languages &lt;- tuesdata$gutenberg_languages\ngutenberg_metadata &lt;- tuesdata$gutenberg_metadata\ngutenberg_subjects &lt;- tuesdata$gutenberg_subjects\n\n# Option 2: Read directly from GitHub\n\ngutenberg_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-06-03')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ngutenberg_authors = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-06-03')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ngutenberg_authors = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv\")\ngutenberg_languages = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv\")\ngutenberg_metadata = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv\")\ngutenberg_subjects = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ngutenberg_authors = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv\", DataFrame)\ngutenberg_languages = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv\", DataFrame)\ngutenberg_metadata = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv\", DataFrame)\ngutenberg_subjects = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_author_id\ninteger\nUnique identifier for the author that can be used to join with the gutenberg_metadata dataset.\n\n\nauthor\ncharacter\nThe agent_name field from the original metadata.\n\n\nalias\ncharacter\nAlias.\n\n\nbirthdate\ninteger\nYear of birth.\n\n\ndeathdate\ninteger\nYear of death.\n\n\nwikipedia\ncharacter\nLink to Wikipedia article on the author. If there are multiple, they are “|”-delimited.\n\n\naliases\ncharacter\nCharacter vector of aliases. If there are multiple, they are “/”-delimited.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nUnique identifier for the work that can be used to join with the gutenberg_metadata dataset.\n\n\nlanguage\nfactor\nLanguage ISO 639 code. Two letter code if one exists, otherwise three letter.\n\n\ntotal_languages\ninteger\nNumber of languages for this work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nNumeric ID, used to retrieve works from Project Gutenberg.\n\n\ntitle\ncharacter\nTitle.\n\n\nauthor\ncharacter\nAuthor, if a single one given. Given as last name first (e.g. “Doyle, Arthur Conan”).\n\n\ngutenberg_author_id\ninteger\nProject Gutenberg author ID.\n\n\nlanguage\nfactor\nLanguage ISO 639 code, separated by / if multiple. Two letter code if one exists, otherwise three letter. See https://en.wikipedia.org/wiki/List_of_ISO_639-2_codes.\n\n\ngutenberg_bookshelf\ncharacter\nWhich collection or collections this is found in, separated by / if multiple.\n\n\nrights\nfactor\nGenerally one of three options: “Public domain in the USA.” (the most common by far), “Copyrighted. Read the copyright notice inside this book for details.”, or “None”.\n\n\nhas_text\nlogical\nWhether there is a file containing digits followed by .txt in Project Gutenberg for this record (as opposed to, for example, audiobooks).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nID describing a work that can be joined with gutenberg_metadata.\n\n\nsubject_type\nfactor\nEither “lcc” (Library of Congress Classification) or “lcsh” (Library of Congress Subject Headings).\n\n\nsubject\ncharacter\nSubject.\n\n\n\n\n\n\n\n# Mostly clean data provided by the {gutenbergr} package.\n# install.packages(\"gutenbergr\")\nlibrary(gutenbergr)\nlibrary(dplyr)\ngutenberg_metadata &lt;- gutenbergr::gutenberg_metadata\ngutenberg_authors &lt;- gutenbergr::gutenberg_authors\ngutenberg_languages &lt;- gutenbergr::gutenberg_languages |&gt;\n  # Fix a typo in the current CRAN version of the package.\n  dplyr::mutate(language = as.factor(language))\ngutenberg_languages$lanuage &lt;- NULL\ngutenberg_subjects &lt;- gutenbergr::gutenberg_subjects"
  },
  {
    "objectID": "data/2025/2025-06-03/readme.html#the-data",
    "href": "data/2025/2025-06-03/readme.html#the-data",
    "title": "Project Gutenberg",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-06-03')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 22)\n\ngutenberg_authors &lt;- tuesdata$gutenberg_authors\ngutenberg_languages &lt;- tuesdata$gutenberg_languages\ngutenberg_metadata &lt;- tuesdata$gutenberg_metadata\ngutenberg_subjects &lt;- tuesdata$gutenberg_subjects\n\n# Option 2: Read directly from GitHub\n\ngutenberg_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-06-03')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ngutenberg_authors = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv')\ngutenberg_languages = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv')\ngutenberg_metadata = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv')\ngutenberg_subjects = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-06-03')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ngutenberg_authors = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv\")\ngutenberg_languages = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv\")\ngutenberg_metadata = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv\")\ngutenberg_subjects = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ngutenberg_authors = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_authors.csv\", DataFrame)\ngutenberg_languages = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_languages.csv\", DataFrame)\ngutenberg_metadata = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_metadata.csv\", DataFrame)\ngutenberg_subjects = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-03/gutenberg_subjects.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-06-03/readme.html#how-to-participate",
    "href": "data/2025/2025-06-03/readme.html#how-to-participate",
    "title": "Project Gutenberg",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-06-03/readme.html#data-dictionary",
    "href": "data/2025/2025-06-03/readme.html#data-dictionary",
    "title": "Project Gutenberg",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ngutenberg_author_id\ninteger\nUnique identifier for the author that can be used to join with the gutenberg_metadata dataset.\n\n\nauthor\ncharacter\nThe agent_name field from the original metadata.\n\n\nalias\ncharacter\nAlias.\n\n\nbirthdate\ninteger\nYear of birth.\n\n\ndeathdate\ninteger\nYear of death.\n\n\nwikipedia\ncharacter\nLink to Wikipedia article on the author. If there are multiple, they are “|”-delimited.\n\n\naliases\ncharacter\nCharacter vector of aliases. If there are multiple, they are “/”-delimited.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nUnique identifier for the work that can be used to join with the gutenberg_metadata dataset.\n\n\nlanguage\nfactor\nLanguage ISO 639 code. Two letter code if one exists, otherwise three letter.\n\n\ntotal_languages\ninteger\nNumber of languages for this work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nNumeric ID, used to retrieve works from Project Gutenberg.\n\n\ntitle\ncharacter\nTitle.\n\n\nauthor\ncharacter\nAuthor, if a single one given. Given as last name first (e.g. “Doyle, Arthur Conan”).\n\n\ngutenberg_author_id\ninteger\nProject Gutenberg author ID.\n\n\nlanguage\nfactor\nLanguage ISO 639 code, separated by / if multiple. Two letter code if one exists, otherwise three letter. See https://en.wikipedia.org/wiki/List_of_ISO_639-2_codes.\n\n\ngutenberg_bookshelf\ncharacter\nWhich collection or collections this is found in, separated by / if multiple.\n\n\nrights\nfactor\nGenerally one of three options: “Public domain in the USA.” (the most common by far), “Copyrighted. Read the copyright notice inside this book for details.”, or “None”.\n\n\nhas_text\nlogical\nWhether there is a file containing digits followed by .txt in Project Gutenberg for this record (as opposed to, for example, audiobooks).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nID describing a work that can be joined with gutenberg_metadata.\n\n\nsubject_type\nfactor\nEither “lcc” (Library of Congress Classification) or “lcsh” (Library of Congress Subject Headings).\n\n\nsubject\ncharacter\nSubject."
  },
  {
    "objectID": "data/2025/2025-06-03/readme.html#cleaning-script",
    "href": "data/2025/2025-06-03/readme.html#cleaning-script",
    "title": "Project Gutenberg",
    "section": "",
    "text": "# Mostly clean data provided by the {gutenbergr} package.\n# install.packages(\"gutenbergr\")\nlibrary(gutenbergr)\nlibrary(dplyr)\ngutenberg_metadata &lt;- gutenbergr::gutenberg_metadata\ngutenberg_authors &lt;- gutenbergr::gutenberg_authors\ngutenberg_languages &lt;- gutenbergr::gutenberg_languages |&gt;\n  # Fix a typo in the current CRAN version of the package.\n  dplyr::mutate(language = as.factor(language))\ngutenberg_languages$lanuage &lt;- NULL\ngutenberg_subjects &lt;- gutenbergr::gutenberg_subjects"
  },
  {
    "objectID": "data/2025/2025-05-20/readme.html",
    "href": "data/2025/2025-05-20/readme.html",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "This week we’re exploring the water quality of Sydney’s iconic beaches. The data is available at the New South Wales State Government Beachwatch website.\n\nBeachwatch and our partners monitor water quality at swim sites to ensure that recreational water environments are managed as safely as possible so that as many people as possible can benefit from using the water.\n\nSydney beaches were in the news this summer with high rainfall causing concerns about the safety of the water.\nThe dataset this week includes both water quality and historical weather data from 1991 until 2025.\n\nHas the water quality declined over this period?\nHow does rainfall impact E-coli bacteria levels?\nAre some swimming sites particularly prone to high bacteria levels following rain?\n\nThank you to Jen Richmond (R-Ladies Sydney) for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-05-20')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 20)\n\nwater_quality &lt;- tuesdata$water_quality\nweather &lt;- tuesdata$weather\n\n# Option 2: Read directly from GitHub\n\nwater_quality &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nweather &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/weather.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-05-20')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nwater_quality = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nweather = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/weather.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-05-20')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nwater_quality = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv\")\nweather = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/weather.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nwater_quality = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv\", DataFrame)\nweather = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/weather.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nregion\ncharacter\nArea of Sydney City\n\n\ncouncil\ncharacter\nCity council responsible for water quality\n\n\nswim_site\ncharacter\nName of beach/swimming location\n\n\ndate\ndate\nDate\n\n\ntime\ntime\nTime of day\n\n\nenterococci_cfu_100ml\ninteger\nEnterococci bacteria levels in colony forming units (CFU) per 100 millilitres of water\n\n\nwater_temperature_c\ninteger\nWater temperature in degrees Celsius\n\n\nconductivity_ms_cm\ninteger\nConductivity in microsiemens per centimetre\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate\n\n\nmax_temp_C\ndouble\nMaximum temperature in degrees Celsius\n\n\nmin_temp_C\ndouble\nMinimum temperature in degrees Celsius\n\n\nprecipitation_mm\ndouble\nRainfall in millimetres\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\n# Historical weather data for Sydney provided by https://open-meteo.com/ API. \n\nweather &lt;- readr::read_csv(here::here(\"data_raw\", \"open-meteo-33.85S151.20E51m.csv\")) |&gt;\n  dplyr::select(date = latitude, \n         max_temp_C = longitude, \n         min_temp_C  = elevation, \n         precipitation_mm = utc_offset_seconds) |&gt;\n  dplyr::slice(-(1:2)) |&gt;\n  dplyr::mutate(date = ymd(date)) |&gt;\n  dplyr::mutate(latitude = -33.848858, \n         longitude = 151.19551) \n  \n# Water quality data for Sydney beaches provided by https://www.beachwatch.nsw.gov.au/waterMonitoring/waterQualityData\n\nwater_quality &lt;- readr::read_csv(here::here(\"data_raw\", \"Water quality-1746064496936.csv\")) |&gt;\n  janitor::clean_names() |&gt;\n  rename(enterococci_cfu_100ml = enterococci_cfu_100m_l, conductivity_ms_cm = conductivity_m_s_cm) |&gt;\n  dplyr::mutate(date = dmy(date)) |&gt;\n  dplyr::mutate(\n    dplyr::across(\n      c(\"enterococci_cfu_100ml\", \"water_temperature_c\", \"conductivity_ms_cm\"),\n      as.integer\n    )\n  )"
  },
  {
    "objectID": "data/2025/2025-05-20/readme.html#the-data",
    "href": "data/2025/2025-05-20/readme.html#the-data",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-05-20')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 20)\n\nwater_quality &lt;- tuesdata$water_quality\nweather &lt;- tuesdata$weather\n\n# Option 2: Read directly from GitHub\n\nwater_quality &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nweather &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/weather.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-05-20')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nwater_quality = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv')\nweather = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/weather.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-05-20')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nwater_quality = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv\")\nweather = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/weather.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nwater_quality = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/water_quality.csv\", DataFrame)\nweather = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-20/weather.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-05-20/readme.html#how-to-participate",
    "href": "data/2025/2025-05-20/readme.html#how-to-participate",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-05-20/readme.html#data-dictionary",
    "href": "data/2025/2025-05-20/readme.html#data-dictionary",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nregion\ncharacter\nArea of Sydney City\n\n\ncouncil\ncharacter\nCity council responsible for water quality\n\n\nswim_site\ncharacter\nName of beach/swimming location\n\n\ndate\ndate\nDate\n\n\ntime\ntime\nTime of day\n\n\nenterococci_cfu_100ml\ninteger\nEnterococci bacteria levels in colony forming units (CFU) per 100 millilitres of water\n\n\nwater_temperature_c\ninteger\nWater temperature in degrees Celsius\n\n\nconductivity_ms_cm\ninteger\nConductivity in microsiemens per centimetre\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate\n\n\nmax_temp_C\ndouble\nMaximum temperature in degrees Celsius\n\n\nmin_temp_C\ndouble\nMinimum temperature in degrees Celsius\n\n\nprecipitation_mm\ndouble\nRainfall in millimetres\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude"
  },
  {
    "objectID": "data/2025/2025-05-20/readme.html#cleaning-script",
    "href": "data/2025/2025-05-20/readme.html#cleaning-script",
    "title": "Water Quality at Sydney Beaches",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\n# Historical weather data for Sydney provided by https://open-meteo.com/ API. \n\nweather &lt;- readr::read_csv(here::here(\"data_raw\", \"open-meteo-33.85S151.20E51m.csv\")) |&gt;\n  dplyr::select(date = latitude, \n         max_temp_C = longitude, \n         min_temp_C  = elevation, \n         precipitation_mm = utc_offset_seconds) |&gt;\n  dplyr::slice(-(1:2)) |&gt;\n  dplyr::mutate(date = ymd(date)) |&gt;\n  dplyr::mutate(latitude = -33.848858, \n         longitude = 151.19551) \n  \n# Water quality data for Sydney beaches provided by https://www.beachwatch.nsw.gov.au/waterMonitoring/waterQualityData\n\nwater_quality &lt;- readr::read_csv(here::here(\"data_raw\", \"Water quality-1746064496936.csv\")) |&gt;\n  janitor::clean_names() |&gt;\n  rename(enterococci_cfu_100ml = enterococci_cfu_100m_l, conductivity_ms_cm = conductivity_m_s_cm) |&gt;\n  dplyr::mutate(date = dmy(date)) |&gt;\n  dplyr::mutate(\n    dplyr::across(\n      c(\"enterococci_cfu_100ml\", \"water_temperature_c\", \"conductivity_ms_cm\"),\n      as.integer\n    )\n  )"
  },
  {
    "objectID": "data/2025/2025-05-06/readme.html",
    "href": "data/2025/2025-05-06/readme.html",
    "title": "National Science Foundation Grant Terminations under the Trump Administration",
    "section": "",
    "text": "This week we’re exploring a dataset of grants for scientific research and education projects from the U.S. National Science Foundation (NSF) that have been terminated by the Trump administration in 2025. In an unprecedented and possibly illegal action, the NSF has terminated over 1,000 such grants starting on April 18, 2025, and terminations continue. These data were collected by Grant Watch by crowdsourcing from researchers and program administrators, as the administration has not released information on these terminations.\nFrom a New York Times article on the terminations:\n\nIn general, the agency provides scientists with the opportunity to dispute its decisions about funding. But researchers were informed that the decision to cancel their grants was final and not subject to appeal.\nScientists expressed fear about the growing disruptions to research and the harm it may do to both academia and the public at large.\n“It’s shocking to see the government do this,”” said Jon Freeman, a psychologist at Columbia University whose grant on studying facial perception was terminated. “It cedes American leadership in science and technology to China and to other countries. I think it is going to take at least 10 years for American scientific and biomedical research to recover from this.”\n\nMore information, as well as similar data on grant terminations from the National Institutes of Health (NIH), can be found at https://grant-watch.us.\nSome questions you might explore are:\n\nHow many grants, and how much money, were terminated by state or congressional district? What institutions? How can you present these on a map?\nGrants from what directorates, divisions, or programs made up most of the projects terminated?\nWhat topics or terms are most common in project titles or abstracts?\n\nMore elaborate analysis could use data on total awards to look at the fraction of awards terminated, or data on educational institutions to look at what kinds of institutions are most affected.\nCheck out the cleaning script below for instructions on fetching the latest version of the data!\nThank you to Noam Ross and Scott Delaney, Grant Watch for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-05-06')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 18)\n\nnsf_terminations &lt;- tuesdata$nsf_terminations\n\n# Option 2: Read directly from GitHub\n\nnsf_terminations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-05-06')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nnsf_terminations = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-05-06')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nnsf_terminations = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nnsf_terminations = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngrant_number\ncharacter\nThe numeric ID of the grant.\n\n\nproject_title\ncharacter\nThe title of the project the grant funds.\n\n\ntermination_letter_date\ndate\nThe date a termination letter was received by the organization.\n\n\norg_name\ncharacter\nThe name of the organization or institution funded to do the project.\n\n\norg_city\ncharacter\nThe name of the organization’s city.\n\n\norg_state\ncharacter\nThe organization’s two-letter state abbreviation.\n\n\norg_district\ncharacter\nThe congressional district (state and number) where the organization is located.\n\n\nusaspending_obligated\ndouble\nThe amount of money, via USAspending.gov, that NSF had committed to funding.\n\n\naward_type\ncharacter\nThe type of grant.\n\n\ndirectorate_abbrev\ncharacter\nThe three-letter abbreviation of the NSF directorate name.\n\n\ndirectorate\ncharacter\nThe NSF directorate (the highest level of organization), which administered the grant.\n\n\ndivision\ncharacter\nThe NSF division (housed within directorate) which administered the grant.\n\n\nnsf_program_name\ncharacter\nThe name of the funding program under which the grant was made.\n\n\nnsf_url\ncharacter\nThe URL pointing to the award information in the public NSF award database.\n\n\nusaspending_url\ncharacter\nThe URL pointing to budget and spending information at the public USAspending.gov website.\n\n\nnsf_startdate\ndate\nThe start date of the project.\n\n\nnsf_expected_end_date\ndate\nThe date the project was expected to end.\n\n\norg_zip\ncharacter\nThe 5- or 9-digit ZIP code of the organization receiving the grant.\n\n\norg_uei\ncharacter\nThe unique entitity identifier (UEI) of the organization recieving the grant, used across U.S. government databases.\n\n\nabstract\ncharacter\nThe text of the project abstract, describing the work to be done.\n\n\nin_cruz_list\nlogical\nWhether the project was in a list of NSF projects named by U.S. Senator Ted Cruz that he claimed “promoted Diversity, Equity, and Inclusion (DEI) or advanced neo-Marxist class warfare propaganda.”\n\n\n\n\n\n\n\n# Fetch data from the CSV download link at https://grant-watch.us/nsf-data.html\nraw_nsf_terminations &lt;- readr::read_csv(\"https://drive.usercontent.google.com/download?id=1TFoyowiiMFZm73iU4YORniydEeHhrsVz&export=download\")\n\n# Clean the data\nnsf_terminations &lt;- raw_nsf_terminations |&gt; \n  janitor::clean_names() |&gt; \n  mutate(usaspending_obligated = stringi::stri_replace_first_fixed(usaspending_obligated, \"$\", \"\") |&gt; \n           readr::parse_number()) |&gt; \n  mutate(in_cruz_list = !is.na(in_cruz_list)) |&gt; \n  mutate(grant_number = as.character(grant_number))"
  },
  {
    "objectID": "data/2025/2025-05-06/readme.html#the-data",
    "href": "data/2025/2025-05-06/readme.html#the-data",
    "title": "National Science Foundation Grant Terminations under the Trump Administration",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-05-06')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 18)\n\nnsf_terminations &lt;- tuesdata$nsf_terminations\n\n# Option 2: Read directly from GitHub\n\nnsf_terminations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-05-06')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nnsf_terminations = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-05-06')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nnsf_terminations = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nnsf_terminations = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-06/nsf_terminations.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-05-06/readme.html#how-to-participate",
    "href": "data/2025/2025-05-06/readme.html#how-to-participate",
    "title": "National Science Foundation Grant Terminations under the Trump Administration",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-05-06/readme.html#data-dictionary",
    "href": "data/2025/2025-05-06/readme.html#data-dictionary",
    "title": "National Science Foundation Grant Terminations under the Trump Administration",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ngrant_number\ncharacter\nThe numeric ID of the grant.\n\n\nproject_title\ncharacter\nThe title of the project the grant funds.\n\n\ntermination_letter_date\ndate\nThe date a termination letter was received by the organization.\n\n\norg_name\ncharacter\nThe name of the organization or institution funded to do the project.\n\n\norg_city\ncharacter\nThe name of the organization’s city.\n\n\norg_state\ncharacter\nThe organization’s two-letter state abbreviation.\n\n\norg_district\ncharacter\nThe congressional district (state and number) where the organization is located.\n\n\nusaspending_obligated\ndouble\nThe amount of money, via USAspending.gov, that NSF had committed to funding.\n\n\naward_type\ncharacter\nThe type of grant.\n\n\ndirectorate_abbrev\ncharacter\nThe three-letter abbreviation of the NSF directorate name.\n\n\ndirectorate\ncharacter\nThe NSF directorate (the highest level of organization), which administered the grant.\n\n\ndivision\ncharacter\nThe NSF division (housed within directorate) which administered the grant.\n\n\nnsf_program_name\ncharacter\nThe name of the funding program under which the grant was made.\n\n\nnsf_url\ncharacter\nThe URL pointing to the award information in the public NSF award database.\n\n\nusaspending_url\ncharacter\nThe URL pointing to budget and spending information at the public USAspending.gov website.\n\n\nnsf_startdate\ndate\nThe start date of the project.\n\n\nnsf_expected_end_date\ndate\nThe date the project was expected to end.\n\n\norg_zip\ncharacter\nThe 5- or 9-digit ZIP code of the organization receiving the grant.\n\n\norg_uei\ncharacter\nThe unique entitity identifier (UEI) of the organization recieving the grant, used across U.S. government databases.\n\n\nabstract\ncharacter\nThe text of the project abstract, describing the work to be done.\n\n\nin_cruz_list\nlogical\nWhether the project was in a list of NSF projects named by U.S. Senator Ted Cruz that he claimed “promoted Diversity, Equity, and Inclusion (DEI) or advanced neo-Marxist class warfare propaganda.”"
  },
  {
    "objectID": "data/2025/2025-05-06/readme.html#cleaning-script",
    "href": "data/2025/2025-05-06/readme.html#cleaning-script",
    "title": "National Science Foundation Grant Terminations under the Trump Administration",
    "section": "",
    "text": "# Fetch data from the CSV download link at https://grant-watch.us/nsf-data.html\nraw_nsf_terminations &lt;- readr::read_csv(\"https://drive.usercontent.google.com/download?id=1TFoyowiiMFZm73iU4YORniydEeHhrsVz&export=download\")\n\n# Clean the data\nnsf_terminations &lt;- raw_nsf_terminations |&gt; \n  janitor::clean_names() |&gt; \n  mutate(usaspending_obligated = stringi::stri_replace_first_fixed(usaspending_obligated, \"$\", \"\") |&gt; \n           readr::parse_number()) |&gt; \n  mutate(in_cruz_list = !is.na(in_cruz_list)) |&gt; \n  mutate(grant_number = as.character(grant_number))"
  },
  {
    "objectID": "data/2025/2025-04-22/readme.html",
    "href": "data/2025/2025-04-22/readme.html",
    "title": "Fatal Car Crashes on 4/20",
    "section": "",
    "text": "Today we’re exploring the (lack of) connection between fatal car crashes in the United States and the “4/20 holiday” (4:20pm to 11:59pm on April 20th). Thank you to @Rmadillo for submitting this dataset 6 years ago!, including a bonus script to generate sample plots!\n\n[In January 2019], Harper and Palayew[1] published a study looking at whether a signal could be detected in fatal car crashes in the United States based on the “4/20” holiday, based on a previous study by Staples and Redelmeier[2] that suggested a strong link. Using more robust methods and a more comprehensive time window, Harper and Palayew could not find a signal for 4/20, but could for other holidays, such as July 4.\n\n\nThis is a great example of how charts can mislead based on choices in analysis and plotting.\n\n[1]. Harper S, Palayew A “The annual cannabis holiday and fatal traffic crashes.” BMJ Injury Prevention. Published Online First: 29 January 2019. doi: 10.1136/injuryprev-2018-043068. Manuscript and original data/code at https://osf.io/qnrg6/\n[2]. Staples JA, Redelmeier DA. “The April 20 cannabis celebration and fatal traffic crashes in the United States.” JAMA Intern Med. 2018 Feb; 178(4):569–72.\nQuestions:\n\nCan you detect any correlations between fatal car crashes and particular days of the year?\nWhat are the most dangerous days of the year for fatal car crashes in the United States?\nWhat other factors might help analyze the data in more detail? You can use the cleaning script to download the full dataset.\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-22')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 16)\n\ndaily_accidents &lt;- tuesdata$daily_accidents\ndaily_accidents_420 &lt;- tuesdata$daily_accidents_420\n\n# Option 2: Read directly from GitHub\n\ndaily_accidents &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-22/daily_accidents.csv')\ndaily_accidents_420 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-22/daily_accidents_420.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-22')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ndaily_accidents = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-22/daily_accidents.csv')\ndaily_accidents_420 = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-22/daily_accidents_420.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of the accident.\n\n\nfatalities_count\ninteger\nTotal count of fatalities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of the accident.\n\n\ne420\nlogical\nDid the accident occur on the 4/20 “holiday” (between 4:20pm and 11:59pm on April 20th)?\n\n\nfatalities_count\ninteger\nTotal count of fatalities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of the accident.\n\n\nd420\nlogical\nDid the accident occur between 4:20pm and 11:59pm on any day of the year)?\n\n\nfatalities_count\ninteger\nTotal count of fatalities.\n\n\n\n\n\n\n\n# Draft cleaning script provided by @Rmadillo at\n# https://github.com/Rmadillo/Harper_and_Palayew/blob/1aaf1333e8fb6a8f3aa7f169a12943c17b3487da/Load_Data_and_Clean.R\n# Script cleaned and modernized by @jonthegeek (the script was submitted as a\n# TidyTuesday issue in 2019!).\n\n#### Load packages -------------------------------------------------------------\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(fs)\n\n#### Acquire raw data ----------------------------------------------------------\n\n# Crash data (from Harper and Palayew)\ndl_path &lt;- withr::local_tempfile(fileext = \".zip\")\ndownload.file(\"https://osf.io/kj7ub/download\", dl_path, mode = \"wb\")\nunzip_path &lt;- withr::local_tempdir()\nunzip(dl_path, exdir = unzip_path)\n\ndta_files &lt;- fs::dir_ls(unzip_path, glob = \"*.dta\")\n\nfars &lt;- purrr::map(dta_files, haven::read_dta) |&gt; \n  purrr::list_rbind(names_to = \"id\") |&gt; \n  dplyr::mutate(id = basename(id))\n\n# Geographic lookup\ngeog &lt;- readr::read_csv(\n  \"https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt\",\n  col_names = c(\n    \"state_name\",\n    \"state_code\",\n    \"county_code\",\n    \"county_name\",\n    \"FIPS_class_code\"\n  )\n) |&gt; \n  dplyr::mutate(\n    state = as.numeric(state_code),\n    count = as.numeric(county_code),\n    FIPS = paste0(state_code, county_code)\n  )\n\n#### Data wrangling ------------------------------------------------------------\n# Used https://osf.io/drbge/ Stata code as a guide for cleaning\n\n# All data\n# This might take a while... go get a coffee\nall_accidents &lt;- fars |&gt; \n  # What are state and county codes/look ups?\n  dplyr::select(\n    \"id\", \"state\", \"county\", \n    \"month\", \"day\", \"hour\", \"minute\", \n    \"st_case\", \"per_no\", \"veh_no\", \"per_typ\", \n    \"age\", \"sex\", \n    \"death_da\", \"death_mo\", \"death_yr\", \"death_hr\", \"death_mn\", \"death_tm\",\n    \"inj_sev\", \"mod_year\", \"lag_hrs\", \"lag_mins\"\n  ) |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      c(\"month\", \"day\", \"hour\", \"minute\"),\n      ~ na_if(.x, 99)\n    ),\n    year = readr::parse_number(id)\n  ) |&gt; \n  dplyr::filter(\n    .data$per_typ == 1,\n    !is.na(.data$year),\n    !is.na(.data$month),\n    !is.na(.data$day)\n  ) |&gt; \n  dplyr::mutate(\n    crashtime = .data$hour * 100 + .data$minute,\n    date = as.Date(paste(.data$year, .data$month, .data$day, sep = \"-\")),\n    time = paste(.data$hour, .data$minute, sep = \":\"),\n    timestamp = as.POSIXct(\n      paste(.data$date, .data$time),\n      format = \"%Y-%m-%d %H:%M\"\n    ),\n    e420 = .data$month == 4 & .data$day == 20 & \n      .data$crashtime &gt;= 1620 & .data$crashtime &lt;= 2359,\n    e420_control = .data$month == 4 & (.data$day == 20 | .data$day == 27) & \n      .data$crashtime &gt;= 1620 & .data$crashtime &lt; 2359,\n    d420 = .data$crashtime &gt;= 1620 & .data$crashtime &lt;= 2359,\n    sex = factor(\n      dplyr::case_when(\n        .data$sex == 2 ~ \"F\",\n        .data$sex == 1 ~ \"M\",\n        .default = NA_character_\n      )\n    ),\n    period = factor(\n      dplyr::case_when(\n        .data$year &lt; 2004  ~ \"Remote (1992-2003)\",\n        .data$year &gt;= 2004 ~ \"Recent (2004-2016)\",\n        .default = NA_character_\n      )\n    ),\n    age_group = factor(\n      dplyr::case_when(\n        .data$age &lt;= 20 ~ \"&lt;20y\",\n        .data$age &lt;= 30 ~ \"21-30y\",\n        .data$age &lt;= 40 ~ \"31-40y\",\n        .data$age &lt;= 50 ~ \"41-50y\",\n        .data$age &lt;= 97 ~ \"51-97y\",\n        .default = NA_character_\n      )\n    )\n  )\n\n# Daily final working data\n# Only use data starting in 1992\ndaily_accidents &lt;- all_accidents |&gt; \n  dplyr::filter(.data$year &gt; 1991) |&gt; \n  summarize(fatalities_count = dplyr::n(), .by = \"date\")\n\n# Daily+Time Group final working data\ndaily_accidents_420 &lt;- all_accidents |&gt; \n  dplyr::filter(.data$year &gt; 1991) |&gt; \n  dplyr::summarize(fatalities_count = dplyr::n(), .by = c(\"date\", \"e420\"))\n  \n# Daily working data for daily 420 period\ndaily_accidents_420_time &lt;- all_accidents |&gt; \n  dplyr::filter(.data$year &gt; 1991) |&gt; \n  dplyr::summarize(fatalities_count = dplyr::n(), .by = c(\"date\", \"d420\"))"
  },
  {
    "objectID": "data/2025/2025-04-22/readme.html#the-data",
    "href": "data/2025/2025-04-22/readme.html#the-data",
    "title": "Fatal Car Crashes on 4/20",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-22')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 16)\n\ndaily_accidents &lt;- tuesdata$daily_accidents\ndaily_accidents_420 &lt;- tuesdata$daily_accidents_420\n\n# Option 2: Read directly from GitHub\n\ndaily_accidents &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-22/daily_accidents.csv')\ndaily_accidents_420 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-22/daily_accidents_420.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-22')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ndaily_accidents = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-22/daily_accidents.csv')\ndaily_accidents_420 = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-22/daily_accidents_420.csv')"
  },
  {
    "objectID": "data/2025/2025-04-22/readme.html#how-to-participate",
    "href": "data/2025/2025-04-22/readme.html#how-to-participate",
    "title": "Fatal Car Crashes on 4/20",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-04-22/readme.html#data-dictionary",
    "href": "data/2025/2025-04-22/readme.html#data-dictionary",
    "title": "Fatal Car Crashes on 4/20",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of the accident.\n\n\nfatalities_count\ninteger\nTotal count of fatalities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of the accident.\n\n\ne420\nlogical\nDid the accident occur on the 4/20 “holiday” (between 4:20pm and 11:59pm on April 20th)?\n\n\nfatalities_count\ninteger\nTotal count of fatalities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of the accident.\n\n\nd420\nlogical\nDid the accident occur between 4:20pm and 11:59pm on any day of the year)?\n\n\nfatalities_count\ninteger\nTotal count of fatalities."
  },
  {
    "objectID": "data/2025/2025-04-22/readme.html#cleaning-script",
    "href": "data/2025/2025-04-22/readme.html#cleaning-script",
    "title": "Fatal Car Crashes on 4/20",
    "section": "",
    "text": "# Draft cleaning script provided by @Rmadillo at\n# https://github.com/Rmadillo/Harper_and_Palayew/blob/1aaf1333e8fb6a8f3aa7f169a12943c17b3487da/Load_Data_and_Clean.R\n# Script cleaned and modernized by @jonthegeek (the script was submitted as a\n# TidyTuesday issue in 2019!).\n\n#### Load packages -------------------------------------------------------------\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(fs)\n\n#### Acquire raw data ----------------------------------------------------------\n\n# Crash data (from Harper and Palayew)\ndl_path &lt;- withr::local_tempfile(fileext = \".zip\")\ndownload.file(\"https://osf.io/kj7ub/download\", dl_path, mode = \"wb\")\nunzip_path &lt;- withr::local_tempdir()\nunzip(dl_path, exdir = unzip_path)\n\ndta_files &lt;- fs::dir_ls(unzip_path, glob = \"*.dta\")\n\nfars &lt;- purrr::map(dta_files, haven::read_dta) |&gt; \n  purrr::list_rbind(names_to = \"id\") |&gt; \n  dplyr::mutate(id = basename(id))\n\n# Geographic lookup\ngeog &lt;- readr::read_csv(\n  \"https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt\",\n  col_names = c(\n    \"state_name\",\n    \"state_code\",\n    \"county_code\",\n    \"county_name\",\n    \"FIPS_class_code\"\n  )\n) |&gt; \n  dplyr::mutate(\n    state = as.numeric(state_code),\n    count = as.numeric(county_code),\n    FIPS = paste0(state_code, county_code)\n  )\n\n#### Data wrangling ------------------------------------------------------------\n# Used https://osf.io/drbge/ Stata code as a guide for cleaning\n\n# All data\n# This might take a while... go get a coffee\nall_accidents &lt;- fars |&gt; \n  # What are state and county codes/look ups?\n  dplyr::select(\n    \"id\", \"state\", \"county\", \n    \"month\", \"day\", \"hour\", \"minute\", \n    \"st_case\", \"per_no\", \"veh_no\", \"per_typ\", \n    \"age\", \"sex\", \n    \"death_da\", \"death_mo\", \"death_yr\", \"death_hr\", \"death_mn\", \"death_tm\",\n    \"inj_sev\", \"mod_year\", \"lag_hrs\", \"lag_mins\"\n  ) |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      c(\"month\", \"day\", \"hour\", \"minute\"),\n      ~ na_if(.x, 99)\n    ),\n    year = readr::parse_number(id)\n  ) |&gt; \n  dplyr::filter(\n    .data$per_typ == 1,\n    !is.na(.data$year),\n    !is.na(.data$month),\n    !is.na(.data$day)\n  ) |&gt; \n  dplyr::mutate(\n    crashtime = .data$hour * 100 + .data$minute,\n    date = as.Date(paste(.data$year, .data$month, .data$day, sep = \"-\")),\n    time = paste(.data$hour, .data$minute, sep = \":\"),\n    timestamp = as.POSIXct(\n      paste(.data$date, .data$time),\n      format = \"%Y-%m-%d %H:%M\"\n    ),\n    e420 = .data$month == 4 & .data$day == 20 & \n      .data$crashtime &gt;= 1620 & .data$crashtime &lt;= 2359,\n    e420_control = .data$month == 4 & (.data$day == 20 | .data$day == 27) & \n      .data$crashtime &gt;= 1620 & .data$crashtime &lt; 2359,\n    d420 = .data$crashtime &gt;= 1620 & .data$crashtime &lt;= 2359,\n    sex = factor(\n      dplyr::case_when(\n        .data$sex == 2 ~ \"F\",\n        .data$sex == 1 ~ \"M\",\n        .default = NA_character_\n      )\n    ),\n    period = factor(\n      dplyr::case_when(\n        .data$year &lt; 2004  ~ \"Remote (1992-2003)\",\n        .data$year &gt;= 2004 ~ \"Recent (2004-2016)\",\n        .default = NA_character_\n      )\n    ),\n    age_group = factor(\n      dplyr::case_when(\n        .data$age &lt;= 20 ~ \"&lt;20y\",\n        .data$age &lt;= 30 ~ \"21-30y\",\n        .data$age &lt;= 40 ~ \"31-40y\",\n        .data$age &lt;= 50 ~ \"41-50y\",\n        .data$age &lt;= 97 ~ \"51-97y\",\n        .default = NA_character_\n      )\n    )\n  )\n\n# Daily final working data\n# Only use data starting in 1992\ndaily_accidents &lt;- all_accidents |&gt; \n  dplyr::filter(.data$year &gt; 1991) |&gt; \n  summarize(fatalities_count = dplyr::n(), .by = \"date\")\n\n# Daily+Time Group final working data\ndaily_accidents_420 &lt;- all_accidents |&gt; \n  dplyr::filter(.data$year &gt; 1991) |&gt; \n  dplyr::summarize(fatalities_count = dplyr::n(), .by = c(\"date\", \"e420\"))\n  \n# Daily working data for daily 420 period\ndaily_accidents_420_time &lt;- all_accidents |&gt; \n  dplyr::filter(.data$year &gt; 1991) |&gt; \n  dplyr::summarize(fatalities_count = dplyr::n(), .by = c(\"date\", \"d420\"))"
  },
  {
    "objectID": "data/2025/2025-04-08/readme.html",
    "href": "data/2025/2025-04-08/readme.html",
    "title": "Timely and Effective Care by US State",
    "section": "",
    "text": "This week we’re exploring state-level results for medicare.gov “timely and effective care” measurements. As of 2025-04-06, the data is available at the Centers for Medicare and Medicaid Services (CMS) website. Thanks to former TidyTuesday team member Tracy Teal (@tracykteal) for the dataset suggestion and the link to a visualization by Kayla Zhu and Christina Kostandi at the Visual Capitalist.\n\nEmergency room wait times vary significantly across the United States depending on factors such as hospital resources, patient volume, and staffing levels, with some states facing delays that can stretch for more than three hours.\n\n\nIs there a connection between state populations and wait times?\nWhich conditions have the longest wait times? The shortest?\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-08')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 14)\n\ncare_state &lt;- tuesdata$care_state\n\n# Option 2: Read directly from GitHub\n\ncare_state &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-08/care_state.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-08')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncare_state = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-08/care_state.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstate\ncharacter\nThe two-letter code for the state (or territory, etc) where the hospital is located.\n\n\ncondition\ncharacter\nThe condition for which the patient was admitted. Six categories of conditions are included in the data.\n\n\nmeasure_id\ncharacter\nThe ID of the thing being measured. Note that there are 22 unique IDs but only 21 unique names.\n\n\nmeasure_name\ncharacter\nThe name of the thing being measured. Note that there are 22 unique IDs but only 21 unique names.\n\n\nscore\ndouble\nThe score of the measure.\n\n\nfootnote\ncharacter\nFootnotes that apply to this measure: 5 = “Results are not available for this reporting period.”, 25 = “State and national averages include Veterans Health Administration (VHA) hospital data.”, 26 = “State and national averages include Department of Defense (DoD) hospital data.”.\n\n\nstart_date\ndate\nThe date on which measurement began for this measure.\n\n\nend_date\ndate\nThe date on which measurement ended for this measure.\n\n\n\n\n\n\n\n# Data downloaded manually from https://data.cms.gov/provider-data/dataset/apyc-v239\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\ncare_state &lt;- readr::read_csv(\n  here::here(\"Timely_and_Effective_Care-State.csv\")\n) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::mutate(\n    score = dplyr::na_if(score, \"Not Available\") |&gt; \n      as.double(),\n    dplyr::across(\n      dplyr::ends_with(\"_date\"),\n      lubridate::mdy\n    )\n  )"
  },
  {
    "objectID": "data/2025/2025-04-08/readme.html#the-data",
    "href": "data/2025/2025-04-08/readme.html#the-data",
    "title": "Timely and Effective Care by US State",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-08')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 14)\n\ncare_state &lt;- tuesdata$care_state\n\n# Option 2: Read directly from GitHub\n\ncare_state &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-08/care_state.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-08')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncare_state = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-08/care_state.csv')"
  },
  {
    "objectID": "data/2025/2025-04-08/readme.html#how-to-participate",
    "href": "data/2025/2025-04-08/readme.html#how-to-participate",
    "title": "Timely and Effective Care by US State",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-04-08/readme.html#data-dictionary",
    "href": "data/2025/2025-04-08/readme.html#data-dictionary",
    "title": "Timely and Effective Care by US State",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nstate\ncharacter\nThe two-letter code for the state (or territory, etc) where the hospital is located.\n\n\ncondition\ncharacter\nThe condition for which the patient was admitted. Six categories of conditions are included in the data.\n\n\nmeasure_id\ncharacter\nThe ID of the thing being measured. Note that there are 22 unique IDs but only 21 unique names.\n\n\nmeasure_name\ncharacter\nThe name of the thing being measured. Note that there are 22 unique IDs but only 21 unique names.\n\n\nscore\ndouble\nThe score of the measure.\n\n\nfootnote\ncharacter\nFootnotes that apply to this measure: 5 = “Results are not available for this reporting period.”, 25 = “State and national averages include Veterans Health Administration (VHA) hospital data.”, 26 = “State and national averages include Department of Defense (DoD) hospital data.”.\n\n\nstart_date\ndate\nThe date on which measurement began for this measure.\n\n\nend_date\ndate\nThe date on which measurement ended for this measure."
  },
  {
    "objectID": "data/2025/2025-04-08/readme.html#cleaning-script",
    "href": "data/2025/2025-04-08/readme.html#cleaning-script",
    "title": "Timely and Effective Care by US State",
    "section": "",
    "text": "# Data downloaded manually from https://data.cms.gov/provider-data/dataset/apyc-v239\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\ncare_state &lt;- readr::read_csv(\n  here::here(\"Timely_and_Effective_Care-State.csv\")\n) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::mutate(\n    score = dplyr::na_if(score, \"Not Available\") |&gt; \n      as.double(),\n    dplyr::across(\n      dplyr::ends_with(\"_date\"),\n      lubridate::mdy\n    )\n  )"
  },
  {
    "objectID": "data/2025/2025-03-25/readme.html",
    "href": "data/2025/2025-03-25/readme.html",
    "title": "Text Data From Amazon’s Annual Reports",
    "section": "",
    "text": "This week we’re exploring text data from Amazon’s annual reports. The PDFs were read into R using the {pdftools} R package, and explored by TidyTuesday participant Gregory Vander Vinne in a post on his website. Note that stop words (e.g., “and”, “the”, “a”) have been removed from the data.\n\nAs a publicly-traded company, Amazon releases an annual report every year (with a December 31st year end). An annual report is essentially a summary of the company’s performance over the past year. It includes details on how well the company did financially, what goals were achieved, and what challenges it faced.\n\n\nHow have the words used change over time?\nAre there meaningful changes in sentiment from year to year?\nWhich words are likely to appear together in the same annual report?\n\nThank you to Gregory Vander Vinne for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-03-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 12)\n\nreport_words_clean &lt;- tuesdata$report_words_clean\n\n# Option 2: Read directly from GitHub\n\nreport_words_clean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-25/report_words_clean.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-03-25')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nreport_words_clean = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-25/report_words_clean.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-03-25/readme.html#the-data",
    "href": "data/2025/2025-03-25/readme.html#the-data",
    "title": "Text Data From Amazon’s Annual Reports",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-03-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 12)\n\nreport_words_clean &lt;- tuesdata$report_words_clean\n\n# Option 2: Read directly from GitHub\n\nreport_words_clean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-25/report_words_clean.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-03-25')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nreport_words_clean = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-25/report_words_clean.csv')"
  },
  {
    "objectID": "data/2025/2025-03-25/readme.html#how-to-participate",
    "href": "data/2025/2025-03-25/readme.html#how-to-participate",
    "title": "Text Data From Amazon’s Annual Reports",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-03-25/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "href": "data/2025/2025-03-25/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "title": "Text Data From Amazon’s Annual Reports",
    "section": "",
    "text": "Exploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-03-11/readme.html",
    "href": "data/2025/2025-03-11/readme.html",
    "title": "Pixar Films",
    "section": "",
    "text": "This week we’re exploring Pixar films! The data this week comes from the {pixarfilms} R package by Eric Leung.\n\nR data package to explore Pixar films, the people, and reception data. This package contains six data sets provided mostly in part by Wikipedia.\n\n\nWhy are some values missing in the datasets?\nWhich films have the highest score in each rating system? Are there distinct differences in ratings?\nDownload the box_office dataset from the {pixarfilms} package. How does the box_office_us_canada value compare to the various ratings? Is the trend different for box_office_worldwide?\n\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-03-11')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 10)\n\npixar_films &lt;- tuesdata$pixar_films\npublic_response &lt;- tuesdata$public_response\n\n# Option 2: Read directly from GitHub\n\npixar_films &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv')\npublic_response &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-03-11')\n\n# Option 2: Read directly from GitHub and assign to an object\n\npixar_films = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv')\npublic_response = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-03-11/readme.html#the-data",
    "href": "data/2025/2025-03-11/readme.html#the-data",
    "title": "Pixar Films",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-03-11')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 10)\n\npixar_films &lt;- tuesdata$pixar_films\npublic_response &lt;- tuesdata$public_response\n\n# Option 2: Read directly from GitHub\n\npixar_films &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv')\npublic_response &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-03-11')\n\n# Option 2: Read directly from GitHub and assign to an object\n\npixar_films = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/pixar_films.csv')\npublic_response = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-11/public_response.csv')"
  },
  {
    "objectID": "data/2025/2025-03-11/readme.html#how-to-participate",
    "href": "data/2025/2025-03-11/readme.html#how-to-participate",
    "title": "Pixar Films",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-03-11/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "href": "data/2025/2025-03-11/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "title": "Pixar Films",
    "section": "",
    "text": "Exploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-02-25/readme.html",
    "href": "data/2025/2025-02-25/readme.html",
    "title": "Academic Literature on Racial and Ethnic Disparities in Reproductive Medicine in the US",
    "section": "",
    "text": "This week we’re exploring data on studies investigating racial and ethnic disparities in reproductive medicine as published in the eight highest impact peer-reviewed Ob/Gyn journals from January 1, 2010 through June 30, 2023. The data were collected as part of a review article Racial and ethnic disparities in reproductive medicine in the United States: a narrative review of contemporary high-quality evidence published in the American Journal of Obstetrics and Gynecology in January 2025.\n\n“There has been increasing debate around how or if race and ethnicity should be used in medical research—including the conceptualization of race as a biological entity, a social construct, or a proxy for racism. The objectives of this narrative review are to identify and synthesize reported racial and ethnic inequalities in obstetrics and gynecology (ob/gyn) and develop informed recommendations for racial and ethnic inequity research in ob/gyn.”\n\nA companion interactive website was published alongside the review article, and the data has also been used by college students to create data art to raise awareness about this research.\nThe data provides an opportunity to critically examine how racial and ethnic disparities in reproductive medicine are framed, measured, and discussed in the academic literature.\n\nExplore how race and ethnicity are categorized in these articles. Which categories are most prominent? Which are missing? How do sample sizes vary across groups?\nHas the sentiment of article titles, abstracts, keywords, and/or aims statements changed over time?\n\nWhat type of health outcomes have been studied? What disparities have been identified?\n\nThank you to Kat Correia from Amherst College for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 8)\n\narticle_dat &lt;- tuesdata$article_dat\nmodel_dat &lt;- tuesdata$model_dat\n\n# Option 2: Read directly from GitHub\n\narticle_dat &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-25/article_dat.csv')\nmodel_dat &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-25/model_dat.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-02-25')\n\n# Option 2: Read directly from GitHub and assign to an object\n\narticle_dat = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-25/article_dat.csv')\nmodel_dat = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-25/model_dat.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-02-25/readme.html#the-data",
    "href": "data/2025/2025-02-25/readme.html#the-data",
    "title": "Academic Literature on Racial and Ethnic Disparities in Reproductive Medicine in the US",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 8)\n\narticle_dat &lt;- tuesdata$article_dat\nmodel_dat &lt;- tuesdata$model_dat\n\n# Option 2: Read directly from GitHub\n\narticle_dat &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-25/article_dat.csv')\nmodel_dat &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-25/model_dat.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-02-25')\n\n# Option 2: Read directly from GitHub and assign to an object\n\narticle_dat = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-25/article_dat.csv')\nmodel_dat = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-25/model_dat.csv')"
  },
  {
    "objectID": "data/2025/2025-02-25/readme.html#how-to-participate",
    "href": "data/2025/2025-02-25/readme.html#how-to-participate",
    "title": "Academic Literature on Racial and Ethnic Disparities in Reproductive Medicine in the US",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-02-25/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "href": "data/2025/2025-02-25/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "title": "Academic Literature on Racial and Ethnic Disparities in Reproductive Medicine in the US",
    "section": "",
    "text": "Exploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-02-11/readme.html",
    "href": "data/2025/2025-02-11/readme.html",
    "title": "CDC Datasets",
    "section": "",
    "text": "This week we’re exploring datasets that the Trump administration has purged.\nThe Trump administration has ordered agencies to purge their websites of any references to topics such as LGBTQ+ rights.\nAn effort is underway to back up this publicly funded data before it is lost. This week’s dataset contains metadata about CDC datasets backed up on archive.org.\n\n“The removal of HIV- and LGBTQ-related resources from the websites of the Centers for Disease Control and Prevention and other health agencies is deeply concerning and creates a dangerous gap in scientific information and data to monitor and respond to disease outbreaks,” the Infectious Disease Society of America said in a statement. “Access to this information is crucial for infectious diseases and HIV health care professionals who care for people with HIV and members of the LGBTQ community and is critical to efforts to end the HIV epidemic.”\n\n\nWhich Bureaus and Programs have the most datasets archived in this collection?\nExplore some of the datasets. What keywords do the datasets have in common?\n\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-11')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 6)\n\ncdc_datasets &lt;- tuesdata$cdc_datasets\nfpi_codes &lt;- tuesdata$fpi_codes\nomb_codes &lt;- tuesdata$omb_codes\n\n# Option 2: Read directly from GitHub\n\ncdc_datasets &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/cdc_datasets.csv')\nfpi_codes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/fpi_codes.csv')\nomb_codes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/omb_codes.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-02-11/readme.html#the-data",
    "href": "data/2025/2025-02-11/readme.html#the-data",
    "title": "CDC Datasets",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-11')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 6)\n\ncdc_datasets &lt;- tuesdata$cdc_datasets\nfpi_codes &lt;- tuesdata$fpi_codes\nomb_codes &lt;- tuesdata$omb_codes\n\n# Option 2: Read directly from GitHub\n\ncdc_datasets &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/cdc_datasets.csv')\nfpi_codes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/fpi_codes.csv')\nomb_codes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/omb_codes.csv')"
  },
  {
    "objectID": "data/2025/2025-02-11/readme.html#how-to-participate",
    "href": "data/2025/2025-02-11/readme.html#how-to-participate",
    "title": "CDC Datasets",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-01-28/readme.html",
    "href": "data/2025/2025-01-28/readme.html",
    "title": "Water Insecurity",
    "section": "",
    "text": "This week we’re exploring water insecurity data featured in the article Mapping water insecurity in R with tidycensus!\n\nWater insecurity can be influenced by number of social vulnerability indicators—from demographic characteristics to living conditions and socioeconomic status —that vary spatially across the U.S. This blog shows how the tidycensus package for R can be used to access U.S. Census Bureau data, including the American Community Surveys, as featured in the “Unequal Access to Water” data visualization from the USGS Vizlab. It offers reproducible code examples demonstrating use of tidycensus for easy exploration and visualization of social vulnerability indicators in the Western U.S.\n\n\nHow does the lack of complete indoor plumbing compare between the 2023 and 2022 Census data?\nWhat counties have the greatest percent of households lacking plumbing?\nAre there differences in indoor plumbing availability between Western U.S and Eastern U.S counties?\n\nThank you to Niha Pereira for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-01-28')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 4)\n\nwater_insecurity_2022 &lt;- tuesdata$water_insecurity_2022\nwater_insecurity_2023 &lt;- tuesdata$water_insecurity_2023\n\n# Option 2: Read directly from GitHub\n\nwater_insecurity_2022 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-28/water_insecurity_2022.csv')\nwater_insecurity_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-28/water_insecurity_2023.csv')\n\n# The geometry columns are saved as text with the code to reproduce them.\nwater_insecurity_2022 &lt;- water_insecurity_2022 |&gt; \n  dplyr::mutate(\n    geometry = purrr::map(geometry, \\(geo) {\n      eval(parse(text = geo))\n    } )\n  )\nwater_insecurity_2023 &lt;- water_insecurity_2023 |&gt; \n  dplyr::mutate(\n    geometry = purrr::map(geometry, \\(geo) {\n      eval(parse(text = geo))\n    } )\n  )\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-01-28/readme.html#the-data",
    "href": "data/2025/2025-01-28/readme.html#the-data",
    "title": "Water Insecurity",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-01-28')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 4)\n\nwater_insecurity_2022 &lt;- tuesdata$water_insecurity_2022\nwater_insecurity_2023 &lt;- tuesdata$water_insecurity_2023\n\n# Option 2: Read directly from GitHub\n\nwater_insecurity_2022 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-28/water_insecurity_2022.csv')\nwater_insecurity_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-28/water_insecurity_2023.csv')\n\n# The geometry columns are saved as text with the code to reproduce them.\nwater_insecurity_2022 &lt;- water_insecurity_2022 |&gt; \n  dplyr::mutate(\n    geometry = purrr::map(geometry, \\(geo) {\n      eval(parse(text = geo))\n    } )\n  )\nwater_insecurity_2023 &lt;- water_insecurity_2023 |&gt; \n  dplyr::mutate(\n    geometry = purrr::map(geometry, \\(geo) {\n      eval(parse(text = geo))\n    } )\n  )"
  },
  {
    "objectID": "data/2025/2025-01-28/readme.html#how-to-participate",
    "href": "data/2025/2025-01-28/readme.html#how-to-participate",
    "title": "Water Insecurity",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-01-14/readme.html",
    "href": "data/2025/2025-01-14/readme.html",
    "title": "posit::conf talks",
    "section": "",
    "text": "This week we’re exploring posit::conf talks! On the day when this dataset is first being shared, the call for speakers for posit::conf(2025) is open. To help inspire you to submit a talk, we’ve collected data about posit::conf talks from 2023 and 2024. Thank you to Rachael Dempsey for the Google sheets!\n\nposit::conf is our annual conference that focuses on the R and Python programming languages and their applications in data science. The conference features a variety of workshops, talks, and networking opportunities for attendees, with a particular emphasis on fostering a sense of community among data science professionals. In addition to providing opportunities for learning and professional development, posit::conf also aims to create a fun and engaging atmosphere that encourages attendees to connect with one another and explore the latest trends and technologies in the field.\n\n\nWhich speakers gave talks in both 2023 and 2024?\nAre there keywords that appear in track titles in both 2023 and 2024?\nWhat is the average sentiment of the descriptions in each track?\n\nBe careful to de-duplicate talk data when necessary! Talks with multiple speakers might appear more than once.\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-01-14')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 2)\n\nconf2023 &lt;- tuesdata$conf2023\nconf2024 &lt;- tuesdata$conf2024\n\n# Option 2: Read directly from GitHub\n\nconf2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-14/conf2023.csv')\nconf2024 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-14/conf2024.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-01-14/readme.html#the-data",
    "href": "data/2025/2025-01-14/readme.html#the-data",
    "title": "posit::conf talks",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-01-14')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 2)\n\nconf2023 &lt;- tuesdata$conf2023\nconf2024 &lt;- tuesdata$conf2024\n\n# Option 2: Read directly from GitHub\n\nconf2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-14/conf2023.csv')\nconf2024 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-14/conf2024.csv')"
  },
  {
    "objectID": "data/2025/2025-01-14/readme.html#how-to-participate",
    "href": "data/2025/2025-01-14/readme.html#how-to-participate",
    "title": "posit::conf talks",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-31/readme.html",
    "href": "data/2024/2024-12-31/readme.html",
    "title": "James Beard Awards",
    "section": "",
    "text": "This week we’re exploring the James Beard Awards! Wikipedia tells us:\n\nThe James Beard Foundation Awards are annual awards presented by the James Beard Foundation to recognize chefs, restaurateurs, authors and journalists in the United States.\n\nThank you to PythonCoderUnicorn for the dataset suggestion!\n\nHow have the subcategories of the various awards changed over time?\nHas anybody won in multiple categories?\nWhich restaurants have the most winners? Which newspapers or networks?\n\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-31')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 53)\n\nbook &lt;- tuesdata$book\nbroadcast_media &lt;- tuesdata$broadcast_media\njournalism &lt;- tuesdata$journalism\nleadership &lt;- tuesdata$leadership\nrestaurant_and_chef &lt;- tuesdata$restaurant_and_chef\n\n# Option 2: Read directly from GitHub\n\nbook &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/book.csv')\nbroadcast_media &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/broadcast_media.csv')\njournalism &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/journalism.csv')\nleadership &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/leadership.csv')\nrestaurant_and_chef &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/restaurant_and_chef.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-31/readme.html#the-data",
    "href": "data/2024/2024-12-31/readme.html#the-data",
    "title": "James Beard Awards",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-31')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 53)\n\nbook &lt;- tuesdata$book\nbroadcast_media &lt;- tuesdata$broadcast_media\njournalism &lt;- tuesdata$journalism\nleadership &lt;- tuesdata$leadership\nrestaurant_and_chef &lt;- tuesdata$restaurant_and_chef\n\n# Option 2: Read directly from GitHub\n\nbook &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/book.csv')\nbroadcast_media &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/broadcast_media.csv')\njournalism &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/journalism.csv')\nleadership &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/leadership.csv')\nrestaurant_and_chef &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-31/restaurant_and_chef.csv')"
  },
  {
    "objectID": "data/2024/2024-12-31/readme.html#how-to-participate",
    "href": "data/2024/2024-12-31/readme.html#how-to-participate",
    "title": "James Beard Awards",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-17/readme.html",
    "href": "data/2024/2024-12-17/readme.html",
    "title": "Dungeons and Dragons Spells (2024)",
    "section": "",
    "text": "This week we’re exploring magical spells from the recently released Dungeons and Dragons Free Rules (2024 edition).\n\nMany characters have the ability to cast spells, which have a huge variety of effects. Some spells are mostly useful in combat, by dealing damage or imposing conditions. Other spells have utility in exploration. If you’re playing a spellcaster, look for a mix of combat-effective and utilitarian spells to help deal with varied challenges.\n\n\nWhich class has the most options for spells to cast on themselves, or on targets they can touch?\nWhich classes have the most spells that require concentration? Which classes have spells that last without concentration?\nAre there any interesting patterns in the text descriptions of the spells?\n\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-17')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 51)\n\nspells &lt;- tuesdata$spells\n\n# Option 2: Read directly from GitHub\n\nspells &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-17/spells.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-17/readme.html#the-data",
    "href": "data/2024/2024-12-17/readme.html#the-data",
    "title": "Dungeons and Dragons Spells (2024)",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-17')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 51)\n\nspells &lt;- tuesdata$spells\n\n# Option 2: Read directly from GitHub\n\nspells &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-17/spells.csv')"
  },
  {
    "objectID": "data/2024/2024-12-17/readme.html#how-to-participate",
    "href": "data/2024/2024-12-17/readme.html#how-to-participate",
    "title": "Dungeons and Dragons Spells (2024)",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-03/readme.html",
    "href": "data/2024/2024-12-03/readme.html",
    "title": "National Highways Traffic Flow",
    "section": "",
    "text": "This week we’re exploring National Highways Traffic Flow data! National Highways operates and maintains motorways and major A roads in England. They directly monitor the speed and flow of roads using on road sensors, and the data can be accessed via the National Highways API.\nThis week’s data has vehicle size and speed information for May 2021 from four different road sensors on the A64 road.\n\nDo vehicles travel faster on certain days or at certain times?\nWhat time of day do large vehicles use this road?\nDo smaller vehicles travel faster?\n\nThank you to Nicola Rennie for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-03')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 49)\n\nA64_traffic &lt;- tuesdata$A64_traffic\n\n# Option 2: Read directly from GitHub\n\nA64_traffic &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-03/A64_traffic.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-03/readme.html#the-data",
    "href": "data/2024/2024-12-03/readme.html#the-data",
    "title": "National Highways Traffic Flow",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-03')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 49)\n\nA64_traffic &lt;- tuesdata$A64_traffic\n\n# Option 2: Read directly from GitHub\n\nA64_traffic &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-03/A64_traffic.csv')"
  },
  {
    "objectID": "data/2024/2024-12-03/readme.html#how-to-participate",
    "href": "data/2024/2024-12-03/readme.html#how-to-participate",
    "title": "National Highways Traffic Flow",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-11-19/readme.html",
    "href": "data/2024/2024-11-19/readme.html",
    "title": "Bob’s Burgers Episodes",
    "section": "",
    "text": "This week we’re exploring Bob’s Burgers dialogue! Thank you to Steven Ponce for the data, and a blog post demonstrating how to visualize the data!\nSee the {bobsburgersR} R Package for the original transcript data, as well as additional information about each episode!\n\nHow have dialogue metrics changed over the seasons?\nCan you find any patterns not shown in Steven Ponce’s original visualization?\n\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-11-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 47)\n\nepisode_metrics &lt;- tuesdata$episode_metrics\n\n# Option 2: Read directly from GitHub\n\nepisode_metrics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-19/episode_metrics.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-11-19/readme.html#the-data",
    "href": "data/2024/2024-11-19/readme.html#the-data",
    "title": "Bob’s Burgers Episodes",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-11-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 47)\n\nepisode_metrics &lt;- tuesdata$episode_metrics\n\n# Option 2: Read directly from GitHub\n\nepisode_metrics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-19/episode_metrics.csv')"
  },
  {
    "objectID": "data/2024/2024-11-19/readme.html#how-to-participate",
    "href": "data/2024/2024-11-19/readme.html#how-to-participate",
    "title": "Bob’s Burgers Episodes",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-11-05/readme.html",
    "href": "data/2024/2024-11-05/readme.html",
    "title": "Democracy and Dictatorship",
    "section": "",
    "text": "It’s Election Day in the United States. If you are able to do so, vote!\nTo celebrate, we’re looking at a dataset about Democracy and Dictatorship.\n\nThis dataset updates pacl with more countries and coverage from 1950 to 2020, as described in C. Bjørnskov and M. Rode. “Regime types and regime change: A new dataset on &lt; democracy, coups, and political institutions”. In: The Review of International Organizations 15.2 (2020), pp. 531-551. DOI: 10.1007/s11558-019-09345-1. The full data and codebook can be downloaded here.\n\n\nHow many countries switched from democracies to non-democracies? Did any of them keep their democratically elected leader after the switch?\nWhich of those countries switched back to democracies? How long did it take?\n\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-11-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 45)\n\ndemocracy_data &lt;- tuesdata$democracy_data\n\n# Option 2: Read directly from GitHub\n\ndemocracy_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-05/democracy_data.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-11-05/readme.html#the-data",
    "href": "data/2024/2024-11-05/readme.html#the-data",
    "title": "Democracy and Dictatorship",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-11-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 45)\n\ndemocracy_data &lt;- tuesdata$democracy_data\n\n# Option 2: Read directly from GitHub\n\ndemocracy_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-05/democracy_data.csv')"
  },
  {
    "objectID": "data/2024/2024-11-05/readme.html#how-to-participate",
    "href": "data/2024/2024-11-05/readme.html#how-to-participate",
    "title": "Democracy and Dictatorship",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-22/readme.html",
    "href": "data/2024/2024-10-22/readme.html",
    "title": "The CIA World Factbook",
    "section": "",
    "text": "This week we’re exploring the CIA World Factbook! The dataset this week comes from the CIA Factbook, Country Comparisons, 2014, via the {openintro} R package, via the {usdatasets} R package, via this post on LinkedIn.\n\nThe World Factbook provides basic intelligence on the history, people, government, economy, energy, geography, environment, communications, transportation, military, terrorism, and transnational issues for 265 world entities.\n\nWhich countries have the highest number of internet users per square kilometer? Which countries have the highest percentage of internet users?\nYou might want to join this dataset with past TidyTueday datasets that featured country information!\n# pak::pak(\"r4ds/ttmeta\")\nlibrary(tidyverse)\nlibrary(ttmeta)\n\ncountry_datasets &lt;- ttmeta::tt_datasets_metadata |&gt; \n  dplyr::mutate(\n    has_country = purrr::map_lgl(\n      .data$variable_details,\n      \\(var_dets) {\n        !is.null(var_dets) && \n          any(stringr::str_detect(tolower(var_dets$variable), \"country\"))\n      }\n    )\n  ) |&gt; \n  dplyr::filter(has_country)\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-22')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 43)\n\ncia_factbook &lt;- tuesdata$cia_factbook\n\n# Option 2: Read directly from GitHub\n\ncia_factbook &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-22/cia_factbook.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-22/readme.html#the-data",
    "href": "data/2024/2024-10-22/readme.html#the-data",
    "title": "The CIA World Factbook",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-22')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 43)\n\ncia_factbook &lt;- tuesdata$cia_factbook\n\n# Option 2: Read directly from GitHub\n\ncia_factbook &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-22/cia_factbook.csv')"
  },
  {
    "objectID": "data/2024/2024-10-22/readme.html#how-to-participate",
    "href": "data/2024/2024-10-22/readme.html#how-to-participate",
    "title": "The CIA World Factbook",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-08/readme.html",
    "href": "data/2024/2024-10-08/readme.html",
    "title": "National Park Species",
    "section": "",
    "text": "This week we’re exploring species at the most visited National Parks in the USA! NPSpecies contains species listed by National Parks maintained by National Parks Service (NPS). Given the size of the dataset, we’re focusing on the 15 most visited parks. The data comes from https://irma.nps.gov/NPSpecies/Search/SpeciesList.\n\nThe information in NPSpecies is available to the public. The exceptions to this are records for some sensitive, threatened, or endangered species, where widespread distribution of information could potentially put a species at risk.\nAn essential component of NPSpecies is evidence; that is, observations, vouchers, or reports that document the presence of a species in a park. Ideally, every species in a park that is designated as “present in park” will have at least one form of credible evidence substantiating the designation\n\nIf you are looking for more detailed information on the dataset, here is the glossary for column names, field options, and tag meanings: https://irma.nps.gov/content/npspecies/Help/docs/NPSpecies_User_Guide.pdf\nTo properly cite NPSpecies use the following: NPSpecies - The National Park Service biodiversity database. https://irma.nps.gov/npspecies/. Accessed date/time. \nThis data was accessed on September 2nd, 2024.\nIf you are interested in additional data, the curated dataset for all national parks is available at https://github.com/frankiethull/NPSpecies.\nThank you to f. hull for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-08')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 41)\n\nmost_visited_nps_species_data &lt;- tuesdata$most_visited_nps_species_data\n\n# Option 2: Read directly from GitHub\n\nmost_visited_nps_species_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-08/most_visited_nps_species_data.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-08/readme.html#the-data",
    "href": "data/2024/2024-10-08/readme.html#the-data",
    "title": "National Park Species",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-08')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 41)\n\nmost_visited_nps_species_data &lt;- tuesdata$most_visited_nps_species_data\n\n# Option 2: Read directly from GitHub\n\nmost_visited_nps_species_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-08/most_visited_nps_species_data.csv')"
  },
  {
    "objectID": "data/2024/2024-10-08/readme.html#how-to-participate",
    "href": "data/2024/2024-10-08/readme.html#how-to-participate",
    "title": "National Park Species",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-09-24/readme.html",
    "href": "data/2024/2024-09-24/readme.html",
    "title": "International Mathematical Olympiad (IMO) Data",
    "section": "",
    "text": "The data for this week comes from International Mathematical Olympiad (IMO).\n\nThe International Mathematical Olympiad (IMO) is the World Championship Mathematics Competition for High School students and is held annually in a different country. The first IMO was held in 1959 in Romania, with 7 countries participating. It has gradually expanded to over 100 countries from 5 continents. The competition consists of 6 problems and is held over two consecutive days with 3 problems each.\n\n\nHow have country rankings shifted over time?\nWhat is the distribution of participation by gender? What’s the distribution of top scores?\nHow does team size or team composition (e.g., number of first-time participants vs. veterans) relate to overall country performance?\n\nThank you to Havisha Khurana for curating this week’s dataset, and to Emi Tanaka for catching the bug in the original script!\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-09-24')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 39)\n\ncountry_results_df &lt;- tuesdata$country_results_df\nindividual_results_df &lt;- tuesdata$individual_results_df\ntimeline_df &lt;- tuesdata$timeline_df\n\n# Option 2: Read directly from GitHub\n\ncountry_results_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-24/country_results_df.csv')\nindividual_results_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-24/individual_results_df.csv')\ntimeline_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-24/timeline_df.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-09-24/readme.html#the-data",
    "href": "data/2024/2024-09-24/readme.html#the-data",
    "title": "International Mathematical Olympiad (IMO) Data",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-09-24')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 39)\n\ncountry_results_df &lt;- tuesdata$country_results_df\nindividual_results_df &lt;- tuesdata$individual_results_df\ntimeline_df &lt;- tuesdata$timeline_df\n\n# Option 2: Read directly from GitHub\n\ncountry_results_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-24/country_results_df.csv')\nindividual_results_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-24/individual_results_df.csv')\ntimeline_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-24/timeline_df.csv')"
  },
  {
    "objectID": "data/2024/2024-09-24/readme.html#how-to-participate",
    "href": "data/2024/2024-09-24/readme.html#how-to-participate",
    "title": "International Mathematical Olympiad (IMO) Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-09-10/readme.html",
    "href": "data/2024/2024-09-10/readme.html",
    "title": "Economic Diversity and Student Outcomes",
    "section": "",
    "text": "College students are back on campus in the US, so we’re exploring economic diversity and student outcomes! The dataset this week comes from Opportunity Insights via an article and associated interactive visualization from the Upshot at the New York Times. Thank you to Havisha Khurana for suggesting this dataset!\n\nA new study, based on millions of anonymous tax records, shows that some colleges are even more economically segregated than previously understood, while others are associated with income mobility.\n\nThis dataset offers an opportunity to explore the three rules that make a dataset “tidy”:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nHow might you pivot this data to make it longer? When might you want to do that? When might you pivot this data to make it wider?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-09-10')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 37)\n\ncollege_admissions &lt;- tuesdata$college_admissions\n\n# Option 2: Read directly from GitHub\n\ncollege_admissions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-10/college_admissions.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-09-10/readme.html#the-data",
    "href": "data/2024/2024-09-10/readme.html#the-data",
    "title": "Economic Diversity and Student Outcomes",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-09-10')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 37)\n\ncollege_admissions &lt;- tuesdata$college_admissions\n\n# Option 2: Read directly from GitHub\n\ncollege_admissions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-10/college_admissions.csv')"
  },
  {
    "objectID": "data/2024/2024-09-10/readme.html#how-to-participate",
    "href": "data/2024/2024-09-10/readme.html#how-to-participate",
    "title": "Economic Diversity and Student Outcomes",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-08-27/readme.html",
    "href": "data/2024/2024-08-27/readme.html",
    "title": "The Power Rangers Franchise",
    "section": "",
    "text": "This week’s dataset comes from Kaggle’s Power Rangers Dataset!\n\nIn 1993, five ordinary teenagers exploded on the pop-culture scene with the launch of Mighty Morphin Power Rangers. Together they broke down barriers. They defeated evil by demonstrating teamwork, inclusivity, and diversity to people of all ages. Today, this grand tradition continues as new Ranger teams and new generations of fans discover these essential values again.\n\nThe series, created by Haim Saban, has one of the most popular taglines in history, “It’s Morphin Time!” The TV series “Mighty Morphin Power Rangers” (MMPR) launched on August 28, 1993. Power Rangers quickly became the #1 kids action brand and a global phenomenon. With its current 25th season, “Power Rangers Super Ninja Steel,” the show is now the second-longest-running, non-soap-opera, scripted program on American TV (after “The Simpsons”). There are also over 830 episodes in its library. Currently, Power Rangers is seen in more than 150 markets around the world. It’s also translated into numerous languages and is a favorite on many indispensable children’s programming blocks around the world. Go Go Power Rangers on 8.28!\nSource: NationalDayCalendar.com\nWhat can you and your data analysis skills tell us about the Power Rangers’ Franchise?\nWhat were the most popular seasons? Which season of rangers lasted the longest on screen? Which was your favourite ranger and why?\nThank you to Tinashe M. Tapera for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 35)\n\npower_rangers_episodes &lt;- tuesdata$power_rangers_episodes\npower_rangers_seasons &lt;- tuesdata$power_rangers_seasons\n\n# Option 2: Read directly from GitHub\n\npower_rangers_episodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-27/power_rangers_episodes.csv')\npower_rangers_seasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-27/power_rangers_seasons.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-08-27/readme.html#the-data",
    "href": "data/2024/2024-08-27/readme.html#the-data",
    "title": "The Power Rangers Franchise",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 35)\n\npower_rangers_episodes &lt;- tuesdata$power_rangers_episodes\npower_rangers_seasons &lt;- tuesdata$power_rangers_seasons\n\n# Option 2: Read directly from GitHub\n\npower_rangers_episodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-27/power_rangers_episodes.csv')\npower_rangers_seasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-27/power_rangers_seasons.csv')"
  },
  {
    "objectID": "data/2024/2024-08-27/readme.html#how-to-participate",
    "href": "data/2024/2024-08-27/readme.html#how-to-participate",
    "title": "The Power Rangers Franchise",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-08-13/readme.html",
    "href": "data/2024/2024-08-13/readme.html",
    "title": "World’s Fairs",
    "section": "",
    "text": "We’re in Seattle this week for posit::conf, so we’re exploring World’s Fairs!\n\nA world’s fair, also known as a universal exhibition or an expo, is a large global exhibition designed to showcase the achievements of nations. These exhibitions vary in character and are held in different parts of the world at a specific site for a period of time, typically between three and six months.\n\nThe data was scraped from Wikipedia’s list of world expositions.\nDoes the length of a Fair depend on the month in which the fair begins? How has the cost per month changed over time? How about the cost per visitor?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-13')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 33)\n\nworlds_fairs &lt;- tuesdata$worlds_fairs\n\n# Option 2: Read directly from GitHub\n\nworlds_fairs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-13/worlds_fairs.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-08-13/readme.html#the-data",
    "href": "data/2024/2024-08-13/readme.html#the-data",
    "title": "World’s Fairs",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-13')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 33)\n\nworlds_fairs &lt;- tuesdata$worlds_fairs\n\n# Option 2: Read directly from GitHub\n\nworlds_fairs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-13/worlds_fairs.csv')"
  },
  {
    "objectID": "data/2024/2024-08-13/readme.html#how-to-participate",
    "href": "data/2024/2024-08-13/readme.html#how-to-participate",
    "title": "World’s Fairs",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-30/readme.html",
    "href": "data/2024/2024-07-30/readme.html",
    "title": "Summer Movies",
    "section": "",
    "text": "This week we’re exploring “summer” movies: movies with “summer” in their title!\nThe data this week comes from the Internet Movie Database. We don’t have an article using exactly this dataset, but you might get inspiration from IMDb’s 2023 Summer Movie Guide.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 31)\n\nsummer_movie_genres &lt;- tuesdata$summer_movie_genres\nsummer_movies &lt;- tuesdata$summer_movies\n\n# Option 2: Read directly from GitHub\n\nsummer_movie_genres &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-30/summer_movie_genres.csv')\nsummer_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-30/summer_movies.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-30/readme.html#the-data",
    "href": "data/2024/2024-07-30/readme.html#the-data",
    "title": "Summer Movies",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 31)\n\nsummer_movie_genres &lt;- tuesdata$summer_movie_genres\nsummer_movies &lt;- tuesdata$summer_movies\n\n# Option 2: Read directly from GitHub\n\nsummer_movie_genres &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-30/summer_movie_genres.csv')\nsummer_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-30/summer_movies.csv')"
  },
  {
    "objectID": "data/2024/2024-07-30/readme.html#how-to-participate",
    "href": "data/2024/2024-07-30/readme.html#how-to-participate",
    "title": "Summer Movies",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-16/readme.html",
    "href": "data/2024/2024-07-16/readme.html",
    "title": "English Women’s Football",
    "section": "",
    "text": "Thanks to Rob Clapp for the The English Women’s Football (EWF) Database, May 2024 dataset this week, and h/t Data is Plural. The dataset lists the date, teams, score, attendance, division, tier, and season of each match, as well as each season’s final standings. Rob took inspiration from the Fjelstul English Football Database, a similarly structured dataset that covers men’s professional football since 1888.\n\nThe English Women’s Football (EWF) Database is an open database of matches played in the top tiers of women’s football in England. It covers all matches played since the 2011 season for the highest division (the Women’s Super League) and since the 2014 season for the second-highest division (the Women’s Championship).\n\nThe dataset contains three datasets:\n\newf_matches contains all matches that have been played and has one observation per match per season.\newf_appearances contains all appearances by a team and has one observation per team per match per season.\newf_standings contains all end-of-the-season division tables and has one observation per team per season.\n\n\nThe data in the English Women’s Football (EWF) Database has been collected from multiple online sources and has been cross-referenced to confirm its accuracy. Information in the database is also cross-referenced with itself to ensure consistency. For example, that a team’s goals_for at the end of the season in ewf_standings is equal to the number of goals they have scored across all games played in ewf_matches.\n\n\nEach team has been given a unique ID in the format of T-###-T. This is to enable the tracking of team performance across multiple seasons, as most teams have changed their name over time. For example, Arsenal Ladies became Arsenal Women before the start of the 2017-2018 season, but the same ID is used for them throughout the database (T-001-T). The name of the team in each dataset is the name of the team at the time. Any generic terms such as ‘Football Club’ or ‘F.C.’ have been removed. Reference to ‘Women’ or ‘Ladies’ is included in the team name, where applicable, to indicate changes that have occurred. However, most teams do not explicitly reference ‘Women’ or ‘Ladies’ in their name unless it is to distinguish between the male and female teams.\n\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 29)\n\newf_appearances &lt;- tuesdata$ewf_appearances\newf_matches &lt;- tuesdata$ewf_matches\newf_standings &lt;- tuesdata$ewf_standings\n\n# Option 2: Read directly from GitHub\n\newf_appearances &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-16/ewf_appearances.csv')\newf_matches &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-16/ewf_matches.csv')\newf_standings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-16/ewf_standings.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-16/readme.html#the-data",
    "href": "data/2024/2024-07-16/readme.html#the-data",
    "title": "English Women’s Football",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 29)\n\newf_appearances &lt;- tuesdata$ewf_appearances\newf_matches &lt;- tuesdata$ewf_matches\newf_standings &lt;- tuesdata$ewf_standings\n\n# Option 2: Read directly from GitHub\n\newf_appearances &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-16/ewf_appearances.csv')\newf_matches &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-16/ewf_matches.csv')\newf_standings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-16/ewf_standings.csv')"
  },
  {
    "objectID": "data/2024/2024-07-16/readme.html#how-to-participate",
    "href": "data/2024/2024-07-16/readme.html#how-to-participate",
    "title": "English Women’s Football",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-02/readme.html",
    "href": "data/2024/2024-07-02/readme.html",
    "title": "TidyTuesday Datasets",
    "section": "",
    "text": "This week we’re exploring our own data! The data this week comes from our {ttmeta} package, which automatically updates with information about the TidyTuesday datasets. The data shared here was compiled on 2024-06-18. If you would like, you can use the package to retrieve updated information about the last few weeks.\nWhat are the most common variable names? Of those variables, which are used to mean the same thing, and which are used to mean different things?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-02')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 27)\n\ntt_datasets &lt;- tuesdata$tt_datasets\ntt_summary &lt;- tuesdata$tt_summary\ntt_urls &lt;- tuesdata$tt_urls\ntt_variables &lt;- tuesdata$tt_variables\n\n# Option 2: Read directly from GitHub\n\ntt_datasets &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-02/tt_datasets.csv')\ntt_summary &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-02/tt_summary.csv')\ntt_urls &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-02/tt_urls.csv')\ntt_variables &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-02/tt_variables.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-02/readme.html#the-data",
    "href": "data/2024/2024-07-02/readme.html#the-data",
    "title": "TidyTuesday Datasets",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-02')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 27)\n\ntt_datasets &lt;- tuesdata$tt_datasets\ntt_summary &lt;- tuesdata$tt_summary\ntt_urls &lt;- tuesdata$tt_urls\ntt_variables &lt;- tuesdata$tt_variables\n\n# Option 2: Read directly from GitHub\n\ntt_datasets &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-02/tt_datasets.csv')\ntt_summary &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-02/tt_summary.csv')\ntt_urls &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-02/tt_urls.csv')\ntt_variables &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-02/tt_variables.csv')"
  },
  {
    "objectID": "data/2024/2024-07-02/readme.html#how-to-participate",
    "href": "data/2024/2024-07-02/readme.html#how-to-participate",
    "title": "TidyTuesday Datasets",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-06-18/readme.html",
    "href": "data/2024/2024-06-18/readme.html",
    "title": "US Federal Holidays",
    "section": "",
    "text": "This week we’re celebrating Juneteenth!\n\n[Juneteenth National Independence Day] Commemorates the emancipation of enslaved people in the United States on the anniversary of the 1865 date when emancipation was announced in Galveston, Texas. Celebratory traditions often include readings of the Emancipation Proclamation, singing traditional songs, rodeos, street fairs, family reunions, cookouts, park parties, historical reenactments, and Miss Juneteenth contests.\n\nJuneteenth became a federal holiday in the United States on June 17, 2021. To commemorate this newest U.S. Federal Holiday, we’re exploring the Wikipedia page about Federal holidays in the United States.\nWhich days of the week do federal holidays fall on this year? What is the longest gap between holidays this year? Is it different in other years?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-06-18')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 25)\n\nfederal_holidays &lt;- tuesdata$federal_holidays\nproposed_federal_holidays &lt;- tuesdata$proposed_federal_holidays\n\n# Option 2: Read directly from GitHub\n\nfederal_holidays &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-18/federal_holidays.csv')\nproposed_federal_holidays &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-18/proposed_federal_holidays.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-06-18/readme.html#the-data",
    "href": "data/2024/2024-06-18/readme.html#the-data",
    "title": "US Federal Holidays",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-06-18')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 25)\n\nfederal_holidays &lt;- tuesdata$federal_holidays\nproposed_federal_holidays &lt;- tuesdata$proposed_federal_holidays\n\n# Option 2: Read directly from GitHub\n\nfederal_holidays &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-18/federal_holidays.csv')\nproposed_federal_holidays &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-18/proposed_federal_holidays.csv')"
  },
  {
    "objectID": "data/2024/2024-06-18/readme.html#how-to-participate",
    "href": "data/2024/2024-06-18/readme.html#how-to-participate",
    "title": "US Federal Holidays",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-06-04/readme.html",
    "href": "data/2024/2024-06-04/readme.html",
    "title": "Cheese",
    "section": "",
    "text": "The data this week comes from cheese.com, inspired by the examples in the {polite} package.\n\nCheese is nutritious food made mostly from the milk of cows but also other mammals, including sheep, goats, buffalo, reindeer, camels and yaks. Around 4000 years ago people have started to breed animals and process their milk. That’s when the cheese was born.\n\n\nExplore this site to find out about different kinds of cheese from all around the world.\n\n248 cheeses have listed fat content. Is there a relationship between fat content and cheese type? What about texture, flavor, or aroma?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-06-04')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 23)\n\ncheeses &lt;- tuesdata$cheeses\n\n# Option 2: Read directly from GitHub\n\ncheeses &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-04/cheeses.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-06-04/readme.html#the-data",
    "href": "data/2024/2024-06-04/readme.html#the-data",
    "title": "Cheese",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-06-04')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 23)\n\ncheeses &lt;- tuesdata$cheeses\n\n# Option 2: Read directly from GitHub\n\ncheeses &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-04/cheeses.csv')"
  },
  {
    "objectID": "data/2024/2024-06-04/readme.html#how-to-participate",
    "href": "data/2024/2024-06-04/readme.html#how-to-participate",
    "title": "Cheese",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-05-21/readme.html",
    "href": "data/2024/2024-05-21/readme.html",
    "title": "Carbon Majors Emissions Data",
    "section": "",
    "text": "This week we’re exploring historical emissions data from Carbon Majors. They have complied a database of emissions data going back to 1854. h/t Data is Plural.\n\nCarbon Majors is a database of historical production data from 122 of the world’s largest oil, gas, coal, and cement producers. This data is used to quantify the direct operational emissions and emissions from the combustion of marketed products that can be attributed to these entities. These entities include:\n\n\n75 Investor-owned Companies, 36 State-owned Companies, 11 Nation States, 82 Oil Producing Entities, 81 Gas Entities, 49 Coal Entities, 6 Cement Entities\n\n\nThe data spans back to 1854 and contains over 1.42 trillion tonnes of CO2e covering 72% of global fossil fuel and cement emissions since the start of the Industrial Revolution in 1751.\n\nThey share data with low, medium and high levels of granularity. This dataset is the ‘medium’ granularity that contains year, entity, entity type, commodity, commodity production, commodity unit, and total emissions.\nAre there any trends or changes that surprised you?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-21')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 21)\n\nemissions &lt;- tuesdata$emissions\n\n\n# Option 2: Read directly from GitHub\n\nemissions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-21/emissions.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-05-21/readme.html#the-data",
    "href": "data/2024/2024-05-21/readme.html#the-data",
    "title": "Carbon Majors Emissions Data",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-21')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 21)\n\nemissions &lt;- tuesdata$emissions\n\n\n# Option 2: Read directly from GitHub\n\nemissions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-21/emissions.csv')"
  },
  {
    "objectID": "data/2024/2024-05-21/readme.html#how-to-participate",
    "href": "data/2024/2024-05-21/readme.html#how-to-participate",
    "title": "Carbon Majors Emissions Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-05-07/readme.html",
    "href": "data/2024/2024-05-07/readme.html",
    "title": "Rolling Stone Album Rankings",
    "section": "",
    "text": "This week we’re looking at album rankings from Rolling Stone. h/t Data is plural. A visual essay from The Pudding looks at what makes an album the greatest of all time, and shares the data they put together for the essay.\n\nA new visual essay from The Pudding compares Rolling Stone’s “500 Greatest Albums of All Time” lists from 2003, 2012, and 2020. A methodology note says the project began with a spreadsheet by Chris Eckert and eventually led the authors to develop a dataset of their own. Theirs lists every album in the rankings — its name, genre, release year, 2003/2012/2020 rank, the artist’s name, birth year, gender, and more — plus each year’s voters. [h/t Jason Kottke]\n\nWhat are the characteristics of artists and genres popular at different times?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-07')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 19)\n\nrolling_stone &lt;- tuesdata$rolling_stone\n\n\n# Option 2: Read directly from GitHub\n\nrolling_stone &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-07/rolling_stone.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-05-07/readme.html#the-data",
    "href": "data/2024/2024-05-07/readme.html#the-data",
    "title": "Rolling Stone Album Rankings",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-07')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 19)\n\nrolling_stone &lt;- tuesdata$rolling_stone\n\n\n# Option 2: Read directly from GitHub\n\nrolling_stone &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-07/rolling_stone.csv')"
  },
  {
    "objectID": "data/2024/2024-05-07/readme.html#how-to-participate",
    "href": "data/2024/2024-05-07/readme.html#how-to-participate",
    "title": "Rolling Stone Album Rankings",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-23/readme.html",
    "href": "data/2024/2024-04-23/readme.html",
    "title": "Objects Launched into Space",
    "section": "",
    "text": "This week we’re exploring the annual number of objects launched into outer space! Our World in Data processed the data from the Online Index of Objects Launched into Outer Space, maintained by the United Nations Office for Outer Space Affairs since 1962. h/t Data is Plural.\n\nWhat you should know about this indicator\n\n\nObjects are defined here as satellites, probes, landers, crewed spacecrafts, and space station flight elements launched into Earth orbit or beyond.\n\n\nThis data is based on national registers of launches submitted to the UN by participating nations. According to UN estimates, the data captures around 88% of all objects launched.\n\n\nWhen an object is marked by the source as launched by a country on behalf of another one, the launch is attributed to the latter country.\n\n\nWhen a launch is made jointly by several countries, it is recorded in each of these countries’ time series but only once in the ‘World’ series.\n\nData Page: Annual number of objects launched into space”, part of the following publication: Edouard Mathieu and Max Roser (2022) - “Space Exploration and Satellites”. Data adapted from United Nations Office for Outer Space Affairs. Retrieved from https://ourworldindata.org/grapher/yearly-number-of-objects-launched-into-outer-space\nDataset: United Nations Office for Outer Space Affairs (2024) – with major processing by Our World in Data. “Annual number of objects launched into space – UNOOSA” [dataset]. United Nations Office for Outer Space Affairs, “Online Index of Objects Launched into Outer Space” [original data]. Retrieved April 21, 2024 from https://ourworldindata.org/grapher/yearly-number-of-objects-launched-into-outer-space\nWhen did each entity start launching objects into space? What years saw the biggest increase in space object launches?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 17)\n\nouter_space_objects &lt;- tuesdata$outer_space_objects\n\n\n# Option 2: Read directly from GitHub\n\nouter_space_objects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-23/outer_space_objects.csv')\n\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-23/readme.html#the-data",
    "href": "data/2024/2024-04-23/readme.html#the-data",
    "title": "Objects Launched into Space",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 17)\n\nouter_space_objects &lt;- tuesdata$outer_space_objects\n\n\n# Option 2: Read directly from GitHub\n\nouter_space_objects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-23/outer_space_objects.csv')"
  },
  {
    "objectID": "data/2024/2024-04-23/readme.html#how-to-participate",
    "href": "data/2024/2024-04-23/readme.html#how-to-participate",
    "title": "Objects Launched into Space",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-09/readme.html",
    "href": "data/2024/2024-04-09/readme.html",
    "title": "2023 & 2024 US Solar Eclipses",
    "section": "",
    "text": "This week we’re looking at the paths of solar eclipses in the United States. The data comes from NASA’s Scientific Visualization Studio.\n\nOn October 14, 2023, an annular solar eclipse [crossed] North, Central, and South America creating a path of annularity. An annular solar eclipse occurs when the Moon passes between the Sun and Earth while at its farthest point from Earth. Because the Moon is farther away from Earth, it does not completely block the Sun. This create[d] a “ring of fire” effect in the sky for those standing in the path of annularity. On April 8, 2024, a total solar eclipse will cross North and Central America creating a path of totality. During a total solar eclipse, the Moon completely blocks the Sun while it passes between the Sun and Earth. The sky will darken as if it were dawn or dusk and those standing in the path of totality may see the Sun’s outer atmosphere (the corona) if weather permits.\n\nWhich places are in both the 2023 path of annularity and the 2024 path of totality? Which place has the longest duration of totality in 2024?\nNote: The data is an approximation as of 2023-03-08. If you are observing the 2024 eclipse within the path of totality, you may want to check times in your exact location.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 15)\n\neclipse_annular_2023 &lt;- tuesdata$eclipse_annular_2023\neclipse_total_2024 &lt;- tuesdata$eclipse_total_2024\neclipse_partial_2023 &lt;- tuesdata$eclipse_partial_2023\neclipse_partial_2024 &lt;- tuesdata$eclipse_partial_2024\n\n# Option 2: Read directly from GitHub\n\neclipse_annular_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-09/eclipse_annular_2023.csv')\neclipse_total_2024 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-09/eclipse_total_2024.csv')\neclipse_partial_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-09/eclipse_partial_2023.csv')\neclipse_partial_2024 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-09/eclipse_partial_2024.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-09/readme.html#the-data",
    "href": "data/2024/2024-04-09/readme.html#the-data",
    "title": "2023 & 2024 US Solar Eclipses",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 15)\n\neclipse_annular_2023 &lt;- tuesdata$eclipse_annular_2023\neclipse_total_2024 &lt;- tuesdata$eclipse_total_2024\neclipse_partial_2023 &lt;- tuesdata$eclipse_partial_2023\neclipse_partial_2024 &lt;- tuesdata$eclipse_partial_2024\n\n# Option 2: Read directly from GitHub\n\neclipse_annular_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-09/eclipse_annular_2023.csv')\neclipse_total_2024 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-09/eclipse_total_2024.csv')\neclipse_partial_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-09/eclipse_partial_2023.csv')\neclipse_partial_2024 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-09/eclipse_partial_2024.csv')"
  },
  {
    "objectID": "data/2024/2024-04-09/readme.html#how-to-participate",
    "href": "data/2024/2024-04-09/readme.html#how-to-participate",
    "title": "2023 & 2024 US Solar Eclipses",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-03-26/readme.html",
    "href": "data/2024/2024-03-26/readme.html",
    "title": "NCAA Men’s March Madness",
    "section": "",
    "text": "March is NCAA basketball March Madness! This week’s data is NCAA Men’s March Madness data from Nishaan Amin’s Kaggle dataset and analysis Bracketology: predicting March Madness.\nMarch Madness is the NCAA Division I basketball tournament for women and men. It’s a single-elimination tournament of 68 teams that compete in six rounds for the national championship. (The “March Madness” branding and logo was only extended to women in 2022.)\nEach round of the tournament has a name:\n\nFirst round: Round of 64\nSecond round: Round of 32\nThird round: Sweet 16\nFourth round: Elite 8\nFifth round: Final 4\nSixth round: Finals\n\n\nThe data is from 2008 - 2024 for the men’s teams. The year 2020 is not included because the tournament was canceled due to Covid. The first column of almost every dataset displays the year the data is from.\n\nNishaan Amin’s dataset contains many different sets of data. For TidyTuesday we’re using past team results and the predictions the public has for this year’s tournament. How have teams done in past years? What teams are people predicting will do well this year? How does past performance correlate with expectations for this year?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-03-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 13)\n\nteam_results &lt;- tuesdata$`team-results`\npublic_picks &lt;- tuesdata$`public-picks`\n\n\n# Option 2: Read directly from GitHub\n\nteam_results &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-26/team-results.csv')\npublic_picks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-26/public-picks.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-03-26/readme.html#the-data",
    "href": "data/2024/2024-03-26/readme.html#the-data",
    "title": "NCAA Men’s March Madness",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-03-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 13)\n\nteam_results &lt;- tuesdata$`team-results`\npublic_picks &lt;- tuesdata$`public-picks`\n\n\n# Option 2: Read directly from GitHub\n\nteam_results &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-26/team-results.csv')\npublic_picks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-26/public-picks.csv')"
  },
  {
    "objectID": "data/2024/2024-03-26/readme.html#how-to-participate",
    "href": "data/2024/2024-03-26/readme.html#how-to-participate",
    "title": "NCAA Men’s March Madness",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-03-12/readme.html",
    "href": "data/2024/2024-03-12/readme.html",
    "title": "Fiscal Sponsors",
    "section": "",
    "text": "TidyTuesday is organized by the R4DS Online Learning Community. We are currently (when this dataset was first posted in March, 2024) seeking a new fiscal sponsor. See our Donate page for details about our current efforts.\nTo aid in our search, the dataset this week comes from the Fiscal Sponsor Directory. The data is fairly messy, but you may be able to extract more information from it.\nThe Fiscal Sponsor Directory analyzed their directory in March 2023.\n\nHas the data changed?\nCan you identify other trends, or represent them more clearly?\nCan you identify sponsors that might be particularly appealing to the R4DS Online Learning Community, a “diverse, friendly, and inclusive community of data science learners and practitioners” (a global community based in Austin, Texas, USA)?\n\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-03-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 11)\n\nfiscal_sponsor_directory &lt;- tuesdata$fiscal_sponsor_directory\n\n\n# Option 2: Read directly from GitHub\n\nfiscal_sponsor_directory &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-12/fiscal_sponsor_directory.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-03-12/readme.html#the-data",
    "href": "data/2024/2024-03-12/readme.html#the-data",
    "title": "Fiscal Sponsors",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-03-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 11)\n\nfiscal_sponsor_directory &lt;- tuesdata$fiscal_sponsor_directory\n\n\n# Option 2: Read directly from GitHub\n\nfiscal_sponsor_directory &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-12/fiscal_sponsor_directory.csv')"
  },
  {
    "objectID": "data/2024/2024-03-12/readme.html#how-to-participate",
    "href": "data/2024/2024-03-12/readme.html#how-to-participate",
    "title": "Fiscal Sponsors",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-02-27/readme.html",
    "href": "data/2024/2024-02-27/readme.html",
    "title": "Leap Day",
    "section": "",
    "text": "Happy Leap Day! This week’s data comes from the February 29 article on Wikipedia.\n\nFebruary 29 is a leap day (or “leap year day”), an intercalary date added periodically to create leap years in the Julian and Gregorian calendars.\n\nOne event that’s missing from Wikipedia’s list: R version 1.0 was released on February 29, 2000.\nWhich cohort of leap day births is most represented in Wikipedia’s data? Are any years surprisingly underrepresented compared to nearby years? What other patterns can you find in the data?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 9)\n\nevents &lt;- tuesdata$events\nbirths &lt;- tuesdata$births\ndeaths &lt;- tuesdata$deaths\n\n# Option 2: Read directly from GitHub\n\nevents &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-27/events.csv')\nbirths &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-27/births.csv')\ndeaths &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-27/deaths.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-02-27/readme.html#the-data",
    "href": "data/2024/2024-02-27/readme.html#the-data",
    "title": "Leap Day",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 9)\n\nevents &lt;- tuesdata$events\nbirths &lt;- tuesdata$births\ndeaths &lt;- tuesdata$deaths\n\n# Option 2: Read directly from GitHub\n\nevents &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-27/events.csv')\nbirths &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-27/births.csv')\ndeaths &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-27/deaths.csv')"
  },
  {
    "objectID": "data/2024/2024-02-27/readme.html#how-to-participate",
    "href": "data/2024/2024-02-27/readme.html#how-to-participate",
    "title": "Leap Day",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-02-13/readme.html",
    "href": "data/2024/2024-02-13/readme.html",
    "title": "Valentine’s Day Consumer Data",
    "section": "",
    "text": "Happy Valentine’s Day! This week we’re exploring Valentine’s Day survey data. The National Retail Federation in the United States conducts surveys and has created a Valentine’s Day Data Center so you can explore the data on how consumers celebrate.\n\nThe NRF has surveyed consumers about how they plan to celebrate Valentine’s Day annually for over a decade. Take a deeper dive into the data from the last 10 years, and use the interactive charts to explore a demographic breakdown of total spending, average spending, types of gifts planned and spending per type of gift.\n\nThe NRF has continued to collect data. The data for this week is from 2010 to 2022, as organized by Suraj Das for a Kaggle dataset. In the historical surveys gender was collected as only ‘Men’ and ‘Women’, which does not accurately include all genders.\nIf you’re looking for other Valentine’s Day type datasets, check out previous datasets on chocolate or board games (a good Valentine’s Day activity!).\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-13')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 7)\n\nhistorical_spending &lt;- tuesdata$historical_spending\ngifts_age &lt;- tuesdata$gifts_age\ngifts_gender &lt;- tuesdata$gifts_gender\n\n\n# Option 2: Read directly from GitHub\n\nhistorical_spending &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-13/historical_spending.csv')\ngifts_age &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-13/gifts_age.csv')\ngifts_gender &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-13/gifts_gender.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-02-13/readme.html#the-data",
    "href": "data/2024/2024-02-13/readme.html#the-data",
    "title": "Valentine’s Day Consumer Data",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-13')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 7)\n\nhistorical_spending &lt;- tuesdata$historical_spending\ngifts_age &lt;- tuesdata$gifts_age\ngifts_gender &lt;- tuesdata$gifts_gender\n\n\n# Option 2: Read directly from GitHub\n\nhistorical_spending &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-13/historical_spending.csv')\ngifts_age &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-13/gifts_age.csv')\ngifts_gender &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-13/gifts_gender.csv')"
  },
  {
    "objectID": "data/2024/2024-02-13/readme.html#how-to-participate",
    "href": "data/2024/2024-02-13/readme.html#how-to-participate",
    "title": "Valentine’s Day Consumer Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-01-30/readme.html",
    "href": "data/2024/2024-01-30/readme.html",
    "title": "Groundhog predictions",
    "section": "",
    "text": "Happy Groundhog Day! This week we’re exploring Groundhog Day Predictions from groundhog-day.com! See if you can find a better way to present the annual data than their table of predictions by year! For anyone not familiar with the Groundhog Day tradition, if the groundhog sees its shadow and goes back into its burrow, that is a prediction of six more weeks of winter. Otherwise spring will come early. We attempted to provide weather data to accompany this dataset, but so far we’ve been unsuccessful. Watch for a follow-up dataset in the future!\nNote: “Oil Springs Ollie” (groundhog #55) has been succeeded by “Heaven’s Wildlife Harvey” (groundhog #70).\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-01-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 5)\n\ngroundhogs &lt;- tuesdata$groundhogs\npredictions &lt;- tuesdata$predictions\n\n# Option 2: Read directly from GitHub\n\ngroundhogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-30/groundhogs.csv')\npredictions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-30/predictions.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-01-30/readme.html#the-data",
    "href": "data/2024/2024-01-30/readme.html#the-data",
    "title": "Groundhog predictions",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-01-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 5)\n\ngroundhogs &lt;- tuesdata$groundhogs\npredictions &lt;- tuesdata$predictions\n\n# Option 2: Read directly from GitHub\n\ngroundhogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-30/groundhogs.csv')\npredictions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-30/predictions.csv')"
  },
  {
    "objectID": "data/2024/2024-01-30/readme.html#how-to-participate",
    "href": "data/2024/2024-01-30/readme.html#how-to-participate",
    "title": "Groundhog predictions",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-01-16/readme.html",
    "href": "data/2024/2024-01-16/readme.html",
    "title": "US Polling Places 2012-2020",
    "section": "",
    "text": "This week we’re honoring the legacy of Martin Luther King Jr. by exploring the US Polling Places.\nThe dataset comes from The Center for Public Integrity. You can read more about the data and how it was collected in their September 2020 article “National data release sheds light on past polling place changes”. Thank you Kelsey E Gonzalez for the dataset suggestion back in 2020!\nNote: Some states do not have data in this dataset. Several states (Colorado, Hawaii, Oregon, Washington and Utah) vote primarily by mail and have little or no data in this colletion, and others were not available for other reasons.\nFor states with data for multiple elections, how have polling location counts per county changed over time?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-01-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 3)\n\npolling_places &lt;- tuesdata$polling_places\n\n# Option 2: Read directly from GitHub\n\npolling_places &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-16/polling_places.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-01-16/readme.html#the-data",
    "href": "data/2024/2024-01-16/readme.html#the-data",
    "title": "US Polling Places 2012-2020",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-01-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 3)\n\npolling_places &lt;- tuesdata$polling_places\n\n# Option 2: Read directly from GitHub\n\npolling_places &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-16/polling_places.csv')"
  },
  {
    "objectID": "data/2024/2024-01-16/readme.html#how-to-participate",
    "href": "data/2024/2024-01-16/readme.html#how-to-participate",
    "title": "US Polling Places 2012-2020",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/readme.html",
    "href": "data/2023/readme.html",
    "title": "2023 Data",
    "section": "",
    "text": "2023 Data\nArchive of datasets and articles from the 2023 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2023-01-03\nBring your own data from 2022!\nNA\nNA\n\n\n2\n2023-01-10\nBird FeederWatch data\nFeederWatch\nOver 30 Years of Standardized Bird Counts at Supplementary Feeding Stations in North America: A Citizen Science Data Report for Project FeederWatch\n\n\n3\n2023-01-17\nArt history data\narthistory data package\nQuantifying Art Historical Narratives\n\n\n4\n2023-01-24\nAlone data\nAlone data package\nAlone R package: Datasets from the survival TV series\n\n\n5\n2023-01-31\nPet Cats UK\nMovebank for Animal Tracking Data\nCats on the Move\n\n\n6\n2023-02-07\nBig Tech Stock Prices\nBig Tech Stock Prices on Kaggle\n5 Charts on Big Tech Stocks’ Collapse\n\n\n7\n2023-02-14\nHollywood Age Gaps\nHollywood Age Gap Download Data\nHollywood Age Gap\n\n\n8\n2023-02-21\nBob Ross Paintings\nBob Ross Paintings data\nBob Ross Colors data package\n\n\n9\n2023-02-28\nAfrican Language Sentiment\nAfriSenti: Sentiment Analysis dataset for 14 African languages\nAfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages\n\n\n10\n2023-03-07\nNumbats in Australia\nAtlas of Living Australia\nNumbat page at the Atlas of Living Australia\n\n\n11\n2023-03-14\nEuropean Drug Development\nEuropean Medicines Agency\nDissecting 28 years of European pharmaceutical development\n\n\n12\n2023-03-21\nProgramming Languages\nProgramming Language DataBase\nDoes every programming language have line comments?\n\n\n13\n2023-03-28\nTime Zones\nIANA tz database\n“What Is Daylight Saving Time”\n\n\n14\n2023-04-04\nPremier League Match Data\nPremier League Match Data 2021-2022\nWho wins the EPL if games end at half time?\n\n\n15\n2023-04-11\nUS Egg Production Data\nUS Egg Production Data 2007-2021\nThe Humane League Labs US Egg Production Dataset\n\n\n16\n2023-04-18\nNeolithic Founder Crops\nThe “Neolithic Founder Crops” in Southwest Asia: Research Compendium\nRevisiting the concept of the ‘Neolithic Founder Crops’ in southwest Asia\n\n\n17\n2023-04-25\nLondon Marathon\nLondon Marathon R package\nScraping London Marathon data with {rvest}\n\n\n18\n2023-05-02\nThe Portal Project\nPortal Project Data\nPortal Project\n\n\n19\n2023-05-09\nChildcare Costs\nNational Database of Childcare Prices\nNational Database of Childcare Prices\n\n\n20\n2023-05-16\nTornados\nNOAA’s National Weather Service Storm Prediction Center Severe Weather Maps, Graphics, and Data Page\nDiving into US Tornado Data\n\n\n21\n2023-05-23\nCentral Park Squirrels\n2018 Central Park Squirrel Census\nThe Squirrel Census\n\n\n22\n2023-05-30\nVerified Oldest People\nfrankiethull: Centenarians\nWikipedia: List of the verified oldest people\n\n\n23\n2023-06-06\nEnergy\nEnergy Data Explorer\nOur World in Data Energy Complete Dataset\n\n\n24\n2023-06-13\nStudying African Farmer-Led Irrigation Survey\nSAFI Teaching Dataset for Data Carpentry Social Sciences\nSAFI Teaching Dataset\n\n\n25\n2023-06-20\nUFO Sightings Redux\nNational UFO Reporting Center, sunrise-sunset.org\nTidyTuesday 2019-06-25\n\n\n26\n2023-06-27\nUS Populated Places\nNational Map Staged Products Directory\nUS Board of Geographic Names\n\n\n27\n2023-07-04\nHistorical Markers\nHistorical Marker Database USA Index\nDatabase Counts and Statistics\n\n\n28\n2023-07-11\nGlobal Surface Temperatures\nNASA GISS Surface Temperature Analysis (GISTEMP v4)\nImprovements in the GISTEMP Uncertainty Model\n\n\n29\n2023-07-18\nGPT detectors\nGPT detectors R package\nGPT Detectors Are Biased Against Non-Native English Writers\n\n\n30\n2023-07-25\nScurvy\nmedicaldata R package\nScurvy Dataset Description\n\n\n31\n2023-08-01\nUS States\nList of states and territories of the United States, List of demonyms for US states and territories, and List of state and territory name etymologies of the United States\nList of states and territories of the United States\n\n\n32\n2023-08-08\nHot Ones Episodes\nHot Ones and List of Hot Ones episodes\nHot Ones\n\n\n33\n2023-08-15\nSpam E-mail\nSpam e-mail\nSpam email database\n\n\n34\n2023-08-22\nRefugees\nRefugees R package\nUnited Nations High Commissioner for Refugees (UNHCR) Refugee Data Finder\n\n\n35\n2023-08-29\nFair Use\nU.S. Copyright Office Fair Use Index\nU.S. Copyright Office Fair Use Index\n\n\n36\n2023-09-05\nUnion Membership in the United States\nUnion Membership, Coverage, and Earnings from the CPS\nFive decades of CPS wages, methods, and union-nonunion wage gaps at Unionstats.com\n\n\n37\n2023-09-12\nThe Global Human Day\nThe Global Human Day dataset\nThe global human day PNAS article\n\n\n38\n2023-09-19\nCRAN Package Authors\nThe CRAN collaboration graph\nThe CRAN collaboration graph README\n\n\n39\n2023-09-26\nRoy Kent F**k count\nDeepsha Menghani posit::conf(2023) talk on data visualization and Quarto\nrichmondway dataset\n\n\n40\n2023-10-03\nUS Government Grant Opportunities\nGrants 101 from Grants.gov\nGrants.gov Search Export\n\n\n41\n2023-10-10\nHaunted Places in the United States\nTim Renner on data.world\nUpcoming changes to popular R packages for spatial data: what you need to do\n\n\n42\n2023-10-17\nTaylor Swift data\ntaylor R package\nTaylor Swift data analysis\n\n\n43\n2023-10-24\nPatient Risk Profiles\nR/Pharma\nR/Pharma\n\n\n44\n2023-10-31\nHorror Legends\nSnopes Horrors Section\nSnopes Horrors Section\n\n\n45\n2023-11-07\nUS House Election Results\nMIT Election Data and Science Lab\nNew Report: How We Voted in 2022\n\n\n46\n2023-11-14\nDiwali Sales Data\nDiwali sales data\nDiwali sales data exploration (in Python)\n\n\n47\n2023-11-21\nR-Ladies Chapter Events\nR-Ladies Chapter Events data\nR-Ladies Chapters: Making talks work for diverse audiences\n\n\n48\n2023-11-28\nDoctor Who Episodes\ndatardis package\nList of Doctor Who episodes\n\n\n49\n2023-12-05\nLife Expectancy\nOur World in Data life expectancy\nOur World in Data key insights on life expectancy)\n\n\n50\n2023-12-12\nHoliday Movies\nIMDb non-commercial datasets\nChristmas Movies blog)\n\n\n51\n2023-12-19\nHoliday Episodes\nIMDb non-commercial datasets\nHoliday Episode TV Trope)\n\n\n52\n2023-12-26\nR Package Structure\nropensci-review-tools/pkgstats\nHistorical Trends in R Package Structure and Interdependency on CRAN)",
    "crumbs": [
      "Datasets",
      "2023"
    ]
  },
  {
    "objectID": "data/2023/2023-12-19/readme.html",
    "href": "data/2023/2023-12-19/readme.html",
    "title": "Holiday Episodes",
    "section": "",
    "text": "Happy holidays! Last week we explored “holiday” movies. This week we’re exploring “holiday” TV episodes: individual episodes of TV shows with “holiday”, “Christmas”, “Hanukkah”, or “Kwanzaa” (or variants thereof) in their title!\nThe data this week again comes from the Internet Movie Database. We don’t have an article using exactly this dataset, but you might get inspiration from the TVTropes Holiday Episode article.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 51)\n\nholiday_episodes &lt;- tuesdata$holiday_episodes\nholiday_episode_genres &lt;- tuesdata$holiday_episode_genres\n\n# Option 2: Read directly from GitHub\n\nholiday_episodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-19/holiday_episodes.csv')\nholiday_episode_genres &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-19/holiday_episode_genres.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-12-19/readme.html#the-data",
    "href": "data/2023/2023-12-19/readme.html#the-data",
    "title": "Holiday Episodes",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 51)\n\nholiday_episodes &lt;- tuesdata$holiday_episodes\nholiday_episode_genres &lt;- tuesdata$holiday_episode_genres\n\n# Option 2: Read directly from GitHub\n\nholiday_episodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-19/holiday_episodes.csv')\nholiday_episode_genres &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-19/holiday_episode_genres.csv')"
  },
  {
    "objectID": "data/2023/2023-12-19/readme.html#how-to-participate",
    "href": "data/2023/2023-12-19/readme.html#how-to-participate",
    "title": "Holiday Episodes",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-12-05/readme.html",
    "href": "data/2023/2023-12-05/readme.html",
    "title": "Life Expectancy",
    "section": "",
    "text": "Our World in Data released a new report on life expectancy, and analysis and interpretations are making the rounds this week, and you can join in too!\nWe had a dataset on Global Life Expectancy on 2018-07-03, so if you’re a long time tidytuesday-er, this is a chance for some new analyses.\nThe data this week comes the Our World in Data Life Expectancy report, specifically the figures in the key insights section. The source for this data is from United Nations World Population Prospects (2022); Human Mortality Database (2023); Zijdeman, Richard and Filipa Ribeira da Silva (2015), Life Expectancy at Birth (Total); Riley, J.C. (2005), Estimates of Regional and Global Life Expectancy 1800-2001, Population and Development Review, 31: 537-543. Minor processing by Our World in Data.\n\nAcross the world, people are living longer. In 1900, the average life expectancy of a newborn was 32 years. By 2021 this had more than doubled to 71 years. But where, when, how, and why has this dramatic change occurred? To understand it, we can look at data on life expectancy worldwide. The large reduction in child mortality has played an important role in increasing life expectancy. But life expectancy has increased at all ages. Infants, children, adults, and the elderly are all less likely to die than in the past, and death is being delayed. This remarkable shift results from advances in medicine, public health, and living standards. Along with it, many predictions of the ‘limit’ of life expectancy have been broken. On the Our World in Data Life Expectancy page, you will find global data and research on life expectancy and related measures of longevity: the probability of death at a given age, the sex gap in life expectancy, lifespan inequality within countries, and more.\n\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 49)\n\nlife_expectancy &lt;- tuesdata$life_expectancy\nlife_expectancy_different_ages &lt;- tuesdata$life_expectancy_different_ages\nlife_expectancy_female_male &lt;- tuesdata$life_expectancy_female_male\n\n# Option 2: Read directly from GitHub\n\nlife_expectancy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-05/life_expectancy.csv')\nlife_expectancy_different_ages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-05/life_expectancy_different_ages.csv')\nlife_expectancy_female_male &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-05/life_expectancy_female_male.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-12-05/readme.html#the-data",
    "href": "data/2023/2023-12-05/readme.html#the-data",
    "title": "Life Expectancy",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 49)\n\nlife_expectancy &lt;- tuesdata$life_expectancy\nlife_expectancy_different_ages &lt;- tuesdata$life_expectancy_different_ages\nlife_expectancy_female_male &lt;- tuesdata$life_expectancy_female_male\n\n# Option 2: Read directly from GitHub\n\nlife_expectancy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-05/life_expectancy.csv')\nlife_expectancy_different_ages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-05/life_expectancy_different_ages.csv')\nlife_expectancy_female_male &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-05/life_expectancy_female_male.csv')"
  },
  {
    "objectID": "data/2023/2023-12-05/readme.html#how-to-participate",
    "href": "data/2023/2023-12-05/readme.html#how-to-participate",
    "title": "Life Expectancy",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-11-21/readme.html",
    "href": "data/2023/2023-11-21/readme.html",
    "title": "R-Ladies Chapter Events",
    "section": "",
    "text": "R-Ladies Global is an inspiring story of community, empowerment, and diversity in the field of data science. Founded by Gabriela de Queiroz, R-Ladies began as a grassroots movement with a simple mission: to promote gender diversity in the R programming community and provide a welcoming space for women and gender minorities to learn, collaborate, and excel in data science. R-Ladies Global Website\nThe data this week comes from Federica Gazzelloni’s presentation on R-Ladies Chapters: Making talks work for diverse audiences with data from the rladies meetup-archive.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-11-21')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 47)\n\nrladies_chapters &lt;- tuesdata$rladies_chapters\n\n# Option 2: Read directly from GitHub\n\nrladies_chapters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-21/rladies_chapters.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-11-21/readme.html#the-data",
    "href": "data/2023/2023-11-21/readme.html#the-data",
    "title": "R-Ladies Chapter Events",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-11-21')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 47)\n\nrladies_chapters &lt;- tuesdata$rladies_chapters\n\n# Option 2: Read directly from GitHub\n\nrladies_chapters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-21/rladies_chapters.csv')"
  },
  {
    "objectID": "data/2023/2023-11-21/readme.html#how-to-participate",
    "href": "data/2023/2023-11-21/readme.html#how-to-participate",
    "title": "R-Ladies Chapter Events",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-11-07/readme.html",
    "href": "data/2023/2023-11-07/readme.html",
    "title": "US House Election Results",
    "section": "",
    "text": "It’s election day in the United States! To celebrate, the data this week comes from the MIT Election Data and Science Lab (MEDSL). Hat tip this week to the RStudio GitHub Copilot integration, which suggested the MEDSL.\nFrom the MEDSL’s report New Report: How We Voted in 2022:\n\nThe Survey of the Performance of American Elections (SPAE) provides information about how Americans experienced voting in the most recent federal election. The survey has been conducted after federal elections since 2008, and is the only public opinion project in the country that is dedicated explicitly to understanding how voters themselves experience the election process.\n\nWe’re specifically providing data on House elections from 1976-2022. Check out the MEDSL website for additional datasets and tools.\nBe sure to cite the MEDSL in your work:\n@data{DVN/IG0UN2_2017,\nauthor = {MIT Election Data and Science Lab},\npublisher = {Harvard Dataverse},\ntitle = {{U.S. House 1976–2022}},\nUNF = {UNF:6:A6RSZvlhh8eRZ4+mvT/HRQ==},\nyear = {2017},\nversion = {V12},\ndoi = {10.7910/DVN/IG0UN2},\nurl = {https://doi.org/10.7910/DVN/IG0UN2}\n}\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-11-07')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 45)\n\nhouse &lt;- tuesdata$house\n\n# Option 2: Read directly from GitHub\n\nhouse &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-07/house.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-11-07/readme.html#the-data",
    "href": "data/2023/2023-11-07/readme.html#the-data",
    "title": "US House Election Results",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-11-07')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 45)\n\nhouse &lt;- tuesdata$house\n\n# Option 2: Read directly from GitHub\n\nhouse &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-07/house.csv')"
  },
  {
    "objectID": "data/2023/2023-11-07/readme.html#how-to-participate",
    "href": "data/2023/2023-11-07/readme.html#how-to-participate",
    "title": "US House Election Results",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-24/readme.html",
    "href": "data/2023/2023-10-24/readme.html",
    "title": "Patient Risk Profiles",
    "section": "",
    "text": "The virtual R/Pharma Conference is happening this week! To celebrate, we’re exploring Patient Risk Profiles. Thank you to Jenna Reps for preparing this week’s data!\n\nThis dataset contains 100 simulated patient’s medical history features and the predicted 1-year risk of 14 outcomes based on each patient’s medical history features. The predictions used real logistic regression models developed on a large real world healthcare dataset.\n\nNote: We did not clean the column names this week. This data looks more like the sort of data you’re likely to encounter in the wild, so we thought it would be good practice to work with it as-is.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-24')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 43)\n\npatient_risk_profiles &lt;- tuesdata$patient_risk_profiles\n\n# Option 2: Read directly from GitHub\n\npatient_risk_profiles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-24/patient_risk_profiles.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-24/readme.html#the-data",
    "href": "data/2023/2023-10-24/readme.html#the-data",
    "title": "Patient Risk Profiles",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-24')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 43)\n\npatient_risk_profiles &lt;- tuesdata$patient_risk_profiles\n\n# Option 2: Read directly from GitHub\n\npatient_risk_profiles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-24/patient_risk_profiles.csv')"
  },
  {
    "objectID": "data/2023/2023-10-24/readme.html#how-to-participate",
    "href": "data/2023/2023-10-24/readme.html#how-to-participate",
    "title": "Patient Risk Profiles",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-10/readme.html",
    "href": "data/2023/2023-10-10/readme.html",
    "title": "Haunted Places in the United States",
    "section": "",
    "text": "Halloween is coming soon, so we’re exploring a spooky dataset: a compilation of Haunted Places in the United States. The dataset was compiled by Tim Renner, using The Shadowlands Haunted Places Index, and shared on data.world.\nWe’re also using this dataset as a reminder that several R packages for spatial data are heading to the graveyard next week. Don’t be tricked by their demise!\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-10')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 41)\n\nhaunted_places &lt;- tuesdata$haunted_places\n\n# Option 2: Read directly from GitHub\n\nhaunted_places &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-10/haunted_places.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-10/readme.html#the-data",
    "href": "data/2023/2023-10-10/readme.html#the-data",
    "title": "Haunted Places in the United States",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-10')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 41)\n\nhaunted_places &lt;- tuesdata$haunted_places\n\n# Option 2: Read directly from GitHub\n\nhaunted_places &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-10/haunted_places.csv')"
  },
  {
    "objectID": "data/2023/2023-10-10/readme.html#how-to-participate",
    "href": "data/2023/2023-10-10/readme.html#how-to-participate",
    "title": "Haunted Places in the United States",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-09-26/readme.html",
    "href": "data/2023/2023-09-26/readme.html",
    "title": "Roy Kent F**k count",
    "section": "",
    "text": "For Deepsha Menghani’s talk on Data Viz animation and interactivity in Quarto, she watched each episode of Ted Lasso at 2X speed and diligently noted down every F*bomb and gesture reference, and then made it into the richmondway R package!\nWhat is Ted Lasso and who is Roy Kent?\nTed Lasso is a TV show that “follows Ted Lasso, an American college football coach who is hired to coach an English soccer team with the secret intention that his inexperience will lead it to failure, but whose folksy, optimistic leadership proves unexpectedly successful.”\nRoy Kent is one of the main characters who goes from captain of AFC Richmond to one of the coaches. Particularly in early seasons, he’s a man of few words, but many of them are f**k, expressed in various moods - mad, sad, happy, amused, loving, surprised, thoughtful, and joyous.\nThis dataset includes the number, percentage, and context of f**k used in the show for each episode.\n\n\n# Scroll to the end of this code block to see how to recombine the data into a\n# graph!\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-09-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 39)\n\nrichmondway &lt;- tuesdata$richmondway\n\n# Option 2: Read directly from GitHub\n\nrichmondway &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-26/richmondway.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\n\n\n\nSource Created by Deepsha Menghani by watching the show and counting the number of F-cks used in sentences and as gestures."
  },
  {
    "objectID": "data/2023/2023-09-26/readme.html#the-data",
    "href": "data/2023/2023-09-26/readme.html#the-data",
    "title": "Roy Kent F**k count",
    "section": "",
    "text": "# Scroll to the end of this code block to see how to recombine the data into a\n# graph!\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-09-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 39)\n\nrichmondway &lt;- tuesdata$richmondway\n\n# Option 2: Read directly from GitHub\n\nrichmondway &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-26/richmondway.csv')"
  },
  {
    "objectID": "data/2023/2023-09-26/readme.html#how-to-participate",
    "href": "data/2023/2023-09-26/readme.html#how-to-participate",
    "title": "Roy Kent F**k count",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\n\n\n\nSource Created by Deepsha Menghani by watching the show and counting the number of F-cks used in sentences and as gestures."
  },
  {
    "objectID": "data/2023/2023-09-12/readme.html",
    "href": "data/2023/2023-09-12/readme.html",
    "title": "The Global Human Day",
    "section": "",
    "text": "The data this week comes from the The Human Chronome Project an initiative based at McGill University in Montreal, from their paper The global human day in PNAS and the associated dataset on Zenodo.\n\nThe daily activities of ≈8 billion people occupy exactly 24 h per day, placing a strict physical limit on what changes can be achieved in the world. These activities form the basis of human behavior, and because of the global integration of societies and economies, many of these activities interact across national borders. This project estimates how all humans spend their time using a generalized, physical outcome–based categorization that facilitates the integration of data from hundreds of diverse datasets.\n\nSee their supplementary materials for details about their methods and additional visualizations.\nThe Zenodo dataset includes the input data and scripts used to create the datasets used in the paper. The datasets are from the outputData file “all_countries.csv”, “global_human_day.csv”, “global_economic_activity.csv” and inputData “country_regions.csv”. The outputData files are aggregated output data from data collected, created from the scripts in the ‘scripts’ directory.\nh/t Data is Plural 2023-07-13 newsletter for the dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-09-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 37)\n\nall_countries &lt;- tuesdata$all_countries\ncountry_regions &lt;- tuesdata$country_regions\nglobal_human_day &lt;- tuesdata$global_human_day\nglobal_economic_activity &lt;- tuesdata$global_economic_activity\n\n# Option 2: Read directly from GitHub\n\nall_countries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-12/all_countries.csv')\ncountry_regions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-12/country_regions.csv')\nglobal_human_day &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-12/global_human_day.csv')\nglobal_economic_activity &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-12/global_economic_activity.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-09-12/readme.html#the-data",
    "href": "data/2023/2023-09-12/readme.html#the-data",
    "title": "The Global Human Day",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-09-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 37)\n\nall_countries &lt;- tuesdata$all_countries\ncountry_regions &lt;- tuesdata$country_regions\nglobal_human_day &lt;- tuesdata$global_human_day\nglobal_economic_activity &lt;- tuesdata$global_economic_activity\n\n# Option 2: Read directly from GitHub\n\nall_countries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-12/all_countries.csv')\ncountry_regions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-12/country_regions.csv')\nglobal_human_day &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-12/global_human_day.csv')\nglobal_economic_activity &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-12/global_economic_activity.csv')"
  },
  {
    "objectID": "data/2023/2023-09-12/readme.html#how-to-participate",
    "href": "data/2023/2023-09-12/readme.html#how-to-participate",
    "title": "The Global Human Day",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-08-29/readme.html",
    "href": "data/2023/2023-08-29/readme.html",
    "title": "Fair Use",
    "section": "",
    "text": "The data this week comes from the U.S. Copyright Office Fair Use Index.\n\nFair use is a longstanding and vital aspect of American copyright law. The goal of the Index is to make the principles and application of fair use more accessible and understandable to the public by presenting a searchable database of court opinions, including by category and type of use (e.g., music, internet/digitization, parody).\n\nThere are two datasets this week for which the rows align, but the values might not precisely line up for a clean join – a case you often have to deal with in real-world data.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-29')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 35)\n\nfair_use_cases &lt;- tuesdata$fair_use_cases\nfair_use_findings &lt;- tuesdata$fair_use_findings\n\n# Option 2: Read directly from GitHub\n\nfair_use_cases &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-29/fair_use_cases.csv')\nfair_use_findings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-29/fair_use_findings.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-08-29/readme.html#the-data",
    "href": "data/2023/2023-08-29/readme.html#the-data",
    "title": "Fair Use",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-29')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 35)\n\nfair_use_cases &lt;- tuesdata$fair_use_cases\nfair_use_findings &lt;- tuesdata$fair_use_findings\n\n# Option 2: Read directly from GitHub\n\nfair_use_cases &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-29/fair_use_cases.csv')\nfair_use_findings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-29/fair_use_findings.csv')"
  },
  {
    "objectID": "data/2023/2023-08-29/readme.html#how-to-participate",
    "href": "data/2023/2023-08-29/readme.html#how-to-participate",
    "title": "Fair Use",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-08-15/readme.html",
    "href": "data/2023/2023-08-15/readme.html",
    "title": "Spam E-mail",
    "section": "",
    "text": "The data this week comes from Vincent Arel-Bundock’s Rdatasets package(https://vincentarelbundock.github.io/Rdatasets/index.html).\n\nRdatasets is a collection of 2246 datasets which were originally distributed alongside the statistical software environment R and some of its add-on packages. The goal is to make these data more broadly accessible for teaching and statistical software development.\n\nWe’re working with the spam email dataset. This is a subset of the spam e-mail database.\nThis is a dataset collected at Hewlett-Packard Labs by Mark Hopkins, Erik Reeber, George Forman, and Jaap Suermondt and shared with the UCI Machine Learning Repository. The dataset classifies 4601 e-mails as spam or non-spam, with additional variables indicating the frequency of certain words and characters in the e-mail.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-15')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 33)\n\nspam &lt;- tuesdata$spam\n\n# Option 2: Read directly from GitHub\n\nspam &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-15/spam.csv')"
  },
  {
    "objectID": "data/2023/2023-08-15/readme.html#the-data",
    "href": "data/2023/2023-08-15/readme.html#the-data",
    "title": "Spam E-mail",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-15')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 33)\n\nspam &lt;- tuesdata$spam\n\n# Option 2: Read directly from GitHub\n\nspam &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-15/spam.csv')"
  },
  {
    "objectID": "data/2023/2023-08-15/readme.html#how-to-participate",
    "href": "data/2023/2023-08-15/readme.html#how-to-participate",
    "title": "Spam E-mail",
    "section": "How to Participate",
    "text": "How to Participate\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-08-01/readme.html",
    "href": "data/2023/2023-08-01/readme.html",
    "title": "US State Names",
    "section": "",
    "text": "Happy Colorado Day! The data this week comes from three Wikipedia articles: List of states and territories of the United States, List of demonyms for US states and territories, and List of state and territory name etymologies of the United States.\nA number of past TidyTuesdays (such as 2018/2018-04-03, 2019/2019-01-29, 2020/2020-03-10, and 2022/2022-11-08 have state columns. Might joining in this state date offer any insights to those datasets?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-01')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 31)\n\nstates &lt;- tuesdata$states\nstate_name_etymology &lt;- tuesdata$state_name_etymology\n\n# Option 2: Read directly from GitHub\n\nstates &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-01/states.csv')\nstate_name_etymology &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-01/state_name_etymology.csv')"
  },
  {
    "objectID": "data/2023/2023-08-01/readme.html#the-data",
    "href": "data/2023/2023-08-01/readme.html#the-data",
    "title": "US State Names",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-01')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 31)\n\nstates &lt;- tuesdata$states\nstate_name_etymology &lt;- tuesdata$state_name_etymology\n\n# Option 2: Read directly from GitHub\n\nstates &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-01/states.csv')\nstate_name_etymology &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-01/state_name_etymology.csv')"
  },
  {
    "objectID": "data/2023/2023-08-01/readme.html#how-to-participate",
    "href": "data/2023/2023-08-01/readme.html#how-to-participate",
    "title": "US State Names",
    "section": "How to Participate",
    "text": "How to Participate\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-07-18/readme.html",
    "href": "data/2023/2023-07-18/readme.html",
    "title": "GPT detectors",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nGPT detectors\nThe data this week comes from Simon Couch’s detectors R package. containing predictions from various GPT detectors.\nThe data is based on the pre-print:\nGPT Detectors Are Biased Against Non-Native English Writers. Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou. arXiv: 2304.02819\n\nThe study authors carried out a series of experiments passing a number of essays to different GPT detection models. Juxtaposing detector predictions for papers written by native and non-native English writers, the authors argue that GPT detectors disproportionately classify real writing from non-native English writers as AI-generated.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-07-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 29)\n\ndetectors &lt;- tuesdata$detectors\n\n# Or read in the data manually\n\ndetectors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-18/detectors.csv')\n\n\nData Dictionary\n\n\n\ndetectors.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nkind\ncharacter\nWhether the essay was written by a “Human” or “AI”.\n\n\n.pred_AI\ndouble\nThe class probability from the GPT detector that the inputted text was written by AI.\n\n\n.pred_class\ncharacter\nThe uncalibrated class prediction, encoded as if_else(.pred_AI &gt; .5, \"AI\", \"Human\")\n\n\ndetector\ncharacter\nThe name of the detector used to generate the predictions.\n\n\nnative\ncharacter\nFor essays written by humans, whether the essay was written by a native English writer or not. These categorizations are coarse; values of \"Yes\" may actually be written by people who do not write with English natively. NA indicates that the text was not written by a human.\n\n\nname\ncharacter\nA label for the experiment that the predictions were generated from.\n\n\nmodel\ncharacter\nFor essays that were written by AI, the name of the model that generated the essay.\n\n\ndocument_id\ndouble\nA unique identifier for the supplied essay. Some essays were supplied to multiple detectors. Note that some essays are AI-revised derivatives of others.\n\n\nprompt\ncharacter\nFor essays that were written by AI, a descriptor for the form of “prompt engineering” passed to the model.\n\n\n\n\nCleaning Script\ncsv file was generated from the detectors tibble in the ‘detectors’ R package."
  },
  {
    "objectID": "data/2023/2023-07-04/readme.html",
    "href": "data/2023/2023-07-04/readme.html",
    "title": "Historical Markers",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nHistorical Markers\nThe data this week comes from the Historical Marker Database USA Index. Learn more about the markers on the HMDb.org site, which includes a number of articles, including Database Counts and Statistics.\nWe included a dataset of places that do not have entries in the Historical Markers Database. You might try to combine that with information from geonames.org (code: HSTS) to find markers that need to be submitted. Thanks to Jesus M. Castagnetto for the geonames tip!\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-07-04')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 27)\n\nhistorical_markers &lt;- tuesdata$`historical_markers`\nno_markers &lt;- tuesdata$`no_markers`\n\n# Or read in the data manually\n\nhistorical_markers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-04/historical_markers.csv')\nno_markers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-04/no_markers.csv')\n\n\nData Dictionary\n\n\n\nhistorical_markers.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmarker_id\ndouble\nUnique ID for this marker in the HMdb.\n\n\nmarker_no\ncharacter\nNumber of this marker in the state numbering scheme.\n\n\ntitle\ncharacter\nMain title of the marker.\n\n\nsubtitle\ncharacter\nSubtitle of the marker, if any.\n\n\naddl_subtitle\ncharacter\nAdditional subtitle text.\n\n\nyear_erected\ninteger\nThe year in which the marker was erected.\n\n\nerected_by\ncharacter\nThe organization which erected the marker.\n\n\nlatitude_minus_s\ndouble\nThe latitude of the marker.\n\n\nlongitude_minus_w\ndouble\nThe longitude of the marker.\n\n\nstreet_address\ncharacter\nThe street address of the marker, if available.\n\n\ncity_or_town\ncharacter\nThe city, town, etc in which the marker is located.\n\n\nsection_or_quarter\ncharacter\nThe section of the city, town, etc, when available.\n\n\ncounty_or_parish\ncharacter\nThe county, parish, or similar designation in which the marker appears.\n\n\nstate_or_prov\ncharacter\nThe state, province, territory, etc in which the marker appears.\n\n\nlocation\ncharacter\nA description of the marker’s location.\n\n\nmissing\ncharacter\nWhether the marker is “Reported missing” or “Confirmed missing”. NA values indicate that the marker has neither been reported missing nor confirmed as missing.\n\n\nlink\ncharacter\nThe HMDb link to the marker. Links include additional details, such as photos and topic lists to which this marker belongs.\n\n\n\n\n\nno_markers.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncounty\ncharacter\nCounty or equivalent.\n\n\nstate\ncharacter\nState or territory.\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(here)\nlibrary(fs)\nlibrary(janitor)\nlibrary(remotes)\n\n# I used the dev version of rvest from\n# https://github.com/tidyverse/rvest/pull/362 as of 2023-06-21 to scrape some\n# data this week.\ninstall_github(\"tidyverse/rvest#362\")\nlibrary(rvest)\n\n# Begin by visiting\n# https://www.hmdb.org/geolists.asp?c=United%20States%20of%20America, opening\n# each state in a new tab, and then clicking the \"Download\" button at the\n# top-right. I saved them all locally in a state_exports folder in this week's\n# data folder, but I won't upload those unprocessed CSVs to github.\n\nexport_path &lt;- here::here(\"data\", \"2023\", \"2023-07-04\", \"state_exports\")\n\nhistorical_markers_raw &lt;- fs::dir_map(\n  export_path,\n  read_csv\n)\n\n# I know there's one problem with each read, which we'll deal with. Check for\n# additional problems.\nprobs &lt;- historical_markers_raw |&gt; \n  purrr::map(\n    \\(hm) {\n      problems(hm) |&gt; \n        filter(!(expected == \"17 columns\" & actual == \"4 columns\")) |&gt; \n        mutate(file = str_remove(file, fixed(export_path)))\n    }\n  )\n\nhas_probs &lt;- vctrs::list_sizes(probs) &gt; 0\n\nprobs[has_probs]\nhistorical_markers_raw[[which(has_probs)]] |&gt; \n  slice(1168:1170) |&gt; \n  glimpse()\n\n# It looks like the \"Section or Quarter\" column is auto-guessed as logical (NA),\n# but should actually be character. We'll use this information to re-parse.\n\nhmdb_cols &lt;- spec(historical_markers_raw[[1]]) |&gt; \n  cols_condense()\nhmdb_cols\n\n# That spec will take care of the problem.\n\nhistorical_markers_raw &lt;- fs::dir_map(\n  export_path,\n  \\(path) {\n    read_csv(path, col_types = hmdb_cols)\n  }\n)\n\n# The remaining problems are all the expected one.\n\nhistorical_markers &lt;- historical_markers_raw |&gt; \n  purrr::list_rbind() |&gt; \n  filter(`Marker No.` != \"Source: Hmdb.org\") |&gt; \n  clean_names()\n\n# HMDb.org also has an interesting dataset about missing data. We'll scrape that\n# as well. \n\nstats_page &lt;- rvest::read_html_live(\"https://www.hmdb.org/stats.asp\")\n\nstats_page$click(\"#NoCArrow\")\n\nno_markers &lt;- stats_page |&gt; \n  rvest::html_element(\"#NoCDiv\") |&gt; \n  rvest::html_table() |&gt; \n  select(X2) |&gt; \n  filter(!X2 == \"\") |&gt; \n  tidyr::separate_wider_delim(X2, \",  \", names = c(\"county\", \"state\"))\n\nwrite_csv(\n  historical_markers,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-07-04\",\n    \"historical_markers.csv\"\n  )\n)\nwrite_csv(\n  no_markers,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-07-04\",\n    \"no_markers.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-06-20/readme.html",
    "href": "data/2023/2023-06-20/readme.html",
    "title": "UFO Sightings Redux",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nUFO Sightings Redux\nThe data this week comes from the National UFO Reporting Center, cleaned and enriched with data from sunrise-sunset.org by Jon Harmon.\nIf this dataset looks familiar, that’s because we used a version of it back in 2019. The new version adds the last several years of data, adds information about time-of-day, and cleans up some errors in the original dataset. We’d love to see visualizations describing the differences between the 2019 dataset and this new dataset!\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-06-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 25)\n\nufo_sightings &lt;- tuesdata$`ufo_sightings`\nplaces &lt;- tuesdata$`places`\nday_parts_map &lt;- tuesdata$`day_parts_map`\n\n# Or read in the data manually\n\nufo_sightings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-06-20/ufo_sightings.csv')\nplaces &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-06-20/places.csv')\nday_parts_map &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-06-20/day_parts_map.csv')\n\n\nData Dictionary\n\n\n\nufo_sightings.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nreported_date_time\ndatetime\nThe time and date of the sighting, as it appears in the original NUFORC data.\n\n\nreported_date_time_utc\ndatetime\nThe time and date of the sighting, normalized to UTC.\n\n\nposted_date\ndatetime\nThe date when the sighting was posted to NUFORC.\n\n\ncity\ncharacter\nThe city of the sighting. Some of these have been cleaned from the original data.\n\n\nstate\ncharacter\nThe state, province, or similar division of the sighting.\n\n\ncountry_code\ncharacter\nThe 2-letter country code of the sighting, normalized from the original data.\n\n\nshape\ncharacter\nThe reported shape of the craft.\n\n\nreported_duration\ncharacter\nThe reported duration of the event, in the reporter’s words.\n\n\nduration_seconds\ndouble\nThe duration normalized to seconds using regex.\n\n\nsummary\ncharacter\nThe reported summary of the event.\n\n\nhas_images\nlogical\nWhether the sighting has images available on NUFORC.\n\n\nday_part\ncharacter\nThe approximate part of the day in which the sighting took place, based on the reported date and time, the place, and data from sunrise-sunset.org. Latitude and longitude were rounded to the 10s digit, and the date was rounded to the week, to match against time points such as “nautical twilight”, “sunrise”, and “sunset.”\n\n\n\n\n\nplaces.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncity\ncharacter\nUnique cities in which sightings took place.\n\n\nalternate_city_names\ncharacter\nComma-separated other names for the city.\n\n\nstate\ncharacter\nThe state, province, or similar division of the sighting.\n\n\ncountry\ncharacter\nThe name of the country.\n\n\ncountry_code\ncharacter\nThe 2-letter country code of the sighting.\n\n\nlatitude\ndouble\nThe latitude for this city, from geonames.org.\n\n\nlongitude\ndouble\nThe longitude for this city, from geonames.org.\n\n\ntimezone\ncharacter\nThe timezone for this city, from geonames.org.\n\n\npopulation\ndouble\nThe population for this city, from geonames.org.\n\n\nelevation_m\ndouble\nThe elevation in meters for this city, from geonames.org.\n\n\n\n\n\nday_parts_map.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrounded_lat\ndouble\nLatitudes rounded to the tens digit.\n\n\nrounded_long\ndouble\nLongitudes rounded to the tens digit.\n\n\nrounded_date\ndouble\nDates rounded to the nearest week.\n\n\nastronomical_twilight_begin\ndouble\nThe UTC time of day when astronomical twilight began on this date in this location. Astronomical twilight begins when the sun is 18 degrees below the horizon before sunrise.\n\n\nnautical_twilight_begin\ndouble\nThe UTC time of day when nautical twilight began on this date (or the next date) in this location. Nautical twilight begins when the sun is 12 degrees below the horizon before sunrise.\n\n\ncivil_twilight_begin\ndouble\nThe UTC time of day when civil twilight began on this date (or the next date) in this location. Civil twilight begins when the sun is 6 degrees below the horizon before sunrise.\n\n\nsunrise\ndouble\nThe UTC time of day when the sun rose on this date (or the next date) in this location.\n\n\nsolar_noon\ndouble\nThe UTC time of day when the sun was at its zenith on this date (or the next date) in this location.\n\n\nsunset\ndouble\nThe UTC time of day when the sun set on this date (or the next date) in this location.\n\n\ncivil_twilight_end\ndouble\nThe UTC time of day when civil twilight ended on this date (or the next date) in this location. Civil twilight ends when the sun is 6 degrees below the horizon after sunset.\n\n\nnautical_twilight_end\ndouble\nThe UTC time of day when nautical twilight ended on this date (or the next date) in this location. Nautical twilight ends when the sun is 12 degrees below the horizon after sunset.\n\n\nastronomical_twilight_end\ndouble\nThe UTC time of day when astronomical twilight ended on this date (or the next date) in this location. Astronomical twilight ends when the sun is 18 degrees below the horizon after sunset.\n\n\n\n\nCleaning Script\nSee Jon Harmon’s cleaning and enriching scripts for most of the (extensive) cleaning.\n# All packages used in this script:\nlibrary(tidyverse)\nlibrary(here)\nlibrary(withr)\n\nurl &lt;- \"https://github.com/jonthegeek/apis/raw/main/data/data_ufo_reports_with_day_part.rds\"\nufo_path &lt;- withr::local_tempfile(fileext = \".rds\")\ndownload.file(url, ufo_path)\n\nufo_data_original &lt;- readRDS(ufo_path)\n\n# We need to make the csv small enough that github won't choke. We'll pull out\n# some of the joined data back into separate tables.\n\nufo_sightings &lt;- ufo_data_original |&gt; \n  dplyr::select(\n    reported_date_time:city,\n    state, \n    country_code,\n    shape:has_images,\n    day_part\n  ) |&gt; \n  # This got normalized after the data was saved, re-normalize.\n  dplyr::mutate(\n    shape = tolower(shape)\n  )\n\nplaces &lt;- ufo_data_original |&gt;\n  dplyr::select(\n    city:country_code, \n    latitude:elevation_m\n  ) |&gt; \n  dplyr::distinct()\n\n# We'll also provide the map of \"day parts\" in case anybody wants to do\n# something with that.\nurl2 &lt;- \"https://github.com/jonthegeek/apis/raw/main/data/data_day_parts_map.rds\"\nday_parts_path &lt;- withr::local_tempfile(fileext = \".rds\")\ndownload.file(url2, day_parts_path)\n\nday_parts_map &lt;- readRDS(day_parts_path)\n\nreadr::write_csv(\n  ufo_sightings,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-06-20\",\n    \"ufo_sightings.csv\"\n  )\n)\n\nreadr::write_csv(\n  places,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-06-20\",\n    \"places.csv\"\n  )\n)\n\nreadr::write_csv(\n  day_parts_map,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-06-20\",\n    \"day_parts_map.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-06-06/readme.html",
    "href": "data/2023/2023-06-06/readme.html",
    "title": "Energy Data",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nEnergy Data\nThe data this week comes from Our World in Data’s Energy Data Explorer. Complete dataset available via https://github.com/owid/energy-data.\n\nThe complete Energy dataset is a collection of key metrics maintained by Our World in Data. It is updated regularly and includes data on energy consumption (primary energy, per capita, and growth rates), energy mix, electricity mix and other relevant metrics.\n\n\nThis data has been collected, aggregated, and documented by Hannah Ritchie, Pablo Rosado, Edouard Mathieu, Max Roser.\n\n\nOur World in Data makes data and research on the world’s largest problems understandable and accessible.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-06-06')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 23)\n\nowid_energy &lt;- tuesdata$`owid-energy`\n\n# Or read in the data manually\n\nowid_energy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-06-06/owid-energy.csv')\n\n\nData Dictionary\n\n\n\nowid-energy.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nGeographic location\n\n\nyear\ndouble\nYear of observation\n\n\niso_code\ncharacter\nISO 3166-1 alpha-3 three-letter country codes\n\n\npopulation\ndouble\nPopulation\n\n\ngdp\ndouble\nTotal real gross domestic product, inflation-adjusted\n\n\nbiofuel_cons_change_pct\ndouble\nAnnual percentage change in biofuel consumption\n\n\nbiofuel_cons_change_twh\ndouble\nAnnual change in biofuel consumption, measured in terawatt-hours\n\n\nbiofuel_cons_per_capita\ndouble\nPer capita primary energy consumption from biofuels, measured in kilowatt-hours\n\n\nbiofuel_consumption\ndouble\nPrimary energy consumption from biofuels, measured in terawatt-hours\n\n\nbiofuel_elec_per_capita\ndouble\nPer capita electricity generation from biofuels, measured in kilowatt-hours\n\n\nbiofuel_electricity\ndouble\nElectricity generation from biofuels, measured in terawatt-hours\n\n\nbiofuel_share_elec\ndouble\nShare of electricity generation that comes from biofuels\n\n\nbiofuel_share_energy\ndouble\nShare of primary energy consumption that comes from biofuels\n\n\ncarbon_intensity_elec\ndouble\nCarbon intensity of electricity production, measured in grams of carbon dioxide emitted per kilowatt-hour\n\n\ncoal_cons_change_pct\ndouble\nAnnual percentage change in coal consumption\n\n\ncoal_cons_change_twh\ndouble\nAnnual change in coal consumption, measured in terawatt-hours\n\n\ncoal_cons_per_capita\ndouble\nPer capita primary energy consumption from coal, measured in kilowatt-hours\n\n\ncoal_consumption\ndouble\nPrimary energy consumption from coal, measured in terawatt-hours\n\n\ncoal_elec_per_capita\ndouble\nPer capita electricity generation from coal, measured in kilowatt-hours\n\n\ncoal_electricity\ndouble\nElectricity generation from coal, measured in terawatt-hours\n\n\ncoal_prod_change_pct\ndouble\nAnnual percentage change in coal production\n\n\ncoal_prod_change_twh\ndouble\nAnnual change in coal production, measured in terawatt-hours\n\n\ncoal_prod_per_capita\ndouble\nPer capita coal production, measured in kilowatt-hours\n\n\ncoal_production\ndouble\nCoal production, measured in terawatt-hours\n\n\ncoal_share_elec\ndouble\nShare of electricity generation that comes from coal\n\n\ncoal_share_energy\ndouble\nhare of primary energy consumption that comes from coal\n\n\nelectricity_demand\ndouble\nElectricity demand, measured in terawatt-hours\n\n\nelectricity_generation\ndouble\nElectricity generation, measured in terawatt-hours\n\n\nelectricity_share_energy\ndouble\nElectricity generation as a share of primary energy\n\n\nenergy_cons_change_pct\ndouble\nAnnual percentage change in primary energy consumption\n\n\nenergy_cons_change_twh\ndouble\nAnnual change in primary energy consumption, measured in terawatt-hours\n\n\nenergy_per_capita\ndouble\nPrimary energy consumption per capita, measured in kilowatt-hours\n\n\nenergy_per_gdp\ndouble\nEnergy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$\n\n\nfossil_cons_change_pct\ndouble\nAnnual percentage change in fossil fuel consumption\n\n\nfossil_cons_change_twh\ndouble\nAnnual change in fossil fuel consumption, measured in terawatt-hours\n\n\nfossil_elec_per_capita\ndouble\nPer capita electricity generation from fossil fuels, measured in kilowatt-hours. This is the sum of electricity generated from coal, oil and gas.\n\n\nfossil_electricity\ndouble\nElectricity generation from fossil fuels, measured in terawatt-hours. This is the sum of electricity generation from coal, oil and gas.\n\n\nfossil_energy_per_capita\ndouble\nPer capita fossil fuel consumption, measured in kilowatt-hours. This is the sum of primary energy from coal, oil and gas.\n\n\nfossil_fuel_consumption\ndouble\nFossil fuel consumption, measured in terawatt-hours. This is the sum of primary energy from coal, oil and gas.\n\n\nfossil_share_elec\ndouble\nShare of electricity generation that comes from fossil fuels (coal, oil and gas combined)\n\n\nfossil_share_energy\ndouble\nShare of primary energy consumption that comes from fossil fuels\n\n\ngas_cons_change_pct\ndouble\nAnnual percentage change in gas consumption\n\n\ngas_cons_change_twh\ndouble\nAnnual change in gas consumption, measured in terawatt-hours\n\n\ngas_consumption\ndouble\nPrimary energy consumption from gas, measured in terawatt-hours\n\n\ngas_elec_per_capita\ndouble\nPer capita electricity generation from gas, measured in kilowatt-hours\n\n\ngas_electricity\ndouble\nElectricity generation from gas, measured in terawatt-hours\n\n\ngas_energy_per_capita\ndouble\nPer capita primary energy consumption from gas, measured in kilowatt-hours\n\n\ngas_prod_change_pct\ndouble\nAnnual percentage change in gas production\n\n\ngas_prod_change_twh\ndouble\nAnnual change in gas production, measured in terawatt-hours\n\n\ngas_prod_per_capita\ndouble\nPer capita gas production, measured in kilowatt-hours\n\n\ngas_production\ndouble\nGas production, measured in terawatt-hours\n\n\ngas_share_elec\ndouble\nShare of electricity generation that comes from gas\n\n\ngas_share_energy\ndouble\nShare of primary energy consumption that comes from gas\n\n\ngreenhouse_gas_emissions\ndouble\nGreenhouse-gas emissions produced in the generation of electricity, measured in million tonnes of CO2 equivalent\n\n\nhydro_cons_change_pct\ndouble\nAnnual percentage change in hydropower consumption\n\n\nhydro_cons_change_twh\ndouble\nAnnual change in hydropower consumption, measured in terawatt-hours\n\n\nhydro_consumption\ndouble\nPrimary energy consumption from hydropower, measured in terawatt-hours\n\n\nhydro_elec_per_capita\ndouble\nPer capita electricity generation from hydropower, measured in kilowatt-hours\n\n\nhydro_electricity\ndouble\nElectricity generation from hydropower, measured in terawatt-hours\n\n\nhydro_energy_per_capita\ndouble\nPer capita primary energy consumption from hydropower, measured in kilowatt-hours\n\n\nhydro_share_elec\ndouble\nShare of electricity generation that comes from hydropower\n\n\nhydro_share_energy\ndouble\nShare of primary energy consumption that comes from hydropower\n\n\nlow_carbon_cons_change_pct\ndouble\nAnnual percentage change in low-carbon energy consumption\n\n\nlow_carbon_cons_change_twh\ndouble\nAnnual change in low-carbon energy consumption, measured in terawatt-hours\n\n\nlow_carbon_consumption\ndouble\nPrimary energy consumption from low-carbon sources, measured in terawatt-hours\n\n\nlow_carbon_elec_per_capita\ndouble\nPer capita electricity generation from low-carbon sources, measured in kilowatt-hours\n\n\nlow_carbon_electricity\ndouble\nElectricity generation from low-carbon sources, measured in terawatt-hours. This is the sum of electricity generation from renewables and nuclear power\n\n\nlow_carbon_energy_per_capita\ndouble\nPer capita primary energy consumption from low-carbon sources, measured in kilowatt-hours\n\n\nlow_carbon_share_elec\ndouble\nShare of electricity generation that comes from low-carbon sources. This is the sum of electricity from renewables and nuclear\n\n\nlow_carbon_share_energy\ndouble\nShare of primary energy consumption that comes from low-carbon sources. This is the sum of primary energy from renewables and nuclear\n\n\nnet_elec_imports\ndouble\nNet electricity imports, measured in terawatt-hours\n\n\nnet_elec_imports_share_demand\ndouble\nNet electricity imports as a share of electricity demand\n\n\nnuclear_cons_change_pct\ndouble\nAnnual percentage change in nuclear consumption\n\n\nnuclear_cons_change_twh\ndouble\nAnnual change in nuclear consumption, measured in terawatt-hours\n\n\nnuclear_consumption\ndouble\nPrimary energy consumption from nuclear power, measured in terawatt-hours\n\n\nnuclear_elec_per_capita\ndouble\nPer capita electricity generation from nuclear power, measured in kilowatt-hours\n\n\nnuclear_electricity\ndouble\nElectricity generation from nuclear power, measured in terawatt-hours\n\n\nnuclear_energy_per_capita\ndouble\nPer capita primary energy consumption from nuclear, measured in kilowatt-hours\n\n\nnuclear_share_elec\ndouble\nShare of electricity generation that comes from nuclear power\n\n\nnuclear_share_energy\ndouble\nShare of primary energy consumption that comes from nuclear power\n\n\noil_cons_change_pct\ndouble\nAnnual percentage change in oil consumption\n\n\noil_cons_change_twh\ndouble\nAnnual change in oil consumption, measured in terawatt-hours\n\n\noil_consumption\ndouble\nPrimary energy consumption from oil, measured in terawatt-hours\n\n\noil_elec_per_capita\ndouble\nPer capita electricity generation from oil, measured in kilowatt-hours\n\n\noil_electricity\ndouble\nElectricity generation from oil, measured in terawatt-hours\n\n\noil_energy_per_capita\ndouble\nPer capita primary energy consumption from oil, measured in kilowatt-hours\n\n\noil_prod_change_pct\ndouble\nAnnual percentage change in oil production\n\n\noil_prod_change_twh\ndouble\nAnnual change in oil production, measured in terawatt-hours\n\n\noil_prod_per_capita\ndouble\nPer capita oil production, measured in kilowatt-hours\n\n\noil_production\ndouble\nOil production, measured in terawatt-hours\n\n\noil_share_elec\ndouble\nShare of electricity generation that comes from oil\n\n\noil_share_energy\ndouble\nShare of primary energy consumption that comes from oil\n\n\nother_renewable_consumption\ndouble\nPrimary energy consumption from other renewables, measured in terawatt-hours\n\n\nother_renewable_electricity\ndouble\nElectricity generation from other renewable sources including biofuels, measured in terawatt-hours\n\n\nother_renewable_exc_biofuel_electricity\ndouble\nElectricity generation from other renewable sources excluding biofuels, measured in terawatt-hours\n\n\nother_renewables_cons_change_pct\ndouble\nAnnual percentage change in energy consumption from other renewables\n\n\nother_renewables_cons_change_twh\ndouble\nAnnual change in other renewable consumption, measured in terawatt-hours\n\n\nother_renewables_elec_per_capita\ndouble\nPer capita electricity generation from other renewables including biofuels, measured in kilowatt-hours\n\n\nother_renewables_elec_per_capita_exc_biofuel\ndouble\nPer capita electricity generation from other renewables excluding biofuels, measured in kilowatt-hours\n\n\nother_renewables_energy_per_capita\ndouble\nPer capita primary energy consumption from other renewables, measured in kilowatt-hours\n\n\nother_renewables_share_elec\ndouble\nShare of electricity generation that comes from other renewables including biofuels\n\n\nother_renewables_share_elec_exc_biofuel\ndouble\nShare of electricity generation that comes from other renewables excluding biofuels\n\n\nother_renewables_share_energy\ndouble\nShare of primary energy consumption that comes from other renewables\n\n\nper_capita_electricity\ndouble\nElectricity generation per capita, measured in kilowatt-hours\n\n\nprimary_energy_consumption\ndouble\nPrimary energy consumption, measured in terawatt-hours\n\n\nrenewables_cons_change_pct\ndouble\nAnnual percentage change in renewable energy consumption\n\n\nrenewables_cons_change_twh\ndouble\nAnnual change in renewable energy consumption, measured in terawatt-hours\n\n\nrenewables_consumption\ndouble\nPrimary energy consumption from renewables, measured in terawatt-hours\n\n\nrenewables_elec_per_capita\ndouble\nPer capita electricity generation from renewables, measured in kilowatt-hours\n\n\nrenewables_electricity\ndouble\nElectricity generation from renewables, measured in terawatt-hours\n\n\nrenewables_energy_per_capita\ndouble\nPer capita primary energy consumption from renewables, measured in kilowatt-hours\n\n\nrenewables_share_elec\ndouble\nShare of electricity generation that comes from renewables\n\n\nrenewables_share_energy\ndouble\nShare of primary energy consumption that comes from renewables\n\n\nsolar_cons_change_pct\ndouble\nAnnual percentage change in solar consumption\n\n\nsolar_cons_change_twh\ndouble\nAnnual change in solar consumption, measured in terawatt-hours\n\n\nsolar_consumption\ndouble\nPrimary energy consumption from solar, measured in terawatt-hours\n\n\nsolar_elec_per_capita\ndouble\nPer capita electricity generation from solar, measured in kilowatt-hours\n\n\nsolar_electricity\ndouble\nElectricity generation from solar, measured in terawatt-hours\n\n\nsolar_energy_per_capita\ndouble\nPer capita primary energy consumption from solar, measured in kilowatt-hours\n\n\nsolar_share_elec\ndouble\nShare of electricity generation that comes from solar\n\n\nsolar_share_energy\ndouble\nShare of primary energy consumption that comes from solar\n\n\nwind_cons_change_pct\ndouble\nAnnual percentage change in wind consumption\n\n\nwind_cons_change_twh\ndouble\nAnnual change in wind consumption\n\n\nwind_consumption\ndouble\nPrimary energy consumption from wind, measured in terawatt-hours\n\n\nwind_elec_per_capita\ndouble\nPer capita electricity generation from wind, measured in kilowatt-hours\n\n\nwind_electricity\ndouble\nElectricity generation from wind, measured in terawatt-hours\n\n\nwind_energy_per_capita\ndouble\nPer capita primary energy consumption from wind, measured in kilowatt-hours\n\n\nwind_share_elec\ndouble\nShare of electricity generation that comes from wind\n\n\nwind_share_energy\ndouble\nShare of primary energy consumption that comes from wind\n\n\n\n\nCleaning Script\nData cleaning was done by Our World in Data"
  },
  {
    "objectID": "data/2023/2023-05-23/readme.html",
    "href": "data/2023/2023-05-23/readme.html",
    "title": "Central Park Squirrel Census",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nCentral Park Squirrel Census\nSquirrel data! The data this week comes from the 2018 Central Park Squirrel Census.\n\nThe Squirrel Census is a multimedia science, design, and storytelling project focusing on the Eastern gray (Sciurus carolinensis). They count squirrels and present their findings to the public. The dataset contains squirrel data for each of the 3,023 sightings, including location coordinates, age, primary and secondary fur color, elevation, activities, communications, and interactions between squirrels and with humans.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-05-23')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 21)\n\nsquirrel_data &lt;- tuesdata$squirrel_data\n\n# Or read in the data manually\n\nsquirrel_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-23/squirrel_data.csv')\n\n\nData Dictionary\n\n\n\nsquirrel_data.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nX\ndouble\nLongitude coordinate for squirrel sighting point\n\n\nY\ndouble\nLatitude coordinate for squirrel sighting point\n\n\nUnique Squirrel ID\ncharacter\nIdentification tag for each squirrel sightings. The tag is comprised of “Hectare ID” + “Shift” + “Date” + “Hectare Squirrel Number.”\n\n\nHectare\ncharacter\nID tag, which is derived from the hectare grid used to divide and count the park area. One axis that runs predominantly north-to-south is numerical (1-42), and the axis that runs predominantly east-to-west is roman characters (A-I).\n\n\nShift\ncharacter\nValue is either “AM” or “PM,” to communicate whether or not the sighting session occurred in the morning or late afternoon.\n\n\nDate\ndouble\nConcatenation of the sighting session day and month.\n\n\nHectare Squirrel Number\ndouble\nNumber within the chronological sequence of squirrel sightings for a discrete sighting session.\n\n\nAge\ncharacter\nValue is either “Adult” or “Juvenile.”\n\n\nPrimary Fur Color\ncharacter\nPrimary Fur Color - value is either “Gray,” “Cinnamon” or “Black.”\n\n\nHighlight Fur Color\ncharacter\nDiscrete value or string values comprised of “Gray,” “Cinnamon” or “Black.”\n\n\nCombination of Primary and Highlight Color\ncharacter\nA combination of the previous two columns; this column gives the total permutations of primary and highlight colors observed.\n\n\nColor notes\ncharacter\nSighters occasionally added commentary on the squirrel fur conditions. These notes are provided here.\n\n\nLocation\ncharacter\nValue is either “Ground Plane” or “Above Ground.” Sighters were instructed to indicate the location of where the squirrel was when first sighted.\n\n\nAbove Ground Sighter Measurement\ncharacter\nFor squirrel sightings on the ground plane, fields were populated with a value of “FALSE.”\n\n\nSpecific Location\ncharacter\nSighters occasionally added commentary on the squirrel location. These notes are provided here.\n\n\nRunning\nlogical\nSquirrel was seen running.\n\n\nChasing\nlogical\nSquirrel was seen chasing another squirrel.\n\n\nClimbing\nlogical\nSquirrel was seen climbing a tree or other environmental landmark.\n\n\nEating\nlogical\nSquirrel was seen eating.\n\n\nForaging\nlogical\nSquirrel was seen foraging for food.\n\n\nOther Activities\ncharacter\nOther activities squirrels were observed doing.\n\n\nKuks\nlogical\nSquirrel was heard kukking, a chirpy vocal communication used for a variety of reasons.\n\n\nQuaas\nlogical\nSquirrel was heard quaaing, an elongated vocal communication which can indicate the presence of a ground predator such as a dog.\n\n\nMoans\nlogical\nSquirrel was heard moaning, a high-pitched vocal communication which can indicate the presence of an air predator such as a hawk.\n\n\nTail flags\nlogical\nSquirrel was seen flagging its tail. Flagging is a whipping motion used to exaggerate squirrel’s size and confuse rivals or predators. Looks as if the squirrel is scribbling with tail into the air.\n\n\nTail twitches\nlogical\nSquirrel was seen twitching its tail. Looks like a wave running through the tail, like a breakdancer doing the arm wave. Often used to communicate interest, curiosity.\n\n\nApproaches\nlogical\nSquirrel was seen approaching human, seeking food.\n\n\nIndifferent\nlogical\nSquirrel was indifferent to human presence.\n\n\nRuns from\nlogical\nSquirrel was seen running from humans, seeing them as a threat.\n\n\nOther Interactions\ncharacter\nSighter notes on other types of interactions between squirrels and humans.\n\n\nLat/Long\ncharacter\nLatitude and longitude\n\n\n\n\nCleaning Script\nNo data cleaning"
  },
  {
    "objectID": "data/2023/2023-05-09/readme.html",
    "href": "data/2023/2023-05-09/readme.html",
    "title": "Childcare Costs",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nChildcare Costs\nHappy Mothers Day! The data this week comes from the National Database of Childcare Prices.\n\nThe National Database of Childcare Prices (NDCP) is the most comprehensive federal source of childcare prices at the county level. The database offers childcare price data by childcare provider type, age of children, and county characteristics. Data are available from 2008 to 2018.\n\nThanks this week to Thomas Mock for the submission, with a hat tip to Jon Schwabish on Twitter for pointing out the lack of labels on the original government-posted map.\nNote: This dataset implies that “both parents” means one man and one woman. We recognize that this does not reflect the reality of every loving family.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-05-09')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 19)\n\nchildcare_costs &lt;- tuesdata$childcare_costs\ncounties &lt;- tuesdata$counties\n\n# Or read in the data manually\n\nchildcare_costs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-09/childcare_costs.csv')\ncounties &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-09/counties.csv')\n\n\nData Dictionary\n\n\n\nchildcare_costs.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncounty_fips_code\ndouble\nFour- or five-digit number that uniquely identifies the county in a state. The first two digits (for five-digit numbers) or 1 digit (for four-digit numbers) refer to the FIPS code of the state to which the county belongs.\n\n\nstudy_year\ndouble\nYear the data collection began for the market rate survey and in which ACS data is representative of, or the study publication date.\n\n\nunr_16\ndouble\nUnemployment rate of the population aged 16 years old or older.\n\n\nfunr_16\ndouble\nUnemployment rate of the female population aged 16 years old or older.\n\n\nmunr_16\ndouble\nUnemployment rate of the male population aged 16 years old or older.\n\n\nunr_20to64\ndouble\nUnemployment rate of the population aged 20 to 64 years old.\n\n\nfunr_20to64\ndouble\nUnemployment rate of the female population aged 20 to 64 years old.\n\n\nmunr_20to64\ndouble\nUnemployment rate of the male population aged 20 to 64 years old.\n\n\nflfpr_20to64\ndouble\nLabor force participation rate of the female population aged 20 to 64 years old.\n\n\nflfpr_20to64_under6\ndouble\nLabor force participation rate of the female population aged 20 to 64 years old who have children under 6 years old.\n\n\nflfpr_20to64_6to17\ndouble\nLabor force participation rate of the female population aged 20 to 64 years old who have children between 6 and 17 years old.\n\n\nflfpr_20to64_under6_6to17\ndouble\nLabor force participation rate of the female population aged 20 to 64 years old who have children under 6 years old and between 6 and 17 years old.\n\n\nmlfpr_20to64\ndouble\nLabor force participation rate of the male population aged 20 to 64 years old.\n\n\npr_f\ndouble\nPoverty rate for families.\n\n\npr_p\ndouble\nPoverty rate for individuals.\n\n\nmhi_2018\ndouble\nMedian household income expressed in 2018 dollars.\n\n\nme_2018\ndouble\nMedian earnings expressed in 2018 dollars for the population aged 16 years old or older.\n\n\nfme_2018\ndouble\nMedian earnings for females expressed in 2018 dollars for the population aged 16 years old or older.\n\n\nmme_2018\ndouble\nMedian earnings for males expressed in 2018 dollars for the population aged 16 years old or older.\n\n\ntotal_pop\ndouble\nCount of the total population.\n\n\none_race\ndouble\nPercent of population that identifies as being one race.\n\n\none_race_w\ndouble\nPercent of population that identifies as being one race and being only White or Caucasian.\n\n\none_race_b\ndouble\nPercent of population that identifies as being one race and being only Black or African American.\n\n\none_race_i\ndouble\nPercent of population that identifies as being one race and being only American Indian or Alaska Native.\n\n\none_race_a\ndouble\nPercent of population that identifies as being one race and being only Asian.\n\n\none_race_h\ndouble\nPercent of population that identifies as being one race and being only Native Hawaiian or Pacific Islander.\n\n\none_race_other\ndouble\nPercent of population that identifies as being one race and being a different race not previously mentioned.\n\n\ntwo_races\ndouble\nPercent of population that identifies as being two or more races.\n\n\nhispanic\ndouble\nPercent of population that identifies as being Hispanic or Latino regardless of race.\n\n\nhouseholds\ndouble\nNumber of households.\n\n\nh_under6_both_work\ndouble\nNumber of households with children under 6 years old with two parents that are both working.\n\n\nh_under6_f_work\ndouble\nNumber of households with children under 6 years old with two parents with only the father working.\n\n\nh_under6_m_work\ndouble\nNumber of households with children under 6 years old with two parents with only the mother working.\n\n\nh_under6_single_m\ndouble\nNumber of households with children under 6 years old with a single mother.\n\n\nh_6to17_both_work\ndouble\nNumber of households with children between 6 and 17 years old with two parents that are both working.\n\n\nh_6to17_fwork\ndouble\nNumber of households with children between 6 and 17 years old with two parents with only the father working.\n\n\nh_6to17_mwork\ndouble\nNumber of households with children between 6 and 17 years old with two parents with only the mother working.\n\n\nh_6to17_single_m\ndouble\nNumber of households with children between 6 and 17 years old with a single mother.\n\n\nemp_m\ndouble\nPercent of civilians employed in management, business, science, and arts occupations aged 16 years old or older in the county.\n\n\nmemp_m\ndouble\nPercent of male civilians employed in management, business, science, and arts occupations aged 16 years old or older in the county.\n\n\nfemp_m\ndouble\nPercent of female civilians employed in management, business, science, and arts occupations aged 16 years old or older in the county.\n\n\nemp_service\ndouble\nPercent of civilians employed in service occupations aged 16 years old and older in the county.\n\n\nmemp_service\ndouble\nPercent of male civilians employed in service occupations aged 16 years old and older in the county.\n\n\nfemp_service\ndouble\nPercent of female civilians employed in service occupations aged 16 years old and older in the county.\n\n\nemp_sales\ndouble\nPercent of civilians employed in sales and office occupations aged 16 years old and older in the county.\n\n\nmemp_sales\ndouble\nPercent of male civilians employed in sales and office occupations aged 16 years old and older in the county.\n\n\nfemp_sales\ndouble\nPercent of female civilians employed in sales and office occupations aged 16 years old and older in the county.\n\n\nemp_n\ndouble\nPercent of civilians employed in natural resources, construction, and maintenance occupations aged 16 years old and older in the county.\n\n\nmemp_n\ndouble\nPercent of male civilians employed in natural resources, construction, and maintenance occupations aged 16 years old and older in the county.\n\n\nfemp_n\ndouble\nPercent of female civilians employed in natural resources, construction, and maintenance occupations aged 16 years old and older in the county.\n\n\nemp_p\ndouble\nPercent of civilians employed in production, transportation, and material moving occupations aged 16 years old and older in the county.\n\n\nmemp_p\ndouble\nPercent of male civilians employed in production, transportation, and material moving occupations aged 16 years old and older in the county.\n\n\nfemp_p\ndouble\nPercent of female civilians employed in production, transportation, and material moving occupations aged 16 years old and older in the county.\n\n\nmcsa\ndouble\nWeekly, full-time median price charged for Center-Based Care for those who are school age based on the results reported in the market rate survey report for the county or the rate zone/cluster to which the county is assigned.\n\n\nmfccsa\ndouble\nWeekly, full-time median price charged for Family Childcare for those who are school age based on the results reported in the market rate survey report for the county or the rate zone/cluster to which the county is assigned.\n\n\nmc_infant\ndouble\nAggregated weekly, full-time median price charged for Center-based Care for infants (i.e. aged 0 through 23 months).\n\n\nmc_toddler\ndouble\nAggregated weekly, full-time median price charged for Center-based Care for toddlers (i.e. aged 24 through 35 months).\n\n\nmc_preschool\ndouble\nAggregated weekly, full-time median price charged for Center-based Care for preschoolers (i.e. aged 36 through 54 months).\n\n\nmfcc_infant\ndouble\nAggregated weekly, full-time median price charged for Family Childcare for infants (i.e. aged 0 through 23 months).\n\n\nmfcc_toddler\ndouble\nAggregated weekly, full-time median price charged for Family Childcare for toddlers (i.e. aged 24 through 35 months).\n\n\nmfcc_preschool\ndouble\nAggregated weekly, full-time median price charged for Family Childcare for preschoolers (i.e. aged 36 through 54 months).\n\n\n\n\n\ncounties.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncounty_fips_code\ndouble\nFour- or five-digit number that uniquely identifies the county in a state. The first two digits (for five-digit numbers) or 1 digit (for four-digit numbers) refer to the FIPS code of the state to which the county belongs.\n\n\ncounty_name\ncharacter\nThe full name of the county.\n\n\nstate_name\ncharacter\nThe full name of the state in which the county is found.\n\n\nstate_abbreviation\ncharacter\nThe two-letter state abbreviation.\n\n\n\n\nCleaning Script\n# All packages used in this script:\nlibrary(tidyverse)\nlibrary(here)\nlibrary(withr)\n\nurl &lt;- \"https://www.dol.gov/sites/dolgov/files/WB/media/nationaldatabaseofchildcareprices.xlsx\"\ntemp_xlsx &lt;- withr::local_tempfile(fileext = \".xlsx\")\ndownload.file(url, temp_xlsx, mode = \"wb\")\n\nchildcare_costs_raw &lt;- readxl::read_xlsx(temp_xlsx) |&gt;\n  janitor::clean_names() |&gt; \n  # There are 15 constant columns. Get rid of those.\n  janitor::remove_constant(quiet = FALSE)\n\n# The file is very large, but it contains a lot of duplicate data. Extract\n# duplications into their own tables.\ncounties &lt;- childcare_costs_raw |&gt; \n  dplyr::distinct(county_fips_code, county_name, state_name, state_abbreviation)\nchildcare_costs &lt;- childcare_costs_raw |&gt; \n  dplyr::select(\n    -county_name,\n    -state_name,\n    -state_abbreviation,\n    # Original data also contained unadjusted + adjusted dollars, let's just\n    # keep the 2018 adjustments.\n    -mhi, -me, -fme, -mme,\n    # A number of columns have fine-grained breakdowns by age, and then also\n    # broader categories. Let's only keep the categories (\"infant\" vs 0-5\n    # months, 6-11 monts, etc)\n    -ends_with(\"bto5\"), -ends_with(\"6to11\"), -ends_with(\"12to17\"), \n    -ends_with(\"18to23\"), -ends_with(\"24to29\"), -ends_with(\"30to35\"),\n    -ends_with(\"36to41\"), -ends_with(\"42to47\"), -ends_with(\"48to53\"),\n    -ends_with(\"54to_sa\"),\n    # Since we aren't worrying about the unaggregated columns, we can ignore the\n    # flags indicating how those columns were aggregated into the combined\n    # columns.\n    -ends_with(\"_flag\"),\n    # Original data has both median and 75th percentile for a number of columns.\n    # We'll simplify.\n    -starts_with(\"x75\"),\n    # While important for wider research, we don't need to keep the (many)\n    # variables describing whether certain data was imputed.\n    -starts_with(\"i_\")\n  )\n\nreadr::write_csv(\n  childcare_costs,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-05-09\",\n    \"childcare_costs.csv\"\n  )\n)\n\nreadr::write_csv(\n  counties,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-05-09\",\n    \"counties.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-04-25/readme.html",
    "href": "data/2023/2023-04-25/readme.html",
    "title": "London Marathon",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nLondon Marathon\nThe data this week comes from Nicola Rennie’s LondonMarathon R package. This is an R package containing two data sets scraped from Wikipedia (1 November 2022) on London Marathon winners, and some general data. How the dataset was created, and some analysis, is described in Nicola’s post “Scraping London Marathon data with {rvest}”. Thank you for putting this dataset together @nrennie!\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-04-25')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 17)\n\nwinners &lt;- tuesdata$winners\nlondon_marathon &lt;- tuesdata$london_marathon\n\n\n# Or read in the data manually\n\nwinners &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-04-25/winners.csv')\nlondon_marathon &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-04-25/london_marathon.csv')\n\n\n\nData Dictionary\n\n\n\nwinners.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCategory\ncharacter\nCategory of race\n\n\nYear\ndouble\nYear\n\n\nAthlete\ncharacter\nName of the winner\n\n\nNationality\ncharacter\nNationality of the winner\n\n\nTime\ndouble\nWinning time\n\n\n\n\n\nlondon_marathon.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nDate\ndouble\nDate of the race\n\n\nYear\ndouble\nYear\n\n\nApplicants\ndouble\nNumber of people who applied\n\n\nAccepted\ndouble\nNumber of people accepted\n\n\nStarters\ndouble\nNumber of people who started\n\n\nFinishers\ndouble\nNumber of people who finished\n\n\nRaised\ndouble\nAmount raised for charity (£ millions)\n\n\nOfficial charity\ncharacter\nOfficial charity\n\n\n\n\nCleaning Script\nNo data cleaning"
  },
  {
    "objectID": "data/2023/2023-04-11/readme.html",
    "href": "data/2023/2023-04-11/readme.html",
    "title": "US Egg Production",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nUS Egg Production\nThe data this week comes from The Humane League’s US Egg Production dataset by Samara Mendez. Dataset and code is available for this project on OSF at US Egg Production Data Set.\nThis dataset tracks the supply of cage-free eggs in the United States from December 2007 to February 2021. For TidyTuesday we’ve used data through February 2021, but the full dataset, with data through the present, is available in the OSF project.\n\nIn this project, they synthesize an analysis-ready data set that tracks cage-free hens and the supply of cage-free eggs relative to the overall numbers of hens and table eggs in the United States. The data set is based on reports produced by the United States Department of Agriculture (USDA), which are published weekly or monthly. They supplement these data with definitions and a taxonomy of egg products drawn from USDA and industry publications. The data include flock size (both absolute and relative) and egg production of cage-free hens as well as all table-egg-laying hens in the US, collected to understand the impact of the industry’s cage-free transition on hens. Data coverage ranges from December 2007 to February 2021.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-04-11')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 15)\n\neggproduction &lt;- tuesdata$`egg-production`\ncagefreepercentages &lt;- tuesdata$`cage-free-percentages`\n\n\n# Or read in the data manually\n\neggproduction  &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-04-11/egg-production.csv')\ncagefreepercentages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-04-11/cage-free-percentages.csv')\n\n\nData Dictionary\n\n\n\negg-production.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nobserved_month\ndouble\nMonth in which report observations are collected,Dates are recorded in ISO 8601 format YYYY-MM-DD\n\n\nprod_type\ncharacter\ntype of egg product: hatching, table eggs\n\n\nprod_process\ncharacter\ntype of production process and housing: cage-free (organic), cage-free (non-organic), all. The value ‘all’ includes cage-free and conventional housing.\n\n\nn_hens\ndouble\nnumber of hens produced by hens for a given month-type-process combo\n\n\nn_eggs\ndouble\nnumber of eggs producing eggs for a given month-type-process combo\n\n\nsource\ncharacter\nOriginal USDA report from which data are sourced. Values correspond to titles of PDF reports. Date of report is included in title.\n\n\n\n\n\ncage-free-percentages.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nobserved_month\ndouble\nMonth in which report observations are collected,Dates are recorded in ISO 8601 format YYYY-MM-DD\n\n\npercent_hens\ndouble\nobserved or computed percentage of cage-free hens relative to all table-egg-laying hens\n\n\npercent_eggs\ndouble\ncomputed percentage of cage-free eggs relative to all table eggs,This variable is not available for data sourced from the Egg Markets Overview report\n\n\nsource\ncharacter\nOriginal USDA report from which data are sourced. Values correspond to titles of PDF reports. Date of report is included in title.\n\n\n\n\nCleaning Script\nThis data was already cleaned for the report. Raw data is also available at US Egg Production Dataset."
  },
  {
    "objectID": "data/2023/2023-03-28/readme.html",
    "href": "data/2023/2023-03-28/readme.html",
    "title": "Time Zones",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nTime Zones\nThe data this week comes from the IANA tz database via the {clock} and {tzdb} packages. Special thanks to Davis Vaughan for the assist in preparing this data!\nMany websites operate using the data in the IANA tz database. “What Is Daylight Saving Time” from timeanddate.com is a good place to start to find interesting information about time zones, such as the strange case of Lord Howe Island, Australia.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-03-28')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 13)\n\ntransitions &lt;- tuesdata$transitions\ntimezones &lt;- tuesdata$timezones\ntimezone_countries &lt;- tuesdata$timezone_countries\ncountries &lt;- tuesdata$countries\n\n# Or read in the data manually\n\ntransitions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-03-28/transitions.csv')\ntimezones &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-03-28/timezones.csv')\ntimezone_countries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-03-28/timezone_countries.csv')\ncountries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-03-28/countries.csv')\n\n\nData Dictionary\n\n\n\ntransitions.csv\nChanges in the conversion of a given time zone to UTC (for example for daylight savings or because the definition of the time zone changed).\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nzone\ncharacter\nThe name of the time zone.\n\n\nbegin\ncharacter\nWhen this definition went into effect, in UTC. Tip: convert to a datetime using lubridate::as_datetime().\n\n\nend\ncharacter\nWhen this definition ended (and the next definition went into effect), in UTC. Tip: convert to a datetime using lubridate::as_datetime().\n\n\noffset\ndouble\nThe offset of this time zone from UTC, in seconds.\n\n\ndst\nlogical\nWhether daylight savings time is active within this definition.\n\n\nabbreviation\ncharacter\nThe time zone abbreviation in use throughout this begin to end range.\n\n\n\n\n\ntimezones.csv\nDescriptions of time zones from the IANA time zone database.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nzone\ncharacter\nThe name of the time zone.\n\n\nlatitude\ndouble\nLatitude of the time zone’s “principal location.”\n\n\nlongitude\ndouble\nLongitude of the time zone’s “principal location.”\n\n\ncomments\ncharacter\nComments from the tzdb definition file.\n\n\n\n\n\ntimezone_countries.csv\nCountries (or other place names) that overlap with each time zone.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nzone\ncharacter\nThe name of the time zone.\n\n\ncountry_code\ncharacter\nThe ISO 3166-1 alpha-2 2-character country code.\n\n\n\n\n\ncountries.csv\nNames of countries and other places.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry_code\ncharacter\nThe ISO 3166-1 alpha-2 2-character country code.\n\n\nplace_name\ncharacter\nThe usual English name for the coded region, chosen so that alphabetic sorting of subsets produces helpful lists. This is not the same as the English name in the ISO 3166 tables.\n\n\n\n\nCleaning Script\n# All packages used in this script:\nlibrary(tidyverse)\nlibrary(clock)\nlibrary(vctrs)\nlibrary(tzdb)\nlibrary(fs)\nlibrary(here)\n\n# Code for extracting transitions provided by David Vaughan at\n# https://gist.github.com/DavisVaughan/95a3bdb6b11e8e8f00ac671384461c72\n# Slightly adapted for use in TidyTuesday.\n\ngenerate_transitions &lt;- function(zone,\n                                 from = date_build(1900, 1, 1),\n                                 to = date_build(2030, 1, 1)) {\n  days &lt;- date_seq(\n    from = from,\n    to = to,\n    by = 1\n  )\n  \n  days &lt;- as_sys_time(days)\n  info &lt;- sys_time_info(days, zone = zone)\n  \n  # Must choose a unified transition time zone to use for display\n  info$begin &lt;- as_date_time(info$begin, zone = \"UTC\")\n  info$end &lt;- as_date_time(info$end, zone = \"UTC\")\n  info$offset &lt;- as.double(info$offset)\n  \n  info &lt;- vec_unique(info)\n  \n  info\n}\n\nzones &lt;- tzdb_names()\nzones &lt;- vec_set_names(zones, zones)\n\ntransitions &lt;- map(zones, generate_transitions)\ntransitions &lt;- list_rbind(transitions, names_to = \"zone\")\ntransitions &lt;- as_tibble(transitions)\n\n# All transitions (as long as more than one didn't happen within the same day)\ntransitions\n\n\n# Add some additional information from the tzdb package.\n\ntzdata &lt;- tzdb::tzdb_path(\"text\")\n\n# zone1970 lists time zones that have been agreed upon since at least 1970. Many\n# lines don't have comments, so expect warnings.\nzone1970 &lt;- read_tsv(\n  fs::path(tzdb::tzdb_path(\"text\"), \"zone1970.tab\"),\n  skip = 34,\n  col_names = c(\n    \"country_code\",\n    \"coordinates\",\n    \"zone\",\n    \"comments\"\n  ),\n  col_types = \"cccc\"\n) |&gt;\n  # There's a comment in the middle of the table.\n  filter(!str_starts(country_code, \"#\")) |&gt; \n  # Break coordinates into lat/long columns. They're in \"ISO 6709\n  # sign-degrees-minutes-seconds format, either ±DDMM±DDDMM or ±DDMMSS±DDDMMSS,\n  # first latitude (+ is north), then longitude (+ is east).\" This is a good\n  # opportunity to practice the new tidyr::separate_wider_regex function!\n  # There's probably a function in a spatial package somewhere to deal with\n  # this, but I'll work it out by hand.\n  separate_wider_regex(\n    coordinates,\n    patterns = c(\n      latitude = \"[+-]\\\\d+\",\n      longitude = \"[+-]\\\\d+\"\n    )\n  ) |&gt; \n  separate_wider_regex(\n    latitude,\n    patterns = c(\n      lat_sign = \"[+-]\",\n      lat_deg = \"\\\\d{2}\",\n      lat_min = \"\\\\d{2}\",\n      lat_sec = \"\\\\d*\"\n    ),\n    cols_remove = FALSE\n  ) |&gt; \n  mutate(\n    lat_sec = as.integer(lat_sec),\n    lat_sec = replace_na(lat_sec, 0),\n    latitude = as.integer(paste0(lat_sign, 1)) * (\n      as.integer(lat_deg) + as.integer(lat_min)/60 + lat_sec/60\n    )\n  ) |&gt; \n  separate_wider_regex(\n    longitude,\n    patterns = c(\n      long_sign = \"[+-]\",\n      long_deg = \"\\\\d{3}\",\n      long_min = \"\\\\d{2}\",\n      long_sec = \"\\\\d*\"\n    ),\n    cols_remove = FALSE\n  ) |&gt; \n  mutate(\n    long_sec = as.integer(long_sec),\n    long_sec = replace_na(long_sec, 0),\n    longitude = as.integer(paste0(long_sign, 1)) * (\n      as.integer(long_deg) + as.integer(long_min)/60 + long_sec/60\n    )\n  ) |&gt; \n  select(\n    -starts_with(\"lat_\"),\n    -starts_with(\"long_\")\n  )\n\n# Let's normalize that dataset a little by splitting it.\ntimezones &lt;- zone1970 |&gt; \n  select(zone, latitude, longitude, comments) |&gt; \n  arrange(zone)\ntimezone_countries &lt;- zone1970 |&gt; \n  select(zone, country_code) |&gt; \n  tidyr::separate_longer_delim(country_code, \",\") |&gt; \n  arrange(zone)\n\ncountries &lt;- read_tsv(\n  fs::path(tzdb::tzdb_path(\"text\"), \"iso3166.tab\"),\n  skip = 34,\n  col_names = c(\n    \"country_code\",\n    \"place_name\"\n  ),\n  col_types = \"cc\"\n)\n\ncolnames(transitions)\ncolnames(timezones)\ncolnames(timezone_countries)\ncolnames(countries)\n\nwrite_csv(\n  transitions,\n  here(\n    \"data\",\n    \"2023\",\n    \"2023-03-28\",\n    \"transitions.csv\"\n  )\n)\nwrite_csv(\n  timezones,\n  here(\n    \"data\",\n    \"2023\",\n    \"2023-03-28\",\n    \"timezones.csv\"\n  )\n)\nwrite_csv(\n  timezone_countries,\n  here(\n    \"data\",\n    \"2023\",\n    \"2023-03-28\",\n    \"timezone_countries.csv\"\n  )\n)\nwrite_csv(\n  countries,\n  here(\n    \"data\",\n    \"2023\",\n    \"2023-03-28\",\n    \"countries.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-03-14/readme.html",
    "href": "data/2023/2023-03-14/readme.html",
    "title": "European Drug Development",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nEuropean Drug Development\nThe data this week comes from the European Medicines Agency via Miquel Anglada Girotto on GitHub. We used the source table of all EPARs for human and veterinary medicines, rather than Miquel’s scraped data.\nMiquel wrote about his exploration of the data.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-03-14')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 11)\n\ndrugs &lt;- tuesdata$drugs\n\n# Or read in the data manually\n\ndrugs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-03-14/drugs.csv')\n\n\nData Dictionary\n\n\n\ndrugs.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncategory\ncharacter\nhuman or veterinary\n\n\nmedicine_name\ncharacter\nbrand name of the medicine\n\n\ntherapeutic_area\ncharacter\nsemicolon-separated list of therapeutic areas; might benefit from further cleaning\n\n\ncommon_name\ncharacter\ninternational non-proprietary name (INN) or common name\n\n\nactive_substance\ncharacter\ncommon name of the active chemical in the drug\n\n\nproduct_number\ncharacter\nEMEA/H/C/ (human) or EMEA/V/C/ (veterinary) number for the drug\n\n\npatient_safety\nlogical\nhas patient safety notices\n\n\nauthorisation_status\ncharacter\nwhether the drug was authorised, withdrawn, or refused (or does not have a reported status)\n\n\natc_code\ncharacter\nanatomical therapeutic chemical code\n\n\nadditional_monitoring\nlogical\nwhen true, the medicine is under additional monitoring, meaning that it is monitored even more intensively than other medicines\n\n\ngeneric\nlogical\nwhether the drug is a generic medicine, which is developed to be the same as a medicine that has already been authorised, called the reference medicine. A generic medicine contains the same active substance(s) as the reference medicine, and is used at the same dose(s) to treat the same disease(s)\n\n\nbiosimilar\nlogical\nwhether the drug is a biosimilar medicine, which is a biological medicine highly similar to another already approved biological medicine called the reference medicine.\n\n\nconditional_approval\nlogical\nwhether the medicine received a conditional marketing authorisation. This was granted in the interest of public health because the medicine addresses an unmet medical need and the benefit of immediate availability outweighs the risk from less comprehensive data than normally required.\n\n\nexceptional_circumstances\nlogical\nwhether the medicine was authorised under exceptional circumstances, because the applicant was unable to provide comprehensive data on the efficacy and safety of the medicine under normal conditions of use. This can happen because the condition to be treated is rare or because collection of full information is not possible or is unethical.\n\n\naccelerated_assessment\nlogical\nwhether the medicine had an accelerated assessment. This means that it is a medicine of major interest for public health, so its timeframe for review was 150 evaluation days rather than 210.\n\n\norphan_medicine\nlogical\nwhether the medicine was designated an orphan medicine. This means that it was developed for use against a rare, life-threatening or chronically debilitating condition or, for economic reasons, it would be unlikely to have been developed without incentives.\n\n\nmarketing_authorisation_date\ndate\ndate on which the drug was authorised\n\n\ndate_of_refusal_of_marketing_authorisation\ndate\ndate on which the drug was refused\n\n\nmarketing_authorisation_holder_company_name\ncharacter\nname of the company that is authorised to market the drug\n\n\npharmacotherapeutic_group\ncharacter\nthe target of the drug; might benefit from further cleaning\n\n\ndate_of_opinion\ndate\ndate on which the opinion was made\n\n\ndecision_date\ndate\ndate on which the latest decision was made\n\n\nrevision_number\ninteger\ninteger revision number\n\n\ncondition_indication\ncharacter\nlanguage describing the specific uses of the drug\n\n\nspecies\ncharacter\nfor veterinary medicines, the target species; might benefit from further cleaning\n\n\nfirst_published\ndatetime\ndatetime when the information was first published\n\n\nrevision_date\ndatetime\ndatetime of the most recent revision\n\n\nurl\ncharacter\nurl for details about the drug and submission\n\n\n\n\nCleaning Script\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(here)\n\n# xlsx file downloaded from https://www.ema.europa.eu/en/medicines/download-medicine-data\n\ndrugs &lt;- readxl::read_excel(\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-03-14\",\n    \"Medicines_output_european_public_assessment_reports.xlsx\"\n  ),\n  skip = 8\n) |&gt; \n  clean_names() |&gt; \n  mutate(\n    category = factor(tolower(category)),\n    # product_number is based on whether it's human or veterinary, so get rid of\n    # the extras.\n    product_number = str_extract(product_number, \"\\\\d+$\"),\n    \n    # These two are often only different in capitalization.\n    international_non_proprietary_name_inn_common_name = tolower(\n      international_non_proprietary_name_inn_common_name\n    ),\n    active_substance = tolower(active_substance),\n    \n    # These are really just dates that got meaningless hours attached.\n    marketing_authorisation_date = lubridate::date(\n      marketing_authorisation_date\n    ),\n    date_of_refusal_of_marketing_authorisation = lubridate::date(\n      date_of_refusal_of_marketing_authorisation\n    ),\n    date_of_opinion = lubridate::date(date_of_opinion),\n    decision_date = lubridate::date(decision_date),\n    \n    # Coalesce this in place then later rename and drop the extra column.\n    human_pharmacotherapeutic_group = coalesce(\n      human_pharmacotherapeutic_group, vet_pharmacotherapeutic_group\n    ),\n    \n    atc_code = coalesce(atc_code, at_cvet_code),\n    \n    # Logicals\n    patient_safety = patient_safety == \"yes\",\n    additional_monitoring = additional_monitoring == \"yes\",\n    generic = generic == \"yes\",\n    biosimilar = biosimilar == \"yes\",\n    conditional_approval = conditional_approval == \"yes\",\n    exceptional_circumstances = exceptional_circumstances == \"yes\",\n    accelerated_assessment = accelerated_assessment == \"yes\",\n    orphan_medicine = orphan_medicine == \"yes\",\n    \n    # Other data types\n    revision_number = as.integer(revision_number),\n    authorisation_status = factor(tolower(authorisation_status))\n  ) |&gt;\n  rename(\n    \"common_name\" = \"international_non_proprietary_name_inn_common_name\",\n    \"pharmacotherapeutic_group\" = \"human_pharmacotherapeutic_group\"\n  ) |&gt; \n  select(\n    -\"vet_pharmacotherapeutic_group\",\n    -\"at_cvet_code\"\n  )\n\nwrite_csv(\n  drugs, \n  file = here::here(\n    \"data\",\n    \"2023\",\n    \"2023-03-14\",\n    \"drugs.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-02-28/readme.html",
    "href": "data/2023/2023-02-28/readme.html",
    "title": "African Language Sentiment",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nAfrican Language Sentiment\nThe data this week comes from AfriSenti: Sentiment Analysis dataset for 14 African languages via @shmuhammad2004 (the corresponding author on the associated paper, and an active member of the R4DS Online Learning Community Slack).\n\nThis repository contains data for the SemEval 2023 Shared Task 12: Sentiment Analysis in African Languages (AfriSenti-SemEval).\n\nThe source repository also includes sentiment lexicons for several languages.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-02-28')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 9)\n\nafrisenti &lt;- tuesdata$afrisenti\nlanguages &lt;- tuesdata$languages\nlanguage_scripts &lt;- tuesdata$language_scripts\nlanguage_countries &lt;- tuesdata$language_countries\ncountry_regions &lt;- tuesdata$country_regions\n\n# Or read in the data manually\n\nafrisenti &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-28/afrisenti.csv')\nlanguages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-28/languages.csv')\nlanguage_scripts &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-28/language_scripts.csv')\nlanguage_countries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-28/language_countries.csv')\ncountry_regions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-28/country_regions.csv')\n\n\nData Dictionary\n\n\n\nafrisenti.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlanguage_iso_code\ncharacter\nThe unique code used to identify the language\n\n\ntweet\ncharacter\nThe text content of a tweet\n\n\nlabel\ncharacter\nA sentiment label of positive, negative, or neutral assigned by a native speaker of that language\n\n\nintended_use\ncharacter\nWhether the data came from the dev, test, or train set for that language\n\n\n\n\n\nlanguages.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlanguage_iso_code\ncharacter\nThe unique code used to identify the language\n\n\nlanguage\ncharacter\nThe name of the language\n\n\n\n\n\nlanguage_scripts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlanguage_iso_code\ncharacter\nThe unique code used to identify the language\n\n\nscript\ncharacter\nThe script used to write the language\n\n\n\n\n\nlanguage_countries.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlanguage_iso_code\ncharacter\nThe unique code used to identify the language\n\n\ncountry\ncharacter\nA country in which the language is spoken\n\n\n\n\n\ncountry_regions.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nA country in which the language is spoken\n\n\nregion\ncharacter\nThe region of Africa in which that country is categorized. Note that Mozambique is categorized as “East Africa”, “Southern Africa”, and “Southeastern Africa”\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\n# Read in the data\nlanguage_abbrevs &lt;- c(\n  \"amh\", \"arq\", \"ary\", \n  \"hau\", \"ibo\", \"kin\",\n  \"oro\", \"pcm\", \"por\",\n  \"swa\", \"tir\", \"tso\", \n  \"twi\", \"yor\"\n)\n# amh == ama, por == pt-MZ, oro == orm\n\nsplits &lt;- c(\"dev\", \"test\", \"train\")\n\nafrisenti &lt;- purrr::map(\n  language_abbrevs,\n  \\(language_abbr) {\n    purrr::map(\n      splits,\n      \\(split) {\n        readr::read_tsv(\n          paste0(\n            \"https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/\",\n            language_abbr, \"/\", split, \".tsv\"\n          )\n        ) |&gt; \n          dplyr::mutate(\n            language_abbreviation = language_abbr,\n            intended_use = split\n          )\n      }\n    ) |&gt; \n      purrr::list_rbind()\n  }\n) |&gt; \n  purrr::list_rbind()\n\nglimpse(afrisenti)\n\n# oro has an extra column.\nafrisenti |&gt; \n  dplyr::filter(!is.na(ID)) |&gt; \n  dplyr::count(language_abbreviation)\n\n# Drop the extra column, and arrange the columns. Also recode the\n# language_abbreviation to formal ISO codes.\nafrisenti &lt;- afrisenti |&gt; \n  dplyr::mutate(\n    language_iso_code = dplyr::recode(\n      language_abbreviation,\n      por = \"pt-MZ\",\n      oro = \"orm\"\n    )\n  ) |&gt; \n  dplyr::select(\n    language_iso_code,\n    tweet,\n    label,\n    intended_use\n  ) |&gt; \n  dplyr::arrange(language_iso_code, label, intended_use)\n\n# Also include information about the languages. Start with Table 2 from\n# https://arxiv.org/pdf/2302.08956.pdf but make it tidy.\nlanguages_untidy &lt;- tibble::tribble(\n  ~language, ~language_iso_code, ~region, ~country, ~script,\n  \"Amharic\", \"amh\", \"East Africa\", \"Ethiopia\", \"Ethiopic\",\n  \"Algerian Arabic/Darja\", \"arq\", \"North Africa\", \"Algeria\", \"Arabic\",\n  \"Hausa\", \"hau\", \"West Africa\", \"Nigeria\", \"Latin\",\n  \"Hausa\", \"hau\", \"West Africa\", \"Ghana\", \"Latin\",\n  \"Hausa\", \"hau\", \"West Africa\", \"Cameroon\", \"Latin\",\n  \"Igbo\", \"ibo\", \"West Africa\", \"Nigeria\", \"Latin\",\n  \"Kinyarwanda\", \"kin\", \"East Africa\", \"Rwanda\", \"Latin\",\n  \"Moroccan Arabic/Darija\", \"ary\", \"Northern Africa\", \"Morocco\", \"Arabic\",\n  \"Moroccan Arabic/Darija\", \"ary\", \"Northern Africa\", \"Morocco\", \"Latin\",\n  \"Mozambican Portuguese\", \"pt-MZ\", \"Southeastern Africa\", \"Mozambique\", \"Latin\",\n  \"Nigerian Pidgin\", \"pcm\", \"West Africa\", \"Nigeria\", \"Latin\",\n  \"Nigerian Pidgin\", \"pcm\", \"West Africa\", \"Ghana\", \"Latin\",\n  \"Nigerian Pidgin\", \"pcm\", \"West Africa\", \"Cameroon\", \"Latin\",\n  \"Oromo\", \"orm\", \"East Africa\", \"Ethiopia\", \"Latin\",\n  \"Swahili\", \"swa\", \"East Africa\", \"Tanzania\", \"Latin\",\n  \"Swahili\", \"swa\", \"East Africa\", \"Kenya\", \"Latin\",\n  \"Swahili\", \"swa\", \"East Africa\", \"Mozambique\", \"Latin\",\n  \"Tigrinya\", \"tir\", \"East Africa\", \"Ethiopia\", \"Ethiopic\",\n  \"Twi\", \"twi\", \"West Africa\", \"Ghana\", \"Latin\",\n  \"Xitsonga\", \"tso\", \"Southern Africa\", \"Mozambique\", \"Latin\",\n  \"Xitsonga\", \"tso\", \"Southern Africa\", \"South Africa\", \"Latin\",\n  \"Xitsonga\", \"tso\", \"Southern Africa\", \"Zimbabwe\", \"Latin\",\n  \"Xitsonga\", \"tso\", \"Southern Africa\", \"Eswatini\", \"Latin\",\n  \"Yorùbá\", \"yor\", \"West Africa\", \"Nigeria\", \"Latin\"\n)\n\nlanguages &lt;- languages_untidy |&gt; \n  dplyr::distinct(language_iso_code, language) |&gt; \n  dplyr::arrange(language_iso_code, language)\n\nlanguage_countries &lt;- languages_untidy |&gt; \n  dplyr::distinct(language_iso_code, country) |&gt; \n  dplyr::arrange(language_iso_code, country)\n\nlanguage_scripts &lt;- languages_untidy |&gt; \n  dplyr::distinct(language_iso_code, script) |&gt; \n  dplyr::arrange(language_iso_code, script)\n\ncountry_regions &lt;- languages_untidy |&gt; \n  dplyr::distinct(country, region) |&gt; \n  dplyr::arrange(country, region)\n\n# Save the data.\nwrite_csv(\n  afrisenti,\n  here::here(\n    \"data\", \"2023\", \"2023-02-28\",\n    \"afrisenti.csv\"\n  )\n)\n\nwrite_csv(\n  languages,\n  here::here(\n    \"data\", \"2023\", \"2023-02-28\",\n    \"languages.csv\"\n  )\n)\n\nwrite_csv(\n  language_scripts,\n  here::here(\n    \"data\", \"2023\", \"2023-02-28\",\n    \"language_scripts.csv\"\n  )\n)\n\nwrite_csv(\n  language_countries,\n  here::here(\n    \"data\", \"2023\", \"2023-02-28\",\n    \"language_countries.csv\"\n  )\n)\n\nwrite_csv(\n  country_regions,\n  here::here(\n    \"data\", \"2023\", \"2023-02-28\",\n    \"country_regions.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-02-14/readme.html",
    "href": "data/2023/2023-02-14/readme.html",
    "title": "Hollywood Age Gaps",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nHollywood Age Gaps\nThe data this week comes from Hollywood Age Gap via Data Is Plural.\n\nAn informational site showing the age gap between movie love interests.\n\nThe data follows certain rules:\n\nThe two (or more) actors play actual love interests (not just friends, coworkers, or some other non-romantic type of relationship)\n\n\nThe youngest of the two actors is at least 17 years old\n\n\nNot animated characters\n\nWe previously provided a dataset about the Bechdel Test. It might be interesting to see whether there is any correlation between these datasets! The Bechdel Test dataset also included additional information about the films that were used in that dataset.\nNote: The age gaps dataset includes “gender” columns, which always contain the values “man” or “woman”. These values appear to indicate how the characters in each film identify. Some of these values do not match how the actor identifies. We apologize if any characters are misgendered in the data!\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-02-14')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 7)\n\nage_gaps &lt;- tuesdata$age_gaps\n\n# Or read in the data manually\n\nage_gaps &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-14/age_gaps.csv')\n\n\nData Dictionary\n\n\n\nage_gaps.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmovie_name\ncharacter\nName of the film\n\n\nrelease_year\ninteger\nRelease year\n\n\ndirector\ncharacter\nDirector of the film\n\n\nage_difference\ninteger\nAge difference between the characters in whole years\n\n\ncouple_number\ninteger\nAn identifier for the couple in case multiple couples are listed for this film\n\n\nactor_1_name\ncharacter\nThe name of the older actor in this couple\n\n\nactor_2_name\ncharacter\nThe name of the younger actor in this couple\n\n\ncharacter_1_gender\ncharacter\nThe gender of the older character, as identified by the person who submitted the data for this couple\n\n\ncharacter_2_gender\ncharacter\nThe gender of the younger character, as identified by the person who submitted the data for this couple\n\n\nactor_1_birthdate\ndate\nThe birthdate of the older member of the couple\n\n\nactor_2_birthdate\ndate\nThe birthdate of the younger member of the couple\n\n\nactor_1_age\ninteger\nThe age of the older actor when the film was released\n\n\nactor_2_age\ninteger\nThe age of the younger actor when the film was released\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\nage_gaps &lt;- read_csv(\n  \"http://hollywoodagegap.com/movies.csv\",\n) |&gt; \n  clean_names()\n\nglimpse(age_gaps)\n\n# Quickly check that the columns make sense.\nlength(vctrs::vec_cast(age_gaps$release_year, integer())) == nrow(age_gaps)\nlength(vctrs::vec_cast(age_gaps$age_difference, integer())) == nrow(age_gaps)\nunique(age_gaps$actor_1_gender)\n!any(is.na(as.Date(age_gaps$actor_1_birthdate)))\nlength(vctrs::vec_cast(age_gaps$actor_1_age, integer())) == nrow(age_gaps)\nunique(age_gaps$actor_2_gender)\n!any(is.na(as.Date(age_gaps$actor_2_birthdate)))\nlength(vctrs::vec_cast(age_gaps$actor_2_age, integer())) == nrow(age_gaps)\n\n# Formally set the dates to dates.\nage_gaps &lt;- age_gaps |&gt; \n  mutate(\n    across(\n      ends_with(\"birthdate\"),\n      as.Date\n    )\n  )\n\n# Try to get a better understanding of the \"gender\" columns.\ncount(age_gaps, actor_1_gender)\ncount(age_gaps, actor_2_gender)\n\n# The order of the characters doesn't seem to be consistent\nage_gaps |&gt; \n  summarize(\n    p_1_older = mean(actor_1_age &gt; actor_2_age),\n    p_1_male = mean(actor_1_gender == \"man\"),\n    p_1_female_2_male = mean(actor_1_gender == \"woman\" & actor_2_gender == \"man\"),\n    p_1_first_alpha = mean(actor_1_name &lt; actor_2_name)\n  )\n\n# For the most part, they put the man first if there's a man in the couple. It\n# doesn't look like there's a strict rule, though. But beware: Some movies have\n# more than 1 couple! Let's use all that to rebuild the data, always putting the\n# older character first.\nage_gaps &lt;- age_gaps |&gt; \n  mutate(\n    couple_number = row_number(),\n    .by = \"movie_name\"\n  ) |&gt; \n  pivot_longer(\n    cols = starts_with(c(\"actor_1_\", \"actor_2_\")),\n    names_to = c(NA, NA, \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  # Put the older actor first.\n  arrange(desc(age_difference), movie_name, birthdate) |&gt; \n  # While we have it pivoted, correct Elliot Page's name. I don't know if other\n  # actors are similarly deadnamed, but at least we can fix this one. Note that\n  # the *characters* played by Elliot in these particular films were women, so\n  # I'll leave the gender as-is.\n  mutate(\n    name = case_match(\n      name,\n      \"Ellen Page\" ~ \"Elliot Page\",\n      .default = name\n    )\n  ) |&gt;\n  mutate(\n    position = row_number(),\n    .by = c(\"movie_name\", \"couple_number\")\n  ) |&gt; \n  pivot_wider(\n    names_from = \"position\",\n    names_glue = \"actor_{position}_{.value}\",\n    values_from = c(\"name\", \"gender\", \"birthdate\", \"age\")\n  )\n\n# The gender isn't really the actor so much as it is the character. Let's\n# correct that.\nage_gaps &lt;- age_gaps |&gt; \n  rename(\n    \"character_1_gender\" = \"actor_1_gender\",\n    \"character_2_gender\" = \"actor_2_gender\"\n  )\n\nglimpse(age_gaps)\n\n# Save the data.\nwrite_csv(\n  age_gaps,\n  here::here(\n    \"data\", \"2023\", \"2023-02-14\",\n    \"age_gaps.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-01-31/readme.html",
    "href": "data/2023/2023-01-31/readme.html",
    "title": "Pet Cats UK",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nPet Cats UK\nThe data this week comes from the Movebank for Animal Tracking Data via Data is Plural. Thanks @jthomasmock for the suggestion!\n\nBetween 2013 and 2017, Roland Kays et al. convinced hundreds of volunteers in the U.S., U.K., Australia, and New Zealand to strap GPS sensors on their pet cats. The aforelinked datasets include each cat’s characteristics (such as age, sex, neuter status, hunting habits) and time-stamped GPS pings.\n\n\nWhen using this dataset, please cite the original article.\n\n\nKays R, Dunn RR, Parsons AW, Mcdonald B, Perkins T, Powers S, Shell L, McDonald JL, Cole H, Kikillus H, Woods L, Tindle H, Roetman P (2020) The small home ranges and large local ecological impacts of pet cats. Animal Conservation. doi:10.1111/acv.12563\n\n\nAdditionally, please cite the Movebank data package:\n\n\nMcDonald JL, Cole H (2020) Data from: The small home ranges and large local ecological impacts of pet cats [United Kingdom]. Movebank Data Repository. doi:10.5441/001/1.pf315732\n\nAdditional datasets for the US, Australia, and New Zealand are also available for download, but they were too large for us to include them directly.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-01-31')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 5)\n\ncats_uk &lt;- tuesdata$cats_uk\ncats_uk_reference &lt;- tuesdata$cats_uk_reference\n\n# Or read in the data manually\n\ncats_uk &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-31/cats_uk.csv')\ncats_uk_reference &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-31/cats_uk_reference.csv')\n\n\nData Dictionary\nFull dictionaries are available on Movebank\n\n\n\ncats_uk.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntag_id\ncharacter\nA unique identifier for the tag, provided by the data owner. If the data owner does not provide a tag ID, an internal Movebank tag identifier may sometimes be shown.\n\n\nevent_id\ndouble\nAn identifier for the set of values associated with each event, i.e. sensor measurement. A unique event ID is assigned to every time-location or other time-measurement record in Movebank. If multiple measurements are included within a single row of a data file, they will share an event ID. If users import the same sensor measurement to Movebank multiple times, a separate event ID will be assigned to each.\n\n\nvisible\nlogical\nDetermines whether an event is visible on the Movebank map. Values are calculated automatically, with TRUE indicating the event has not been flagged as an outlier by algorithm_marked_outlier, import_marked_outlier or manually_marked_outlier, or that the user has overridden the results of these outlier attributes using manually_marked_valid = TRUE. Allowed values are TRUE or FALSE.\n\n\ntimestamp\ndouble\nThe date and time corresponding to a sensor measurement or an estimate derived from sensor measurements.\n\n\nlocation_long\ndouble\nThe geographic longitude of the location as estimated by the sensor. Positive values are east of the Greenwich Meridian, negative values are west of it.\n\n\nlocation_lat\ndouble\nThe geographic longitude of the location as estimated by the sensor. Positive values are east of the Greenwich Meridian, negative values are west of it.\n\n\nground_speed\ndouble\nThe estimated ground speed provided by the sensor or calculated between consecutive locations. Units are reportedly m/s, which indicates that there is likely a problem with this data (either the units were reported erroneously or their is an issue with the sensor data).\n\n\nheight_above_ellipsoid\ndouble\nThe estimated height above the ellipsoid, typically estimated by the tag. Units: meters\n\n\nalgorithm_marked_outlier\nlogical\nIdentifies events marked as outliers using a user-selected filter algorithm in Movebank. Outliers have the value TRUE.\n\n\nmanually_marked_outlier\nlogical\nIdentifies events flagged manually as outliers, typically using the Event Editor in Movebank, and may also include outliers identified using other methods. Outliers have the value TRUE.\n\n\nstudy_name\ncharacter\nThe name of the study in Movebank.\n\n\n\n\n\ncats_uk_reference.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntag_id\ncharacter\nA unique identifier for the tag, provided by the data owner. If the data owner does not provide a tag ID, an internal Movebank tag identifier may sometimes be shown.\n\n\nanimal_id\ncharacter\nAn individual identifier for the animal, provided by the data owner. If the data owner does not provide an Animal ID, an internal Movebank animal identifier is sometimes shown.\n\n\nanimal_taxon\ncharacter\nThe scientific name of the species on which the tag was deployed, as defined by the Integrated Taxonomic Information System (ITIS, www.itis.gov). If the species name can not be provided, this should be the lowest level taxonomic rank that can be determined and that is used in the ITIS taxonomy.\n\n\ndeploy_on_date\ndouble\nThe timestamp when the tag deployment started.\n\n\ndeploy_off_date\ndouble\nThe timestamp when the tag deployment ended.\n\n\nhunt\nlogical\nWhether the animal was allowed to hunt.\n\n\nprey_p_month\ndouble\nApproximate number of prey caught by the animal per month.\n\n\nanimal_reproductive_condition\ncharacter\nThe reproductive condition of the animal at the beginning of the deployment.\n\n\nanimal_sex\ncharacter\nThe sex of the animal, as “m” or “f”.\n\n\nhrs_indoors\ndouble\nThe average number of hours which the animal was indoors per day.\n\n\nn_cats\ndouble\nThe number of cats in the house.\n\n\nfood_dry\nlogical\nWhether the cat was fed dry food.\n\n\nfood_wet\nlogical\nWhether the cat was fed wet food.\n\n\nfood_other\nlogical\nWhether the cat was fed other food.\n\n\nstudy_site\ncharacter\nA location such as the deployment site or colony, or a location-related group such as the herd or pack name.\n\n\nage_years\ndouble\nThe age of the animal at the beginning of the deployment, in years. “0” indicates that the animal was &lt; 1 year old.\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\ncats_uk &lt;- read_csv(\"https://www.datarepository.movebank.org/bitstream/handle/10255/move.883/Pet%20Cats%20United%20Kingdom.csv?sequence=3\") |&gt; \n  clean_names() |&gt; \n  # Standardize things and reorder columns.\n  select(\n    tag_id = tag_local_identifier,\n    event_id:location_lat,\n    ground_speed,\n    height_above_ellipsoid,\n    algorithm_marked_outlier,\n    manually_marked_outlier,\n    study_name\n  ) |&gt; \n  # Explicitly encode FALSE in the outlier columns.\n  tidyr::replace_na(\n    list(\n      algorithm_marked_outlier = FALSE,\n      manually_marked_outlier = FALSE\n    )\n  )\n\ncats_uk_reference &lt;- read_csv(\"https://www.datarepository.movebank.org/bitstream/handle/10255/move.884/Pet%20Cats%20United%20Kingdom-reference-data.csv?sequence=1\") |&gt;\n  clean_names() |&gt; \n  mutate(\n    # animal_life_stage is ALMOST just age in years.\n    age_years = case_when(\n      str_detect(animal_life_stage, fixed(\"&lt;\")) ~ 0L,\n      str_detect(animal_life_stage, \"year\") ~ str_extract(\n        animal_life_stage, \"\\\\d+\"\n      ) |&gt; \n        as.integer(),\n      TRUE ~ NA_integer_\n    )\n  ) |&gt; \n  # There are only a handful of unique values for the comments, extract those.\n  separate_wider_delim(\n    animal_comments,\n    \"; \",\n    names = c(\"hunt\", \"prey_p_month\")\n  ) |&gt; \n  mutate(\n    hunt = case_when(\n      str_detect(hunt, \"Yes\") ~ TRUE,\n      str_detect(hunt, \"No\") ~ FALSE,\n      TRUE ~ NA\n    ),\n    prey_p_month = as.numeric(\n      str_remove(prey_p_month, \"prey_p_month: \")\n    )\n  ) |&gt; \n  # manipulation_comments similarly has a pattern.\n  separate_wider_delim(\n    manipulation_comments,\n    \"; \",\n    names = c(\"hrs_indoors\", \"n_cats\", \"food\")\n  ) |&gt; \n  mutate(\n    hrs_indoors = as.numeric(\n      str_remove(hrs_indoors, \"hrs_indoors: \")\n    ),\n    n_cats = as.integer(\n      str_remove(n_cats, \"n_cats: \")\n    )\n  ) |&gt; \n  separate_wider_delim(\n    food,\n    \",\",\n    names = c(\"food_dry\", \"food_wet\", \"food_other\")\n  ) |&gt; \n  mutate(\n    food_dry = case_when(\n      str_detect(food_dry, \"Yes\") ~ TRUE,\n      str_detect(food_dry, \"No\") ~ FALSE,\n      TRUE ~ NA\n    ),\n    food_wet = case_when(\n      str_detect(food_wet, \"Yes\") ~ TRUE,\n      str_detect(food_wet, \"No\") ~ FALSE,\n      TRUE ~ NA\n    ),\n    food_other = case_when(\n      str_detect(food_other, \"Yes\") ~ TRUE,\n      str_detect(food_other, \"No\") ~ FALSE,\n      TRUE ~ NA\n    )\n  ) |&gt;\n  # Drop uninteresting fields.\n  select(\n    -animal_life_stage,\n    -attachment_type,\n    -data_processing_software,\n    -deployment_end_type,\n    -duty_cycle,\n    -deployment_id,\n    -manipulation_type,\n    -tag_manufacturer_name,\n    -tag_mass,\n    -tag_model,\n    -tag_readout_method\n  )\n\nglimpse(cats_uk)\nglimpse(cats_uk_reference)\n\ncats_uk |&gt; write_csv(\n  here(\n    \"data\",\n    \"2023\",\n    \"2023-01-31\",\n    \"cats_uk.csv\"\n  )\n)\n\ncats_uk_reference |&gt; write_csv(\n  here(\n    \"data\",\n    \"2023\",\n    \"2023-01-31\",\n    \"cats_uk_reference.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-01-17/readme.html",
    "href": "data/2023/2023-01-17/readme.html",
    "title": "Art History",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nArt History\nThe data this week comes from the arthistory data package\n\nThis dataset contains data that was used for Holland Stam’s thesis work, titled Quantifying art historical narratives. The data was collected to assess the demographic representation of artists through editions of Janson’s History of Art and Gardner’s Art Through the Ages, two of the most popular art history textbooks used in the American education system. In this package specifically, both artist-level and work-level data was collected along with variables regarding the artists’ demographics and numeric metrics for describing how much space they or their work took up in each edition of each textbook.\n\n\nThis package contains three datasets:\n\n\n\nworksjanson: Contains individual work-level data by edition of Gardner’s art history textbook from 1963 until 2011. For each work, there is information about the size of the work and text as displayed in the textbook as well as details about the work’s medium and year created. Demographic data about the artist is also included.\n\n\n\n\nworksgardner: Contains individual work-level data by edition of Gardner’s art history textbook from 1926 until 2020. For each work, there is information about the size of the work as displayed in the textbook as well as the size of the accompanying descriptive text. Demographic data about the artist is also included.\n\n\n\n\nartists: Contains various information about artists by edition of Gardner or Janson’s art history textbook from 1926 until 2020. Data includes demographic information, space occupied in the textbook, as well as presence in the MoMA and Whitney museums.\n\n\nAcknowledging arthistory\n\nCitation\n\n\nLemus S, Stam H (2022). arthistory: Art History Textbook Data. https://github.com/saralemus7/arthistory, https://saralemus7.github.io/arthistory/.\n\nExamples of analyses are included in Holland Stam’s thesis in Quarto files.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-01-17')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 03)\n\narthistory &lt;- tuesdata$arthistory\n\n# Or read in the data manually\n\nartists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-17/artists.csv')\n\n\nData Dictionary\n\n\n\nartists.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nartist_name\ncharacter\nThe name of each artist\n\n\nedition_number\ndouble\nThe edition number of the textbook from either Janson’s History or Art or Gardner’s Art Through the Ages.\n\n\nyear\ndouble\nThe year of publication for a given edition of Janson or Gardner.\n\n\nartist_nationality\ncharacter\nThe nationality of a given artist.\n\n\nartist_nationality_other\ncharacter\nThe nationality of the artist. Of the total count of artists through all editions of Janson’s History of Art and Gardner’s Art Through the Ages, 77.32% account for French, Spanish, British, American and German. Therefore, the categorical strings of this variable are French, Spanish, British, American, German and Other\n\n\nartist_gender\ncharacter\nThe gender of the artist\n\n\nartist_race\ncharacter\nThe race of the artist\n\n\nartist_ethnicity\ncharacter\nThe ethnicity of the artist\n\n\nbook\ncharacter\nWhich book, either Janson or Gardner the particular artist at that particular time was included.\n\n\nspace_ratio_per_page_total\ndouble\nThe area in centimeters squared of both the text and the figure of a particular artist in a given edition of Janson’s History of Art divided by the area in centimeters squared of a single page of the respective edition. This variable is continuous.\n\n\nartist_unique_id\ndouble\nThe unique identifying number assigned to artists across books is denoted in alphabetical order. This variable is discrete.\n\n\nmoma_count_to_year\ndouble\nThe total count of exhibitions ever held by the Museum of Modern Art (MoMA) of a particular artist at a given year of publication. This variable is discrete.\n\n\nwhitney_count_to_year\ndouble\nThe count of exhibitions held by The Whitney of a particular artist at a particular moment of time, as highlighted by year. This variable in discrete.\n\n\nartist_race_nwi\ncharacter\nThe non-white indicator for artist race, meaning if an artist’s race is denoted as either white or non-white.\n\n\n\n\nCleaning Script\nNo data cleaning"
  },
  {
    "objectID": "data/2023/2023-01-03/readme.html",
    "href": "data/2023/2023-01-03/readme.html",
    "title": "Week 1",
    "section": "",
    "text": "Week 1\nThis was really just a bring your own dataset week.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2023-01-03\nBring your own data from 2022!"
  },
  {
    "objectID": "data/2022/2022-12-27/readme.html",
    "href": "data/2022/2022-12-27/readme.html",
    "title": "Star Trek Timelines",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nStar Trek Timelines\nThe data this week comes from the {rtrek} package. Thank you Georgios Karamanis for suggesting the dataset!\n\nThe rtrek package provides datasets related to the Star Trek fictional universe and functions for working with those datasets. It interfaces with the Star Trek API (STAPI), Memory Alpha and Memory Beta to retrieve data, metadata and other information relating to Star Trek.\n\nWe directly included two of the datasets from the {rtrek} package, but we encourage you to install the package and explore the trekverse of Star-Trek-related R packages!\nWarnings:\n\nWe filtered the tlFootnotes dataset and renamed columns, so you won’t see exactly the same dataset if you load directly from the package.\nSome of the years do not fit into R integers, and will be coerced to NA if you attempt to convert the years to integer.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-12-27')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 52)\n\ntlBooks &lt;- tuesdata$tlBooks\ntlFootnotes &lt;- tuesdata$tlFootnotes\n\n# Or read in the data manually\n\ntlBooks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-12-27/tlBooks.csv')\ntlFootnotes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-12-27/tlFootnotes.csv')\n\n# Or load this data and more directly from the rtrek package! \n# Install it from CRAN via: install.packages(\"rtrek\")\n# See the cleaning script for more details.\n\n\nData Dictionary\n\n\n\ntlBooks.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nyear (CE)\n\n\ntitle\ncharacter\nbook (or story or episode) title\n\n\nseries\ncharacter\nassociated series abbreviation\n\n\nanthology\ncharacter\nanthology\n\n\nformat\ncharacter\nbook, story, or episode\n\n\nnumber\ninteger\nnumber of this book, story, or episode within its collection\n\n\nnovelization\nlogical\nTRUE if this thing did not begin as a book\n\n\nsetting\ncharacter\nuniverse in which the book takes place\n\n\nstardate_start\ndouble\nfirst stardate mentioned or implied\n\n\nstardate_end\ndouble\nlast stardate mentioned or implied\n\n\ndetailed_date\ncharacter\nother information about the date\n\n\nsection\ncharacter\nsection of the book, story, or episode in which the event takes place\n\n\nprimary_entry_year\ninteger\nyear of the main events of the book, story, or episode\n\n\nfootnote\ninteger\nfootnote number\n\n\n\n\n\ntlFootnotes.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nfootnote\ninteger\nfootnote number\n\n\ntext\ncharacter\ntext\n\n\n\n\nCleaning Script\n# Slight cleaning of footnotes.\n\nlibrary(rtrek)\n\ntlBooks |&gt; \n  readr::write_csv(\"tlBooks.csv\")\n\ntlFootnotes |&gt; \n  purrr::set_names(\n    c(\"association\", \"footnote\", \"text\")\n  ) |&gt; \n  dplyr::filter(association == \"book\") |&gt; \n  dplyr::select(-association) |&gt; \n  readr::write_csv(\"tlFootnotes.csv\")"
  },
  {
    "objectID": "data/2022/2022-12-13/readme.html",
    "href": "data/2022/2022-12-13/readme.html",
    "title": "Monthly State Retail Sales",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nMonthly State Retail Sales\nThe data this week comes from the United States Census Bureau’s Monthly State Retail Sales.\n\nThe Monthly State Retail Sales (MSRS) is the Census Bureau’s new experimental data product featuring modeled state-level retail sales.\n\nThe data updates monthly. We pulled the data on 2022-12-10, so this dataset runs through August of 2022.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-12-13')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 50)\n\nstate_retail &lt;- tuesdata$state_retail\ncoverage_codes &lt;- tuesdata$coverage_codes\n\n# Or read in the data manually\n\nstate_retail &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-12-13/state_retail.csv',  col_types = \"cciciiccc\")\ncoverage_codes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-12-13/coverage_codes.csv')\n\n\nData Dictionary\n\n\n\nstate_retail.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nfips\ncharacter\n2-digit State Federal Information Processing Standards (FIPS) code. For more information on FIPS Codes, please reference this document. Note: The US is assigned a “00”” State FIPS code\n\n\nstate_abbr\ncharacter\nStates are assigned 2-character official U.S. Postal\n\n\nService Code. The United States is assigned “USA” as its state_abbr value. For more information, please reference this document.\n\n\n\n\nnaics\ninteger\nThree-digit numeric NAICS value for retail subsector\n\n\ncode\n\n\n\n\nsubsector\ncharacter\nRetail subsector.\n\n\nyear\ninteger\nyear\n\n\nmonth\ninteger\nmonth\n\n\nchange_yoy\ncharacter\nNumeric year-over-year percent change in retail sales value\n\n\nchange_yoy_se\ncharacter\nNumeric standard error for year-over-year percentage change in retail sales value\n\n\ncoverage_code\ncharacter\nCharacter values assigned based on the non-imputed coverage of the data.\n\n\n\n\n\ncoverage_codes.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncoverage_code\ncharacter\nCharacter values assigned based on the non-imputed coverage of the data.\n\n\ncoverage\ncharacter\nDefinition of the codes.\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(here)\n\n# Read datasets\nstate_retail_yy &lt;- read_csv(\n  \"https://www.census.gov/retail/mrts/www/statedata/state_retail_yy.csv\"\n)\nstate_retail_se &lt;- read_csv(\n  \"https://www.census.gov/retail/mrts/www/statedata/state_retail_se.csv\"\n)\nstate_retail_coverage &lt;- read_csv(\n  \"https://www.census.gov/retail/mrts/www/statedata/state_retail_coverage.csv\"\n)\n\nstate_retail_yy_clean &lt;- state_retail_yy |&gt; \n  pivot_longer(\n    starts_with(\"yy\"),\n    names_to = \"date_raw\",\n    values_to = \"change_yoy\"\n  ) |&gt; \n  mutate(\n    year = as.integer(str_extract(date_raw, \"yy(\\\\d{4})(\\\\d{2})\", group = 1)),\n    month = as.integer(str_extract(date_raw, \"yy(\\\\d{4})(\\\\d{2})\", group = 2)),\n  ) |&gt; \n  select(-date_raw)\n\nstate_retail_se_clean &lt;- state_retail_se |&gt; \n  pivot_longer(\n    starts_with(\"se\"),\n    names_to = \"date_raw\",\n    values_to = \"change_yoy_se\"\n  ) |&gt; \n  mutate(\n    year = as.integer(str_extract(date_raw, \"se(\\\\d{4})(\\\\d{2})\", group = 1)),\n    month = as.integer(str_extract(date_raw, \"se(\\\\d{4})(\\\\d{2})\", group = 2)),\n  ) |&gt; \n  select(-date_raw)\n\nstate_retail_coverage_clean &lt;- state_retail_coverage |&gt; \n  pivot_longer(\n    starts_with(\"cov\"),\n    names_to = \"date_raw\",\n    values_to = \"coverage_code\"\n  ) |&gt; \n  mutate(\n    year = as.integer(str_extract(date_raw, \"cov(\\\\d{4})(\\\\d{2})\", group = 1)),\n    month = as.integer(str_extract(date_raw, \"cov(\\\\d{4})(\\\\d{2})\", group = 2)),\n  ) |&gt; \n  select(-date_raw)\n\n# There are only 5 coverage codes. Let's put them into a table, not just in the\n# data dictionary, in case people want to use them in labels.\ncoverage_codes &lt;- tibble::tribble(\n  ~coverage_code, ~coverage,\n  \"A\", \"non-imputed coverage is less than 10% of the state/NAICS total.\",\n  \"B\", \"non-imputed coverage is greater than or equal to 10% and less than 25% of the state/NAICS total\",\n  \"C\", \"non-imputed coverage is greater than or equal to 25% and less than 50% of the state/NAICS total\",\n  \"D\", \"non-imputed coverage is greater than or equal to 50% of the state/NAICS total.\",\n  \"S\", \"Suppressed due to data quality concerns\"\n)\n\n# Attempts to download a source of NAICS codes weren't fruitful, but there are\n# only a handful so we'll get them from the documentation at\n# https://www.census.gov/retail/mrts/www/statedata/msrs_overview.pdf.\n\n# We are publishing year-over-year percent changes for each state and the\n# District of Columbia for Total Retail Sales excluding Nonstore Retailers as\n# well as for 11 three-digit retail subsectors as classified by the North\n# American Industry Classification System (NAICS). These NAICS include Motor\n# vehicle and parts dealers (NAICS 441), Furniture and Home Furnishing (NAICS\n# 442), Electronics and Appliances (NAICS 443), Building Materials and Supplies\n# Dealers (NAICS 444), Food and Beverage (NAICS 445), Health and Personal Care\n# (NAICS 446), Gasoline Stations (NAICS 447), Clothing and Clothing Accessories\n# (NAICS 448), Sporting Goods and Hobby (NAICS 451), General Merchandise (NAICS\n# 452), and Miscellaneous Store Retailers (NAICS 453).\nnaics_code_text &lt;- \"Motor vehicle and parts dealers (NAICS 441), Furniture\nand Home Furnishing (NAICS 442), Electronics and Appliances (NAICS 443), Building Materials and Supplies Dealers\n(NAICS 444), Food and Beverage (NAICS 445), Health and Personal Care (NAICS 446), Gasoline Stations (NAICS 447),\nClothing and Clothing Accessories (NAICS 448), Sporting Goods and Hobby (NAICS 451), General Merchandise\n(NAICS 452), Miscellaneous Store Retailers (NAICS 453).\"\nnaics_codes &lt;- tibble(\n  raw = naics_code_text |&gt; \n    str_squish() |&gt; \n    strsplit(\", \")\n) |&gt; \n  unnest(raw) |&gt; \n  transmute(\n    naics = str_extract(raw, \"NAICS (\\\\d{3})\", group = 1),\n    subsector = str_extract(raw, \"[^(]+\") |&gt; str_squish()\n  )\n\n# Combine the three tables.\nstate_retail &lt;- state_retail_yy_clean |&gt; \n  left_join(state_retail_se_clean) |&gt; \n  left_join(state_retail_coverage_clean) |&gt; \n  # Add the NAICS translations and clean some things up.\n  left_join(naics_codes, by = \"naics\") |&gt; \n  mutate(\n    subsector = case_when(\n      naics == \"TOTAL\" ~ \"total\",\n      TRUE ~ subsector\n    ),\n    naics = as.integer(\n      na_if(\n        naics,\n        \"TOTAL\"\n      )\n    )\n  ) |&gt; \n  select(\n    fips,\n    state_abbr = stateabbr,\n    naics,\n    subsector,\n    year,\n    month,\n    change_yoy,\n    change_yoy_se,\n    coverage_code\n  )\n\nstate_retail |&gt; write_csv(\n  here(\n    \"data\",\n    \"2022\",\n    \"2022-12-13\",\n    \"state_retail.csv\"\n  )\n)\ncoverage_codes |&gt; write_csv(\n  here(\n    \"data\",\n    \"2022\",\n    \"2022-12-13\",\n    \"coverage_codes.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2022/2022-11-29/readme.html",
    "href": "data/2022/2022-11-29/readme.html",
    "title": "World Cup",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nWorld Cup\nThe data this week comes from FIFA World Cup.\n\nAs we count down to the start of the FIFA World Cup in Qatar on November 20, this new dataset covers every single World Cup match played in its history from Uruguay in 1930 to Russia in 2018.\n\n\nThe wcmatches dataset contains every football match played in the World Cup. Columns inside the dataset include city from which city was the match being played in, outcome which team claimed victory or did the match result in a draw, win_condition showing if the winning side need added extra time of penalties to win the game.\n\n\nIt also includes a summary of each World Cup held, the cups dataset contains columns winners, games, goals_scored, and more.\n\nSome data explorations: https://www.kaggle.com/datasets/evangower/fifa-world-cup/code\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-11-29')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 48)\n\nwcmatches &lt;- tuesdata$wcmatches\n\n# Or read in the data manually\n\nwcmatches &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-29/wcmatches.csv')\nworldcups &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-29/worldcups.csv')\n\n\nData Dictionary\n\n\n\nwcmatches.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nyear\n\n\ncountry\ncharacter\ncountry\n\n\ncity\ncharacter\ncity\n\n\nstage\ncharacter\nstage\n\n\nhome_team\ncharacter\nhome_team\n\n\naway_team\ncharacter\naway_team\n\n\nhome_score\ndouble\nhome_score\n\n\naway_score\ndouble\naway_score\n\n\noutcome\ncharacter\noutcome\n\n\nwin_conditions\ncharacter\nwin_conditions\n\n\nwinning_team\ncharacter\nwinning_team\n\n\nlosing_team\ncharacter\nlosing_team\n\n\ndate\ndouble\ndate\n\n\nmonth\ncharacter\nmonth\n\n\ndayofweek\ncharacter\ndayofweek\n\n\n\n\n\nworldcups.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nyear\n\n\nhost\ncharacter\nhost\n\n\nwinner\ncharacter\nwinner\n\n\nsecond\ncharacter\nsecond\n\n\nthird\ncharacter\nthird\n\n\nfourth\ncharacter\nfourth\n\n\ngoals_scored\ndouble\ngoals_scored\n\n\nteams\ndouble\nteams\n\n\ngames\ndouble\ngames\n\n\nattendance\ndouble\nattendance\n\n\n\n\nCleaning Script\nClean data - no script"
  },
  {
    "objectID": "data/2022/2022-11-15/readme.html",
    "href": "data/2022/2022-11-15/readme.html",
    "title": "Page Metrics",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nPage Metrics\nThe data this week comes from httparchive.org by way of Data is Plural. As seen in: “Why web pages can have a size problem” (Datawrapper).\nFull detail available via BigQuery, but aggregate data used for this week.\nFull data available via instructions at: https://github.com/HTTPArchive/httparchive.org/blob/main/docs/gettingstarted_bigquery.md\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-11-15')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 46)\n\nimage_alt &lt;- tuesdata$image_alt\n\n# Or read in the data manually\n\nimage_alt &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-15/image_alt.csv')\ncolor_contrast &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-15/color_contrast.csv')\nally_scores &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-15/ally_scores.csv')\nbytes_total &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-15/bytes_total.csv')\nspeed_index &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-15/speed_index.csv')\n\n\nData Dictionary\n\n\n\nally_scores.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmeasure\ncharacter\nmeasure\n\n\nclient\ncharacter\nclient\n\n\ndate\ncharacter\ndate\n\n\np10\ncharacter\np10\n\n\np25\ncharacter\np25\n\n\np50\ncharacter\np50\n\n\np75\ncharacter\np75\n\n\np90\ncharacter\np90\n\n\ntimestamp\ncharacter\ntimestamp\n\n\n\n\n\nimage_alt.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmeasure\ncharacter\nmeasure\n\n\nclient\ncharacter\nclient\n\n\ndate\ncharacter\ndate\n\n\np10\ncharacter\np10\n\n\np25\ncharacter\np25\n\n\np50\ncharacter\np50\n\n\np75\ncharacter\np75\n\n\np90\ncharacter\np90\n\n\ntimestamp\ncharacter\ntimestamp\n\n\n\n\n\ncolor_contrast.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmeasure\ncharacter\nmeasure\n\n\nclient\ncharacter\nclient\n\n\ndate\ncharacter\ndate\n\n\np10\ncharacter\np10\n\n\np25\ncharacter\np25\n\n\np50\ncharacter\np50\n\n\np75\ncharacter\np75\n\n\np90\ncharacter\np90\n\n\ntimestamp\ncharacter\ntimestamp\n\n\n\n\n\nbytes_total.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmeasure\ncharacter\nmeasure\n\n\nclient\ncharacter\nclient\n\n\ndate\ncharacter\ndate\n\n\np10\ncharacter\np10\n\n\np25\ncharacter\np25\n\n\np50\ncharacter\np50\n\n\np75\ncharacter\np75\n\n\np90\ncharacter\np90\n\n\ntimestamp\ncharacter\ntimestamp\n\n\n\n\n\nspeed_index.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmeasure\ncharacter\nmeasure\n\n\nclient\ncharacter\nclient\n\n\ndate\ncharacter\ndate\n\n\np10\ncharacter\np10\n\n\np25\ncharacter\np25\n\n\np50\ncharacter\np50\n\n\np75\ncharacter\np75\n\n\np90\ncharacter\np90\n\n\ntimestamp\ncharacter\ntimestamp\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(jsonlite)\n\n\n\n# ally-scores -------------------------------------------------------------\n\nget_df &lt;- function(url){\n  raw_list &lt;- fromJSON(url, simplifyVector = FALSE)\n  \n  tibble(data = raw_list) |&gt; \n    unnest_wider(data) |&gt; \n    mutate(\n      measure = fs::path_file(url) |&gt; tools::file_path_sans_ext(),\n      .before = 1)\n}\n\n\n\nally_scores_url &lt;- \"https://cdn.httparchive.org/reports/a11yScores.json\"\ncolor_contrast_url &lt;- \"https://cdn.httparchive.org/reports/a11yColorContrast.json\"\nimage_alt_url &lt;- \"https://cdn.httparchive.org/reports/a11yImageAlt.json\"\n\n\nally_scores &lt;- get_df(ally_scores_url)\ncolor_contrast &lt;- get_df(color_contrast_url)\nimage_alt &lt;- get_df(image_alt_url)\n\nally_scores |&gt; write_csv(\"2022/2022-11-15/ally_scores.csv\")\ncolor_contrast |&gt; write_csv(\"2022/2022-11-15/color_contrast.csv\")\nimage_alt |&gt; write_csv(\"2022/2022-11-15/image_alt.csv\")\n\n# speed-size --------------------------------------------------------------\n\nbytes_total_url &lt;- \"https://cdn.httparchive.org/reports/bytesTotal.json\"\nspeed_index_url &lt;- \"https://cdn.httparchive.org/reports/speedIndex.json\"\n\nbytes_total &lt;- get_df(bytes_total_url)\nspeed_index &lt;- get_df(speed_index_url)\n\nbytes_total |&gt; write_csv(\"2022/2022-11-15/bytes_total.csv\")\nspeed_index |&gt; write_csv(\"2022/2022-11-15/speed_index.csv\")"
  },
  {
    "objectID": "data/2022/2022-11-01/readme.html",
    "href": "data/2022/2022-11-01/readme.html",
    "title": "Horror Movies",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nHorror Movies\n\nPurpose is to explore a dataset about horror movies dating back to the 1950s. Data set was extracted from The Movie Datbase via the tmdb API using R httr. There are ~35K movie records in this dataset.\n\nNote - to use the images, append the poster path to the following url: https://www.themoviedb.org/t/p/w1280\nRead more about Tanya’s workshop on her GitHub: Horror Movies\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-11-01')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 44)\n\nhorror_movies &lt;- tuesdata$horror_movies\n\n# Or read in the data manually\n\nhorror_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-01/horror_movies.csv')\n\n\nData Dictionary\n\n\n\nhorror_movies.csv\n\n\n\n\n\n\n\n\n\nVariable\nType\nDefinition\nExample\n\n\n\n\nid\nint\nunique movie id\n4488\n\n\noriginal_title\nchar\noriginal movie title\nFriday the 13th\n\n\ntitle\nchar\nmovie title\nFriday the 13th\n\n\noriginal_language\nchar\nmovie language\nen\n\n\noverview\nchar\nmovie overview/desc\nCamp counselors are stalked…\n\n\ntagline\nchar\ntagline\nThey were warned…\n\n\nrelease_date\ndate\nrelease date\n1980-05-09\n\n\nposter_path\nchar\nimage url\n/HzrPn1gEHWixfMOvOehOTlHROo.jpg\n\n\npopularity\nnum\npopularity\n58.957\n\n\nvote_count\nint\ntotal votes\n2289\n\n\nvote_average\nnum\naverage rating\n6.4\n\n\nbudget\nint\nmovie budget\n550000\n\n\nrevenue\nint\nmovie revenue\n59754601\n\n\nruntime\nint\nmovie runtime (min)\n95\n\n\nstatus\nchar\nmovie status\nReleased\n\n\ngenre_names\nchar\nlist of genre tags\nHorror, Thriller\n\n\ncollection\nnum\ncollection id (nullable)\n9735\n\n\ncollection_name\nchar\ncollection name (nullable)\nFriday the 13th Collection\n\n\n\n\nCleaning Script\nTanya Shapiro - Horror Movies workshop"
  },
  {
    "objectID": "data/2022/2022-10-18/readme.html",
    "href": "data/2022/2022-10-18/readme.html",
    "title": "Stranger Things Dialogue",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nStranger Things Dialogue\nThe data this week comes from 8flix.com - prepped by Dan Fellowes & Jonathan Kitt.\nRepo with cleaning scripts + data: https://github.com/filmicaesthetic/stringr-things\n8flix: https://8flix.com/stranger-things/ Wikipedia - Stranger Things episodes: https://en.wikipedia.org/wiki/List_of_Stranger_Things_episodes\n\nStranger Things is an American science fiction fantasy horror drama television series created by the Duffer Brothers that is streaming on Netflix. The brothers serve as showrunners and are executive producers along with Shawn Levy and Dan Cohen. The first season of the series was released on Netflix on July 15, 2016, with the second, third, and fourth seasons following in October 2017, July 2019, and May and July 2022, respectively.\n\nA Statistical Curiosity Voyage Through the Emotion of Stranger Things by Jordan Dworkin:\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-10-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 42)\n\nepisodes &lt;- tuesdata$episodes\n\n# Or read in the data manually\n\nepisodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-10-18/episodes.csv')\n\n\nData Dictionary\n\n\nstranger_things_all_dialogue.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ninteger\nSeason number\n\n\nepisode\ninteger\nEpisode number within the season\n\n\nline\ninteger\nOrder in which line appears in episode\n\n\nraw_text\ncharacter\nOriginal text with both dialogue and stage directions\n\n\nstage_direction\ncharacter\nText describing what’s happening, or who is talking\n\n\ndialogue\ncharacter\nDialogue spoken within the episode\n\n\nstart_time\ncharacter\nTime within the episode when the line starts being spoken\n\n\nend_time\ncharacter\nTime within the episode when the line stops being spoken\n\n\n\n\n\nepisodes.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ninteger\nSeason number\n\n\nepisode\ninteger\nEpisode number within the season\n\n\ntitle\ncharacter\nTitle of the episode\n\n\ndirected_by\ncharacter\nDirector(s) of the episode\n\n\nwritten_by\ncharacter\nWriter(s) of the episode\n\n\noriginal_release_date\ncharacter\nRelease date of the episode\n\n\n\n\n\nCleaning Script\nScripts contributed: https://github.com/filmicaesthetic/stringr-things/tree/main/scripts\n# load packages\nlibrary(pdftools)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# tidy script function\n\ntidy_script &lt;- function(pdftext) {\n  \n  # paste as a single string\n  ep_string &lt;- paste(pdftext, sep = \" \", collapse = \"\\\\\\n\")\n  \n  # split by line with \\n\n  ep_byline &lt;- str_extract_all(ep_string, \"\\\\\\n.*\")\n  \n  # convert to dataframe\n  ep_byline_df &lt;- data.frame(ep_byline[[1]])\n  \n  # tidy text \n  ep_squish &lt;-  ep_byline_df |&gt;\n    mutate(ep_byline..1.. = str_squish(ep_byline..1..))\n  \n  # get row containing page 1 text\n  start_line &lt;- which(ep_squish$ep_byline..1.. == \"Page |1\")\n  \n  # remove text above page 1\n  ep_script &lt;- data.frame(ep_squish[start_line:nrow(ep_squish),])\n  \n  colnames(ep_script) &lt;- \"raw_text\"\n  \n  # filter out irrelevant lines\n  ep_filt &lt;- ep_script |&gt;\n    filter(!(raw_text %in% c(\"8FLiX.com TRANSCRIPT DATABASE\", \"FOR EDUCATIONAL USE ONLY\", \"\\\\\", \"\", \"This transcript is for educational use only.\", \"Not to be sold or auctioned.\")),\n           !(substr(raw_text, 1, 6) == \"Page |\"),\n           !(substr(raw_text, 1, 6) == \"P a g \"))\n  \n  ep_split &lt;- ep_filt |&gt;\n    mutate(time = ifelse(grepl(\"--&gt;\", raw_text) == TRUE, raw_text, NA),\n           line = ifelse(grepl(\"^[0-9]+$\", raw_text) == TRUE, as.numeric(raw_text), NA),\n           stage_direction = str_squish(str_extract(raw_text, \"\\\\[(.*?)\\\\]\")),\n           dialogue = str_squish(gsub(\"\\\\[(.*?)\\\\]\", \"\", gsub(\"-\", \"\", raw_text)))) |&gt;\n    fill(time, .direction = \"down\") |&gt;\n    fill(line, .direction = \"down\")\n  \n  ep_timesplit &lt;- ep_split\n  ep_timesplit[c(\"start_time\", \"end_time\")] &lt;- str_split_fixed(string = ep_timesplit$time, pattern = \" --&gt; \", n = 2)\n  ep_timesplit &lt;- ep_timesplit |&gt;\n    select(-time)\n  \n  \n  ep_tidy &lt;- ep_timesplit |&gt;\n    filter(grepl(\"[a-zA-Z]\", raw_text) == TRUE)\n  \n  ep_groupline &lt;- ep_tidy |&gt;\n    group_by(line) |&gt;\n    summarise(raw_text = paste0(raw_text, collapse = \" \"),\n              stage_direction = paste0(stage_direction[!is.na(stage_direction)], collapse = \" \"),\n              dialogue = paste0(dialogue, collapse = \" \"),\n              start_time = first(start_time),\n              end_time = first(end_time)) |&gt;\n    arrange(line) |&gt;\n    filter(!is.na(line)) |&gt;\n    select(season, episode, line, raw_text, stage_direction, dialogue, start_time, end_time)\n  \n  return(ep_groupline)\n  \n}\n\n# Process scripts for all seasons\n\n\n# read PDFs for season 1 scripts\ne101 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-101-Chapter-One-The-Vanishing-of-Will-Byers.pdf\")\ne102 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-102-Chapter-Two-The-Weirdo-on-Maple-Street.pdf\")\ne103 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-103-Chapter-Three-Holly-Jolly.pdf\")\ne104 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-104-Chapter-Four-The-Body.pdf\")\ne105 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-105-Chapter-Five-The-Flea-and-the-Acrobat.pdf\")\ne106 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-106-Chapter-Six-The-Monster.pdf\")\ne107 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-107-Chapter-Seven-The-Bathtub.pdf\")\ne108 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-108-Chapter-Eight-The-Upside-Down.pdf\")\n\n# read PDFs for season 2 scripts\ne201 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-201-Chapter-One-MADMAX.pdf\")\ne202 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-202-Chapter-Two-Trick-or-Treat-Freak.pdf\")\ne203 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-203-Chapter-Three-The-Pollywog.pdf\")\ne204 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-204-Chapter-Four-Will-the-Wise.pdf\")\ne205 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-205-Chapter-Five-Dig-Dug.pdf\")\ne206 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-206-Chapter-Six-The-Spy.pdf\")\ne207 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-207-Chapter-Seven-The-Lost-Sister.pdf\")\ne208 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-208-Chapter-Eight-The-Mind-Flayer.pdf\")\ne209 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-209-Chapter-Nine-The-Gate.pdf\")\n\n# read PDFs for season 3 scripts\ne301 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-301-Chapter-One-Suzie-Do-You-Copy.pdf\")\ne302 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-302-Chapter-Two-The-Mall-Rats.pdf\")\ne303 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-303-Chapter-Three-The-Case-of-the-Missing-Lifeguard.pdf\")\ne304 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-304-Chapter-Four-The-Sauna-Test.pdf\")\ne305 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-305-Chapter-Five-The-Flayed.pdf\")\ne306 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-306-Chapter-Six-E-Pluribus-Unum.pdf\")\ne307 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-307-Chapter-Seven-The-Bite.pdf\")\ne308 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-308-Chapter-Eight-The-Battle-of-Starcourt.pdf\")\n\n# read PDFs for season 4 scripts\ne401 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-401-Chapter-One-The-Hellfire-Club.pdf\")\ne402 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-402-Chapter-Two-Vecnas-Curse.pdf\")\ne403 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-403-Chapter-Three-The-Monster-and-the-Superhero.pdf\")\ne404 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-404-Chapter-Four-Dear-Billy.pdf\")\ne405 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-405-Chapter-Five-The-Nina-Project.pdf\")\ne406 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-406-Chapter-Six-The-Dive.pdf\")\ne407 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-407-Chapter-Seven-The-Massacre-at-Hawkins-Lab.pdf\")\ne408 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-408-Chapter-Eight-Papa.pdf\")\ne409 &lt;- pdf_text(\"https://8flix.com/assets/transcripts/s/tt4574334/Stranger-Things-transcript-409-Chapter-Nine-The-Piggyback.pdf\")\n\n# list of episodes by season\ns1 &lt;- list(e101, e102, e103, e104, e105, e106, e107, e108)\ns2 &lt;- list(e201, e202, e203, e204, e205, e206, e207, e208, e209)\ns3 &lt;- list(e301, e302, e303, e304, e305, e306, e307, e308)\ns4 &lt;- list(e401, e402, e403, e404, e405, e406, e407, e308, e209)\n\n# list of seasons\nseasons &lt;- list(s1, s2, s3, s4)\n\n# create blank dataframe\nall_dialogue &lt;- data.frame()\n\n# process scripts into single dataframe\nfor (s in 1:length(seasons)) {\n  \n  # tidy scripts\n  for (i in 1:length(seasons[[s]])) {\n    \n    it &lt;- tidy_script(seasons[[s]][[i]])\n    \n    it$episode &lt;- i\n    it$season &lt;- s\n    \n    all_dialogue &lt;- rbind(all_dialogue, it)\n    \n  }\n}\n\n# save dataframe\nwrite.csv(all_dialogue, \"data/stranger_things_all_dialogue.csv\", row.names = FALSE)"
  },
  {
    "objectID": "data/2022/2022-10-04/readme.html",
    "href": "data/2022/2022-10-04/readme.html",
    "title": "Product Hunt",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nProduct Hunt\nThe data this week comes from components.one by way of Data is Plural.\n\nFor “The Gamer and the Nihilist,” an essay in Components, Andrew Thompson and collaborators created a dataset of 76,000+ tech products on Product Hunt, a popular social network for launching and promoting such things. The dataset includes the name, description, launch date, upvote count, and other details for every product from 2014 to 2021 in the platform’s sitemap. (“Based on experience, not every product that appears on Product Hunt seems to appear on the sitemap,” the authors caution.)\n\nA report on the result of anaylsis from 2014 to 2021.\nGet the full dataset (280 mB) at https://components.one/datasets/product-hunt-products\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-10-04')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 40)\n\nproduct_hunt &lt;- tuesdata$product_hunt\n\n# Or read in the data manually\n\nproduct_hunt &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-10-04/product_hunt.csv')\n\n\nData Dictionary\n\n\n\nproduct_hunt.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ncharacter\nID\n\n\nname\ncharacter\nName of product\n\n\nproduct_description\ncharacter\nProduct description\n\n\nrelease_date\ndouble\nRelease date\n\n\nproduct_of_the_day_date\ndouble\nProduct of the day date\n\n\nproduct_ranking\ndouble\nProduct ranking\n\n\nmain_image\ncharacter\nMain image\n\n\nupvotes\ndouble\nUpvotes\n\n\ncategory_tags\ncharacter\nCategory tags - multiple tags per line\n\n\nhunter\ncharacter\nHunters, who sponsor a product by promoting it to their follower\n\n\nmakers\ncharacter\nMakers, who build and release products on the site\n\n\nlast_updated\ndouble\nLast updated\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nraw_df &lt;- read_csv(\"2022/2022-10-04/product-hunt-prouducts-1-1-2014-to-12-31-2021.csv\")\n\nfull_df &lt;- raw_df |&gt; \n  rename(id = 1) |&gt; \n  mutate(id = str_remove(id, \"https://www.producthunt.com/posts/\"))\n\nraw_df |&gt; \n  select(-comments, -websites, -images) |&gt; \n  lobstr::obj_size() |&gt; \n  unclass() |&gt; \n  prettyunits::compute_bytes()\n\nfull_df |&gt; \n  select(id:main_image, upvotes, category_tags, hunter:last_updated) |&gt; \n  write_csv(\"2022/2022-10-04/product_hunt.csv\")"
  },
  {
    "objectID": "data/2022/2022-09-20/readme.html",
    "href": "data/2022/2022-09-20/readme.html",
    "title": "Wastewater Plants",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nWastewater Plants\nThe data this week comes from Macedo et al, 2022 by way of Data is Plural.\nSee the Distribution and characteristics of wastewater treatment plants within the global river network\n\nHow to cite.\n\nEhalt Macedo, H., Lehner, B., Nicell, J., Grill, G., Li, J., Limtong, A., and Shakya, R.: Distribution and characteristics of wastewater treatment plants within the global river network, Earth Syst. Sci. Data, 14, 559–577, https://doi.org/10.5194/essd-14-559-2022, 2022.\n\nThe main objective of wastewater treatment plants (WWTPs) is to remove pathogens, nutrients, organics, and other pollutants from wastewater. After these contaminants are partially or fully removed through physical, biological, and/or chemical processes, the treated effluents are discharged into receiving waterbodies. However, since WWTPs cannot remove all contaminants, especially those of emerging concern, they inevitably represent concentrated point sources of residual contaminant loads into surface waters. To understand the severity and extent of the impact of treated-wastewater discharges from such facilities into rivers and lakes, as well as to identify opportunities of improved management, detailed information about WWTPs is required, including (1) their explicit geospatial locations to identify the waterbodies affected and (2) individual plant characteristics such as the population served, flow rate of effluents, and level of treatment of processed wastewater. These characteristics are especially important for contaminant fate models that are designed to assess the distribution of substances that are not typically included in environmental monitoring programs. Although there are several regional datasets that provide information on WWTP locations and characteristics, data are still lacking at a global scale, especially in developing countries. Here we introduce a spatially explicit global database, termed HydroWASTE, containing 58,502 WWTPs and their characteristics. This database was developed by combining national and regional datasets with auxiliary information to derive or complete missing WWTP characteristics, including the number of people served. A high-resolution river network with streamflow estimates was used to georeference WWTP outfall locations and calculate each plant’s dilution factor (i.e., the ratio of the natural discharge of the receiving waterbody to the WWTP effluent discharge). The utility of this information was demonstrated in an assessment of the distribution of treated wastewater at a global scale. Results show that 1,200,000 km of the global river network receives wastewater input from upstream WWTPs, of which more than 90 000 km is downstream of WWTPs that offer only primary treatment. Wastewater ratios originating from WWTPs exceed 10 % in over 72,000 km of rivers, mostly in areas of high population densities in Europe, the USA, China, India, and South Africa. In addition, 2533 plants show a dilution factor of less than 10, which represents a common threshold for environmental concern. HydroWASTE can be accessed at https://doi.org/10.6084/m9.figshare.14847786.v1 (Ehalt Macedo et al., 2021).\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-09-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 38)\n\nHydroWASTE_v10 &lt;- tuesdata$HydroWASTE_v10\n\n# Or read in the data manually\n\nHydroWASTE_v10 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-09-20/HydroWASTE_v10.csv')\n\n\nData Dictionary\n\n\n\nHydroWASTE_v10.csv\nColumn description:\n\n\n\n\n\n\n\ncol\ndescription\n\n\n\n\nWASTE_ID\nID of WWTP in HydroWASTE\n\n\nSOURCE\n\n\n\nORG_ID\n\n\n\nWWTP_NAME\nName of the WWTP from national/regional dataset (empty if not reported)\n\n\nCOUNTRY\n\n\n\nCNTRY_ISO\nCountry ISO\n\n\nLAT_WWTP\nLatitude of reported WWTP location\n\n\nLON_WWTP\nLongitude of reported WWTP location\n\n\nQUAL_LOC\nQuality indicator related to reported WWTP location (see SI of reference paper for more information): 1 = high (tests indicated &gt;80% of reported WWTP locations in country/region to be accurate); 2 = medium (tests indicated between 50% and 80% of reported WWTP locations in country/region to be accurate); 3 = low (tests indicated &lt;50% of reported WWTP locations in country/region to be accurate); 4 = Quality of WWTP locations in country/region not analysed\n\n\nLAT_OUT\n\n\n\nLON_OUT\n\n\n\nSTATUS\n\n\n\nPOP_SERVED\nPopulation served by the WWTP\n\n\nQUAL_POP\nQuality indicator related to the attribute “population served” (see reference paper for more information): 1 = Reported as ‘population served’ by national/regional dataset; 2 = Reported as ‘population equivalent’ by national/regional dataset; 3 = Estimated (with wastewater discharge available); 4 = Estimated (without wastewater discharge available)\n\n\nWASTE_DIS\nTreated wastewater discharged by the WWTP in m3 d-1\n\n\nQUAL_WASTE\nQuality indicator related to the attribute “Treated wastewater discharged” (see reference paper for more information): 1 = Reported as ‘treated’ by national/regional dataset; 2 = Reported as ‘design capacity’ by national/regional dataset; 3 = Reported but type not identified; 4 = Estimated\n\n\nLEVEL\n\n\n\nQUAL_LEVEL\nQuality indicator related to the attribute “level of treatment” (see reference paper for more information): 1 = Reported by national/regional dataset; 2 = Estimated\n\n\nDF\n\n\n\nHYRIV_ID\nID of associated river reach in RiverATLAS at estimated outfall location (link to HydroATLAS database; empty if estimated outfall location is the ocean or an endorheic sink)\n\n\nRIVER_DIS\nEstimated river discharge at the WWTP outfall location in m3 s-1 (derived from HydroATLAS database; empty if estimated outfall location is the ocean)\n\n\nCOAST_10KM\n1 = Estimated outfall location within 10 km of the ocean or a large lake (surface area larger than 500 km2); 0 = Estimated outfall location further than 10 km of the ocean or a large lake (surface area larger than 500 km2)\n\n\nCOAST_50KM\n1 = Estimated outfall location within 50 km of the ocean or a large lake (surface area larger than 500 km2); 0 = Estimated outfall location further than 50 km of the ocean or a large lake (surface area larger than 500 km2)\n\n\nDESIGN_CAP\nDesign capacity of WWTP as reported in national/regional dataset (empty if not reported)\n\n\nQUAL_CAP\nQuality indicator related to the attribute “design capacity”: 1 = Reported as design capacity in m3 d-1; 2 = Reported as design capacity in ‘population equivalent’; 3 = Not reported\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-09-06/readme.html",
    "href": "data/2022/2022-09-06/readme.html",
    "title": "LEGO sets",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nLEGO sets\nThe data this week comes from rebrickable courtesy of Georgios Karamanis.\n\nThe LEGO Parts/Sets/Colors and Inventories of every official LEGO set in the Rebrickable database is available for download as csv files here. These files are automatically updated daily. If you need more details, you can use the API which provides real-time data, but has rate limits that prevent bulk downloading of data.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-09-06')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 36)\n\ninventories &lt;- tuesdata$inventories\n\n# Or read in the data manually\n\ninventories &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-09-06/inventories.csv.gz')\ninventory_sets &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-09-06/inventory_sets.csv.gz')\nsets &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-09-06/sets.csv.gz')\n\n\nData Dictionary\n\n\n\ninventories.csv.gz\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nvariable\n\n\nversion\ndouble\nvariable\n\n\nset_num\ncharacter\nvariable\n\n\n\n\n\ninventory_sets.csv.gz\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ninventory_id\ndouble\nvariable\n\n\nset_num\ncharacter\nvariable\n\n\nquantity\ndouble\nvariable\n\n\n\n\n\nsets.csv.gz\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nset_num\ncharacter\nvariable\n\n\nname\ncharacter\nvariable\n\n\nyear\ndouble\nvariable\n\n\ntheme_id\ndouble\nvariable\n\n\nnum_parts\ndouble\nvariable\n\n\nimg_url\ncharacter\nvariable\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nall_csvs &lt;- list.files(\"2022/2022-09-06\") |&gt; \n  stringr::str_subset(\".csv\")\n  \nall_csvs\n\ninventories &lt;- read_csv(\"2022/2022-09-06/inventories.csv.gz\")\ninventory_sets &lt;- read_csv(\"2022/2022-09-06/inventory_sets.csv.gz\")\nsets &lt;- read_csv(\"2022/2022-09-06/sets.csv.gz\")\n\nall_df &lt;- left_join(inventories, inventory_sets, by = \"set_num\") |&gt;\n  left_join(sets, by = \"set_num\") \n\nex_plot &lt;- all_df |&gt; \n  ggplot(aes(x = num_parts)) +\n  geom_density() +\n  scale_x_log10()\n\nggsave(\"2022/2022-09-06/pic2.png\", ex_plot, dpi = \"retina\", height = 4, width = 6)"
  },
  {
    "objectID": "data/2022/2022-08-23/readme.html",
    "href": "data/2022/2022-08-23/readme.html",
    "title": "CHIP Dataset",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nCHIP Dataset\nThe data this week comes from CHIP Dataset.\n\nMoore’s Law.)\nWikipedia - Moore’s Law\n\n\nMoore’s law is the observation that the number of transistors in a dense integrated circuit (IC) doubles about every two years. Moore’s law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship linked to gains from experience in production.\n\nPaper for citation: Summarizing CPU and GPU Design Trends with Product Data\nNote that the authors prohibit resharing the dataset, so I’ve created a simple summary. You can easily download the full dataset at the bottom of: https://chip-dataset.vercel.app/\n\nHere are some interesting findings:\n\nMoore’s Law still holds, especially in GPUs.\nDannard Scaling is still valid in general.\nCPUs have higher frequencies, but GPUs are catching up.\nGPU performance doubles every 1.5 years.\nGPU performance improvement is a joint effect of smaller transistors, larger die size, and higher frequency.\nHigh-end GPUs tends to first use new semiconductor technologies. Low-end GPUs may use old technologies for a few years.\n\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-08-23')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 34)\n\nchips &lt;- tuesdata$chips\n\n# Or read in the data manually\n\nchips &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-08-23/chips.csv')\n\n\nData Dictionary\n\n\n\nchips.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndouble\nDate of release\n\n\ntype\ncharacter\nType of chip\n\n\nfoundry\ncharacter\nCreator\n\n\nvendor\ncharacter\nVendor\n\n\nprocess_size_nm_mean\ndouble\nProcess size in nanometer\n\n\nprocess_size_nm_sd\ndouble\nProcess size in nanometer\n\n\ntdp_w_mean\ndouble\nThermal design profile\n\n\ntdp_w_sd\ndouble\nThermal design profile\n\n\ndie_size_mm_2_mean\ndouble\nDie size in millimeters^2\n\n\ndie_size_mm_2_sd\ndouble\nDie size in millimeters^2\n\n\ntransistors_million_mean\ndouble\nTransitor count in millions\n\n\ntransistors_million_sd\ndouble\nTransitor count in millions\n\n\nfreq_m_hz_mean\ndouble\nFrequency (Mhz)\n\n\nfreq_m_hz_sd\ndouble\nFrequency (Mhz)\n\n\nn\ninteger\nTotal number of observations for date, type, foundry, vendor grouping\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-08-09/readme.html",
    "href": "data/2022/2022-08-09/readme.html",
    "title": "Ferris Wheels",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nFerris Wheels\nThe data this week comes from ferriswheels package by Emil Hvitfeldt.\nMake sure to tag @Emil_Hvitfeldt so he can see all the cute dataviz y’all make!\n\nThe goal of ferriswheels is to provide a fun harmless little data set to play with\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-08-09')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 32)\n\nwheels &lt;- tuesdata$wheels\n\n# Or read in the data manually\n\nwheels &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-08-09/wheels.csv')\n\n\nData Dictionary\n\n\n\nwheels.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName\n\n\nheight\ndouble\nHeight in feet\n\n\ndiameter\ndouble\nDiameter in feet\n\n\nopened\ndouble\nISO Date opened\n\n\nclosed\ndouble\nISO date closed\n\n\ncountry\ncharacter\nCountry\n\n\nlocation\ncharacter\nLocation/city/region\n\n\nnumber_of_cabins\ninteger\nNumber of cabins\n\n\npassengers_per_cabin\ninteger\nPassengers per cabin\n\n\nseating_capacity\ninteger\nSeating capacity\n\n\nhourly_capacity\ninteger\nHourly capacity\n\n\nride_duration_minutes\ndouble\nRide duration minutes\n\n\nclimate_controlled\ncharacter\nclimate_controlled\n\n\nconstruction_cost\ncharacter\nconstruction_cost\n\n\nstatus\ncharacter\nstatus\n\n\ndesign_manufacturer\ncharacter\ndesign_manufacturer\n\n\ntype\ncharacter\ntype\n\n\nvip_area\ncharacter\nvip_area\n\n\nticket_cost_to_ride\ncharacter\nticket_cost_to_ride\n\n\nofficial_website\ncharacter\nofficial_website\n\n\nturns\ndouble\nturns\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-07-19/readme.html",
    "href": "data/2022/2022-07-19/readme.html",
    "title": "Technology Adoption",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nTechnology Adoption\nThe data this week comes from data.nber.org.\nThis data is released under CCBY 4.0, please reference the CHAT dataset via: 10.3386/w15319.\nPer the working paper:\n\nWe present data on the global diffusion of technologies over time, updating and adding to Comin and Mestieri’s ‘CHAT’ database. We analyze usage primarily based on per capita measures and divide technologies into the two broad categories of production and consumption. We conclude that there has been strong convergence in use of consumption technologies with somewhat slower and more partial convergence in production technologies. This reflects considerably stronger global convergence in quality of life than in income, but we note that universal convergence in use of production technologies is not required for income convergence (only that countries are approaching the technology frontier in the goods and services that they produce).\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-07-19')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 29)\n\ntechnology &lt;- tuesdata$technology\n\n# Or read in the data manually\n\ntechnology &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-07-19/technology.csv')\n\n\nData Dictionary\n\n\n\ntechnology.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nvariable\ncharacter\nVariable name\n\n\nlabel\ncharacter\nLabel for variable\n\n\niso3c\ncharacter\nCountry code\n\n\nyear\ndouble\nYear\n\n\ngroup\ncharacter\nGroup (consumption/production)\n\n\ncategory\ncharacter\nCategory\n\n\nvalue\ndouble\nValue (related to label)\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-07-05/readme.html",
    "href": "data/2022/2022-07-05/readme.html",
    "title": "SF Rents",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nSF Rents\nThe data this week comes from Kate Pennington, data.sfgov.org, Vital Signs.\nIf using Dr. Pennington’s data, please cite:\n\nPennington, Kate (2018). Bay Area Craigslist Rental Housing Posts, 2000-2018. Retrieved from https://github.com/katepennington/historic_bay_area_craigslist_housing_posts/blob/master/clean_2000_2018.csv.zip.\n\nHer methodology can be found at her website.\n\nWhat impact does new housing have on rents, displacement, and gentrification in the surrounding neighborhood? Read our interview with economist Kate Pennington about her article, “Does Building New Housing Cause Displacement?:The Supply and Demand Effects of Construction in San Francisco.” - Kate Pennington on Gentrification and Displacement in San Francisco\n\nAll building permits can be found at the Socrata API endpoint.\nDoes Building New Housing Cause Displacement?: The Supply and Demand Effects of Construction in San Francisco, Kate Pennington\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-07-05')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 27)\n\nrent &lt;- tuesdata$rent\n\n# Or read in the data manually\n\nrent &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-07-05/rent.csv')\npermits &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-07-05/sf_permits.csv')\nnew_construction &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-07-05/new_construction.csv')\n\n\n\nData Dictionary\n\n\n\nrent.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npost_id\ncharacter\nUnique ID\n\n\ndate\ndouble\ndate\n\n\nyear\ndouble\nyear\n\n\nnhood\ncharacter\nneighborhood\n\n\ncity\ncharacter\ncity\n\n\ncounty\ncharacter\ncounty\n\n\nprice\ndouble\nprice in USD\n\n\nbeds\ndouble\nn of beds\n\n\nbaths\ndouble\nn of baths\n\n\nsqft\ndouble\nsquare feet of rental\n\n\nroom_in_apt\ndouble\nroom in apartment\n\n\naddress\ncharacter\naddress\n\n\nlat\ndouble\nlatitude\n\n\nlon\ndouble\nlongitude\n\n\ntitle\ncharacter\ntitle of listing\n\n\ndescr\ncharacter\ndescription\n\n\ndetails\ncharacter\nadditional details\n\n\n\n\n\nsf_permits.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npermit_number\ncharacter\npermit_number\n\n\npermit_type\ndouble\npermit_type\n\n\npermit_type_definition\ncharacter\npermit_type_definition\n\n\npermit_creation_date\ndouble\npermit_creation_date\n\n\nblock\ncharacter\nblock\n\n\nlot\ncharacter\nlot\n\n\nstreet_number\ndouble\nstreet_number\n\n\nstreet_number_suffix\ncharacter\nstreet_number_suffix\n\n\nstreet_name\ncharacter\nstreet_name\n\n\nstreet_suffix\ncharacter\nstreet_suffix\n\n\nunit\ndouble\nunit\n\n\nunit_suffix\ncharacter\nunit_suffix\n\n\ndescription\ncharacter\ndescription\n\n\nstatus\ncharacter\nstatus\n\n\nstatus_date\ndouble\nstatus_date\n\n\nfiled_date\ndouble\nfiled_date\n\n\nissued_date\ndouble\nissued_date\n\n\ncompleted_date\ndouble\ncompleted_date\n\n\nfirst_construction_document_date\ndouble\nfirst_construction_document_date\n\n\nstructural_notification\ncharacter\nstructural_notification\n\n\nnumber_of_existing_stories\ndouble\nnumber_of_existing_stories\n\n\nnumber_of_proposed_stories\ndouble\nnumber_of_proposed_stories\n\n\nvoluntary_soft_story_retrofit\ncharacter\nvoluntary_soft_story_retrofit\n\n\nfire_only_permit\ncharacter\nfire_only_permit\n\n\npermit_expiration_date\ndouble\npermit_expiration_date\n\n\nestimated_cost\ndouble\nestimated_cost\n\n\nrevised_cost\ndouble\nrevised_cost\n\n\nexisting_use\ncharacter\nexisting_use\n\n\nexisting_units\ndouble\nexisting_units\n\n\nproposed_use\ncharacter\nproposed_use\n\n\nproposed_units\ndouble\nproposed_units\n\n\nplansets\ndouble\nplansets\n\n\ntidf_compliance\nlogical\ntidf_compliance\n\n\nexisting_construction_type\ndouble\nexisting_construction_type\n\n\nexisting_construction_type_description\ncharacter\nexisting_construction_type_description\n\n\nproposed_construction_type\ndouble\nproposed_construction_type\n\n\nproposed_construction_type_description\ncharacter\nproposed_construction_type_description\n\n\nsite_permit\ncharacter\nsite_permit\n\n\nsupervisor_district\ndouble\nsupervisor_district\n\n\nneighborhoods_analysis_boundaries\ncharacter\nneighborhoods_analysis_boundaries\n\n\nzipcode\ndouble\nzipcode\n\n\nlocation\ncharacter\nlocation\n\n\nrecord_id\ndouble\nrecord_id\n\n\ndate\ndouble\ndate\n\n\n\n\n\nnew_construction.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncartodb_id\ninteger\nID\n\n\nthe_geom\nlogical\ntype of geom\n\n\nthe_geom_webmercator\nlogical\nvariable\n\n\ncounty\ncharacter\nCountry\n\n\nyear\ninteger\nYear\n\n\ntotalproduction\ninteger\nTotal production of housing\n\n\nsfproduction\ninteger\nSingle family production\n\n\nmfproduction\ninteger\nmulti family production\n\n\nmhproduction\ninteger\nmobile home production\n\n\nsource\ncharacter\nsource\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(httr)\n\nraw_df &lt;- read_csv(\"2022/2022-07-05/clean_2000_2018.csv.zip\")\n\n\n# https://www.vitalsigns.mtc.ca.gov/housing-production\nurl &lt;- \"https://mtc.carto.com/api/v2/sql?q=SELECT%20*%20FROM%20nyee_uw6v%20ORDER%20BY%20county%2C%20year\"\n\nraw_json &lt;- url |&gt;\n  httr::GET() |&gt;\n  content()\n\nnew_construction &lt;- tibble(data = raw_json$rows) |&gt;\n  unnest_wider(data)\n\nnew_construction |&gt;\n  write_csv(\"2022/2022-07-05/new_construction.csv\")\n\npermit_url &lt;- \"https://data.sfgov.org/resource/i98e-djp9.json?permit_number=201602179765\"\n\npermit_url &lt;- \"https://data.sfgov.org/resource/i98e-djp9.json?$where=filed_date &gt; '2000-01-01T12:00:00'\"\n\ndownload.file(\"https://data.sfgov.org/resource/i98e-djp9.csv?$where=filed_date%20%3E%20%272000-01-01T12:00:00%27&$limit=800000\", \"2022/2022-07-05/permits.csv\")\n\npermits_raw &lt;- read_csv(\"2022/2022-07-05/permits.csv\")\n\npermit_build &lt;- permits_raw |&gt;\n  filter(permit_type %in% c(1, 2, 3, 6)) |&gt;\n  mutate(date = lubridate::date(permit_creation_date)) |&gt;\n  filter(date &lt;= as.Date(\"2018-12-31\"))\n\npermit_build |&gt;\n  write_csv(\"2022/2022-07-05/sf-permits.csv\")"
  },
  {
    "objectID": "data/2022/2022-06-21/readme.html",
    "href": "data/2022/2022-06-21/readme.html",
    "title": "American Slavery and Juneteenth",
    "section": "",
    "text": "Please add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nAbolition of slavery celebration, Washington DC April 19 1866"
  },
  {
    "objectID": "data/2022/2022-06-21/readme.html#african-american-achievements",
    "href": "data/2022/2022-06-21/readme.html#african-american-achievements",
    "title": "American Slavery and Juneteenth",
    "section": "African American Achievements",
    "text": "African American Achievements\nAdditional data this week comes from Wikipedia & Wikipedia. This will be a celebration of Black Lives, their achievements, and many of their battles against racism across their lives. This is in emphasis that Black Lives Matter and we’re focusing on a celebration of these lives. Each of those Wikipedia articles above have additional details and sub-links that are highly worth reading through.\nFor additional datasets related to describing racial problems that still exist in the US, please see a few of our previous #TidyTuesday datasets:\n- Note, if you decide to use these datasets please read through the source material to better understand the nuance behind the data.\n- School Diversity\n- Vera Institute Incarceration Trends\n- The Stanford Open Policing Project\nThe article for this week is the obituary for David Blackwell - Fought racism; became world famous statistician."
  },
  {
    "objectID": "data/2022/2022-06-21/readme.html#suggested-reading-material",
    "href": "data/2022/2022-06-21/readme.html#suggested-reading-material",
    "title": "American Slavery and Juneteenth",
    "section": "Suggested Reading Material",
    "text": "Suggested Reading Material\nPlease use your best judgement when working with this data. There is a lot of pain and suffering that we cannot fully capture in simple numbers and charts. We believe that it is important to understand how wide-spread slavery was, how many people were affected then, and how this continually impacts the world.\nTeaching Hard History - American Slavery &gt; The consequences of slavery continue to distort and stunt lives in America, so it’s quite right that we should engage in what can be an agonizing conversation about this history. Only when our history is faced squarely can removing Confederate monuments be properly understood, as a small but significant step toward ending the celebration of treason and white supremacy, if not toward ameliorating their effects.\nWe are influenced by the “Teaching Hard History” resources. Please take the time to review these materials, examine some of the quick Summary Videos\nThe full article around these education materials can be found here."
  },
  {
    "objectID": "data/2022/2022-06-21/readme.html#juneteenth-details",
    "href": "data/2022/2022-06-21/readme.html#juneteenth-details",
    "title": "American Slavery and Juneteenth",
    "section": "Juneteenth details",
    "text": "Juneteenth details\n\n\n\nJuneteenth Inscription\n\n\nTexas Historical Commission Juneteenth Inscription\n\nCOMMEMORATED ANNUALLY ON JUNE 19TH, JUNETEENTH IS THE OLDEST KNOWN CELEBRATION OF THE END OF SLAVERY IN THE U.S. THE EMANCIPATION PROCLAMATION, ISSUED BY PRESIDENT ABRAHAM LINCOLN ON SEP. 22, 1862, ANNOUNCED, “THAT ON THE 1ST DAY OF JANUARY, A.D. 1863, ALL PERSONS HELD AS SLAVES WITHIN ANY STATE…IN REBELLION AGAINST THE U.S. SHALL BE THEN, THENCEFORWARD AND FOREVER FREE.” HOWEVER, IT WOULD TAKE THE CIVIL WAR AND PASSAGE OF THE 13TH AMENDMENT TO THE CONSTITUTION TO END THE BRUTAL INSTITUTION OF AFRICAN AMERICAN SLAVERY. AFTER THE CIVIL WAR ENDED IN APRIL 1865 MOST SLAVES IN TEXAS WERE STILL UNAWARE OF THEIR FREEDOM. THIS BEGAN TO CHANGE WHEN UNION TROOPS ARRIVED IN GALVESTON. MAJ. GEN. GORDON GRANGER, COMMANDING OFFICER, DISTRICT OF TEXAS, FROM HIS HEADQUARTERS IN THE OSTERMAN BUILDING (STRAND AND 22ND ST.), READ ‘GENERAL ORDER NO. 3’ ON JUNE 19, 1865. THE ORDER STATED “THE PEOPLE OF TEXAS ARE INFORMED THAT, IN ACCORDANCE WITH A PROCLAMATION FROM THE EXECUTIVE OF THE UNITED STATES, ALL SLAVES ARE FREE. THIS INVOLVES AN ABSOLUTE EQUALITY OF PERSONAL RIGHTS AND RIGHTS OF PROPERTY BETWEEN FORMER MASTERS AND SLAVES.” WITH THIS NOTICE, RECONSTRUCTION ERA TEXAS BEGAN. FREED AFRICAN AMERICANS OBSERVED “EMANCIPATION DAY,” AS IT WAS FIRST KNOWN, AS EARLY AS 1866 IN GALVESTON. AS COMMUNITY GATHERINGS GREW ACROSS TEXAS, CELEBRATIONS INCLUDED PARADES, PRAYER, SINGING, AND READINGS OF THE PROCLAMATION. IN THE MID-20TH CENTURY, COMMUNITY CELEBRATIONS GAVE WAY TO MORE PRIVATE COMMEMORATIONS. A RE-EMERGENCE OF PUBLIC OBSERVANCE HELPED JUNETEENTH BECOME A STATE HOLIDAY IN 1979. INITIALLY OBSERVED IN TEXAS, THIS LANDMARK EVENT’S LEGACY IS EVIDENT TODAY BY WORLDWIDE COMMEMORATIONS THAT CELEBRATE FREEDOM AND THE TRIUMPH OF THE HUMAN SPIRIT."
  },
  {
    "objectID": "data/2022/2022-06-07/readme.html",
    "href": "data/2022/2022-06-07/readme.html",
    "title": "Pride Donations",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nPride Donations\nThe data this week comes from Data For Progress.\n\nEach year, hundreds of corporations around the country participate in Pride, an annual celebration of the LGBTQ+ community’s history and progress. They present themselves as LGBTQ+ allies, but new research from Data for Progress finds that in between their yearly parade appearances, dozens of these corporations are giving to state politicians behind some of the most bigoted and harmful policies in over a decade.\nActivists and allies wishing to hold these politicians accountable for bigotry can begin by holding their corporate backers accountable. In a new project series, Data for Progress has compiled a set of resources for activists, employees, community leaders, and lawmakers to push back on these policies and the prejudice powering them. We provide research tying the political giving of specific Fortune 500 companies to anti-LGBTQ+ politicians in six states, polling showing that such giving hurts the brands’ favorability, and upcoming policy memos to understand the issue and to take action.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-06-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 23)\n\ndonations &lt;- tuesdata$pride_aggregates\n\n# Or read in the data manually\n\npride_aggregates &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-07/pride_aggregates.csv')\nfortune_aggregates &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-07/fortune_aggregates.csv')\nstatic_list &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-07/static_list.csv')\npride_sponsors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-07/pride_sponsors.csv')\ncorp_by_politician &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-07/corp_by_politician.csv')\ndonors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-07/donors.csv')\n\n\n\n\nData Dictionary\n\n\n\npride_aggregates.csv\n\nPride sponsors who have donated to Anti-LGBTQ Campaigns\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCompany\ncharacter\nCompany\n\n\nTotal Contributed\ndouble\nTotal Contributed to anti-LBGTQ politicians\n\n\n# of Politicians Contributed to\ndouble\n# of politicians contributed, anti-LBGTQ\n\n\n# of States Where Contributions Made\ndouble\n# of states where contributions were made to anti-LBGTQ\n\n\n\n\n\nfortune_aggregates.csv\n\nFortune 500, Pride sponsors who have donated to Anti-LGBTQ Campaigns\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCompany\ncharacter\nCompany name\n\n\nTotal Contributed\ndouble\nTotal contributed to anti-LBGTQ politicians\n\n\n# of Politicians Contributed to\ndouble\n# of politicians contributed to\n\n\n# of States Where Contributions Made\ndouble\n# of states where contributions made to anti-LBGTQ\n\n\n\n\n\nstatic_list.csv\n\nOverview of company, pride sponsors, HRC Business pledge\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCompany\ncharacter\nCompany\n\n\nPride?\nlogical\nDonated to pride\n\n\nHRC Business Pledge\ncharacter\nHRC Business pledge\n\n\nAmount Contributed Across States\ndouble\nAmount contributed across states to anti-LBGTQ politicians\n\n\n# of Politicians Contributed to\ndouble\n# of politicians contributed\n\n\n# of States Where Contributions Made\ndouble\n# of states\n\n\n\n\n\npride_sponsors.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCompany\ncharacter\nCompany\n\n\nPride Event Sponsored\ncharacter\nEvent sponsored\n\n\nSponsorship Amount, where available\ncharacter\nSponsorship amount level\n\n\nYear\ndouble\nYear\n\n\nSource\ncharacter\nSource\n\n\nTrue donor value\ncharacter\nTrue donor name/value\n\n\n\n\n\ncorp_by_politician.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nPolitician\ncharacter\nPolitician\n\n\nSUM of Amount\ndouble\nSum of amount in USD to anti-LBGTQ\n\n\nTitle\ncharacter\nTitle of politician\n\n\nState\ncharacter\nState\n\n\n\n\n\ndonors.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nDonor Name\ncharacter\nvariable\n\n\n“True” Donor - Pride Sponsor Match Only\ncharacter\nvariable\n\n\n“True” Donor - Fortune Match Only\nlogical\nvariable\n\n\nPride and Sponsor Match?\ncharacter\nvariable\n\n\nDonor Name - Combined\ncharacter\nvariable\n\n\n\n\n\ncontribution_data_all_states.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCompany\ncharacter\nCompany\n\n\nPride and Sponsor Match?\ncharacter\nPride and Sponsorship Match\n\n\nPride?\nlogical\nPride event sponsor\n\n\nHRC Business Pledge\ncharacter\nHRC Busines pledge\n\n\nDonor Name\ncharacter\nDonor name\n\n\nPolitician\ncharacter\nPolitician\n\n\nState\ncharacter\nstate\n\n\nAmount\ndouble\nAmount in USD\n\n\nDate\ndouble\nDate\n\n\nCitation\nlogical\nCitation\n\n\nDonor Type\ncharacter\nDonor Type\n\n\nComments\nlogical\nComments\n\n\nARCHIVE - Company Manually Determined (May Not Match Pride Sponsors List)\nlogical\nArchive\n\n\n\n\nCleaning Script\nlibrary(httr)\nlibrary(tidyverse)\n\n# overall URL\norig_url &lt;- \"https://www.dataforprogress.org/accountable-allies\"\n\n# direct link to the iFrame that builds the table\niframe_url &lt;- \"https://dfp-accountable-allies.netlify.app/fortune\"\n\n# Found the link to the GoogleSheets\npar_url &lt;- \"https://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw\"\n\n# Extract data from JSON\nget_content &lt;- function(url){\n  raw_content &lt;- GET(url) |&gt; content()\n  \n  nm_content &lt;- raw_content$values[1] |&gt; unlist() |&gt; janitor::make_clean_names()\n  \n  raw_df &lt;- raw_content$values[2:length(raw_content$values)] |&gt;\n    tibble(data = _) |&gt;\n    mutate(data = map(data, ~set_names(.x, nm = nm_content[1:length(.x)]))) |&gt;\n    unnest_wider(data) |&gt;\n    readr::type_convert()\n  \n  raw_df\n}\n\n\n# get individual datasets\nfort_agg_url &lt;- \"https://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw/values/Fortune%20Aggregates\"\n\nfort_agg_df &lt;- get_content(pride_agg_url)\n\npride_agg_url &lt;- \"https://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw/values/Pride%20Aggregates\"\n\npride_agg_df &lt;- get_content(pride_agg_url)\n\nstatic_list_url &lt;- \"https://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw/values/Static%20List\"\n\nstatic_list_df &lt;- get_content(static_list_url)\n\npride_sponsor_url &lt;- \"https://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw/values/Pride%20Sponsors\"\n\npride_sponsor_df &lt;- get_content(pride_sponsor_url)\n\npride_sponsor_df |&gt; glimpse()\n\n\nraw_fort_agg &lt;- GET(fort_agg_url) |&gt; content()\n\nfort_agg_nm &lt;- raw_fort_agg$values[1] |&gt; unlist() |&gt; janitor::make_clean_names()\n\nraw_fort_agg_df &lt;- raw_fort_agg$values[2:length(raw_fort_agg$values)] |&gt;\n  tibble(data = _) |&gt;\n  mutate(data = map(data, ~set_names(.x, nm = fort_agg_nm))) |&gt;\n  unnest_wider(data) |&gt;\n  readr::type_convert() |&gt;\n  mutate(total_contributed = parse_number(total_contributed))\n\nurl_builder &lt;- function(text){\n  \n  text &lt;- stringr::str_replace_all(text, \" \", \"%20\")\n  text &lt;- stringr::str_replace_all(text, \"/\", \"%2F\")\n  glue::glue(\"https://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw/values/{text}\")\n}\n\ncorp_pol_df &lt;- url_builder(\"Corp by Politician\") |&gt;\n  get_content()\n\nlibrary(httr)\nlibrary(tidyverse)\nraw_sheet &lt;- \"https://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw\" |&gt;\n  GET() |&gt;\n  content()\n\nall_sheets &lt;- raw_sheet |&gt;\n  tibble(data = _) |&gt;\n  unnest_longer(data) |&gt;\n  unnest_wider(data) |&gt;\n  unnest_wider(properties) |&gt;\n  unnest_wider(gridProperties) |&gt;\n  janitor::clean_names() |&gt;\n  select(sheet_id, title, row_count:column_count)\n\nall_sheets\n\nurl_builder &lt;- function(text){\n  \n  text &lt;- stringr::str_replace_all(text, \" \", \"%20\")\n  text &lt;- stringr::str_replace_all(text, \"/\", \"%2F\")\n  glue::glue(\"https://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw/values/{text}\")\n}\n\nall_sheets |&gt;\n  pull(title) |&gt;\n  url_builder()\n\nall_sheets |&gt;\n  mutate(sheet_url = url_builder(title)) |&gt;\n  pull(sheet_url)\n\nhttps://sheets.googleapis.com/v4/spreadsheets/1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw/values/Fortune%201000%20List/Search%20Keys\n\nall_sheets |&gt;\n  slice(11) |&gt;\n  pull(title) |&gt;\n  url_builder()\n\n###### googlesheets\n\nlibrary(googlesheets4)\n\ngs4_deauth()\n\n# courtesty of Jenny Bryan\nssid &lt;- \"1Bj8YMaqxYrh2PxVhI1M1kLSbIMSN7vTpR2OZxg1DoXw\"\n(ss &lt;- gs4_get(ssid))\n\nread_sheet(ss, \"Fortune Aggregates\") |&gt; \n  write_csv('2022/2022-06-07/fortune_agg.csv')\n\nread_sheet(ss, \"Pride Sponsors\")\n\n# grab and clean sheets\nsub_sheets &lt;- ss$sheets |&gt; \n  slice(2:6, 12:13) |&gt; \n  select(name) |&gt; \n  mutate(title = paste0(janitor::make_clean_names(name), \".csv\")) \n\nall_df &lt;- sub_sheets |&gt; \n  mutate(data = map(name, ~read_sheet(ss, .x)))\n\nwrite_df &lt;- function(title, data){\n  write_csv(x = data, file =glue::glue(\"2022/2022-06-07/{title}\"))\n}\n\n# write out the datasets\nall_df |&gt; \n  select(title, data) |&gt; \n  pwalk(write_df)\n  \nall_df |&gt; \n  pull(data) |&gt; \n  map(create_tidytuesday_dictionary)"
  },
  {
    "objectID": "data/2022/2022-05-24/readme.html",
    "href": "data/2022/2022-05-24/readme.html",
    "title": "Women’s Rugby",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nWomen’s Rugby\nThe data this week comes from ScrumQueens by way of Jacquie Tran.\nScrumqueen can be found on Twitter @ScrumQueens\n\nWe write about women’s rugby & women in rugby. Volunteers with a passion for equality in our brilliant sport - by @alidonnelly & @johnlbirch.\n\nPer Wikipedia\n\nThe series, the women’s counterpart to the World Rugby Sevens Series, provides elite-level women’s competition between rugby nations. As with the men’s Sevens World Series, teams compete for the title by accumulating points based on their finishing position in each tournament.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-05-24')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 21)\n\nsevens &lt;- tuesdata$sevens\nfifteens &lt;- tuesdata$fifteens\n\n# Or read in the data manually\n\nsevens &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-24/sevens.csv')\nfifteens &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-24/fifteens.csv')\n\n\nData Dictionary\n\n\n\nsevens.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrow_id\ndouble\nRow ID for each observation\n\n\ndate\ndouble\nISO date\n\n\nteam_1\ncharacter\nTeam 1\n\n\nscore_1\ncharacter\nScore for Team 1\n\n\nscore_2\ncharacter\nScore for team 2\n\n\nteam_2\ncharacter\nTeam 2\n\n\nvenue\ncharacter\nLocation of game\n\n\ntournament\ncharacter\nTournament name\n\n\nstage\ncharacter\nStage of tournament\n\n\nt1_game_no\ndouble\nTeam 1 game number\n\n\nt2_game_no\ndouble\nTeam 2 game number\n\n\nseries\ndouble\nSeries number\n\n\nmargin\ndouble\nMargin of victory (diff between score 1/2)\n\n\nwinner\ncharacter\nWinner of match\n\n\nloser\ncharacter\nLoser of match\n\n\nnotes\ncharacter\nMisc notes\n\n\n\n\n\nfifteens.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntest_no\ndouble\nTest number\n\n\ndate\ndouble\nISO date\n\n\nteam_1\ncharacter\nTeam 1 name\n\n\nscore_1\ndouble\nScore for team 1\n\n\nscore_2\ndouble\nScore for team 2\n\n\nteam_2\ncharacter\nTeam 2 name\n\n\nvenue\ncharacter\nLocation of tournament\n\n\nhome_test_no\ndouble\nHome number\n\n\naway_test_no\ndouble\nAway game number\n\n\nseries_no\ndouble\nSeries number\n\n\ntournament\ncharacter\nTournament type\n\n\nmargin_of_victory\ndouble\nMargin of victory (diff of score 1/2)\n\n\nhome_away_win\ncharacter\nHome or away team won\n\n\nwinner\ncharacter\nWinner name\n\n\nloser\ncharacter\nLoser name\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nraw_df &lt;- read_csv(\"2022/2022-05-24/Scrumqueens-data-2022-05-23.csv\")\n\nclean_df &lt;- raw_df |&gt; \n  janitor::clean_names() |&gt; \n  glimpse() |&gt; \n  rename(row_id = x1)\n\nclean_df |&gt; \n  write_csv('2022/2022-05-24/sevens.csv')\n\nraw_15 &lt;- read_csv(\"2022/2022-05-24/Scrumqueens-data-2022-05-23 (1).csv\")\n\nclean_15 &lt;- raw_15 |&gt; \n  janitor::clean_names() \n\nclean_15 |&gt; \n  write_csv('2022/2022-05-24/fifteens.csv')\n\ncreate_tidytuesday_dictionary(clean_df)"
  },
  {
    "objectID": "data/2022/2022-05-10/readme.html",
    "href": "data/2022/2022-05-10/readme.html",
    "title": "NY Times bestsellers list",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization:\nChart type\nIt’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph\nType of data\nWhat data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year\nReason for including the chart\nThink about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales\nLink to data or source\nDon’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nNY Times bestsellers list\nThe data this week comes from Post45 Data by way of Sara Stoudt.\nSee their Data Description for full details.\n\nEach peer-reviewed dataset has an accompanying curatorial statement, which provides an overview of the data that explains its contents, construction, and some possible uses.\nPlease cite data from the Post45 Data Collective and use the following six components in your citation: \n\nauthor name(s)  \ndate published in the Post45 repository \ntitle  \nglobal persistent &gt; identifier: DOI   \nPost45 Data Collective \nversion number \n\nExample replication data citation from The Program Era Project, Kelly, White, and Glass, 2021:\nKelly, Nicholas; White, Nicole, Glass, Loren, 03/01/2021, “The Program Era &gt; Project,” DOI: https://doi.org/10.18737/CNJV1733p4520210415, Post45 Data &gt; Collective, V1.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-05-10')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 19)\n\nnyt_titles &lt;- tuesdata$nyt_titles\n\n# Or read in the data manually\n\nnyt_titles &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-10/nyt_titles.tsv')\nnyt_full &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-10/nyt_full.tsv')\n\n\nData Dictionary\n\n\n\nnyt_titles.tsv\nThis is a subset/summary of the larger dataset.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nBook id\n\n\ntitle\ncharacter\nTitle of book\n\n\nauthor\ncharacter\nAuthor\n\n\nyear\ndouble\nYear\n\n\ntotal_weeks\ndouble\nTotal weeks on best sellers list\n\n\nfirst_week\ndouble\nFirst week on list (date)\n\n\ndebut_rank\ndouble\nDebut rank on the list\n\n\nbest_rank\ndouble\nBest rank\n\n\n\n\n\nnyt_full.tsv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\nweek\ndouble\nWeek (as date)\n\n\nrank\ndouble\nRank (1 &gt; 18)\n\n\ntitle_id\ndouble\nTitle ID\n\n\ntitle\ncharacter\nTitle of book\n\n\nauthor\ncharacter\nAuthor\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-04-26/readme.html",
    "href": "data/2022/2022-04-26/readme.html",
    "title": "Kaggle Hidden Gems",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization:\nChart type\nIt’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph\nType of data\nWhat data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year\nReason for including the chart\nThink about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales\nLink to data or source\nDon’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nKaggle Hidden Gems\nThe data this week comes from Kaggle by way of Martin Henze (Heads or Tails). You can find some of their initial analysis in a starter R notebook.\nThis dataset is born out of the Notebooks of the Week - Hidden Gems project.\n\nOn Kaggle, many great Notebooks (aka “Kernels”) can be found that have a large number of well-deserved upvotes. I’m publishing a weekly series of “Hidden Gems” in which I aspire to find and showcase underrated Notebooks on Kaggle in an effort to illuminate the diversity and creativity of our community.\nThis series looks beyond those well-recognised works to find Notebooks which I personally like and which, somehow, didn’t receive the attention that I think they deserve. My selections are always highly subjective; reflecting my own views and preferences. I have managed to publish an episode of “Hidden Gems” every week since the first post back in May 2020 and I hope to be able to keep this weekly schedule for the foreseeable future.\n\n\nWelcome to this special Kaggle Community competition on the Hidden Gems dataset!\nTo celebrate 100 episodes of Hidden Gems we are hosting a Notebooks contest aimed at exploring all 300 Notebook entries and their (meta) characteristics. This is the place for you to showcase and practice your skills when it comes to exploratory data analysis, data visualisation, and crafting engaging narratives. Discover insights, publish your notebooks, and win cool prizes!\nAs an introduction & AMA to this competition, we will be talking very shortly with Sanyam Bhutani about the Hidden Gems series and the Notebooks challenge: Tune in at 6pm CET today on April 5th!\nSee all the details below, and don’t hesitate to ask for clarification. I hope you’ll enjoy this competition.\n\nTo participate in the crossover challenge, please check out the Kaggle page.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-04-26')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 17)\n\nhidden_gems &lt;- tuesdata$hidden_gems\n\n# Or read in the data manually\n\nhidden_gems &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-04-26/hidden_gems.csv')\n\n\nData Dictionary\n\n\n\nhidden_gems.csv\nThe file of interest is kaggle_hidden_gems.csv, containing the following columns:\n\nvol and date: The consecutive number of the Hidden Gems episode and when it was first published.\nlink_forum and link_twitter: The hyperlinks to the Kaggle Forum post and Twitter post for the episode.\nnotebook and author: The hyperlinks to the Notebook itself, as well as to the Kaggle profile of the author.\ntitle: The Notebook title as a string.\nreview: My brief review of the Notebook.\nauthor_name: The name of the Notebook author as listed on their Kaggle profile at the time the episode was published.\nauthor_twitter and author_linkedin: The social media links of the author, if listed on their Kaggle profile.\nnotes: Notes about special episodes.\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-04-12/readme.html",
    "href": "data/2022/2022-04-12/readme.html",
    "title": "Indoor Air pollution",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization:\nChart type\nIt’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph\nType of data\nWhat data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year\nReason for including the chart\nThink about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales\nLink to data or source\nDon’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nIndoor Air pollution\nThe data this week comes from Our World in Data.\n\nIndoor air pollution is caused by burning solid fuel sources – such as firewood, crop waste, and dung – for cooking and heating.\nThe burning of such fuels, particularly in poor households, results in air pollution that leads to respiratory diseases which can result in premature death. The WHO calls indoor air pollution “the world’s largest single environmental health risk.”\n\n\nIndoor air pollution is a leading risk factor for premature death.\n4.1% of global deaths are attributed to indoor air pollution.\nDeath rates from air pollution are highest in low-income countries. There’s a greater than 1000-fold difference between low- and high-income countries.\nThe world is making progress: global deaths from indoor air pollution have declined substantially since 1990.\nDeath rates from indoor air pollution have declined in almost every country in the world since 1990.\nIndoor air pollution results from a reliance of solid fuels for cooking.\nOnly 60% of the world has access to clean fuels for cooking. This share has been steadily increasing.\nThe use of solid fuels for cooking has been declining across world regions, but is still high.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-04-12')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 15)\n\nindoor_pollution &lt;- tuesdata$indoor_pollution\n\n# Or read in the data manually\n\nindoor_pollution &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-04-12/indoor_pollution.csv')\n\n\nData Dictionary\n\n\n\nfuel_access.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry name\n\n\nCode\ncharacter\nCountry code\n\n\nYear\ndouble\nYear\n\n\nAccess to clean fuels and technologies for cooking (% of population)\ndouble\nAccess to clean fuels and technologies for cooking (% of population)\n\n\n\n\n\nfuel_gdp.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\n.\n\n\nCode\ncharacter\n.\n\n\nYear\ndouble\n.\n\n\nAccess to clean fuels and technologies for cooking (% of population)\ndouble\n.\n\n\nGDP per capita, PPP (constant 2017 international $)\ndouble\n.\n\n\nPopulation (historical estimates)\ndouble\n.\n\n\nContinent\ncharacter\n.\n\n\n\n\n\ndeath_source.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\n.\n\n\nCode\ncharacter\n.\n\n\nYear\ndouble\n.\n\n\nDeaths - Cause: All causes - Risk: Household air pollution from solid fuels - Sex: Both - Age: Age-standardized (Rate)\ndouble\n.\n\n\n\n\n\ndeath_full.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\n.\n\n\nCode\ncharacter\n.\n\n\nYear\ndouble\n.\n\n\nDeaths - Cause: All causes - Risk: Household air pollution from solid fuels - Sex: Both - Age: Age-standardized (Rate)\ndouble\n.\n\n\nAccess to clean fuels and technologies for cooking (% of population)\ndouble\n.\n\n\nPopulation (historical estimates)\ndouble\n.\n\n\nContinent\ncharacter\n.\n\n\n\n\n\ndeath_timeseries.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\n.\n\n\nCode\ncharacter\n.\n\n\nYear…3\ndouble\n.\n\n\nDeaths - Cause: All causes - Risk: Household air pollution from solid fuels - Sex: Both - Age: All Ages (Number)…4\ndouble\n.\n\n\nDeaths - Cause: All causes - Risk: Household air pollution from solid fuels - Sex: Both - Age: All Ages (Number)…5\ndouble\n.\n\n\nYear…6\ndouble\n.\n\n\nContinent\ncharacter\n.\n\n\n\n\n\nindoor_pollution.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\n.\n\n\nCode\ncharacter\n.\n\n\nYear…3\ndouble\n.\n\n\nDeaths - Cause: All causes - Risk: Household air pollution from solid fuels - Sex: Both - Age: All Ages (Number)…4\ndouble\n.\n\n\nDeaths - Cause: All causes - Risk: Household air pollution from solid fuels - Sex: Both - Age: All Ages (Number)…5\ndouble\n.\n\n\nYear…6\ndouble\n.\n\n\nContinent\ncharacter\n.\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-03-29/readme.html",
    "href": "data/2022/2022-03-29/readme.html",
    "title": "Collegiate sports",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nCollegiate sports\nThe data this week comes from Equity in Athletics Data Analysis, hattip to Data is Plural.\nAn additional article can be found at USA Facts.\nPlease note that I have only used a subset of all available columns, there are MANY other columns of interest like coaching staff and other statistics. Please see the Schoolsdoc file in each folder (EADA_2016-2017, etc) for the definitions of additional columns.\nAdditional articles from US News and NPR.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-03-29')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 13)\n\nsports &lt;- tuesdata$sports\n\n# Or read in the data manually\n\nsports &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-03-29/sports.csv')\n\n\nData Dictionary\n\n\n\nsports.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nyear, which is year: year + 1, eg 2015 is 2015 to 2016\n\n\nunitid\ndouble\nSchool ID\n\n\ninstitution_name\ncharacter\nSchool name\n\n\ncity_txt\ncharacter\nCity name\n\n\nstate_cd\ncharacter\nState abbreviation\n\n\nzip_text\ncharacter\nZip of school\n\n\nclassification_code\ndouble\nCode for school classification\n\n\nclassification_name\ncharacter\nSchool classification\n\n\nclassification_other\ncharacter\nSchool classification other\n\n\nef_male_count\ndouble\nTotal male student\n\n\nef_female_count\ndouble\nTotal Female student\n\n\nef_total_count\ndouble\nTotal student for binary male/female gender (sum of previous two cols)\n\n\nsector_cd\ndouble\nSector code\n\n\nsector_name\ncharacter\nSector name\n\n\nsportscode\ndouble\nSport code\n\n\npartic_men\ndouble\nParticipation men\n\n\npartic_women\ndouble\nParticipation women\n\n\npartic_coed_men\ndouble\nParticipation as coed men\n\n\npartic_coed_women\ndouble\nParticipation for coed women\n\n\nsum_partic_men\ndouble\nSum of participation for men\n\n\nsum_partic_women\ndouble\nSum of participation women\n\n\nrev_men\ndouble\nRevenue in USD for men\n\n\nrev_women\ndouble\nRevenue in USD for women\n\n\ntotal_rev_menwomen\ndouble\nTotal revenue for both\n\n\nexp_men\ndouble\nExpenditures in USD for men\n\n\nexp_women\ndouble\nExpenditures in USD for women\n\n\ntotal_exp_menwomen\ndouble\nTotal Expenditure for both\n\n\nsports\ncharacter\nSport name\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nread_clean &lt;- function(year) {\n  raw_df &lt;- readxl::read_excel(glue::glue(\"2022/2022-03-29/EADA_{year}-{as.double(year)+1}/Schools.xlsx\"))\n\n  clean_df &lt;- raw_df %&gt;%\n    select(\n      unitid, institution_name, city_txt:SUM_PARTIC_WOMEN,\n      REV_MEN:TOTAL_REV_MENWOMEN, EXP_MEN:TOTAL_EXP_MENWOMEN, Sports\n    ) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(year = as.integer(year), .before = 1) %&gt;%\n    type_convert()\n\n  clean_df\n}\n\nall_df &lt;- 2015:2019 %&gt;%\n  map_dfr(read_clean)\n\nall_df %&gt;% \n  write_csv(\"2022/2022-03-29/sports.csv\")"
  },
  {
    "objectID": "data/2022/2022-03-15/readme.html",
    "href": "data/2022/2022-03-15/readme.html",
    "title": "R vignettes",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nR vignettes\nThe data this week comes from Robert Flight.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-03-15')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 11)\n\nbioc &lt;- tuesdata$bioc\n\n# Or read in the data manually\n\nbioc &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-03-15/bioc.csv')\ncran &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-03-15/cran.csv')\n\n\n\nData Dictionary\n\n\n\nbioc.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndatetime\nDate and time when package was uploaded/updated\n\n\nrnw\ninteger\nRNW (Sweave) based vignette count\n\n\nrmd\ninteger\nRMD based vignette count\n\n\npackage\ncharacter\nPackage name\n\n\n\n\n\ncran.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npackage\ncharacter\nPackage name\n\n\nversion\ncharacter\nPkg version number\n\n\ndate\ndatetime\nDate and time when package was uploaded/updated\n\n\nrnw\ninteger\nRNW (Sweave) based vignette count\n\n\nrmd\ninteger\nRMD based vignette count"
  },
  {
    "objectID": "data/2022/2022-03-01/readme.html",
    "href": "data/2022/2022-03-01/readme.html",
    "title": "Alternative fuel stations",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nAlternative fuel stations\nThe data this week comes from US DOT. An interactive map can be used at https://www.arcgis.com/home/webmap/viewer.html?panel=gallery&layers=cc51698ab9d94d67b4ec5dc5b8d97f34 with a short article from the EIA.\nHattip to Data is Plural for sharing this resource.\nNote there is also a shapefile\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-03-01')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 9)\n\nstations &lt;- tuesdata$stations\n\n# Or read in the data manually\n\nstations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-03-01/stations.csv')\n\n\nData Dictionary\n\n\n\nstations.csv\nSee full LARGE data dictionary at: https://afdc.energy.gov/data_download/alt_fuel_stations_format\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-02-15/readme.html",
    "href": "data/2022/2022-02-15/readme.html",
    "title": "Du Bois Visualization Challenge: 2022",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization:\nChart type\nIt’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph\nType of data\nWhat data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year\nReason for including the chart\nThink about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales\nLink to data or source\nDon’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nDuBois Challenge\nThe data this week comes from Anthony Starks as part of the #DuBoisChallenge2022. Credit to Anthony Starks, Allen Hillery and Sekou Tyler for creating and executing this challenge!\nThe WEB DuBois challenge is live for 10 weeks, and you can use the #DuBoisChallenge2022 hashtag to participate in this challenge. Full details available in the DuBois data portraits repo. Anthony Starks has written an excellent article covering the details of last year’s challenge and this year’s new challenge in the Nightingale by DVS.\nWhile there are several datasets in that repo, there is also the option to BYOD (Bring your own data)! Details below from the repo. The primary request is to try and recreate the “Duboisian” style.\n\n\n\n\n\n\nA screenshot of the 10 visualizations from the DuBois Challenge. Each has a color scheme of bright yellow, blue, black, or green. A unique aspect is the use of curves/circles to indicate the number.\n\n\n\nThe goal of the challenge is to celebrate the data visualization legacy of W.E.B Du Bois by recreating the visualizations from the 1900 Paris Exposition using modern tools.\n\n\nThis directory contains the data and original plates from the exposition; your goal is to re-create the visualizations using modern tools of your choice (Tableau, R, ggplot, Stata, PowerBI, decksh, etc)\n\n\nThere is a folder for each challenge, which includes the images of the 1900 original plates along with the corresponding data. You may submit your re-creations to twitter using the hash tag #DuBoisChallenge2022\n\n\nThe Challenges\n\n\n\nchallenge01: The Georgia Negro (plate 1)\nchallenge02: Assessed Valuation of all Taxable Property Owned by Georgia Negroes (plate 22)\nchallenge03: Relative Negro Population of the States of the United States (plate 2)\nchallenge04: Valuation of Town and City Property Owned by Georgia Negroes (plate 21)\nchallenge05: Slave and Free Negroes (plate 12)\nchallenge06: Illiteracy (plate 14)\nchallenge07: Conjugal condition of American Negroes according to age periods (plate 53)\nchallenge08: Assessed Value of Household and Kitchen Furniture Owned by Georgia Negroes. (plate 25)\nchallenge09: Number Of Negro Students Taking The Various Courses Of Study Offered In Georgia Schools (plate 17)\nchallenge10: Proportion Of Total Negro Children Of School Age Who Are Enrolled In The Public Schools (plate 49)\n\nReferences\n\n\nTo learn about how I re-created the visualizations using decksh, see: Recreating the Dubois Data Portraits. This presentation contains the full catalog of re-creations at the end.\n\n\nAlso, here is a quick guide to the “Duboisian” style.\n\nData available in the GitHub repo.\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-02-01/readme.html",
    "href": "data/2022/2022-02-01/readme.html",
    "title": "Dog breeds",
    "section": "",
    "text": "Please add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media."
  },
  {
    "objectID": "data/2022/2022-02-01/readme.html#data-dictionary",
    "href": "data/2022/2022-02-01/readme.html#data-dictionary",
    "title": "Dog breeds",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\nbreed_traits.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nBreed\ncharacter\nDog Breed\n\n\nAffectionate With Family\ncharacter\nPlacement on scale of 1-5 for the breed’s tendancy to be “Affectionate With Family” (Trait_Score)\n\n\nGood With Young Children\ncharacter\nPlacement on scale of 1-5 for the breed’s tendancy to be “Good With Young Children” (Trait_Score)\n\n\nGood With Other Dogs\ncharacter\nPlacement on scale of 1-5 for the breed’s tendancy to be “Good With Other Dogs” (Trait_Score)\n\n\nShedding Level\ncharacter\nPlacement on scale of 1-5 for the breed’s “Shedding Level” (Trait_Score)\n\n\nCoat Grooming Frequency\ncharacter\nPlacement on scale of 1-5 for the breed’s “Coat Grooming Frequency” (Trait_Score)\n\n\nDrooling Level\ncharacter\nPlacement on scale of 1-5 for the breed’s “Drooling Level” (Trait_Score)\n\n\nCoat Type\ncharacter\nDescription of the breed’s coat type (Trait_Score)\n\n\nCoat Length\ncharacter\nDescription of the breed’s coat length (Trait_Score)\n\n\nOpenness To Strangers\ncharacter\nPlacement on scale of 1-5 for the breed’s tendancy to be open to strangers (Trait_Score)\n\n\nPlayfulness Level\ncharacter\nPlacement on scale of 1-5 for the breed’s tendancy to be playful (Trait_Score)\n\n\nWatchdog/Protective Nature\ncharacter\nPlacement on scale of 1-5 for the breed’s “Watchdog/Protective Nature” (Trait_Score)\n\n\nAdaptability Level\ncharacter\nPlacement on scale of 1-5 for the breed’s tendancy to be adaptable (Trait_Score)\n\n\nTrainability Level\ncharacter\nPlacement on scale of 1-5 for the breed’s tendancy to be adaptable (Trait_Score)\n\n\nEnergy Level\ncharacter\nPlacement on scale of 1-5 for the breed’s “Energy Level” (Trait_Score)\n\n\nBarking Level\ncharacter\nPlacement on scale of 1-5 for the breed’s “Barking Level” (Trait_Score)\n\n\nMental Stimulation Needs\ncharacter\nPlacement on scale of 1-5 for the breed’s “Mental Stimulation Needs” (Trait_Score)\n\n\n\n\n\ntrait_description.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nTrait\ncharacter\nDog Breed\n\n\nTrait_1\ncharacter\nValue corresponding to Trait when Trait_Score = 1\n\n\nTrait_5\ncharacter\nValue corresponding to Trait when Trait_Score = 5\n\n\nDescription\ncharacter\nLong description of trait\n\n\n\n\n\nbreed_rank_all.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nBreed\ncharacter\nDog Breed\n\n\n2013 Rank\ncharacter\nPopularity of breed based on AKC registration statistics in 2013\n\n\n2014 Rank\ncharacter\nPopularity of breed based on AKC registration statistics in 2014\n\n\n2015 Rank\ncharacter\nPopularity of breed based on AKC registration statistics in 2015\n\n\n2016 Rank\ncharacter\nPopularity of breed based on AKC registration statistics in 2016\n\n\n2017 Rank\ncharacter\nPopularity of breed based on AKC registration statistics in 2017\n\n\n2018 Rank\ncharacter\nPopularity of breed based on AKC registration statistics in 2018\n\n\n2019 Rank\ncharacter\nPopularity of breed based on AKC registration statistics in 2019\n\n\n2020 Rank\ncharacter\nPopularity of breed based on AKC registration statistics in 2020\n\n\nlinks\ncharacter\nLink to the dog breed’s AKC webpage\n\n\nImage\ncharacter\nLink to image of dog breed\n\n\n\n\nScripts to retrieve the data: - webscraping_script-1.Rmd - webscraping_script-2.Rmd\nLast retrieved on Dec. 23, 2021"
  },
  {
    "objectID": "data/2022/2022-01-18/readme.html",
    "href": "data/2022/2022-01-18/readme.html",
    "title": "Chocolate Ratings",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nChocolate Ratings\nThe data this week comes from Flavors of Cacao by way of Georgios and Kelsey.\nSome analysis from 2017 on this data at Kaggle.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-01-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 3)\n\nchocolate &lt;- tuesdata$chocolate\n\n# Or read in the data manually\n\nchocolate &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-01-18/chocolate.csv')\n\n\nData Dictionary\n\n\n\nchocolate.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nref\ninteger\nReference ID, The highest REF numbers were the last entries made.\n\n\ncompany_manufacturer\ncharacter\nManufacturer name\n\n\ncompany_location\ncharacter\nManufacturer region\n\n\nreview_date\ninteger\nReview date (year)\n\n\ncountry_of_bean_origin\ncharacter\nCountry of origin\n\n\nspecific_bean_origin_or_bar_name\ncharacter\nSpecific bean or bar name\n\n\ncocoa_percent\ncharacter\nCocoa percent (% chocolate)\n\n\ningredients\ncharacter\nIngredients, (“#” = represents the number of ingredients in the chocolate; B = Beans, S = Sugar, S* = Sweetener other than white cane or beet sugar, C = Cocoa Butter, V = Vanilla, L = Lecithin, Sa = Salt)\n\n\nmost_memorable_characteristics\ncharacter\nMost Memorable Characteristics column is a summary review of the most memorable characteristics of that bar. Terms generally relate to anything from texture, flavor, overall opinion, etc. separated by ‘,’\n\n\nrating\ndouble\nrating between 1-5\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-01-04/readme.html",
    "href": "data/2022/2022-01-04/readme.html",
    "title": "Week 1",
    "section": "",
    "text": "Week 1\nThis was really just a bring your own dataset week.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2022-01-04\nBring your own data from 2021!"
  },
  {
    "objectID": "data/2021/2021-12-21/readme.html",
    "href": "data/2021/2021-12-21/readme.html",
    "title": "Starbucks",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nStarbucks\nOfficial Starbucks Nutritional dataset from the pdf Starbucks Coffee Company Beverage Nutrition Information. The pdf version is 22 pages and only steamed milk data is omitted for this dataset.\nHat-tip to PythonCoderUnicorn for their contribution!\nThere’s some infographics from Behance.net\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-12-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 52)\n\nstarbucks &lt;- tuesdata$starbucks\n\n# Or read in the data manually\n\nstarbucks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-12-21/starbucks.csv')\n\n\nData Dictionary\n\n\n\nstarbucks.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nProduct_Name\ncharacter\nProduct Name\n\n\nSize\ncharacter\nSize of drink (short, tall, grande, venti)\n\n\nMilk\ndouble\nMilk Type type of milk used\n\n\n\n\n- 0 none\n\n\n\n\n- 1 nonfat\n\n\n\n\n- 2 2%\n\n\n\n\n- 3 soy\n\n\n\n\n- 4 coconut\n\n\n\n\n- 5 whole\n\n\nWhip\ndouble\nWhip added or not (binary 0/1)\n\n\nServ_Size_mL\ndouble\nServing size in ml\n\n\nCalories\ndouble\nKCal\n\n\nTotal_Fat_g\ndouble\nTotal fat grams\n\n\nSaturated_Fat_g\ndouble\nSaturated fat grams\n\n\nTrans_Fat_g\ncharacter\nTrans fat grams\n\n\nCholesterol_mg\ndouble\nCholesterol mg\n\n\nSodium_mg\ndouble\nSodium milligrams\n\n\nTotal_Carbs_g\ndouble\nTotal Carbs grams\n\n\nFiber_g\ncharacter\nFiber grams\n\n\nSugar_g\ndouble\nSugar grams\n\n\nCaffeine_mg\ndouble\nCaffeine in milligrams\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-12-07/readme.html",
    "href": "data/2021/2021-12-07/readme.html",
    "title": "Spiders",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nSpiders\nThe data this week comes from World Spider Database.\n\nSpecies export World Spider Catalog by https://wsc.nmbe.ch is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nMajer et al, 2015\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-12-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 50)\n\nspiders &lt;- tuesdata$spiders\n\n# Or read in the data manually\n\nspiders &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-12-07/spiders.csv')\n\n\nData Dictionary\n\n\n\nspiders.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspeciesId\ndouble\nSpecies unique id\n\n\nspecies_lsid\ncharacter\nSpecies lsid\n\n\nfamily\ncharacter\nFamily\n\n\ngenus\ncharacter\nGenus\n\n\nspecies\ncharacter\nSpecies\n\n\nsubspecies\ncharacter\nSubspecies\n\n\nauthor\ncharacter\nAuthor\n\n\nyear\ndouble\nYear\n\n\nparentheses\ndouble\nParentheses\n\n\ndistribution\ncharacter\nDistribution/region, comma separated\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-11-23/readme.html",
    "href": "data/2021/2021-11-23/readme.html",
    "title": "Dr. Who",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nDr. Who\nThe data this week comes from the datardis package by way of Jonathan Kitt.\nThey have a short blogpost on the package at: https://randomics.netlify.app/posts/2021-11-16-datardis/ (post no longer available as of 2024-05-06).\nAdditional brief articlet from the Independent.ie\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-11-23')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 48)\n\ndirectors &lt;- tuesdata$directors\n\n# Or read in the data manually\n\ndirectors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-11-23/directors.csv')\nepisodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-11-23/episodes.csv')\nwriters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-11-23/writers.csv')\nimdb &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-11-23/imdb.csv')\n\n\n\n\nData Dictionary\n\n\n\ndirectors.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstory_number\ncharacter\nStory number\n\n\ndirector\ncharacter\nDirector\n\n\n\n\n\nepisodes.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nera\ncharacter\nEra = Classic or revived\n\n\nseason_number\ndouble\nSeason number\n\n\nserial_title\ncharacter\nSerial title\n\n\nstory_number\ncharacter\nStoru number\n\n\nepisode_number\ninteger\nEpisode number\n\n\nepisode_title\ncharacter\nEpisode title\n\n\ntype\ncharacter\nType\n\n\nfirst_aired\ndouble\nFirst aired date\n\n\nproduction_code\ncharacter\nProduction code\n\n\nuk_viewers\ndouble\nUK Viewership in Millions\n\n\nrating\ndouble\nRating\n\n\nduration\ndouble\nDuration in minutes\n\n\n\n\n\nwriters.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstory_number\ncharacter\nStory number\n\n\nwriter\ncharacter\nWriter\n\n\n\n\n\nimdb.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ninteger\nSeason number\n\n\nep_num\ndouble\nEpisode number\n\n\nair_date\ncharacter\nAir date\n\n\nrating\ndouble\nRating\n\n\nrating_n\ndouble\nNumber of ratings\n\n\ndesc\ncharacter\nEpisode description\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(rvest)\n\nseason &lt;- 1\n\nget_imdb &lt;- function(season){\n  url &lt;- glue::glue(\"https://www.imdb.com/title/tt0436992/episodes?season={season}\")\n  \n  raw_html &lt;- read_html(url)\n  \n  raw_div &lt;- raw_html %&gt;% \n    html_elements(\"div.list.detail.eplist\") %&gt;% \n    html_elements(\"div.info\")\n  \n  ep_num &lt;- raw_div %&gt;% \n    html_elements(\"meta\") %&gt;% \n    html_attr(\"content\")\n  \n  air_date &lt;- raw_div %&gt;% \n    html_elements(\"div.airdate\") %&gt;% \n    html_text() %&gt;% \n    str_squish()\n  \n  ratings &lt;- raw_div %&gt;% \n    html_elements(\"div.ipl-rating-star.small &gt; span.ipl-rating-star__rating\") %&gt;% \n    html_text()\n  rate_ct &lt;- raw_div %&gt;% \n    html_elements(\"div.ipl-rating-star.small &gt; span.ipl-rating-star__total-votes\")%&gt;% \n    html_text() %&gt;% \n    str_remove_all(\"\\\\(|\\\\)|,\")\n  \n  descrip &lt;- raw_div %&gt;% \n    html_elements(\"div.item_description\") %&gt;% \n    html_text() %&gt;% \n    str_squish()\n  \n  tibble(\n    season = season,\n    ep_num = ep_num,\n    air_date = air_date,\n    rating = ratings, rating_n = rate_ct, desc = descrip)\n  \n}\n\nall_season &lt;- 1:12 %&gt;% \n  map_dfr(get_imdb)\n\nclean_season &lt;- all_season %&gt;% \n  type_convert()\n\nclean_season %&gt;% \n  write_csv(\"2021/2021-11-23/imdb.csv\")"
  },
  {
    "objectID": "data/2021/2021-11-09/readme.html",
    "href": "data/2021/2021-11-09/readme.html",
    "title": "afrimapr",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nafrimapr\nThe data this week comes from the afrimapr team and the learning/teaching of spatial techniques with the afrilearndata package.\nAdditional larger dataset on &gt; 100K health facility points for Africa.\n\nGet the data here\n# Get the Data\n\n# Data accessed via the afrimapr or afrilearndata packages\n\n# afrilearndata\nremotes::install_github(\"afrimapr/afrilearndata\")\n# afrihealthsites`\nremotes::install_github(\"afrimapr/afrihealthsites\")\n\n\nData Dictionary\nPlease see:\n\nafrilearndata\n\nafrimapr\n\nafrihealthsites\n\n\n\nCleaning Script\nTo prep this week, please install the following packages:\n\nafrilearndata\n\nremotes::install_github(\"afrimapr/afrilearndata\")\n\nafrihealthsites\n\nremotes::install_github(\"afrimapr/afrihealthsites\")"
  },
  {
    "objectID": "data/2021/2021-10-26/readme.html",
    "href": "data/2021/2021-10-26/readme.html",
    "title": "Ultra Trail Running",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nUltra Trail Running\nThe data this week comes from Benjamin Nowak by way of International Trail Running Association (ITRA). Their original repo is available on GitHub.\nA nice overview of some similar running statistics is available RunRepeat.com.\n\nIn this study, we explore the trends in ultra running over the last 23 years. We have analyzed 5,010,730 results from 15,451 ultra running events, making this the largest study ever done on the sport.\n\n\nKey results\n\n\n\nFemale ultra runners are faster than male ultra runners at distances over 195 miles. The longer the distance the shorter the gender pace gap. In 5Ks men run 17.9% faster than women, at marathon distance the difference is just 11.1%, 100-mile races see the difference shrink to just .25%, and above 195 miles, women are actually 0.6% faster than men.\n\n\n\n\nParticipation has increased by 1676% in the last 23 years from 34,401 to 611,098 yearly participations and 345% in the last 10 years from 137,234 to 611,098. There have never been more ultra runners.\n\n\n\n\nMore ultra runners are competing in multiple events per year. In 1996, only 14% of runners participated in multiple races a year, now 41% of participants run more than one event per year. There is also a significant increase in the % of people who run 2 races a year, 17.2% (from 7.7% to 24.9%) and 3 races, 6.7% (from 2.8% to 9.5%).\n\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-10-26')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 44)\n\nultra_rankings &lt;- tuesdata$ultra_rankings\n\n# Or read in the data manually\n\nultra_rankings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-26/ultra_rankings.csv')\nrace &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-26/race.csv')\n\n\nData Dictionary\n\n\n\nultra_rankings.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrace_year_id\ndouble\nRace ID\n\n\nrank\ndouble\nRacer rank\n\n\nrunner\ncharacter\nRunner name\n\n\ntime\ndouble\nRunner time, hour:minute:seconds\n\n\nage\ndouble\nRunner age\n\n\ngender\ncharacter\nRunner gender (Man = M, Woman = W)\n\n\nnationality\ncharacter\nNationality of runner\n\n\n\n\n\nrace.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrace_year_id\ndouble\nRace ID for join\n\n\nevent\ncharacter\nEvent name\n\n\nrace\ncharacter\nRace Name\n\n\ncity\ncharacter\nCity name\n\n\ncountry\ncharacter\nCountry Name\n\n\ndate\ndouble\nDate\n\n\nstart_time\ndouble\nStart Time\n\n\nparticipation\ncharacter\nParticipation type\n\n\ndistance\ndouble\nDistance traveled in Km\n\n\nelevation_gain\ndouble\nElevation gains in meters\n\n\nelevation_loss\ndouble\nEleveation loss in meters\n\n\naid_stations\ndouble\nAid station count\n\n\nparticipants\ndouble\nTotal N of participants\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-10-12/readme.html",
    "href": "data/2021/2021-10-12/readme.html",
    "title": "Global Fishing",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nA photo of several fishermen standing over about a dozen large green buckets of fish. The fish are light grey and white, very long and each is about 12 inches long. They are on a beach, with the ocean in the background.\n\n\n\n\nGlobal Fishing\nThe data this week comes from OurWorldinData.org. More details can be found on their site.\n\nIncreasing pressures on fish populations mean one-third of global fish stocks are overexploited – this has increased from 10% in the 1970s.\n\nThe world now produces more than 155 million tonnes of seafood each year.\n\nThere are large differences in per capita fish consumption across the world.\n\nThe world now produces more seafood from aquaculture (fish farming) than from wild catch. This has played a key role in alleviating pressure on wild fish populations.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-10-12')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 42)\n\nconsumption &lt;- tuesdata$`fish-and-seafood-consumption-per-capita`\n\n# Or read in the data manually\n\nfarmed &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-12/aquaculture-farmed-fish-production.csv')\ncaptured_vs_farmed &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-12/capture-fisheries-vs-aquaculture.csv')\ncaptured &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-12/capture-fishery-production.csv')\nconsumption &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-12/fish-and-seafood-consumption-per-capita.csv')\nstock &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-12/fish-stocks-within-sustainable-levels.csv')\nfishery &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-12/global-fishery-catch-by-sector.csv')\nproduction &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-12/seafood-and-fish-production-thousand-tonnes.csv')\n\n\nData Dictionary\n\n\n\nseafood_consumption.csv\n\n\naquaculture-farmed-fish-production.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry/entity\n\n\nCode\ncharacter\nCountry code (see countrycode R package)\n\n\nYear\ndouble\n.\n\n\nAquaculture production (metric tons)\ndouble\n.\n\n\n\n\n\ncapture-fisheries-vs-aquaculture.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry/entity\n\n\nCode\ncharacter\nCountry code (see countrycode R package)\n\n\nYear\ndouble\nYear\n\n\nAquaculture production (metric tons)\ndouble\nProduction of aquaculture animals\n\n\nCapture fisheries production (metric tons)\ndouble\nCaptured aquaculture\n\n\n\n\n\ncapture-fishery-production.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry/entity\n\n\nCode\ncharacter\nCountry code (see countrycode R package)\n\n\nYear\ndouble\nYear\n\n\nCapture fisheries production (metric tons)\ndouble\nCaptured fisheres production\n\n\n\n\n\nfish-and-seafood-consumption-per-capita.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry/entity\n\n\nCode\ncharacter\nCountry code (see countrycode R package)\n\n\nYear\ndouble\nYear\n\n\nFish, Seafood- Food supply quantity (kg/capita/yr) (FAO, 2020)\ndouble\nFood supply in fish in kg/capita/year\n\n\n\n\n\nfish-stocks-within-sustainable-levels.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry/entity\n\n\nCode\ncharacter\nCountry code (see countrycode R package)\n\n\nYear\ndouble\nYear\n\n\nShare of fish stocks within biologically sustainable levels (FAO, 2020)\ndouble\nShare of sustainable fish stock\n\n\nShare of fish stocks that are overexploited\ndouble\nShare of fish stock that are overexploited\n\n\n\n\n\nglobal-fishery-catch-by-sector.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry/entity\n\n\nCode\ncharacter\nCountry code (see countrycode R package)\n\n\nYear\ndouble\nYear\n\n\nArtisanal (small-scale commercial)\ndouble\nCatch by small-scale commercial\n\n\nDiscards\ndouble\nDiscarded quantities\n\n\nIndustrial (large-scale commercial)\ndouble\nLarge scale commercial\n\n\nRecreational\ndouble\nRecreational fishing\n\n\nSubsistence\ndouble\nFood caught to feed self/family\n\n\n\n\n\nseafood-and-fish-production-thousand-tonnes.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry/entity\n\n\nCode\ncharacter\nCountry code (see countrycode R package)\n\n\nYear\ndouble\n.\n\n\nPelagic Fish - 2763 - Production - 5510 - tonnes\ndouble\nPelagic Fish\n\n\nCrustaceans - 2765 - Production - 5510 - tonnes\ndouble\nCrustaceans\n\n\nCephalopods - 2766 - Production - 5510 - tonnes\ndouble\nCephalopods\n\n\nDemersal Fish - 2762 - Production - 5510 - tonnes\ndouble\nDemersal\n\n\nFreshwater Fish - 2761 - Production - 5510 - tonnes\ndouble\nFreshwater\n\n\nMolluscs, Other - 2767 - Production - 5510 - tonnes\ndouble\nMolluscs\n\n\nMarine Fish, Other - 2764 - Production - 5510 - tonnes\ndouble\nMarine\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-09-28/readme.html",
    "href": "data/2021/2021-09-28/readme.html",
    "title": "Economic Papers",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nThe logo for the National Bureau of Economic Research (NBER)\n\n\n\n\nEconomic Papers\nThe data this week comes from the National Bureau of Economic Research NBER by way of the nberwp package by Ben Davies.\n\nNew research by NBER affiliates, circulated for discussion and comment. The NBER distributes more than 1,200 working papers each year. These papers have not been peer reviewed. Papers issued more than 18 months ago are open access. More recent papers are available without charge to affiliates of subscribing academic institutions, employees of NBER Corporate Associates, government employees in the US, journalists, and residents of low-income countries.\n\nBen also has a detailed blogpost looking over this data.\nThis is a good dataset to again better understand joining datasets in R. There are the combined datasets and the original tables.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-09-28')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 40)\n\npapers &lt;- tuesdata$papers\n\n# Or read in the data manually\n\npapers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-28/papers.csv')\nauthors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-28/authors.csv')\nprograms &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-28/programs.csv')\npaper_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-28/paper_authors.csv')\npaper_programs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-28/paper_programs.csv')\n\n\nData Dictionary\n\n\n\npapers.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npaper\ncharacter\nPaper ID\n\n\ncatalogue_group\ncharacter\nCatalogue group, either Historical, Technical or General\n\n\nyear\ninteger\nYear\n\n\nmonth\ninteger\nMonth\n\n\ntitle\ncharacter\nTitle of the paper\n\n\nauthor\ncharacter\nAuthor ID\n\n\nname\ncharacter\nAuthor Name\n\n\nuser_nber\ncharacter\nAuthor nber ID\n\n\nuser_repec\ncharacter\nAuthor repec ID\n\n\nprogram\ncharacter\nProgram\n\n\nprogram_desc\ncharacter\nDescription of program\n\n\nprogram_category\ncharacter\nProgram category\n\n\n\n\nCleaning Script\nlibrary(nberwp)\nlibrary(tidyverse)\n\npapers %&gt;% \n  write_csv(\"2021/2021-09-28/papers.csv\")\n\nauthors %&gt;% \n  write_csv(\"2021/2021-09-28/authors.csv\")\n\nprograms %&gt;% \n  write_csv(\"2021/2021-09-28/programs.csv\")\n\npaper_authors %&gt;% \n  write_csv('2021/2021-09-28/paper_authors.csv')\n\npaper_programs %&gt;% \n  write_csv(\"2021/2021-09-28/paper_programs.csv\")\n\njoined_df &lt;- left_join(papers, paper_authors) %&gt;% \n  left_join(authors) %&gt;% \n  left_join(paper_programs) %&gt;% \n  left_join(programs)%&gt;% \n  mutate(\n    catalogue_group = str_sub(paper, 1, 1),\n    catalogue_group = case_when(\n      catalogue_group == \"h\" ~ \"Historical\",\n      catalogue_group == \"t\" ~ \"Technical\",\n      catalogue_group == \"w\" ~ \"General\"\n    ),\n    .after = paper\n  ) \n\njoined_df"
  },
  {
    "objectID": "data/2021/2021-09-14/readme.html",
    "href": "data/2021/2021-09-14/readme.html",
    "title": "Top 100 Billboard",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nBillboard Hot 100 logo as of 1958, it is the word Billboard ocver the words Hot 100.\n\n\n\n\nTop 100 Billboard\nThe data this week comes from Data.World by way of Sean Miller, Billboard.com and Spotify.\nBillboard Top 100 - Wikipedia\n\nThe Billboard Hot 100 is the music industry standard record chart in the United States for songs, published weekly by Billboard magazine. Chart rankings are based on sales (physical and digital), radio play, and online streaming in the United States.\n\nBillboard Top 100 Article\n\nDrake rewrites the record for the most entries ever on the Billboard Hot 100, as he lands his 208th career title on the latest list, dated March 21\n\nThePudding has a neat interactive experience by year with audio!\nSome exploratory graphs by Azhad Syed.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-09-14')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 38)\n\nbillboard &lt;- tuesdata$billboard\n\n# Or read in the data manually\n\nbillboard &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-14/billboard.csv')\n\n\nData Dictionary\n\n\n\nbillboard.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nurl\ncharacter\nBillboard Chart URL\n\n\nweek_id\ncharacter\nWeek ID\n\n\nweek_position\ndouble\nWeek position 1: 100\n\n\nsong\ncharacter\nSong name\n\n\nperformer\ncharacter\nPerformer name\n\n\nsong_id\ncharacter\nSong ID, combo of song/singer\n\n\ninstance\ndouble\nInstance (this is used to separate breaks on the chart for a given song. Example, an instance of 6 tells you that this is the sixth time this song has appeared on the chart)\n\n\nprevious_week_position\ndouble\nPrevious week position\n\n\npeak_position\ndouble\nPeak position as of that week\n\n\nweeks_on_chart\ndouble\nWeeks on chart as of that week\n\n\n\n\n\naudio_features.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsong_id\ncharacter\nSong ID\n\n\nperformer\ncharacter\nPerformer name\n\n\nsong\ncharacter\nSong\n\n\nspotify_genre\ncharacter\nGenre\n\n\nspotify_track_id\ncharacter\nTrack ID\n\n\nspotify_track_preview_url\ncharacter\nSpotify URL\n\n\nspotify_track_duration_ms\ndouble\nDuration in ms\n\n\nspotify_track_explicit\nlogical\nIs explicit\n\n\nspotify_track_album\ncharacter\nAlbum name\n\n\ndanceability\ndouble\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nkey\ndouble\nThe estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\n\n\nloudness\ndouble\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n\n\nmode\ndouble\nMode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n\n\nspeechiness\ndouble\nSpeechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n\n\nacousticness\ndouble\nA confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n\n\ninstrumentalness\ndouble\nPredicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n\n\nliveness\ndouble\nDetects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\ntempo\ndouble\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n\n\ntime_signature\ndouble\nTime signature\n\n\nspotify_track_popularity\ndouble\nPopularity\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-08-31/readme.html",
    "href": "data/2021/2021-08-31/readme.html",
    "title": "Bird Baths",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nA screenshot of a birdbath with 6 birds shaking their feathers and bathing in the water. The text “Bath bullies, bacteria and battlegrounds: the secret world of bird baths” is overlaid on the image, and the bird bath is a shallow red bowl with a single large rock off center.\n\n\n\n\nBird Baths\nThe data this week comes from Cleary et al, 2016 with the corresponding article Avian Assemblages at Bird Baths: A Comparison of Urban and Rural Bird Baths in Australia.\nhttps://doi.org/10.1371/journal.pone.0150899.\nBig thanks to Alison Hill for suggesting this data and finding another article.\n\nBird baths are a familiar sight in Australian gardens but surprisingly little is known about the precise role they play in the lives of birds.\nIn a dry continent such as Australia, bird baths may be vital to supporting an otherwise stressed bird population. We wanted to find out more, so we enlisted the help of thousands of citizen scientists across Australia to gather as much data as we could on how birds use bird baths.\nAnd so the Bathing Birds Study was born. Started by researchers at Deakin University and Griffith University in 2014, this study involved collecting data online from 2,500 citizen scientists on bathing birds all over Australia.\n\n\nData on bird occurrence at bird baths were collected during “The Bathing Birds Study” that ran for a four week period in each of two seasons: austral winter (June 24th to July 26th 2014) and summer (January 27th to February 29th 2015). The study was promoted throughout Australia to recruit citizen scientists via: (1) media coverage (television, radio and newspapers), (2) social media (particularly via Facebook), (3) communication networks of a range of project partners, and (4) by targeting specific ornithological associations across Australia. Participants used an on-line data portal hosted on the Atlas of Living Australia (ALA) website (ala.org.au) to report the presence of birds visiting their bird bath during the survey. Other data collected included location of the bird bath, number of bird visits and photographs for validation of sightings. To aid participants with identification, an online field guide was available and participants could email photo and descriptions of birds to aid identification.\nDuring the two survey periods, citizen scientists monitored their bird baths for 20 minutes, once per day and three times per week for four weeks (surveys which did not meet these criteria were not considered further) to detect all or most species visiting bird baths [38]. Due to the difficulty in accurately surveying birds in rain or high winds, we asked our citizen scientists to conduct surveys in relatively calm and rain-free weather. Data were pooled (sightings of each species across all surveys with each survey period) within each bird bath and expressed as a binary indicator of whether a given species was present or absent at a bird bath. Due to limitations of the technology platform used to collect data, we were only able to record occurrence/presence of birds at baths. It was not possible to capture surveys where there were no sightings present although such surveys are regarded as highly unlikely. Bird baths were assigned to:\nBioregion (Interim Biogeographic Regionalisation for Australia) regions, henceforth ‘bioregion’, a classification based on climate, vegetation and soil (National Land and Water Resources Audit, 2001). Bird data were collected from 42 bioregions but due to low participation (&lt; 3 participants in an urban or rural area), assessment of differences were conducted on 8 (winter) and 13 (summer) bioregions. “Rural” or “urban” areas according to the Australian Bureau of Statistics (ABS) classification which uses an Australian Statistical Geography Standard that defines “urban” areas as having human populations of 1,000–100,000+ people while “rural” areas have &lt; 999 people. Thus, rural areas may contain large natural areas as well as low-density human settlement.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-08-31')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 36)\n\nbird_baths &lt;- tuesdata$bird_baths\n\n# Or read in the data manually\n\nbird_baths &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-08-31/bird_baths.csv')\n\n\nData Dictionary\n\n\n\nbird_baths.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsurvey_year\ndouble\nYear of survey\n\n\nurban_rural\ncharacter\nUrban or rural location\n\n\nbioregions\ncharacter\nRegion of Australia\n\n\nbird_type\ncharacter\nBird species\n\n\nbird_count\ndouble\n\n\n\n\nNote: Rows in the data that are missing survey_year, urban_rural, and bioregions are totals from the source data.\n\nCleaning Script\nlibrary(tidyverse)\nraw_df &lt;- readxl::read_excel(\"2021/2021-08-31/S1File.xlsx\")\n\nnames(raw_df)\n\nbird_df &lt;- raw_df %&gt;% \n  pivot_longer(names_to = \"bird_type\", values_to = \"bird_count\", cols = 4:last_col()) %&gt;% \n  janitor::clean_names()\n\nbird_df"
  },
  {
    "objectID": "data/2021/2021-08-17/readme.html",
    "href": "data/2021/2021-08-17/readme.html",
    "title": "Star Trek commands",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nImage of three characters from Star Trek huddled around the onboard computer, reading through some of the on screen prompts.\n\n\n\n\nStar Trek commands\nThe data this week comes from SpeechInteraction.org. H/t to Sara Stoudt for sharing this dataset.\n\nTea, Earl Grey, Hot: Designing Speech Interactions from the Imagined Ideal of Star Trek\n\n\nSpeech is now common in daily interactions with our devices, thanks to voice user interfaces (VUIs) like Alexa. Despite their seeming ubiquity, designs often do not match users’ expectations. Science fiction, which is known to influence design of new technologies, has included VUIs for decades. Star Trek: The Next Generation is a prime example of how people envisioned ideal VUIs. Understanding how current VUIs live up to Star Trek’s utopian technologies reveals mismatches between current designs and user expectations, as informed by popular fiction. Combining conversational analysis and VUI user analysis, we study voice interactions with the Enterprise’s computer and compare them to current interactions. Independent of futuristic computing power, we find key design-based differences: Star Trek interactions are brief and functional, not conversational, they are highly multimodal and context-driven, and there is often no spoken computer response. From this, we suggest paths to better align VUIs with user expectations.\n\nExample commands are available on YouTube.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-08-17')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 34)\n\ncomputer &lt;- tuesdata$computer\n\n# Or read in the data manually\n\ncomputer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-08-17/computer.csv')\n\n\nData Dictionary\nSee full details at SpeechInteraction.org\n\n\n\ncomputer.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nID name\n\n\nchar\ncharacter\nThe name of the speaking character\n\n\nline\ncharacter\nThe complete line of dialog (may contain more speech than the speech interaction). Parentheticals are directions and not spoken.\n\n\ndirection\ncharacter\nStage directions as written in the episode script\n\n\ntype\ncharacter\nThe type of interaction, see detailed definitions below\n\n\npri_type\ncharacter\nThe primary interaction type as defined by the below ranking\n\n\ndomain\ncharacter\nThe domain of interaction, see detailed definitions below\n\n\nsub_domain\ncharacter\nThe sub-domain of interaction, generally a specific setting, see below\n\n\nnv_resp\nlogical\nOn a person’s line when the computer completes the query but without speaking a response.\n\n\ninteraction\ncharacter\nThe actual speech interaction. May be shorter or longer than the line of dialog\n\n\nchar_type\ncharacter\nEither Person or Computer\n\n\nis_fed\nlogical\nIndicates whether an interaction is with the standard Enterprise system (true) or another (false), which is akin to having a few interactions with a Google Home in an Alexa dataset.\n\n\nerror\nlogical\nThe interaction resulted in an error\n\n\nvalue_id\ncharacter\nValue ID\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(jsonlite)\n\nurl &lt;- \"http://www.speechinteraction.org/TNG/teaearlgreyhotdataset.json\"\n\nraw_json &lt;- parse_json(url(url))\n\nraw_json %&gt;% \n  listviewer::jsonedit()\n\nclean_df &lt;- raw_json %&gt;% \n  enframe() %&gt;% \n  unnest_longer(value) %&gt;% \n  unnest_wider(value) %&gt;% \n  unnest_longer(type) %&gt;% \n  unnest_longer(domain) %&gt;% \n  unnest_longer(`sub-domain`) %&gt;% \n  janitor::clean_names()\n\nclean_df %&gt;% write_csv(\"2021/2021-08-17/computer.csv\")"
  },
  {
    "objectID": "data/2021/2021-08-03/readme.html",
    "href": "data/2021/2021-08-03/readme.html",
    "title": "Paralympics",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nParalympic logo which consists of 3 non-overlapping curved stripes colored red, blue, and green\n\n\n\n\nParalympics\nThe data this week comes from the International Paralympic Committee.\nData webscraped for the games of 1980 - 2016 for the athletes for all of their listed sports. There are additional summary-level datasets for medals by country/sport etc that are also available on the site.\nPer Wikipedia\n\nThe Paralympic Games or Paralympics are a periodic series of international multi-sport events involving athletes with a range of disabilities, including impaired muscle power (e.g. paraplegia and quadriplegia, muscular dystrophy, post-polio syndrome, spina bifida), impaired passive range of movement, limb deficiency (e.g. amputation or dysmelia), leg length difference, short stature, hypertonia, ataxia, athetosis, vision impairment and intellectual impairment. There are Winter and Summer Paralympic Games, which since the 1988 Summer Olympics in Seoul, South Korea, are held almost immediately following the respective Olympic Games.\n\nArticle - 1964 to 1988 — It was all about Zipora Rubin-Rosenbaum’s dominance\nParalympic categories article.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-08-03')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 32)\n\nathletes &lt;- tuesdata$athletes\n\n# Or read in the data manually\n\nathletes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-08-03/athletes.csv')\n\n\nData Dictionary\n\n\n\nathletes.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngender\ncharacter\nBinary gender\n\n\nevent\ncharacter\nEvent name\n\n\nmedal\ncharacter\nMedal type\n\n\nathlete\ncharacter\nAthlete name (LAST NAME first name\n\n\nabb\ncharacter\nCountry abbreviation\n\n\ncountry\ncharacter\nCountry name\n\n\ngrp_id\ninteger\nGroup ID as a count within team sports\n\n\ntype\ncharacter\nType of sport\n\n\nyear\ndouble\nyear of games\n\n\nguide\ncharacter\nGuide (for vision impaired athletes)\n\n\npilot\ncharacter\nPilot (for vision impaired athletes)\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(rvest)\n\ncore_url &lt;- \"https://db.ipc-services.org/sdms/hira/web/competition/code/PG2016/sport/AR\"\n\n\"body &gt; div.container.bg-white.border.border-header.border-top-0.pb-3 &gt; table &gt; tbody &gt; tr:nth-child(1) &gt; td:nth-child(7) &gt; div &gt; a\"\n\nraw_html &lt;- \"https://db.ipc-services.org/sdms/hira/web/competition/code/PG2016\" %&gt;% \n  read_html()\n\n\n# Archery -----------------------------------------------------------------\n\n\nclean_arrow &lt;- function(year){\n  \n  arrows &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/AR\") %&gt;% \n    read_html()\n  \n  raw_arrow &lt;- arrows %&gt;% \n    html_table() %&gt;% \n    .[[2]] %&gt;% \n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;% \n    rename(event = Event) %&gt;% \n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    )\n  \n  ind_arrow &lt;- raw_arrow %&gt;% \n    separate(athlete, into = c(\"athlete\", \"abb\"), sep = \" \\\\(\") %&gt;% \n    separate(event, into = c(\"gender\", \"event\"), sep = \"'s | \", fill = \"left\", extra = \"merge\") %&gt;% \n    mutate(abb = str_remove(abb, \"\\\\)\")) %&gt;% \n    filter(str_detect(event, \"Mixed|Team\", negate = TRUE)) \n  \n  team_arrow &lt;- raw_arrow %&gt;% \n    filter(str_detect(event, \"Mixed|Team\")) %&gt;% \n    separate(athlete, into = c(\"country\", \"abb\"), sep = \" \\\\(\") %&gt;% \n    separate(abb, into = c(\"abb\", \"athlete\"), sep = \"\\\\)\") %&gt;% \n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;% \n    group_by(event, medal, abb) %&gt;% \n    mutate(grp_id = row_number()) %&gt;% \n    ungroup()\n  \n  bind_rows(ind_arrow, team_arrow) %&gt;% \n    mutate(type = \"Archery\", year = year) \n}\n\nclean_arrow(2008) %&gt;% \n  print(n = 100)\n\nyear_vec &lt;- seq(1980, 2016, by = 4)\n\ntry_arrow &lt;- safely(clean_arrow)\n\nall_arrow &lt;- year_vec %&gt;% \n  map(try_arrow) \n\narrow_years &lt;- all_arrow %&gt;% \n  map_dfr(\"result\")\n\n# athletics ---------------------------------------------------------------\n\nsport_vec &lt;- raw_html %&gt;% \n  html_nodes(\"td:nth-child(7) &gt; div &gt; a\") %&gt;% \n  html_attr(\"href\") %&gt;% \n  str_remove(\"/sdms/hira/web/competition/code/PG2016/sport/\")\n\n\nclean_ath &lt;- function(year){\n  \n  ath &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/AT\") %&gt;% \n    read_html()\n  \n  raw_ath &lt;- ath %&gt;% \n    html_table() %&gt;% \n    .[[2]] %&gt;% \n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;% \n    rename(event = Event) %&gt;% \n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;% \n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\")\n  \n  raw_ind &lt;- raw_ath %&gt;% \n    filter(str_detect(event, \"4x\", negate = TRUE)) %&gt;% \n    separate(athlete, into = c(\"athlete\", \"abb\"), sep = \" \\\\(\", fill = \"left\", extra = \"merge\") %&gt;% \n    separate(abb, into = c(\"abb\", \"guide\"), sep = \"\\\\)\") %&gt;% \n    mutate(guide = str_remove(guide, \"Guide: \"),\n           guide = if_else(guide == \"\", NA_character_, guide)) %&gt;% \n    filter(!is.na(athlete)) \n  \n  clean_ind &lt;- raw_ind %&gt;% \n    add_row(\n      raw_ind %&gt;% \n        filter(str_detect(guide, \"\\\\(\")) %&gt;% \n        select(gender:medal, athlete = guide) %&gt;% \n        separate(athlete, c(\"athlete\", \"abb\"), sep = \" \\\\(\")\n    ) %&gt;% \n    filter(!is.na(athlete)) %&gt;% \n    mutate(guide = if_else(str_detect(guide, \"\\\\(\"), NA_character_, guide)) \n  \n  \n  clean_grp &lt;- raw_ath %&gt;% \n    filter(str_detect(event, \"4x\")) %&gt;% \n    separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;% \n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;% \n    separate_rows(athlete, sep = \"(?&lt;=[A-Z]\\\\.)(?=[A-Z])\") %&gt;% \n    mutate(\n      guide = if_else(\n        str_detect(lead(athlete), \"Guide\"), \n        lead(athlete),\n        NA_character_\n      )\n    ) %&gt;% \n    filter(str_detect(athlete, \"Guide\", negate = TRUE)) %&gt;%\n    group_by(event, medal, country) %&gt;% \n    mutate(grp_id = row_number()) %&gt;% \n    ungroup()\n  \n  bind_rows(clean_ind, clean_grp) %&gt;% \n    mutate(type = \"Athletics\", year = year)\n  \n}\n\ntest_df &lt;- clean_ath(1980) \ntest_df %&gt;% \n  filter(str_detect(event, \"4x60\")) %&gt;% \n  print(n = 100)\n\nsafe_ath &lt;- safely(clean_ath)\n\nath_years &lt;- year_vec %&gt;% \n  map(safe_ath) %&gt;% \n  map_dfr(\"result\")\n\n# Cycling -----------------------------------------------------------------\n\nclean_cycle &lt;- function(year){\n  \n  url_c &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/CY\") %&gt;% \n    read_html()\n  \n  raw_cycle &lt;- url_c %&gt;% \n    html_table() %&gt;% \n    .[[2]] %&gt;% \n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;% \n    rename(event = Event) %&gt;% \n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;% \n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\")\n  \n  raw_ind_c &lt;- raw_cycle %&gt;% \n    filter(str_detect(event, \"Team|Tandem\", negate = TRUE)) %&gt;% \n    separate(athlete, into = c(\"athlete\", \"abb\"), sep = \" \\\\(\", fill = \"left\", extra = \"merge\") %&gt;% \n    separate(abb, into = c(\"abb\", \"pilot\"), sep = \"Pilot: |\\\\(PT\\\\):\") %&gt;% \n    separate(athlete, sep = \"(?&lt;=[PT\\\\)])(?=[A-Z])\", into = c(\"athlete\", \"pilot\")) %&gt;%\n    separate(athlete, sep = \"(?&lt;=[a-z]+)(?=[A-Z])\", into = c(\"athlete\", \"pilot\")) %&gt;%\n    mutate(pilot = if_else(pilot == \"\", NA_character_, pilot)) %&gt;% \n    filter(!is.na(athlete)) \n  \n  clean_ind_c &lt;- raw_ind_c %&gt;% \n    add_row(\n      raw_ind_c %&gt;% \n        filter(str_detect(pilot, \"\\\\(\")) %&gt;% \n        select(gender:medal, athlete = pilot) %&gt;% \n        separate(athlete, c(\"athlete\", \"abb\"), sep = \" \\\\(\")\n    ) %&gt;% \n    filter(!is.na(athlete)) %&gt;% \n    mutate(pilot = if_else(str_detect(pilot, \"\\\\(\"), NA_character_, pilot)) %&gt;% \n    mutate(abb = str_remove(abb, \"\\\\)\"))\n  \n  \n  clean_grp_c &lt;- raw_cycle %&gt;% \n    filter(str_detect(event, \"Team\")) %&gt;% \n    separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;% \n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;% \n    mutate(\n      guide = if_else(\n        str_detect(lead(athlete), \"Pilot|PT\\\\)\"), \n        lead(athlete),\n        NA_character_\n      )\n    ) %&gt;% \n    filter(str_detect(athlete, \"Pilot|PT\\\\)\", negate = TRUE)) %&gt;%\n    group_by(event, medal, country) %&gt;% \n    mutate(grp_id = row_number()) %&gt;% \n    ungroup()\n  \n  bind_rows(clean_ind_c, clean_grp_c) %&gt;% \n    mutate(year = year, type = \"Cycling\") %&gt;% \n    rename(guide = pilot)\n  \n}\n\nsafe_cycle &lt;- safely(clean_cycle)\n\ncycle_years &lt;- year_vec %&gt;% \n  map(safe_cycle) %&gt;% \n  map_dfr(\"result\")\n\n\n# Powerlifting ------------------------------------------------------------\n\n\nclean_power &lt;- function(year){\n  \n  raw_power &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/PO\") %&gt;% \n    read_html()\n  \n  clean_pwr &lt;- raw_power %&gt;% \n    html_table() %&gt;% \n    .[[2]] %&gt;% \n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;% \n    rename(event = Event) %&gt;% \n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;% \n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;% \n    separate(athlete, into = c(\"athlete\", \"abb\"), sep = \" \\\\(\", fill = \"left\", extra = \"merge\") %&gt;% \n    separate_rows(abb, sep = \" \\\\)\") \n  \n  clean_abb_power &lt;- clean_pwr %&gt;%\n    mutate(abb = str_remove(abb, \"\\\\)\")) %&gt;%\n    bind_rows(clean_pwr %&gt;%\n                filter(str_detect(abb, \"\\\\(\")) %&gt;%\n                separate_rows(abb, sep = \"\\\\(\") %&gt;%\n                mutate(athlete = ifelse(str_length(abb) &lt;= 4, lag(abb), athlete)) %&gt;%\n                mutate(\n                  abb = str_remove(abb, \"\\\\).*\"),\n                  athlete = str_remove(athlete, \"&gt;*\\\\)\") %&gt;% str_trim()\n                )) %&gt;%\n    filter(str_detect(abb, \"\\\\(\", negate = TRUE), !is.na(athlete)) %&gt;% \n    mutate(abb = str_remove(abb, \"\\\\)\"))\n  \n  \n  clean_abb_power %&gt;% \n    mutate(year = year, type = \"Powerlifting\")\n}\n\nclean_power(1984)\n\nsafe_power &lt;- safely(clean_power)\n\npower_years &lt;- year_vec %&gt;% \n  map(safe_power) %&gt;% \n  map_dfr(\"result\")\n\n\n# Swimming ----------------------------------------------------------------\n\nclean_swim &lt;- function(year){\n  \n  raw_swim &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/SW\") %&gt;%\n  read_html()\n\nraw_ind_sw &lt;- raw_swim %&gt;%\n  html_table() %&gt;%\n  .[[2]] %&gt;%\n  pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n  rename(event = Event) %&gt;%\n  mutate(\n    medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n  ) %&gt;%\n  filter(str_detect(event, \"4x\", negate = TRUE)) %&gt;%\n  separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;% \n  separate_rows(athlete, sep = \"(?&lt;=[A-Z]\\\\))(?=[A-Z])\") %&gt;%\n  separate(athlete, into = c(\"athlete\", \"abb\"), sep = \" \\\\(\", fill = \"left\", extra = \"merge\")\n\n\nclean_ind_sw &lt;- raw_ind_sw %&gt;%\n  mutate(abb = str_remove(abb, \"\\\\)\")) %&gt;%\n  bind_rows(raw_ind_sw %&gt;%\n    filter(str_detect(abb, \"\\\\(\")) %&gt;%\n    separate_rows(abb, sep = \"\\\\(\") %&gt;%\n    mutate(athlete = ifelse(str_length(abb) &lt;= 4, lag(abb), athlete)) %&gt;%\n    mutate(\n      abb = str_remove(abb, \"\\\\).*\"),\n      athlete = str_remove(athlete, \"&gt;*\\\\)\") %&gt;% str_trim()\n    )) %&gt;%\n    filter(str_detect(abb, \"\\\\(\", negate = TRUE), !is.na(athlete))\n\n\nclean_grp_sw &lt;- raw_swim %&gt;%\n  html_table() %&gt;%\n  .[[2]] %&gt;%\n  pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n  rename(event = Event) %&gt;%\n  mutate(\n    medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n  ) %&gt;%\n  filter(str_detect(event, \"4x\")) %&gt;%\n  separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n  separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;%\n  separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;%\n  group_by(event, medal, country) %&gt;%\n  mutate(grp_id = row_number()) %&gt;%\n  ungroup()\n\nbind_rows(clean_ind_sw, clean_grp_sw) %&gt;% \n  mutate(year = year, type = \"Swimming\") \n\n  }\n\nsafe_swim &lt;- safely(clean_swim)\n\nswim_years &lt;- year_vec %&gt;% \n  map(safe_swim) %&gt;% \n  map_dfr(\"result\")\n\n\n\n# Table Tennis ------------------------------------------------------------\n\nclean_tab_tennis &lt;- function(year){\n  \n  raw_tab &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/TT\") %&gt;%\n    read_html()\n  \n  clean_ind_tab &lt;- raw_tab %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;%\n    filter(str_detect(event, \"Team\", negate = TRUE)) %&gt;%\n    separate(\n      event,\n      into = c(\"gender\", \"event\"),\n      sep = \" |'s \",\n      fill = \"left\",\n      extra = \"merge\"\n    ) %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[\\\\)])(?=[A-Z])\") %&gt;%\n    separate(\n      athlete,\n      into = c(\"athlete\", \"abb\"),\n      sep = \" \\\\(\",\n      fill = \"left\",\n      extra = \"merge\"\n    ) %&gt;%\n    mutate(abb = str_remove(abb, \"\\\\)\"))\n  \n  \n  clean_grp_tab &lt;- raw_tab %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;%\n    filter(str_detect(event, \"Team\")) %&gt;%\n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[A-Z]\\\\.)(?=[A-Z])\") %&gt;% \n    group_by(event, medal, country) %&gt;%\n    mutate(grp_id = row_number()) %&gt;%\n    ungroup()\n  \n  bind_rows(clean_ind_tab, clean_grp_tab) %&gt;% \n    mutate(year = year, type = \"Table Tennis\") \n  \n}\n\nsafe_table &lt;- safely(clean_tab_tennis)\n\ntable_years &lt;- year_vec %&gt;% \n  map(safe_table) %&gt;% \n  map_dfr(\"result\")\n\ncheck_fun(table_years)\n\n# Volleyball --------------------------------------------------------------\n\n\n\nclean_volleyball &lt;- function(year){\n  \n  raw_vb &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/VO\") %&gt;%\n    read_html()\n  \n  clean_grp_vb &lt;- raw_vb %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;%\n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[A-Z]\\\\.)(?=[A-Z])\") %&gt;% \n    group_by(event, medal, country) %&gt;%\n    mutate(grp_id = row_number()) %&gt;%\n    ungroup()\n  \n  clean_grp_vb %&gt;% \n    mutate(year = year, type = \"Volleyball\")\n  \n}\n\nsafe_vb &lt;- safely(clean_volleyball)\n\nvb_years &lt;- year_vec %&gt;% \n  map(safe_vb) %&gt;% \n  map_dfr(\"result\")\n\ncheck_fun(vb_years)\n\n# Basketball --------------------------------------------------------------\n\nclean_basketball &lt;- function(year){\n  \n  raw_bb &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/WB\") %&gt;%\n    read_html()\n  \n  clean_grp_bb &lt;- raw_bb %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;%\n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;%\n    group_by(event, medal, country) %&gt;%\n    mutate(grp_id = row_number()) %&gt;%\n    ungroup()\n  \n  clean_grp_bb %&gt;% \n    mutate(year = year, type = \"Basketball\")\n  \n}\n\nsafe_bb &lt;- safely(clean_basketball)\n\nbb_years &lt;- year_vec %&gt;% \n  map(safe_bb) %&gt;% \n  map_dfr(\"result\")\n\ncheck_fun(bb_years)\n\n# Fencing -----------------------------------------------------------------\n\nclean_fencing &lt;- function(year){\n  \n  raw_f &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/WF\") %&gt;%\n    read_html()\n  \n  clean_ind_f &lt;- raw_f %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;%\n    filter(str_detect(event, \"Team\", negate = TRUE)) %&gt;%\n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate(athlete, into = c(\"athlete\", \"abb\"), sep = \" \\\\(\", fill = \"left\", extra = \"merge\") %&gt;% \n    mutate(abb = str_remove(abb, \"\\\\)\"))\n  \n  clean_grp_f &lt;- raw_f %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;%\n    filter(str_detect(event, \"Team\")) %&gt;%\n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[A-Z]\\\\.)(?=[A-Z])\") %&gt;% \n    group_by(event, medal, country) %&gt;%\n    mutate(grp_id = row_number()) %&gt;%\n    ungroup()\n  \n  bind_rows(clean_ind_f, clean_grp_f) %&gt;% \n    mutate(year = year, type = \"Fencing\")\n  \n}\n  \nsafe_fence &lt;- safely(clean_fencing)\n\nfence_years &lt;- year_vec %&gt;% \n  map(safe_fence) %&gt;% \n  map_dfr(\"result\")\n\ncheck_fun(fence_years)\n\n\n# Rugby -------------------------------------------------------------------\n\nclean_rugby &lt;- function(year){\n  \n  raw_rug &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/WR\") %&gt;%\n    read_html()\n  \n  clean_grp_rug &lt;- raw_rug %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\"),\n      event = paste(event, \"Wheelchair Rugby\")\n    ) %&gt;%\n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[A-Z]\\\\.)(?=[A-Z])\") %&gt;% \n    group_by(event, medal, country) %&gt;%\n    mutate(grp_id = row_number()) %&gt;% \n    ungroup()\n  \n  clean_grp_rug %&gt;% \n    mutate(year = year, type = \"Rugby\")\n  \n}\n\nsafe_rugby &lt;- safely(clean_rugby)\n\nrugby_years &lt;- year_vec %&gt;% \n  map(safe_rugby) %&gt;% \n  map_dfr(\"result\")\n\ncheck_fun(rugby_years)\n\n\n# Tennis ------------------------------------------------------------------\n\nclean_tennis &lt;- function(year){\n  \n  raw_ten &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/TT\") %&gt;%\n    read_html()\n  \n  clean_ind_ten &lt;- raw_ten %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;%\n    filter(str_detect(event, \"Double|Team\", negate = TRUE)) %&gt;%\n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[\\\\)])(?=[A-Z])\") %&gt;%\n    separate(athlete, into = c(\"athlete\", \"abb\"), sep = \" \\\\(\", fill = \"left\", extra = \"merge\") %&gt;% \n    mutate(abb = str_remove(abb, \"\\\\)\"))\n  \n  \n  clean_grp_ten &lt;- raw_ten %&gt;%\n    html_table() %&gt;%\n    .[[2]] %&gt;%\n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;%\n    rename(event = Event) %&gt;%\n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;%\n    filter(str_detect(event, \"Double|Team\")) %&gt;%\n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\") %&gt;%\n    separate(athlete, into = c(\"country\", \"abb\", \"athlete\"), sep = \" \\\\(|\\\\)\") %&gt;%\n    separate_rows(athlete, sep = \"(?&lt;=[a-z])(?=[A-Z])\") %&gt;%\n    group_by(event, medal, country) %&gt;%\n    mutate(grp_id = row_number()) %&gt;%\n    ungroup()\n  \n  bind_rows(clean_ind_ten, clean_grp_ten) %&gt;% \n    mutate(year = year, type = \"Wheelchair Tennis\")\n  \n}\n\nsafe_tennis &lt;- safely(clean_tennis)\n\ntennis_years &lt;- year_vec %&gt;% \n  map(safe_tennis) %&gt;% \n  map_dfr(\"result\")\n\ncheck_fun(tennis_years)\n\n# Triathlon ---------------------------------------------------------------\n\nclean_triathlon &lt;- function(year){\n  \n  tri_url &lt;- glue::glue(\"https://db.ipc-services.org/sdms/hira/web/competition/code/PG{year}/sport/TR\") %&gt;% \n    read_html()\n  \n  raw_tri &lt;- tri_url %&gt;% \n    html_table() %&gt;% \n    .[[2]] %&gt;% \n    pivot_longer(names_to = \"medal\", values_to = \"athlete\", cols = -Event) %&gt;% \n    rename(event = Event) %&gt;% \n    mutate(\n      medal = str_remove(medal, \" Medallist\\\\(s\\\\)\")\n    ) %&gt;% \n    separate(event, into = c(\"gender\", \"event\"), sep = \" |'s \", fill = \"left\", extra = \"merge\")\n  \n  raw_ind_tri &lt;- raw_tri %&gt;% \n    filter(str_detect(event, \"4x\", negate = TRUE)) %&gt;% \n    separate(athlete, into = c(\"athlete\", \"abb\"), sep = \" \\\\(\", fill = \"left\", extra = \"merge\") %&gt;% \n    separate(abb, into = c(\"abb\", \"guide\"), sep = \"\\\\)\") %&gt;% \n    mutate(guide = str_remove(guide, \"Guide: \"),\n           guide = if_else(guide == \"\", NA_character_, guide)) %&gt;% \n    filter(!is.na(athlete)) \n  \n  clean_ind_tri &lt;- raw_ind_tri %&gt;% \n    add_row(\n      raw_ind_tri %&gt;% \n        filter(str_detect(guide, \"\\\\(\")) %&gt;% \n        select(gender:medal, athlete = guide) %&gt;% \n        separate(athlete, c(\"athlete\", \"abb\"), sep = \" \\\\(\")\n    ) %&gt;% \n    filter(!is.na(athlete)) %&gt;% \n    mutate(guide = if_else(str_detect(guide, \"\\\\(\"), NA_character_, guide)) %&gt;% \n    mutate(year = year, type = \"Triathlon\")\n  \n  \n  clean_ind_tri\n}\n\nsafe_tri &lt;- safely(clean_triathlon)\n\ntri_years &lt;- year_vec %&gt;% \n  map(safe_tri) %&gt;% \n  map_dfr(\"result\")\n\ncheck_fun(tri_years)\n\nall_sports &lt;- bind_rows(\n  list(arrow_years, ath_years, bb_years, cycle_years, fence_years, power_years,\n       rugby_years, swim_years, table_years, tennis_years, tri_years, vb_years)\n)\n\nall_sports %&gt;% \n  skimr::skim()\n\nall_sports %&gt;% \n  write_csv(\"2021/2021-08-03/athletes.csv\")\n\ncheck_fun(all_sports)\n\ncheck_abb &lt;- function(df_in) {\ndf_in %&gt;% \n    filter(str_length(abb) &gt; 3) %&gt;% \n    select(athlete, abb, type, year)\n  \n}\n\nall_sports %&gt;% check_abb() %&gt;% distinct(type, year)"
  },
  {
    "objectID": "data/2021/2021-07-20/readme.html",
    "href": "data/2021/2021-07-20/readme.html",
    "title": "US Droughts",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nArtistic cutout of a drought map over the midwest US - a continuous color palette from light brown to dark red is overlaid a cutout of the state-level map of the US\n\n\n\n\nUS Droughts\nThe data this week comes from the U.S. Drought Monitor. Many more datasets including county-level data can be accessed there. In the interest of size we stuck with state-level data.\nThis dataset was covered by the NY Times and CNN.\nThe dataset for today ranges from 2001 to 2021, but again more data is available at the Drought Monitor.\nDrought classification can be found on the US Drought Monitor site.\nPlease reference the data as seen below:\n\nThe U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the University of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and Atmospheric Administration. Map courtesy of NDMC.\n\nSome maps and other interesting summaries can be found on the Drought Monitor site and their Map Collection.\nSome limitations of the data expanded on the Drought Monitor site.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-07-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 30)\n\ndrought &lt;- tuesdata$drought\n\n# Or read in the data manually\n\ndrought &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-07-20/drought.csv')\n\n\nData Dictionary\n\n\n\ndrought.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmap_date\ndouble\nDate map released\n\n\nstate_abb\ncharacter\nState Abbreviation\n\n\nvalid_start\ndouble\nStart of weekly data\n\n\nvalid_end\ndouble\nEnd of weekly data\n\n\nstat_fmt\ndouble\nStatistic format (2 for “categorical”, 1 for “cumulative”“)\n\n\ndrought_lvl\ncharacter\nDrought level (None, DO, D1, D2, D3, D4) which corresponds to no drought, abnormally dry, moderate drought, severe drought, extreme drought or exceptional drought.\n\n\narea_pct\ndouble\nPercent of state currently in that drought category\n\n\narea_total\ndouble\nTotal land area (sq miles) of state currently in that drought category\n\n\npop_pct\ndouble\nPopulation percent of total state population in that drought category\n\n\npop_total\ndouble\nPopulation total of that state in that drought category\n\n\n\nA note on “cumulative” data. For pop_pct, pop_total, area_pct, and area_total fields, this means that when drought_lvl == \"D0\", the value on the field is the sum of the drought levels D0 through D4 (but not including None). If you use the cumulative data and are looking to use actual per-level values, you will want to subtract each drought level from the next higher: D0 from D1, D1 from D2, and so on.\n\nCleaning Script\nlibrary(tidyverse)\n\nread_and_prep &lt;- function(file, type){\n  read_csv(paste0(\"2021/2021-07-20/\", file)) %&gt;% \n    pivot_longer(cols = None:D4, names_to = \"drought_lvl\", values_to = type) %&gt;% \n    janitor::clean_names()\n  \n}\n\nstate_area_pct &lt;- read_and_prep(\"drought_area_pct.csv\", \"area_pct\")\nstate_area &lt;- read_and_prep(\"drought_area_total.csv\", \"area_total\")\nstate_pct_pop &lt;- read_and_prep(\"drought_pop_pct.csv\", \"pop_pct\")\nstate_pop &lt;- read_and_prep(\"drought_pop_total.csv\", \"pop_total\")\n\nall_df &lt;- state_area_pct %&gt;% \n  left_join(state_area) %&gt;% \n  left_join(state_pct_pop) %&gt;% \n  left_join(state_pop) %&gt;% \n  rename(state_abb = state_abbreviation, stat_fmt = statistic_format_id)\n\nall_df %&gt;% \n  write_csv(\"2021/2021-07-20/drought.csv\")"
  },
  {
    "objectID": "data/2021/2021-07-06/readme.html",
    "href": "data/2021/2021-07-06/readme.html",
    "title": "Independence Days",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nIndependence Day celebrations in the Dominican Republic. Image is of a Dominican Republic citizen celebrating independence day in a half mask, smiling, waving a Dominican Republic flag and wearing a Dominican Republic flag t shirt. Editorial credit: Tina Andros / Shutterstock.com\n\n\n\n\nIndependence Days\nThe data this week comes from Wikipedia and thank you to Isabella Velasquez for prepping this week’s dataset.\n\nAn independence day is an annual event commemorating the anniversary of a nation’s independence or statehood, usually after ceasing to be a group or part of another nation or state, or more rarely after the end of a military occupation. Many countries commemorate their independence from a colonial empire. American political commentator Walter Russell Mead notes that, “World-wide, British Leaving Day is never out of season.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-07-06')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 28)\n\nholidays &lt;- tuesdata$holidays\n\n# Or read in the data manually\n\nholidays &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-07-06/holidays.csv')\n\n\nData Dictionary\n\n\n\nholidays.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry Name\n\n\ndate_parsed\ndouble\nDate parsed\n\n\nweekday\ncharacter\nweekday\n\n\nday\ninteger\nDay of month\n\n\nmonth\ninteger\nMonth number\n\n\nname_of_holiday\ncharacter\nName of holiday\n\n\ndate_of_holiday\ncharacter\nDate of holiday\n\n\nyear_of_event\ninteger\nYear of event\n\n\nindependence_from\ncharacter\nIndependence from what country\n\n\nevent_commemorated_and_notes\ncharacter\nEvent commemorated and other notes\n\n\nyear\ninteger\nYear\n\n\ndate_mdy\ncharacter\nDate as month day year\n\n\n\n\nCleaning Script\nCode credit goes to Isabella Velasquez.\n# Independence Days Around the World --------------------------------------\n\n# Libraries ---------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr)\nlibrary(polite)\nlibrary(lubridate)\nlibrary(janitor)\n\n# Scrape data -------------------------------------------------------------\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_national_independence_days\"\nurl_bow &lt;- polite::bow(url)\n\nind_html &lt;-\n  polite::scrape(url_bow) %&gt;%\n  rvest::html_nodes(\"table.wikitable\") %&gt;%\n  rvest::html_table(fill = TRUE)\n\nind_tab &lt;-\n  ind_html[[1]][1:6] %&gt;%\n  as_tibble() %&gt;%\n  clean_names()\n\nraw_html &lt;- polite::scrape(url_bow) \n\nraw_html %&gt;%\n  # rvest::html_nodes(\"table.wikitable\") %&gt;%\n  rvest::html_nodes(\"span.flagicon\") %&gt;% \n  length()\n  rvest::html_table(fill = TRUE)\n\n# Clean data --------------------------------------------------------------\n\nind_clean &lt;-\n  ind_tab %&gt;%\n  # Cleaning up some dates\n  mutate(\n    date_of_holiday = case_when(\n      country == \"Croatia\" ~ \"May 30\",\n      country == \"Mexico\" ~ \"September 16\",\n      country == \"Mongolia\" ~ \"December 29\",\n      country == \"Paraguay\" ~ \"May 14\",\n      country == \"Israel\" ~ \"May 14\", # Independence Day exists within a range, but this was the original date.\n      country == \"Slovenia\" ~ \"June 25\", # Slovenia has two dates; this one is \"Statehood Day\".\n      TRUE ~ date_of_holiday\n    ),\n    year = str_sub(year_of_event, start = 1, end = 4),\n    date_mdy = case_when(\n      date_of_holiday != \"\" ~ paste0(date_of_holiday, \", \", year),\n      TRUE ~ \"\"\n    ),\n    date_parsed = mdy(date_mdy),\n    weekday = weekdays(date_parsed),\n    day = day(date_parsed),\n    month = month(date_parsed, label = TRUE),\n    year_of_event = as.integer(year_of_event),\n    year = as.integer(year)\n  ) %&gt;%\n  relocate(date_parsed:month, .after = country)\n\nind_clean %&gt;% \n  glimpse()\n\nind_clean %&gt;% \n  write_csv(\"2021/2021-07-06/holidays.csv\")"
  },
  {
    "objectID": "data/2021/2021-06-22/readme.html",
    "href": "data/2021/2021-06-22/readme.html",
    "title": "Park Access",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nCrowds socially distance at Mission Dolores Park in San Francisco in May 2020. Photographer: Scott Strazzante/The San Francisco Chronicle via Getty Images\n\n\n\n\nPark Access\nThe data this week comes from The Trust for Public Land. Citylab also wrote an article about park access and these datasets.\n\nWhen the pandemic forced millions of Americans to shelter indoors, parks saw a surge in popularity. Parks in North Carolina received an all-time high of 20 million visitors in 2020, an uptick of one million from the year before. In New York, the number of park-goers topped 78 million. Cities also mobilized their green spaces in the pandemic-fighting effort, using them to distribute PPE, meals and even vaccines. In Memphis, Tennessee, a 360-foot long tent was erected on Liberty Park fairgrounds in April, aiming to administer 21,000 vaccines weekly.\nBut even as green spaces proved to be a crucial element to people’s physical and mental well-being — especially for urban dwellers — the pandemic further exposed the disparity in who has access to parks. New data from the Trust for Public Land shows that in the 100 most populated U.S. cities, neighborhoods that are majority nonwhite have, on average, access to 44% less park acreage than majority white neighborhoods. Low-income communities have access to 42% less than high-income neighborhoods.\nSince 2011, The Trust for Public Land (TPL) has kept track of green space availability across U.S. metros, using the ParkScore index, which measures how well cities are meeting their residents’ need for parks based on four metrics: park access, acreage, investment and amenities. This year for the first time, the group added equity as a fifth metric.\nIts assessment is grounded in the idea that Americans should have park access within a 10-minute walk from their home. Some 100 million Americans do not live within that distance, according to this year’s analysis. The equity score builds on that, and compares park acreage and access between neighborhoods of color and those that are mostly white, and between low- and high-income communities.\n\nExample PDFs can be found in the following format:\nhttps://parkserve.tpl.org/mapping/historic/2020_ParkScoreRank.pdf\nYou just need to replace the year (ie 2020) with the specific year of interest.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-06-22')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 26)\n\nparks &lt;- tuesdata$parks\n\n# Or read in the data manually\n\nparks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-06-22/parks.csv')\n\n\nData Dictionary\n\n\n\nparks.csv\nNote that “points” are essentially their yearly normalized values (higher points = better).\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear of measurement\n\n\nrank\ndouble\nYearly rank\n\n\ncity\ncharacter\nCity Name\n\n\nmed_park_size_data\ndouble\nMedian park size acres\n\n\nmed_park_size_points\ndouble\nMedian park size in points\n\n\npark_pct_city_data\ncharacter\nParkland as percentage of city area\n\n\npark_pct_city_points\ndouble\nParkland as % of city area points\n\n\npct_near_park_data\ncharacter\nPercent of residents within a 10 minute walk to park\n\n\npct_near_park_points\ndouble\nPercent of residents within a 10 minute walk to park points\n\n\nspend_per_resident_data\ncharacter\nSpending per resident in USD\n\n\nspend_per_resident_points\ndouble\nSpending per resident in points\n\n\nbasketball_data\ndouble\nBasketball hoops per 10,000 residents\n\n\nbasketball_points\ndouble\nBasketball hoops per 10,000 residents points\n\n\ndogpark_data\ndouble\nDog parks per 100,000 residents\n\n\ndogpark_points\ndouble\nDog parks per 100,000 residents points\n\n\nplayground_data\ndouble\nPlaygrounds per 10,000 residents\n\n\nplayground_points\ndouble\nPlaygrounds per 10,000 residents points\n\n\nrec_sr_data\ndouble\nRecreation and senior centers per 20,000 residents\n\n\nrec_sr_points\ndouble\nRecreation and senior centers per 20,000 residents points\n\n\nrestroom_data\ndouble\nRestrooms per 10,000 residents\n\n\nrestroom_points\ndouble\nRestrooms per 10,000 residents points\n\n\nsplashground_data\ndouble\nSplashgrounds and splashpads per 100,000 residents\n\n\nsplashground_points\ndouble\nSplashgrounds and splashpads per 100,000 residents points\n\n\namenities_points\ndouble\nAmenities points total (ie play areas)\n\n\ntotal_points\ndouble\nTotal points (varies in denominator per/year)\n\n\ntotal_pct\ndouble\nTotal points as a percentage\n\n\ncity_dup\ncharacter\nCity duplicated name\n\n\npark_benches\ndouble\nNumber of park benches\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(pdftools)\n\nraw_pdf &lt;- pdftools::pdf_text(\"https://parkserve.tpl.org/mapping/historic/2020_ParkScoreRank.pdf\")\n\nraw_text &lt;- raw_pdf[[1]] %&gt;% \n  str_split(\"\\n\") %&gt;% \n  unlist()\n\ntable_trimmed &lt;- raw_text %&gt;% \n  .[13:(length(raw_text)-1)] %&gt;% \n  str_trim()\n\nall_col_names &lt;- c(\n  \"rank\",\n  \"city\",\n  \"med_park_size_data\",\n  \"med_park_size_points\",\n  \"park_pct_city_data\",\n  \"park_pct_city_points\",\n  \"pct_near_park_data\",\n  \"pct_near_park_points\",\n  \"spend_per_resident_data\",\n  \"spend_per_resident_points\",\n  \"basketball_data\",\n  \"basketball_points\",\n  \"dogpark_data\",\n  \"dogpark_points\",\n  \"playground_data\",\n  \"playground_points\",\n  \"rec_sr_data\",\n  \"rec_sr_points\",\n  \"restroom_data\",\n  \"restroom_points\",\n  \"splashground_data\",\n  \"splashground_points\",\n  \"amenities_points\",\n  \"total_points\",\n  \"total_pct\",\n  \"city_dup\"\n)\n\ntab_names &lt;- fwf_empty(\n  table_trimmed,\n  col_names = all_col_names\n)\n\npark_2020_1 &lt;- table_trimmed %&gt;% \n  read_fwf(\n    tab_names\n  ) \n\npark_2020_2 &lt;- raw_pdf[[2]] %&gt;% \n  str_split(\"\\n\") %&gt;% \n  unlist() %&gt;% \n  .[1:41] %&gt;% \n  str_trim() %&gt;% \n  str_replace_all(\"\\\\s{2,}\", \"|\") %&gt;% \n  read_delim(\n    delim = \"|\", \n    col_names = all_col_names\n  )\n\nall_2020 &lt;- bind_rows(park_2020_1, park_2020_2) \n\nraw_pdf_19 &lt;- pdftools::pdf_text(\"https://parkserve.tpl.org/mapping/historic/2019_ParkScoreRank.pdf\")\n\nraw_pdf_19[[1]] %&gt;% \n  str_split(\"\\n\") %&gt;% \n  unlist() %&gt;% \n  .[13:53] %&gt;% \n  str_trim() %&gt;% \n  str_replace_all(\"\\\\s{2,}\", \"|\") %&gt;%\n  str_replace_all(\"% \", \"|\") %&gt;% \n  read_delim(\n    delim = \"|\", \n    col_names = FALSE\n  ) %&gt;% \n  set_names(all_col_names[str_detect(all_col_names, \"total_pct\", negate = TRUE)]) %&gt;% \n  glimpse()\n\npark_2019_2 &lt;- raw_pdf_19[[2]] %&gt;% \n  str_split(\"\\n\") %&gt;% \n  unlist() %&gt;% \n  .[1:44] %&gt;% \n  str_trim() %&gt;% \n  str_replace_all(\"\\\\s{2,}\", \"|\") %&gt;%\n  str_replace_all(\"% \", \"|\") %&gt;% \n  read_delim(\n    delim = \"|\", \n    col_names = FALSE\n  ) %&gt;% \n  set_names(all_col_names[str_detect(all_col_names, \"total_pct\", negate = TRUE)]) %&gt;% \n  glimpse()\n\nread_and_clean &lt;- function(year, page2 = TRUE){\n  \n  raw_pdf_in &lt;- pdftools::pdf_text(glue::glue(\"https://parkserve.tpl.org/mapping/historic/{year}_ParkScoreRank.pdf\"))\n  \n  df1 &lt;- raw_pdf_in[[1]] %&gt;% \n    str_split(\"\\n\") %&gt;% \n    unlist() %&gt;% \n    # .[range1] %&gt;% \n    str_trim() %&gt;% \n    str_subset(\"^[[:digit:]]+ \") %&gt;% \n    str_subset(\"Ranking|ParkScore\", negate = TRUE) %&gt;% \n    str_replace_all(\"\\\\s{2,}\", \"|\") %&gt;%\n    str_replace_all(\"% \", \"|\") %&gt;% \n    read_delim(\n      delim = \"|\", \n      col_names = FALSE\n    ) \n  \n  if(isTRUE(page2)){\n      df2 &lt;- raw_pdf_in[[2]] %&gt;% \n        str_split(\"\\n\") %&gt;% \n        unlist() %&gt;% \n        # .[range2] %&gt;% \n        str_trim() %&gt;% \n        str_subset(\"^[[:digit:]]+ \") %&gt;% \n        str_subset(\"Ranking|ParkScore\", negate = TRUE) %&gt;% \n        str_replace_all(\"\\\\s{2,}\", \"|\") %&gt;%\n        str_replace_all(\"% \", \"|\") %&gt;% \n        read_delim(\n          delim = \"|\", \n          col_names = FALSE\n        ) \n      \n      bind_rows(df1, df2)\n    } else {\n      df1\n    }\n     \n    }\n\nall_2020 &lt;- read_and_clean(2020) %&gt;% \n  set_names(nm = all_col_names) %&gt;% \n  mutate(year = 2020)\nall_2019 &lt;- read_and_clean(2019) %&gt;% \n  set_names(all_col_names[str_detect(all_col_names, \"total_points\", negate = TRUE)]) %&gt;% \n  mutate(year = 2019)\nall_2018 &lt;- read_and_clean(2018) %&gt;% \n  set_names(nm = all_col_names) %&gt;% \n  mutate(year = 2018)\nall_2017 &lt;- read_and_clean(2017) %&gt;% \n  set_names(nm = all_col_names[c(1:18, 23:26)]) %&gt;% \n  rename(park_benches = total_pct) %&gt;% \n  mutate(year = 2017)\nall_2016 &lt;- read_and_clean(2016) %&gt;% \n  set_names(nm = c(all_col_names[c(1:18, 23:26)], \"city_dup2\")) %&gt;% \n  rename(park_benches = city_dup, city_dup = city_dup2) %&gt;% \n  mutate(year = 2016)\nall_2015 &lt;- read_and_clean(2015, FALSE) %&gt;% \n  set_names(nm = c(all_col_names[c(1:18, 23:25)], \"park_benches\")) %&gt;% \n  mutate(year = 2015)\nall_2014 &lt;- read_and_clean(2014, FALSE) %&gt;% \n  set_names(nm = c(all_col_names[c(1:10, 15:16, 25)], \"park_benches\")) %&gt;% \n  mutate(year = 2014)\nall_2013 &lt;- read_and_clean(2013, FALSE) %&gt;% \n  set_names(nm = c(all_col_names[c(1:10, 15:16, 24:25)], \"park_benches\")) %&gt;% \n  mutate(year = 2013)\nall_2012 &lt;- read_and_clean(2012, FALSE) %&gt;% \n  separate(X1, c(\"rank\", \"city\"), extra = \"merge\") %&gt;% \n  mutate(rank = as.double(rank)) %&gt;% \n  set_names(nm = c(all_col_names[c(1:10, 15:16, 24:25)], \"park_benches\")) %&gt;% \n  mutate(year = 2012)\n\nall_data &lt;- bind_rows(list(all_2020, all_2019, all_2018, all_2017, all_2016, all_2015, all_2014, all_2013, all_2012)) %&gt;% \n  select(year, everything())\n\nall_data %&gt;% \n  ggplot(aes(x = year, y = med_park_size_data, group = year)) +\n  geom_boxplot()\n\nall_data %&gt;% glimpse()\n\nall_data %&gt;% \n  write_csv(\"2021/2021-06-22/parks.csv\")\n\nupdate_data_type(\"parks.csv\", \",\")"
  },
  {
    "objectID": "data/2021/2021-06-08/readme.html",
    "href": "data/2021/2021-06-08/readme.html",
    "title": "Commercial Fishing",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nImage of two men holding a fishing net on their boat in one of the Great Lakes\n\n\n\n\nCommercial Fishing\nThe data this week comes from Great Lakes Fishery Commission. Full details on the data can be found on their statistic notes and background notes.\n\nCommercial fish catch data (called production) were published by the Great Lakes Fishery Commission in 1962 (Technical Report No.3) and covered the period 1867-1960. A supplement covering the years 1961-1968 was released in 1970, and a revised edition covering the years 1867-1977 was published in 1979. This third update of a web-based version covers the period 1867-2015. The intent is to update at approximately five-year intervals. The files are intended for open use by the public. We ask only that the commission be acknowledged when these records are used in presentations and publications.\n\nAnother article from the Detroit Free Press on King salmon reign becomes more precarious on changing Great Lakes.\n\nIt’s the undisputed king sport fish of the Great Lakes — it says so right in its name.\nFor a half-century, the chinook, or king salmon — an ocean fish transplanted into the Great Lakes from the Pacific Northwest — has sent fisherman piling into boats every spring and summer, or queuing up on the banks of inland rivers every fall, its fierce fight on the line like a siren’s song.\nFirst stocked regularly in the Great Lakes beginning in 1966, the kings led a complete turnaround in the Great Lakes fishery, helping create a $7 billion economic impact. As recently as 2012, Michigan’s Great Lakes fishing charters averaged 7.4 king salmon caught per trip.\nIt’s unlikely it will ever be like that again.\nInvasive zebra and quagga mussels have spread so pervasively throughout the Great Lakes, their filtration of nutrients from lake water has caused ripple effects throughout the food web. It’s led to steep declines in the populations of another invasive species that is the chinook salmon’s almost exclusive diet — the alewife, a silvery herring.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-06-08')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 24)\n\nfishing &lt;- tuesdata$fishing\n\n# Or read in the data manually\n\nfishing &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-06-08/fishing.csv')\nstocked &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-06-08/stocked.csv')\n\n\nData Dictionary\n\n\n\nfishing.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear of measurement\n\n\nlake\ncharacter\nLake Name\n\n\nspecies\ncharacter\nSpecies of fish\n\n\ngrand_total\ndouble\nGrand total of observed\n\n\ncomments\ncharacter\nComments from the dataset providers\n\n\nregion\ncharacter\nRegion of the US/Canada, note there is some inconsistency\n\n\nvalues\ndouble\nProduction amounts have been rounded to the nearest thousand pounds\n\n\n\n\n\nstocked.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nSID\ndouble\nUnique ID\n\n\nYEAR\ndouble\nYear\n\n\nMONTH\ndouble\nMonth\n\n\nDAY\ndouble\nDay\n\n\nLAKE\ncharacter\nLake\n\n\nSTATE_PROV\ncharacter\nState Province\n\n\nSITE\ncharacter\nSite name\n\n\nST_SITE\ncharacter\nSite code\n\n\nLATITUDE\nlogical\nLatitude\n\n\nLONGITUDE\nlogical\nLongitude\n\n\nGRID\ndouble\nArmy Corps of Engineers grid\n\n\nSTAT_DIST\ncharacter\nStatistical district\n\n\nLS_MGMT\ncharacter\nLake trout management units\n\n\nSPECIES\ncharacter\nSpecies\n\n\nSTRAIN\ncharacter\nStrain of fish\n\n\nNO_STOCKED\ndouble\nNumber of fish stocked\n\n\nYEAR_CLASS\ndouble\nClosest year in which hatching and onset of exogenous feeding occurs.\n\n\nSTAGE\ncharacter\nLife history of stage\n\n\nAGEMONTH\ndouble\nAge in months\n\n\nMARK\ncharacter\nMarking method\n\n\nMARK_EFF\ndouble\nMarking efficiency\n\n\nTAG_NO\nlogical\nTag Number\n\n\nTAG_RET\ndouble\nTag Retention in percent\n\n\nLENGTH\ndouble\nAverage length of stock in mm\n\n\nWEIGHT\ndouble\nAverage weight of fish stocked in kg\n\n\nCONDITION\ndouble\nQualitative condition of fish at stocking\n\n\nLOT_CODE\ncharacter\nLot Code\n\n\nSTOCK_METH\ncharacter\nStock method code\n\n\nAGENCY\ncharacter\nStocking agency code\n\n\nVALIDATION\ndouble\nValidation accuracy\n\n\nNOTES\ncharacter\nNotes/comments\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(readxl)\n\nraw_df &lt;- read_excel(\"commercial.xls\")\n\nraw_df %&gt;%\n  pivot_longer(\n    names_to = \"region\",\n    values_to = \"values\",\n    cols = 4:9\n  ) %&gt;% \n  janitor::clean_names()\n\n\nclean_and_pivot &lt;- function(sheet, cols_long){\n  \n  raw_df &lt;- read_excel(\"commercial.xls\", sheet = sheet)\n  \n  raw_df %&gt;%\n    select(-contains(\"...\")) %&gt;% \n    pivot_longer(\n      names_to = \"region\",\n      values_to = \"values\",\n      cols = all_of(cols_long)\n    ) %&gt;% \n    janitor::clean_names() %&gt;% \n    rename_with(~str_replace(.x, \"totals\", \"total\"))\n  \n}\n\nclean_and_pivot(6, 4:5)\n\nall_data &lt;- tribble(\n  ~sheet, ~cols_long,\n  1, 4:9,\n  2, 4:5,\n  3, 4:10,\n  4, 4:8,\n  5, 4:12,\n  6, 4:5\n) %&gt;% \n  pmap_dfr(clean_and_pivot) \n\nall_data %&gt;% \n  filter(str_detect(region, \"U.S. Total\")) %&gt;% \n  filter(!is.na(region)) %&gt;% \n  mutate(species = fct_lump(species, 12)) %&gt;% \n  filter(species != \"Other\") %&gt;% \n  ggplot(aes(x = year, y = values, color = lake)) +\n  geom_line() +\n  facet_wrap(~species) +\n  theme(legend.position = \"top\")\n\nall_data %&gt;% \n  write_csv(\"2021/2021-06-08/fishing.csv\")"
  },
  {
    "objectID": "data/2021/2021-05-25/readme.html",
    "href": "data/2021/2021-05-25/readme.html",
    "title": "Mario Kart 64 World Records",
    "section": "",
    "text": "Please add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization:\n\nIt’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nBackground splash art for Mario Kart, highlighting Mario driving a small kart in front of Peach and Bowser also driving karts on a rainbow-colored racetrack"
  },
  {
    "objectID": "data/2021/2021-05-25/readme.html#data-dictionary",
    "href": "data/2021/2021-05-25/readme.html#data-dictionary",
    "title": "Mario Kart 64 World Records",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\nworld-records.csv\n\nCurrent world records in Mario Kart 64 with date achieved and player’s name\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack\ncharacter\nTrack name\n\n\ntype\nfactor\nSingle or three lap record\n\n\nshortcut\nfactor\nShortcut or non-shortcut record\n\n\nplayer\ncharacter\nPlayer’s name\n\n\nsystem_played\ncharacter\nUsed system (NTSC or PAL)\n\n\ndate\ndate\nWorld record date\n\n\ntime_period\nperiod\nTime as hms period\n\n\ntime\ndouble\nTime in seconds\n\n\nrecord_duration\ndouble\nRecord duration in days\n\n\n\n\n\ndrivers.csv\n\nPlayer’s data. Except nationality, this could be constructed with the above dataset.\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nposition\ninteger\nPlayer’s current leader board position\n\n\nplayer\ncharacter\nPlayer’s name\n\n\ntotal\ninteger\nTotal world records\n\n\nyear\ndouble\nYear\n\n\nrecords\ninteger\nNumber of world records\n\n\nnation\ncharacter\nPlayer’s nationality"
  },
  {
    "objectID": "data/2021/2021-05-25/readme.html#some-fun-questions-to-explore",
    "href": "data/2021/2021-05-25/readme.html#some-fun-questions-to-explore",
    "title": "Mario Kart 64 World Records",
    "section": "Some fun questions to explore",
    "text": "Some fun questions to explore\n\nHow did the world records develop over time?\nWhich track is the fastest?\nFor which track did the world record improve the most?\nFor how many tracks have shortcuts been discovered?\nWhen were shortcuts discovered?\nOn which track does the shortcut save the most time?\nWhich is the longest standing world record?\nWho is the player with the most world records?\nWho are recent players?\n\nCredit: Benedikt Claus\nCleaning script on Benedikt’s GitHub."
  },
  {
    "objectID": "data/2021/2021-05-11/readme.html",
    "href": "data/2021/2021-05-11/readme.html",
    "title": "Internet Access",
    "section": "",
    "text": "A US Map highlighting regions that are underserved with broadband internet access, where broadband speeds are 25 Mbps or more. Overall, there are large swaths of the US where internet access is very poor.\n\n\n\nInternet Access\nThe data this week comes from Microsoft by way of The Verge.\n\nIf broadband access was a problem before 2020, the pandemic turned it into a crisis. As everyday businesses moved online, city council meetings or court proceedings became near-inaccessible to anyone whose connection couldn’t support a Zoom call. Some school districts started providing Wi-Fi hotspots to students without a reliable home connection. In other districts, kids set up in McDonald’s parking lots just to get a reliable enough signal to do their homework. After years of slowly widening, the broadband gap became impossible to ignore.\n\nNote that to map the areas you will likely need to geocode by zipcode, which can be accomplished with the {zipcodeR} package. The {tigris} can also help with Census shapefiles.\nzipcodeR\n\nzipcodeR is an R package that makes working with ZIP codes in R easier. It provides data on all U.S. ZIP codes using multiple open data sources, making it easier for social science researchers and data scientists to work with ZIP code-level data in data science projects using R.\nThe latest update to zipcodeR includes new functions for searching ZIP codes at various geographic levels & geocoding.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-05-11')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 20)\n\nbroadband &lt;- tuesdata$broadband\n\n# Or read in the data manually\n\nbroadband &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-05-11/broadband.csv')\n\n\nData Dictionary\n\n\n\nbroadband.csv\n\nData contained in the data table includes counties in the United States\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nst\ncharacter\nState\n\n\ncounty_id\ndouble\nCounty ID\n\n\ncounty_name\ncharacter\nCounty Name\n\n\nbroadband_availability_per_fcc\ncharacter\npercent of people per county with access to fixed terrestrial broadband at speeds of 25 Mbps/3 Mbps as of the end of 2017 https://www.fcc.gov/document/broadband-deployment-report-digital-divide-narrowing-substantially-0\n\n\nbroadband_usage\ncharacter\npercent of people per county that use the internet at broadband speeds based on the methodology explained above. Data is from November 2019.\n\n\n\n\n\nbroadband_zip.csv\n\nData contained in the zip code data table includes the following fields:\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nst\ncharacter\nis the 2 letter abbreviation of states in the United States https://www.iso.org/obp/ui/#iso:code:3166:US\n\n\ncounty_name\ncharacter\nCounty Name\n\n\ncounty_id\ndouble\n4 to 5 digit code used to represent the county (last 3 digits) and the state (first digit or first 2 digits) https://www.census.gov/geographies/reference-files.html\n\n\npostal_code\ndouble\nPostal Code\n\n\nbroadband_usage\ndouble\npercent of people per county that use the internet at broadband speeds based on the methodology explained above. Data is from October 2020.\n\n\nerror_range_mae\ndouble\nmean absolute error (MAE). The non-private broadband coverage estimate will be, on average, within the mean absolute error (MAE) error range\n\n\nerror_range_95_percent\ndouble\n95th percentile error range. For 95% of the time, the non-private broadband coverage estimate for zip codes with a similar number of households will be within 95th percentile error range.\n\n\nmsd\ndouble\nWe also provide the mean signed deviation (MSD). The mean signed deviation offers an estimate of bias introduced by the process.\n\n\n\n\nCleaning Script\nNo cleaning this week!"
  },
  {
    "objectID": "data/2021/2021-04-27/readme.html",
    "href": "data/2021/2021-04-27/readme.html",
    "title": "CEO Departures",
    "section": "",
    "text": "Image of blurred moving people walking through an office lobby\n\n\n\nCEO Departures\nThe data this week comes from Gentry et al. by way of DataIsPlural.\n\nWe introduce an open‐source dataset documenting the reasons for CEO departure in S&P 1500 firms from 2000 through 2018. In our dataset, we code for various forms of voluntary and involuntary departure. We compare our dataset to three published datasets in the CEO succession literature to assess both the qualitative and quantitative differences among them and to explore how these differences impact empirical findings associated with the performance‐CEO dismissal relationship. The dataset includes eight different classifications for CEO turnover, a narrative description of each departure event, and links to sources used in constructing the narrative so that future researchers can validate or adapt the coding. The resulting data are available at (https://doi.org/10.5281/zenodo.4543893).\nThis revision includes potentially relevant 8k filings from 270 days before and after the CEO’s departure date. These filings were not all useful for understanding the departure, but might be useful in general.\n\nAnother article from investors.com.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-27')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 18)\n\ndepartures.csv &lt;- tuesdata$departures.csv\n\n# Or read in the data manually\n\ndepartures &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-27/departures.csv')\n\n\nData Dictionary\n\n\n\ndepartures.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndismissal_dataset_id\ndouble\nThe primary key. This will change from one version to the next. gvkey-year is also a unique identifier\n\n\nconame\ncharacter\nThe Compustat Company Name\n\n\ngvkey\ndouble\nThe Compustat Company identifier\n\n\nfyear\ndouble\nThe fiscal year in which the event occured\n\n\nco_per_rol\ndouble\nThe executive/company identifier from Execucomp\n\n\nexec_fullname\ncharacter\nThe executive full name as listed in Execucomp\n\n\ndeparture_code\ndouble\nThe departure reason coded from criteria above\n\n\nceo_dismissal\ndouble\nA dummy code for involuntary, non-health related turnover (Codes 3 & 4).\n\n\ninterim_coceo\ncharacter\nA descriptor of whether the CEO was listed as co-CEO or as an interim CEO (sometimes interim positions last a couple years)\n\n\ntenure_no_ceodb\ndouble\nFor CEOs who return, this value should capture whether this is the first or second time in office\n\n\nmax_tenure_ceodb\ndouble\nFor this CEO, how many times did s/he serve as CEO\n\n\nfyear_gone\ndouble\nAn attempt to determine the fiscal year of the CEO’s effective departure date. Occasionally, looking at departures on Execucomp does not agree with the leftofc date that we have. They apparently try to balance between the CEO serving one month in the fiscal year against documenting who was CEO on the date of record. I would stick to the Execucomp’s fiscal year, departure indication for consistency with prior work\n\n\nleftofc\ndouble\nLeft office of CEO, modified occasionally from execucomp but same interpretation. The date of effective departure from the office of CEO\n\n\nstill_there\ncharacter\nA date that indicates the last time we checked to see if the CEO was in office. If no date, then it looks like the CEO is still in office but we are in the process of checking\n\n\nnotes\ncharacter\nLong-form description and justification for the coding scheme assignment.\n\n\nsources\ncharacter\nURL(s) of relevant sources from internet or library sources.\n\n\neight_ks\ncharacter\nURL(s) of 8k filing from the Securities and Exchange Commission from 270 days before through 270 days after the CEO’s leftofc date which might relate to the turnover. Included here are any 8k filing 5.02 (departure of directors or principal executives) or simply item 5 if it is an older filing. These were collected without examining their content.\n\n\ncik\ndouble\nThe company’s Central Index Key\n\n\n_merge\ncharacter\nMerge details\n\n\n\n\nCEO Departure Code\n\n\n\n\n\n\n\n\nCode Number\nType\ndescription\n\n\n\n\n1\nInvoluntary - CEO death\nThe CEO died while in office and did not have an opportunity to resign before health failed.\n\n\n2\nInvoluntary - CEO illness\nRequired announcement that the CEO was leaving for health concerns rather than removed during a health crisis.\n\n\n3\nInvoluntary – CEO dismissed for job performance\nThe CEO stepped down for reasons related to job performance. This included situations where the CEO was immediately terminated as well as when the CEO was given some transition period, but the media coverage was negative. Often the media cited financial performance or some other failing of CEO job performance (e.g., leadership deficiencies, innovation weaknesses, etc.).\n\n\n4\nInvoluntary - CEO dismissed for legal violations or concerns\nThe CEO was terminated for behavioral or policy-related problems. The CEO’s departure was almost always immediate, and the announcement cited an instance where the CEO violated company HR policy, expense account cheating, etc.\n\n\n5\nVoluntary - CEO retired\nVoluntary retirement based on how the turnover was reported in the media. Here the departure did not sound forced, and the CEO often had a voice or comment in the succession announcement. Media coverage of voluntary turnover was more valedictory than critical. Firms use different mandatory retirement ages, so we could not use 65 or older and facing mandatory retirement as a cut off. We examined coverage around the event and subsequent coverage of the CEO’s career when it sounded unclear.\n\n\n6\nVoluntary - new opportunity (new career driven succession)\nThe CEO left to pursue a new venture or to work at another company. This frequently occurred in startup firms and for founders.\n\n\n7\nOther\nInterim CEOs, CEO departure following a merger or acquisition, company ceased to exist, company changed key identifiers so it is not an actual turnover, and CEO may or may not have taken over the new company.\n\n\n8\nMissing\nDespite attempts to collect information, there was not sufficient data to assign a code to the turnover event. These will remain the subject of further investigation and expansion.\n\n\n9\nExecucomp error\nIf a researcher were to create a dataset of all potential turnovers using execucomp (co_per_rol != l.co_per_rol), several instances will appear of what looks like a turnover when there was no actual event. This code captures those.\n\n\n\n\n\nCleaning Script\nNo cleaning script, although see details at: CEO Dismissal Database."
  },
  {
    "objectID": "data/2021/2021-04-13/readme.html",
    "href": "data/2021/2021-04-13/readme.html",
    "title": "US Post Offices",
    "section": "",
    "text": "Map of post offices by year in the US, where post offices spread from east to West over the years\n\n\n\nUS Post Offices\nThe data this week comes from Cameron Blevins and Richard W. Helbock. Their website has more details:\n\nUS Post Offices\n\nPlease Cite them when using this data:\n“Blevins, Cameron; Helbock, Richard W., 2021,”US Post Offices”, https://doi.org/10.7910/DVN/NUKCNA, Harvard Dataverse, V1, UNF:6:8ROmiI5/4qA8jHrt62PpyA== [fileUNF]”\n\nUS Post Offices is a spatial-historical dataset containing records for 166,140 post offices that operated in the United States between 1639 and 2000. The dataset provides a year-by-year snapshot of the national postal system over multiple centuries, making it one of the most fine-grained and expansive datasets currently available for studying the historical geography of the United States\n\nH/t to Bob Rudis for sharing.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-13')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 16)\n\npost_offices &lt;- tuesdata$post_offices\n\n# Or read in the data manually\n\npost_offices &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-13/post_offices.csv')\n\n\nData Dictionary\n\n\n\npost_offices.csv\n\n\n\n\n\n\n\n\n\nfield_name\nfield_example\nfield_type\nfield_description\n\n\n\n\nname\nHINES CORNER\ncharacter\nThe modified name of a post office based on Helbock’s original transcription, optimized for finding matches within the GNIS database (often this means punctuation has been removed from the original transcription).\n\n\nalt_name\nHINES’S CORNER\ncharacter\nAlternative name formats for Helbock’s original transcription of a post office name.\n\n\norig_name\nHINES(’S) CORNER\ncharacter\nThe original name of a post office transcribed by Richard Helbock.\n\n\nstate\nPA\ncharacter\nState in which the post office was located.\n\n\ncounty1\nWAYNE\ncharacter\nPrimary county in which the post office was located\n\n\ncounty2\nNA\ncharacter\nHelbock sometimes recorded multiple counties for a single post office - ex. “Piscataquia/Somerset” - so I divided those into multiple fields\n\n\ncounty3\nNA\ncharacter\nHelbock sometimes recorded multiple counties for a single post office - ex. “Piscataquia/Somerset” - so I divided those into multiple fields\n\n\norig_county\nWayne\ncharacter\nThe original county in which the post office was located as transcribed by Richard Helbock\n\n\nestablished\n1873\ninteger\nThe year in which a post office was established. Sometimes this can represent the year in which a post office re-opened after being closed for ten years or more, or the year in which the same post office started operating under a new name. See Data Biography for more information.\n\n\ndiscontinued\n1896\nnumeric\nThe year in which a post office was discontinued. Sometimes this can represent the year in which a post office stopped operating under a particular name and started operating under a new name. See Data Biography for more information.\n\n\ncontinuous\nTRUE\nlogical\nDid the post office operate continuously between its Established and Discontinued dates? Many post offices temporarily closed before re-opening (sometimes multiple times). For post offices that closed and then re-opened less than ten years later, Helbock did not create a separate record but used this field to note that it was operating continously between its established and discontinued date.\n\n\nstamp_index\n5\ncharacter\nThe Stamp Scarcity index, on a scale of 0-9: this was an arbitrary field designated by Helbock to indicate how rare a postmark was that originated from that post office.\n\n\nid\n26772\ninteger\nA unique ID assigned by Helbock to each post office record\n\n\ncoordinates\nTRUE\nlogical\nWas a post office successfully geolocated?\n\n\nduration\n23\nnumeric\nThis is a calculated field representing how long a post office operated for (its Discontinued year minus its Established year). Note that some post offices never shut down, and therefore have a blank value for this field.\n\n\ngnis_match\nTRUE\nlogical\nWas a post office successfully matched to a feature within the GNIS database?\n\n\ngnis_name\nHINES CORNERS\ncharacter\nThe modified name of the GNIS feature that was successfully matched to the post office field of Name or AltName.\n\n\ngnis_county\nWAYNE\ncharacter\nThe modified county of the GNIS feature that was successfully matched to the post office field of County1, County2, or County3.\n\n\ngnis_state\nPA\ncharacter\nThe state of the GNIS feature that was successfully matched to the post office field of State.\n\n\ngnis_feature_id\n1177135\ninteger\nThe unique ID of the GNIS feature within the GNIS database.\n\n\ngnis_feature_class\nLocale\ncharacter\nThe type of feature class of the GNIS feature. These are categories of locations, ex. “Post Office”, “Populated Place”, etc.\n\n\ngnis_orig_name\nHines Corners\ncharacter\nThe original spelling of the name of the GNIS feature that was successfully matched to the post office field of Name or AltName.\n\n\ngnis_orig_county\nWayne\ncharacter\nThe original spelling of the county of the GNIS feature that was successfully matched to the post office field of County1, County2, or County3.\n\n\ngnis_latitude\n41.82286\nnumeric\nThe latitude of the GNIS feature in decimal degrees\n\n\ngnis_longitude\n-75.44824\nnumeric\nThe longitude of the GNIS feature in decimal degrees\n\n\ngnis_elev_in_m\n619\ninteger\nThe elevation of the GNIS feature in meters\n\n\ngnis_dist\n0.9166667\nnumeric\nA fuzzy matching score between 0.75 to 1 that was generated from the “Levenshtein distance” between the name of the post office and the name of the GNIS feature. A score of 1 is a perfect match. This is based on the length of the post office name. In this example, the post office name “HINES CORNER” is 12 characters long. To “fuzzy match” to “HINES CORNERS” required one change (adding “S”). 12 total characters minus (1 change/12 total characters) = 0.91667. I used a score threshold of 0.75, meaning that any potential matches that fell below this score were discarded.\n\n\nlatitude\n41.82286\nnumeric\nThe best inference for the latitude of a post office in decimal degrees\n\n\nlongitude\n-75.44824\nnumeric\nThe best inference for the latitude of a post office in decimal degrees\n\n\nrandom_coords_flag\nFALSE\nlogical\nThis field only appears in us-post-offices-random-coords.csv. It is a marker for whether or not the values for Latitude and Longitude were generated from GNIS features or whether they were assigned from a list of randomly distributed points in the surrounding county. Any records marked TRUE mean that the coordinates for that record are semi-random and should be treated as such.\n\n\n\n\nCleaning Script\nNo cleaning this week, although check out the original authors data cleaning/prep steps on their GitHub."
  },
  {
    "objectID": "data/2021/2021-03-30/readme.html",
    "href": "data/2021/2021-03-30/readme.html",
    "title": "Makeup Shades",
    "section": "",
    "text": "Photo of makeup compact of various shades on marble table, photo credit to Element5 Digital by way of Unsplash\n\n\n\nMakeup Shades\nThe data this week comes from The Pudding. They have a corresponding article related to this data.\n\nFirst Place. Lead Role. Number One. When things are arranged in a sequence, we have a mild obsession with being the “first.” You want the blue ribbon. To be on the first page of search results. To have your story above the fold. Afterall, we prioritize the things that come first.\nWhen beauty brands label their foundation shades with sequential numbers, they are implicitly prioritizing those at the beginning of the sequence. These products become more accessible to customers because they are often higher on store shelves and are not hidden behind the “See More” button on websites..\nWe found 130 products on Sephora’s and Ulta’s websites that use a sequential number system to label their shades. Of those, 97% put their lighter shades, and thus the customers that use those shades, first.\n\nThis is an interesting dataset, and many thanks to Ofunne Amaka and Amber Thomas for sharing the article, the data, and the code behind the article. There’s a lot to the actual data collection itself, as there’s a lot of regex, data cleaning, web scraping, etc.\nYou can work with the text data here, counts, or try and recreate some of the plots from the Pudding.\nAnother note is that Offune and Amber have optionally allowed for the “scrollytelling” to be turned off. This is in an effort to have better accessibility of the article.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-03-30')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 14)\n\nsephora &lt;- tuesdata$sephora\n\n# Or read in the data manually\n\nsephora &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-30/sephora.csv')\nulta &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-30/ulta.csv')\nallCategories &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-30/allCategories.csv')\nallShades &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-30/allShades.csv')\nallNumbers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-30/allNumbers.csv')\n\n\nData Dictionary\n\n\n\nsephora.csv\n\n\n\n\n\n\n\n\nHeader\nDescription\nData Type\n\n\n\n\nbrand\nThe brand of foundation\ncharacter\n\n\nproduct\nThe product name\ncharacter\n\n\nurl\nURL to the product page\ncharacter\n\n\ndescription\nThe description associated with a particular swatch (e.g., “Shade 1 (fair cool)”) as displayed on the product page\ncharacter\n\n\nimgSrc\nThe incomplete url to the image displaying a swatch of this particular foundation shade (note: to complete the url, https://sephora.com needs to be appended to the beginning)\ncharacter\n\n\nimgAlt\nThe alt text attribute for a particular swatch, as is made available to assistive technology\ncharacter\n\n\nname\nThe programmatically extracted word-based name of this particular shade\ncharacter\n\n\nspecific\nThe number or number/letter combination (e.g., “12CN”) used to label a particular shade\ncharacter\n\n\n\n\n\nulta.csv\n\n\n\n\n\n\n\n\nHeader\nDescription\nData Type\n\n\n\n\nbrand\nThe brand of foundation\ncharacter\n\n\nproduct\nThe product name\ncharacter\n\n\nurl\nURL to the product page\ncharacter\n\n\ndescription\nThe description associated with a particular swatch (e.g., “Shade 1 (fair cool)”) as displayed on the product page\ncharacter\n\n\nimgSrc\nThe complete url to the image displaying a swatch of this particular foundation shade\ncharacter\n\n\nimgAlt\nThe alt text attribute for a particular swatch, as is made available to assistive technology\ncharacter\n\n\nname\nThe programmatically extracted word-based name of this particular shade\ncharacter\n\n\nspecific\nThe number or number/letter combination (e.g., “12CN”) used to label a particular shade\ncharacter\n\n\n\n\n\nallShades.csv\n\n\n\n\n\n\n\n\nHeader\nDescription\nData Type\n\n\n\n\nbrand\nThe brand of foundation\ncharacter\n\n\nproduct\nThe product name\ncharacter\n\n\nurl\nURL to the product page\ncharacter\n\n\ndescription\nThe description associated with a particular swatch (e.g., “Shade 1 (fair cool)”) as displayed on the product page\ncharacter\n\n\nimgSrc\nThe url to the image displaying a swatch of this particular foundation shade\ncharacter\n\n\nimgAlt\nThe alt text attribute for a particular swatch, as is made available to assistive technology\ncharacter\n\n\nname\nThe programmatically extracted word-based name of this particular shade\ncharacter\n\n\nspecific\nThe number or number/letter combination (e.g., “12CN”) used to label this particular shade\ncharacter\n\n\ncolorspace\nThe colorspace used to analyze the shade (e.g., “RGB”)\ncharacter\n\n\nhex\nThe hexadecimal color code for the most prevalent color in the imgSrc swatch image (e.g., #4F322C)\ncharacter\n\n\nhue\nThe hue value from the HSL color space. This is represented as a number from 0 to 360 degrees around the color wheel\nnumeric\n\n\nsat\nThe saturuation value from the HSL color space. This represents the amount of gray in a color from 0 to 100 percent (Note: here, it is represented as a decimal from 0 to 1)\nnumeric\n\n\nlightness\nThe lightness value from the HSL color space. This is represented as a decimal from 0 to 1 where 0 is pure black and 1 is pure white\nnumeric\n\n\n\n\n\nallCategories.csv\n\n\n\n\n\n\n\n\nHeader\nDescription\nData Type\n\n\n\n\nbrand\nThe brand of foundation\ncharacter\n\n\nproduct\nThe product name\ncharacter\n\n\nurl\nURL to the product page\ncharacter\n\n\nimgSrc\nThe url to the image displaying a swatch of this particular foundation shade\ncharacter\n\n\nname\nThe programmatically extracted word-based name of this particular shade\ncharacter\n\n\ncategories\nComma separated categories that were assigned to a given label (e.g., food, color)\ncharacter\n\n\nspecific\nThe number or number/letter combination (e.g., “12CN”) used to label this particular shade\ncharacter\n\n\nhex\nThe hexadecimal color code for the most prevalent color in the imgSrc swatch image (e.g., #4F322C)\ncharacter\n\n\nlightness\nThe lightness value from the HSL color space. This is represented as a decimal from 0 to 1 where 0 is pure black and 1 is pure white\nnumeric\n\n\n\n\n\nallNumbers.csv\n\n\n\n\n\n\n\n\nHeader\nDescription\nData Type\n\n\n\n\nbrand\nThe brand of foundation\ncharacter\n\n\nproduct\nThe product name\ncharacter\n\n\nname\nThe programmatically extracted word-based name of this particular shade\ncharacter\n\n\nspecific\nThe number or number/letter combination (e.g., “12CN”) used to label a particular shade\ncharacter\n\n\nlightness\nThe lightness value from the HSL color space. This is represented as a decimal from 0 to 1\nnumeric\n\n\nhex\nThe hexadecimal color code for the most prevalent color in the imgSrc swatch image (e.g., #4F322C)\ncharacter\n\n\nlightToDark\nWhether this product line organizes their colors from light to dark (Note: a value of NA indicates that a product uses a number-based naming system, but not a sequential numbering system)\nlogical\n\n\nnumbers\nThe numbers associated with a particular shade\nnumeric\n\n\nid\nA generated ID number assigned to each individual product\nnumeric\n\n\n\n\nCleaning Script\nThe actual cleaning script from Amber Thomas is available on The Pudding’s Github."
  },
  {
    "objectID": "data/2021/2021-03-16/readme.html",
    "href": "data/2021/2021-03-16/readme.html",
    "title": "Video Games and Sliced",
    "section": "",
    "text": "Picture of controllers\n\n\n\nVideo Games and Sliced\nThe data this week comes from Steam by way of Kaggle and originally came from SteamCharts. The data was scraped and uploaded to Kaggle.\nNote there is a different dataset based on video games from 2019’s TidyTuesday, check it out here, there’s a possibility that some of the data could be joined on “name”.\nAdditionally we are doing a crossover with the “Sliced” data science challenge this week!\nMake sure to tune in to “Sliced” on Nick Wan’s Twitch stream, Tuesday March 16th at 8:30 pm ET!\nWhat is Sliced? It’s like Chopped but for Data Science!\n\nData scientists get data they have never seen and have 2 hours to make a predictive model. Create the best data science or be sliced!\n\nThis is inline with the TidyTuesday efforts, and I look forward to seeing what they do with the stream.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-03-16')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 12)\n\ngames &lt;- tuesdata$games\n\n# Or read in the data manually\n\ngames &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-16/games.csv')\n\n\nData Dictionary\n\n\n\ngames.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngamename\ncharacter\nName of video games\n\n\nyear\ndouble\nYear of measure\n\n\nmonth\ncharacter\nMonth of measure\n\n\navg\ndouble\nAverage number of players at the same time\n\n\ngain\ndouble\nGain (or loss) Difference in average compared to the previous month (NA = 1st month)\n\n\npeak\ndouble\nHighest number of players at the same time\n\n\navg_peak_perc\ncharacter\nShare of the average in the maximum value (avg / peak) in %\n\n\n\n\nCleaning Script\nNo cleaning this week!"
  },
  {
    "objectID": "data/2021/2021-03-02/readme.html",
    "href": "data/2021/2021-03-02/readme.html",
    "title": "Superbowl commercials",
    "section": "",
    "text": "Superbowl commercials\nThe data this week comes from FiveThirtyEight. They have a corresponding article on the topic. Note that the original source was superbowl-ads.com. You can watch all the ads via the FiveThirtyEight article above.\n\nLike millions of viewers who tune into the big game year after year, we at FiveThirtyEight LOVE Super Bowl commercials. We love them so much, in fact, that we wanted to know everything about them … by analyzing and categorizing them, of course. We dug into the defining characteristics of a Super Bowl ad, then grouped commercials based on which criteria they shared — and let me tell you, we found some really weird clusters of commercials.\nWe watched 233 ads from the 10 brands that aired the most spots in all 21 Super Bowls this century, according to superbowl-ads.com.1 While we watched, we evaluated ads using seven specific criteria, marking every spot as a “yes” or “no” for each:\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-03-02')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 10)\n\nyoutube &lt;- tuesdata$youtube\n\n# Or read in the data manually\n\nyoutube &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-02/youtube.csv')\n\n\nData Dictionary\n\n\n\nyoutube.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nSuperbowl year\n\n\nbrand\ncharacter\nBrand for commercial\n\n\nsuperbowl_ads_dot_com_url\ncharacter\nSuperbowl ad URL\n\n\nyoutube_url\ncharacter\nYoutube URL\n\n\nfunny\nlogical\nContains humor\n\n\nshow_product_quickly\nlogical\nShows product quickly\n\n\npatriotic\nlogical\nPatriotic\n\n\ncelebrity\nlogical\nContains celebrity\n\n\ndanger\nlogical\nContains danger\n\n\nanimals\nlogical\nContains animals\n\n\nuse_sex\nlogical\nUses sexuality\n\n\nid\ncharacter\nYoutube ID\n\n\nkind\ncharacter\nYoutube Kind\n\n\netag\ncharacter\nYoutube etag\n\n\nview_count\ninteger\nYoutube view count\n\n\nlike_count\ninteger\nYoutube like count\n\n\ndislike_count\ninteger\nYoutube dislike count\n\n\nfavorite_count\ninteger\nYoutube favorite count\n\n\ncomment_count\ninteger\nYoutube comment count\n\n\npublished_at\ncharacter\nYoutube when published\n\n\ntitle\ncharacter\nYoutube title\n\n\ndescription\ncharacter\nYoutube description\n\n\nthumbnail\ncharacter\nYoutube thumbnail\n\n\nchannel_title\ncharacter\nYoutube channel name\n\n\ncategory_id\ncharacter\nYoutube content category id\n\n\n\n\nCleaning Script\nNote this is optional, and NOT required. I downloaded the youtube data via httr from the youtube API and an API key.\nlibrary(tidyverse)\nlibrary(tuber)\nlibrary(rvest)\nlibrary(httr)\n\nraw_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/superbowl-ads/main/superbowl-ads.csv\")\n\nall_ids &lt;-raw_data$youtube_url %&gt;% \n  str_remove_all(\"https://www.youtube.com/watch\") %&gt;% \n  str_remove(\"\\\\?v=\") %&gt;% \n  str_subset(\"NA\", negate = TRUE)\n\nall_ids\n\napi_key = \"SUPER_SECRET_API\"\n\nget_youtube_data &lt;- function(ids_in, query_type = \"statistics\"){\n  \n  url_in &lt;- modify_url(\"https://www.googleapis.com/youtube/v3/videos\", \n                     query = list(\n                       \"part\" = query_type,\n                       \"id\" = paste(ids_in, collapse=\",\"),\n                       \"key\" = api_key)\n  )\n  out_content &lt;- content(GET(url_in), as = \"parsed\", type = \"application/json\")\n  \n  if(query_type == \"statistics\"){\n    out_content$items %&gt;% \n      enframe() %&gt;% \n      unnest_wider(value) %&gt;% \n      unnest_wider(statistics) %&gt;% \n      janitor::clean_names()\n    \n  }\n}\n\nget_youtube_details &lt;- function(ids_in){\n  \n  url_in &lt;- modify_url(\"https://www.googleapis.com/youtube/v3/videos\", \n                       query = list(\n                         \"part\" = \"snippet\",\n                         \"id\" = paste(ids_in, collapse=\",\"),\n                         \"key\" = api_key)\n  )\n  out_content &lt;- content(GET(url_in), as = \"parsed\", type = \"application/json\")\n  \n  out_content$items %&gt;% \n    enframe() %&gt;% \n    unnest_wider(value) %&gt;% \n    unnest_wider(snippet) %&gt;% \n    janitor::clean_names()\n}\n\nall_vid_stats &lt;- list(\n  all_ids[1:50],\n  all_ids[51:100],\n  all_ids[101:150],\n  all_ids[151:200],\n  all_ids[200:length(all_ids)]\n) %&gt;% \n  map_dfr(get_youtube_data)\n\nall_vid_details &lt;- list(\n    all_ids[1:50],\n    all_ids[51:100],\n    all_ids[101:150],\n    all_ids[151:200],\n    all_ids[200:length(all_ids)]\n    ) %&gt;% \n  map_dfr(get_youtube_details)\n\nclean_details &lt;- all_vid_details %&gt;% \n  hoist(thumbnails, \n        thumbnail = list(\"standard\", \"url\")) %&gt;% \n  select(id, published_at, title:category_id, -tags, -thumbnails)\n\ncombo_vid &lt;- all_vid_stats %&gt;% \n  left_join(clean_details) %&gt;% \n  select(-name)\n\nall_youtube &lt;- raw_data %&gt;% \n  mutate(id = str_remove(youtube_url, \"https://www.youtube.com/watch\") %&gt;% \n           str_remove(\"\\\\?v=\")) %&gt;% \n  left_join(combo_vid) %&gt;% \n  mutate(across(view_count:comment_count, as.integer)) \n\nwrite_csv(all_youtube, \"2021/2021-03-02/youtube.csv\")\n\nall_youtube %&gt;% \n  glimpse()"
  },
  {
    "objectID": "data/2021/2021-02-16/readme.html",
    "href": "data/2021/2021-02-16/readme.html",
    "title": "Dubois Challenge",
    "section": "",
    "text": "Dubois Challenge\nThe data this week comes from Anthony Starks, Allen Hillery Sekou Tyler. Note that Anthony Starks has provided many examples and data preparation, including a “style guide” and article.\nAll credit goes to Anthony, Allen, and Sekou for the preparation of this week’s datasets/challenge.\nAnthony, Allen, and Sekou have put together a very cool data challenge as part of a longer celebration of “the data visualization legacy of W.E.B DuBois by recreating the visualizations from the 1900 Paris Exposition using modern tools.” A quick guide to the “Dubosian style” by Anthony Starks.\nTheir original readme and source of data can be found on their GitHub. Please check it out for even more detail, but I have duplicated the core details below:\nI have included a repeat of TidyTuesday census data from week 23 of 2020, which covers the US census data broken down by race from, 1790 to 1990, where the 13th Amendment “officially abolished” slavery in 1865.\nPlease use the #DuBoisChallenge hashtag to fully interact/overlap with their event and note that it will continue for several weeks!\n\nThe goal of the challenge is to celebrate the data visualization legacy of W.E.B DuBois by recreating the visualizations from the 1900 Paris Exposition using modern tools.\nThis directory contains the data and original plates from the exposition; your goal is to re-create the visualizations using modern tools of your choice (Tableau, PowerBi, decksh, etc)\nIn this repo, there is a folder for each challenge, which includes the images of the 1900 original plates along with the corresponding data. You may submit your re-creations to twitter using the hash tag #DuBoisChallenge\n\n\nThe Challenges\n\nchallenge01: Comparative Increase of White and Colored Population in Georgia\nchallenge02: Conjugal Condition\nchallenge03: Occupations of Negroes and Whites in Georgia\nchallenge04: Proportion of Freeman and Slaves Among American Negroes\nchallenge05: Income and Expenditure of 150 Negro Families in Atlanta, GA, USA\nchallenge06: City and Rural Population 1890\nchallenge07: Assessed Value of Household and Kitchen Furniture Owned by Georgia Negroes.\nchallenge08: The Georgia Negro. A Social Study by W.E.Burghardt Du Bois\nchallenge09: Migration of Negroes\nchallenge10: Negro Population of Georgia by Counties\n\n\nAnthony’s article on Recreating W.E.B. Du Bois’s Data Portraits.\n\nIn May 2017, I attended a talk: “Historical development of W.E.B. Du Bois’s graphical narrative” at the Data Visualization New York meetup. As an African-American, I was intrigued by the subject matter and fascinated by the visualization choices — especially, the now-iconic spiral charts. I made a mental note to attempt to recreate these.\n\n\nWho is W.E.B. Du Bois?\nOverall, W.E.B. Du Bois was a true data visualization expert and visionary who sought with data and data visualization to challenge the incorrect thinking and racist views prevalent in the early 20th century, all only a few decades after the freeing of American slaves.\nFor folks who are not familiar with the works of Du Bois, please read through the articles below:\nData Journalism in the study of W.E.B. Du Bois\n\nOne of the most powerful examples of data visualization was made 118 years ago by an all-black team led by W.E.B. Du Bois only 37 years after the end of slavery in the United States. “The Exhibit of American Negroes” was a sociological display at the 1900 Exposition Universelle in Paris and was a collaboration by noted African-American sociologist W. E. B. Du Bois, educator and social leader Booker T Washington, prominent black lawyer Thomas J. Calloway and students from historically black college Atlanta University.\n\nWEB Du Bois: retracing his attempt to challenge racism with data\n\nAny African American to be admitted to Harvard University in 1888 had to be exceptionally gifted. But that description doesn’t come close to capturing the talent of WEB Du Bois, a man who managed to write 21 books, as well as over 100 essays while being a professor and a relentless civil rights activist.\nDu Bois saw no trade-off between those pursuits – his scholarship was protest and his protest was scholarship. He deeply understood something that every activist scrawling a banner in Washington knows today – messaging matters.\nAt the 1900 World Exposition in Paris, Du Bois went armed with beautiful photographs that showed African Americans posing with dignity. But he also knew that the photographs, however elegant, would fail to persuade many. So, he also took data “to show: (a) The history of the American Negro. (b) His present condition. (c) His education. (d) His literature.”\nWith the help of students at the Atlanta University where he was a professor of economics and history, Du Bois created dozens of illustrations that put many of today’s data visualization experts to shame. The work was the inspiration for Theaster Gates’ latest exhibition in Los Angeles which started this month, and for Black History Month, I wanted to update four of Du Bois’ visualizations with the most recent data available.\n\nW.E.B. Du Bois’ Visionary Infographics Come Together for the First Time in Full Color\n\nAfter three decades of emancipation, the gains made by African-Americans, those that existed at all, presented a decidedly mixed picture about the state of racial progress in the country. The political obstacles were voluminous, with the failure of Reconstruction still lingering, and Jim Crow institutional racism ascendant. In 1897, the United States Supreme Court would rule in Plessy v. Ferguson that separate was indeed equal. All the while, new generations of African-Americans found ways to uplift themselves, despite discrimination, through grassroots efforts in education, work and community building.\nAfter graduating with a Ph.D. in history from Harvard University, W.E.B. Du Bois, the prominent African-American intellectual, sought a way to process all this information showing why the African disapora in America was being held back in a tangible, contextualized form. “It is not one problem,” as Du Bois wrote in 1898, “but rather a plexus of social problems, some new, some old, some simple, some complex; and these problems have their one bond of unity in the act that they group themselves above those Africans whom two centuries of slave-trading brought into the land.”\nTo accomplish this goal, Du Bois turned to the burgeoning field of sociology. Sociology’s scope in history, statistics, and demographics held the potential to quantifiably reveal “life within the Veil,” as Du Bois called the structural forces of oppressions that separated black and white populations, whether that came to educational attainment, voting rights or land ownership.\n\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-02-16')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 8)\n\ngeorgia_pop &lt;- tuesdata$georgia_pop\n\n# Or read in the data manually\n\ngeorgia_pop &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-16/georgia_pop.csv')\ncensus &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-16/census.csv')\nfurniture &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-16/furniture.csv')\ncity_rural &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-16/city_rural.csv')\nincome &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-16/income.csv')\nfreed_slaves &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-16/freed_slaves.csv')\noccupation &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-16/occupation.csv')\nconjugal &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-16/conjugal.csv')\n\n\nData Dictionary\nPlease note that I am leaving the data as it was found/presented in 1900, and specifically the use of the offensive term “colored” for Black/African-American.\n\n\n\ngeorgia_pop.csv\nPopulation change by race in Georgia.\nOriginal Image\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndouble\nYear\n\n\nColored\ndouble\nBlack population in Georgia\n\n\nWhite\ndouble\nWhite population in Georgia\n\n\n\n\n\nconjugal.csv\nMarriage status, original image\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nPopulation\ncharacter\nPopulation Group\n\n\nAge\ncharacter\nAge group\n\n\nSingle\ndouble\nPercent single\n\n\nMarried\ndouble\nPercent Married\n\n\nDivorced and Widowed\ndouble\nPercent Divorced or Widowed\n\n\n\n\n\noccupation.csv\nOccupation by race, original image\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nGroup\ncharacter\nRacial Group\n\n\nOccupation\ncharacter\nOccupation\n\n\nPercentage\ndouble\nPercentage\n\n\n\n\n\nfreed_slaves.csv\nProportion of freemen vs slaves in US by year, original image\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndouble\nYear\n\n\nSlave\ndouble\nProportion enslaved\n\n\nFree\ndouble\nProportion freed\n\n\n\nNote that the census.csv all provides similar data for longer range.\n\n\ncensus.csv\nNot an official Du Bois dataset (as it goes farther than 1900), but includes more detail, comes from TidyTuesday 2020-week 23.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nregion\ncharacter\nUS Region\n\n\ndivision\ncharacter\nUS Division\n\n\nyear\ndouble\nYear of Census\n\n\ntotal\ndouble\nTotal population\n\n\nwhite\ndouble\nWhite population\n\n\nblack\ndouble\nBlack population\n\n\nblack_free\ndouble\nBlack and free population\n\n\nblack_slaves\ndouble\nBlack and enslaved population\n\n\n\n\n\nincome.csv\nIncome brackets and annual expenditures, original image\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nClass\ncharacter\nIncome class\n\n\nActual Average\ndouble\nActual average income\n\n\nRent\ndouble\nRent\n\n\nFood\ndouble\nFood\n\n\nClothes\ndouble\nClothes\n\n\nTax\ndouble\nTax\n\n\nOther\ndouble\nOther\n\n\n\n\n\ncity_rural.csv\nBlack population split between city and rural areas, original image\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCategory\ncharacter\nCategory of city\n\n\nPopulation\ndouble\nPopulation by category\n\n\n\n\n\nfurniture.csv\nAssessed value of household and kitchen furniture owned by Black American in Georgia, original image\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndouble\nYear\n\n\nHoushold Value (Dollars)\ndouble\nFurniture value\n\n\n\n\nCleaning Script\nNo cleaning script this week, but massive credit to Anthony Stark for his efforts in aggregating, cleaning, and putting it all on GitHub. See his process in his article on Recreating W.E.B. Du Bois’s Data Portraits."
  },
  {
    "objectID": "data/2021/2021-02-02/readme.html",
    "href": "data/2021/2021-02-02/readme.html",
    "title": "College Enrollment",
    "section": "",
    "text": "College Enrollment\nThe data this week comes from Data.World and Data.World and was originally from the NCES.\n\nHigh school completion and bachelor’s degree attainment among persons age 25 and over by race/ethnicity & sex 1910-2016\n\n\nFall enrollment in degree-granting historically Black colleges and universities (HBCU)\n\nConsider donating to HBCUs, to help fund student’s financial assistance programs.\nDonation link: https://thehbcufoundation.org/donate/\nThere’s other additional HBCU datasets at Data.World as well.\nHBCU Donations Article\n\n… Donation will be placed in an endowment for students to fund need-based scholarships. President Reynold Verret believes the donation will provide an opportunity for students who don’t have the same financial support as others.\n\n\n“Xavier has roughly more than half of our students who are Pell-eligible. Which means they are in the lowest fifth of the socioeconomic ladder in the country. The lowest quintile. So these students really have significant family needs,” said Verret. “They’re often the first generation in their families to attend college, and meeting the gap between what Pell and the small loans provide and making it affordable is where that need-based is, which is not just based on merit, on your highest ACT or GPA, but basically to qualify students who are able who have the talent and the ability to succeed at Xavier.”\n\nI’ve left the datasets relatively “untidy” this week so you can practice some of the pivot_longer() functions from tidyr. Note that all of the individual CSVs that are duplicates of the raw Excel files.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-02-02')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 6)\n\nhbcu_all &lt;- tuesdata$hbcu_all\n\n# Or read in the data manually\n\nhbcu_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-02/hbcu_all.csv')\n\n\nData Dictionary\n\n\n\nhbcu.csv\n\n\nhs_students.csv\n\nThe percentage of students broken down by race/ethnicity, aged 25 and over who have graduated HS.\n\nbach_students, female_bach_students, female_hs_students, male_bach_students, male_hs_students:\n\nSame as above, but for specific gender and education combination.\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nTotal\ndouble\nYear\n\n\nTotal, percent of all persons age 25 and over\ndouble\nTotal combined population,\n\n\nStandard Errors - Total, percent of all persons age 25 and over\ncharacter\nStandard errors (SE)\n\n\nWhite1\ncharacter\nWhite students\n\n\nStandard Errors - White1\ncharacter\nSE\n\n\nBlack1\ncharacter\nBlack students\n\n\nStandard Errors - Black1\ncharacter\nSE\n\n\nHispanic\ncharacter\nHispanic students\n\n\nStandard Errors - Hispanic\ncharacter\nSE\n\n\nTotal - Asian/Pacific Islander\ncharacter\nAsian Pacific Islander Total students\n\n\nStandard Errors - Total - Asian/Pacific Islander\ncharacter\nSE\n\n\nAsian/Pacific Islander - Asian\ncharacter\nAsian Pacific Islandar - Asian students\n\n\nStandard Errors - Asian/Pacific Islander - Asian\ncharacter\nSE\n\n\nAsian/Pacific Islander - Pacific Islander\ncharacter\nAsian/Pacific Islander - Pacific Islander\n\n\nStandard Errors - Asian/Pacific Islander - Pacific Islander\ncharacter\nSE\n\n\nAmerican Indian/ Alaska Native\ncharacter\nAmerican Indian/ Alaska Native Students\n\n\nStandard Errors - American Indian/Alaska Native\ncharacter\nSE\n\n\nTwo or more race\ncharacter\nTwo or more races students\n\n\nStandard Errors - Two or more race\ncharacter\nSE\n\n\n\n\n\nhbcu_all.csv\n\nEnrollment by year for types of HBCUs\nNote that hbcu_black.csv has duplicate information, but specific to black-student enrollment only.\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndouble\nYear\n\n\nTotal enrollment\ndouble\nTotal enrollment\n\n\nMales\ndouble\nMale enrollment\n\n\nFemales\ndouble\nFemale Enrollment\n\n\n4-year\ndouble\n4 Year college enrollment\n\n\n2-year\ndouble\n2 Year college enrollment\n\n\nTotal - Public\ndouble\nTotal public school enrollment\n\n\n4-year - Public\ndouble\n4 year public school enrollment\n\n\n2-year - Public\ndouble\n2 Year public college enrollment\n\n\nTotal - Private\ndouble\nTotal private college enrollment\n\n\n4-year - Private\ndouble\n4 year private school enrollment\n\n\n2-year - Private\ndouble\n2 Year private college enrollment\n\n\n\n\nCleaning Script\nThis is an optional cleaning script, but shows examples of how to take the raw data this week and prep it for analysis.\nSee more expansive walkthroughs of cleaning this data by Jack Davison and Alex Cookson.\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(glue)\n\n# student data\n\nhs_students &lt;- read_excel(\"2021/2021-02-02/104.10.xlsx\", sheet = 1)\nbach_students &lt;- read_excel(\"2021/2021-02-02/104.10.xlsx\", sheet = 2)\nmale_hs_students &lt;- read_excel(\"2021/2021-02-02/104.10.xlsx\", sheet = 3)\nmale_bach_students &lt;- read_excel(\"2021/2021-02-02/104.10.xlsx\", sheet = 4)\nfemale_hs_students &lt;- read_excel(\"2021/2021-02-02/104.10.xlsx\", sheet = 5)\nfemale_bach_students &lt;- read_excel(\"2021/2021-02-02/104.10.xlsx\", sheet = 6)\n\n# HBCU data\nhbcu_all &lt;- read_excel(\"2021/2021-02-02/tabn313.20.xls\", sheet = 1)\nhbcu_black &lt;- read_excel(\"2021/2021-02-02/tabn313.20.xls\", sheet = 2)\n\n\nlist(\n  datasets = list(\n    bach_students,\n    female_bach_students,\n    female_hs_students,\n    hbcu_all,\n    hbcu_black,\n    hs_students,\n    male_bach_students,\n    male_hs_students\n  ),\n  names = ls()\n) %&gt;%\n  pmap(\n    .f = function(datasets, names) {\n      write_csv(datasets, glue::glue(\"2021/2021-02-02/{names}.csv\"))\n    }\n  )\n\n# Example 1\nhs_students %&gt;% \n  mutate(Total = if_else(Total &gt; 10000, str_sub(Total, 1, 4) %&gt;% as.double(), Total)) %&gt;% \n  rename(year = Total) %&gt;% \n  select(!contains(\"Standard\")) %&gt;% \n  select(!contains(\"Total\")) %&gt;% \n  mutate(across(White1:last_col(), as.double)) %&gt;% \n  pivot_longer(cols = 2:last_col(), names_to = \"group\", values_to = \"percentage\") %&gt;% \n  filter(year &gt;= 1980) %&gt;% \n  ggplot(aes(x = year, y = percentage, color = group)) +\n  geom_line()\n\n# example 2\nhbcu_all %&gt;% \n  select(Year, `4-year`, `2-year`) %&gt;% \n  pivot_longer(cols = `4-year`:`2-year`) %&gt;% \n  ggplot(aes(x = Year, y = value, color = name)) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(1980, 2020, by = 4))\n  \n# Alex Cookson Examples\n\n### Load packages -------------------------------------------------------------\nlibrary(tidyverse) # General-purpose cleaning\nlibrary(janitor) # For the clean_names() function\n\n### Import data ---------------------------------------------------------------\nhbcu_all &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-02/hbcu_all.csv') %&gt;%\n  # clean_names() converts field names to snake_case\n  clean_names()\n\n\n### Clean data ----------------------------------------------------------------\n# We can separate by gender OR by program length and public/private, not both\n\n### Gender breakdown\nhbcu_by_gender &lt;- hbcu_all %&gt;%\n  # We only need year and gender columns\n  select(year, males, females) %&gt;%\n  # Convert to tidy format, collapsing male/female into one descriptor field\n  pivot_longer(males:females,\n               names_to = \"gender\",\n               values_to = \"students\") %&gt;%\n  # Convert from plural to singular for cleaner data\n  # \"s%\" specifies an s character at the end of a string\n  # (\"$\" is end of string in regular expressions)\n  mutate(gender = str_remove(gender, \"s$\"))\n\n\n### Program breakdown\nhbcu_by_program &lt;- hbcu_all %&gt;%\n  # We need fields with \"public\" or \"private\" in the name\n  # (They also have 2- vs 4-year)\n  # We DON'T need fields with \"total\" in the name, since this is redundant\n  select(year,\n         contains(c(\"public\", \"private\")),\n         -contains(\"total\")) %&gt;%\n  # names_pattern argument does the heavy lifting\n  # It separates names into groups, as specified by parentheses \"(group)\"\n  # Field names are structured so that program length is followed by public/private\n  # We also specift \"x_\" as an optional argument using regular expressions\n  pivot_longer(cols = x4_year_public:x2_year_private,\n               names_pattern = \"[x_]?(.*)_(.*)\",\n               names_to = c(\"program_length\", \"public_private\"),\n               values_to = \"students\") %&gt;%\n  mutate(program_length = paste(parse_number(program_length), \"years\"))"
  },
  {
    "objectID": "data/2021/2021-01-19/readme.html",
    "href": "data/2021/2021-01-19/readme.html",
    "title": "Kenya Census",
    "section": "",
    "text": "Kenya Census\nThe data this week comes from rKenyaCensus courtesy of Shelmith Kariuki. Shelmith wrote about these datasets on her blog.\n\nrKenyaCensus is an R package that contains the 2019 Kenya Population and Housing Census results. The results were released by the Kenya National Bureau of Statistics in February 2020, and published in four different pdf files (Volume 1 - Volume 4).\n\n\nThe 2019 Kenya Population and Housing Census was the eighth to be conducted in Kenya since 1948 and was conducted from the night of 24th/25th to 31st August 2019. Kenya leveraged on technology to capture data during cartographic mapping, enumeration and data transmission, making the 2019 Census the first paperless census to be conducted in Kenya\n\nAdditional details about Kenya can be found on Wikipedia.\n\nKenya, officially the Republic of Kenya (Swahili: Jamhuri ya Kenya), is a country in Eastern Africa. At 580,367 square kilometres (224,081 sq mi), Kenya is the world’s 48th largest country by total area. With a population of more than 47.6 million people in the 2019 census, Kenya is the 29th most populous country. Kenya’s capital and largest city is Nairobi, while its oldest city and first capital is the coastal city of Mombasa.\n\nTo access ALL the data, install the package from Github:\n(if needed, install.packages(\"remotes\"))\nremotes::install_github(\"Shelmith-Kariuki/rKenyaCensus\")\nShe’s also put together a Shiny app to download various datasets!\nI’ve taken 3 indicators below just as a teaser, but go ahead and try the package for ALL the data!\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-01-19')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 4)\n\ngender &lt;- tuesdata$gender\n\n# Or read in the data manually\n\ngender &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-01-19/gender.csv')\ncrops &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-01-19/crops.csv')\nhouseholds &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-01-19/households.csv')\n\n\nData Dictionary\n\n\n\ngender.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCounty\ncharacter\nCounty Name\n\n\nMale\ndouble\nPopulation of Male identifying\n\n\nFemale\ndouble\nPopulation of Female identifying\n\n\nIntersex\ndouble\nPopulation of Interesex identifying\n\n\nTotal\ndouble\nTotal population\n\n\n\n\n\ncrops.csv\n\nDistribution of Households Growing Permanent Crops by Type and County.\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nSubCounty\ncharacter\nSubCounty name\n\n\nFarming\ndouble\nPopulation growing farming crops\n\n\nTea\ndouble\nPopulation growing farming tea\n\n\nCoffee\ndouble\nPopulation growing farming coffee\n\n\nAvocado\ndouble\nPopulation growing farming avocado\n\n\nCitrus\ndouble\nPopulation growing farming citrus\n\n\nMango\ndouble\nPopulation growing farming Mango\n\n\nCoconut\ndouble\nPopulation growing farming Coconut\n\n\nMacadamia\ndouble\nPopulation growing farming Macadamia\n\n\nCashew Nut\ndouble\nPopulation growing farming cashew nut\n\n\nKhat (Miraa)\ndouble\nPopulation growing farming Khat (Miraa)\n\n\n\n\n\nhouseholds.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nCounty\ncharacter\nCounty Name\n\n\nPopulation\ndouble\nPopulation\n\n\nNumberOfHouseholds\ndouble\nNumber of households\n\n\nAverageHouseholdSize\ndouble\nAverage houshold size\n\n\n\n\n\nData Catalogue\nFull catalogue from the package about all the available datasets!\n\n\n\n\n\n\n\n\n\nDataset\nVolume\nTable.No.in.PDFs\nDataset.Description\n\n\n\n\nDataCatalogue\n\n\nShows the table number for each of the datasets\n\n\nCountyGPS\n\n\nShows the County GPS centroids\n\n\nKenyaCounties_SHP\n\n\nShapefiles of Kenya County boundaries\n\n\nV1_T2.1\nV1\nTable 2. 1\nCensus Indicators at a Glance, 2019\n\n\nV1_T2.2\nV1\nTable 2. 2\nDistribution of Population by Sex and County\n\n\nV1_T2.3\nV1\nTable 2. 3\nDistribution of Population, Number of Households and Average\n\n\nV1_T2.4\nV1\nTable 2. 4\nDistribution of Population, Land Area and Population Density by County\n\n\nV1_T2.5\nV1\nTable 2. 5\nDistribution of Population by Sex and Sub-County\n\n\nV1_T2.6\nV1\nTable 2. 6\nDistribution of Population, Number of Households and Average Household Size by Sub- County\n\n\nV1_T2.7\nV1\nTable 2. 7\nDistribution of Population by Land Area and Population Density by Sub-County\n\n\nV2_T1.1\nV2\nTable 1.1\nSummary of Census Counts in Kenya\n\n\nV2_T1.2\nV2\nTable 1.2\nList of Counties and Sub-Counties\n\n\nV2_T2.1\nV2\nTable 2.1\nSub-locations with no People on the Census Night by Status/Reason\n\n\nV2_T2.2\nV2\nTable 2.2\nDistribution of Population by Sex, Number of Households, Land Area, Population Density and County\n\n\nV2_T2.2a\nV2\nTable 2.2a\nDistribution of Rural Population by Sex, Number of Households, Land Area, Population Density and County\n\n\nV2_T2.2b\nV2\nTable 2.2b\nDistribution of Urban Population by Sex, Number of Households, Land Area, Population Density and County\n\n\nV2_T2.3\nV2\nTable 2.3\nDistribution of Population by Sex, Number of Households, Land Area, Population Density and Sub County\n\n\nV2_T2.5\nV2\nTable 2.5\nDistribution of Population by Urban Centres, Sex and County\n\n\nV3_T1.1\nV3\nTable 1.1\nSummary of Census Counts in Kenya\n\n\nV3_T1.2\nV3\nTable 1.2\nList of Counties and Sub-Counties\n\n\nV3_T2.1\nV3\nTable 2.1\nSub-locations with no People on the Census Night by Status/Reason\n\n\nV3_T2.2\nV3\nTable 2.2\nDistribution of Population by Age and Sex, Kenya\n\n\nV3_T2.2a\nV3\nTable 2.2a\nDistribution of Rural Population by Age and Sex, Kenya\n\n\nV3_T2.2b\nV3\nTable 2.2b\nDistribution of Urban Population by Age and Sex, Kenya\n\n\nV3_T2.3\nV3\nTable 2.3\nDistribution of Population by Age, Sex, County and Sub- County\n\n\nV3_T2.4a\nV3\nTable 2.4a\nDistribution of Rural Population by Age, Sex and County\n\n\nV3_T2.4b\nV3\nTable 2.4b\nDistribution of Urban Population by Age, Sex and County\n\n\nV4_T1.1\nV4\nTable 1. 1\nSummary of Census Counts in Kenya.\n\n\nV4_T1.9\nV4\nTable 1.9\nList of Counties and Sub-Counties\n\n\nV4_T2.2\nV4\nTable2.2\nDistribution of Population Aged 3 Years and Above by School Attendance Status, Area of Residence, Sex, County and SubCounty.\n\n\nV4_T2.3\nV4\nTable 2.3\nDistribution of Population Aged 3 Years and Above Currently Attending School/ Learning Institution by Education Level, Area of Residence, Sex, County and Sub-County\n\n\nV4_T2.4\nV4\nTable 2.4\nDistribution of Population Aged 3 Years and Above by Highest Level of Education Reached, Area of Residence, Sex, County and Sub-County..\n\n\nV4_T2.5\nV4\nTable 2.5\nDistribution of Population Aged 3 Years and Above by Highest Level of Education Completed, Area of Residence, Sex, County and Sub-County..\n\n\nV4_T2.6a\nV4\nTable 2.6a\nDistribution of Population Aged 3 Years and Above by School Attendance Status, Sex and Special Age Groups\n\n\nV4_T2.6b\nV4\nTable 2.6b\nDistribution of Population Aged 3 Years and Above by School Attendance Status, Sex, Special Age Groups and County..\n\n\nV4_T2.7\nV4\nTable 2.7\nDistribution of Population Aged 15 years and Above by Sex and Main Training Acquired and Qualified for\n\n\nV4_T2.8a\nV4\nTable 2.8a\nDistribution of Population Aged 5 Years and above by Activity Status, Sex, County and Sub-County\n\n\nV4_T2.8b\nV4\nTable 2.8b\nDistribution of Urban Population Aged 5 Years and above by Activity Status, Sex, County and Sub-County…..\n\n\nV4_T2.8c\nV4\nTable 2.8c\nDistribution of Rural Population Aged 5 Years and above by Activity Status, Sex, County and Sub-County..\n\n\nV4_T2.9a\nV4\nTable 2.9a\nDistribution of Population Aged 5 years and above by Activity Status, Broad Age Groups and County..\n\n\nV4_T2.9b\nV4\nTable 2.9b\nDistribution of Rural Population Aged 5 years and above by Activity Status, Broad Age Groups and County…….\n\n\nV4_T2.9c\nV4\nTable 2.9c\nDistribution of Urban Population Aged 5 years and above by Activity Status, Broad Age Groups and County…….\n\n\nV4_T2.10\nV4\nTable 2.10\nDistribution of Households and Tenure Status of Main Dwelling Unit by Area of Residence, County and Sub-County\n\n\nV4_T2.11a\nV4\nTable 2.11a\nDistribution of Households Owning the Main Dwelling Unit by Mode of Acquisition, Area of Residence, County and Sub-County\n\n\nV4_T2.11b\nV4\nTable 2.11b\nDistribution of Households Renting/Provided with their Main Tenure Status of Main Dwelling Unit by Provider, Area of Residence, County and Sub- County..\n\n\nV4_T2.12\nV4\nTable 2.12\nPercentage Distribution of Conventional Households by Dominant Roofing Material of Main Dwelling Unit, Area of Residence, County and Sub-County…..\n\n\nV4_T2.13\nV4\nTable 2.13\nPercentage Distribution of Conventional Households by Dominant Wall Material of Main Dwelling Unit, Area of Residence, County and Sub-County…….\n\n\nV4_T2.14\nV4\nTable 2.14\nPercentage Distribution of Conventional Households by Dominant Floor Material of the Main Dwelling Unit, Area of Residence, County and Sub County……\n\n\nV4_T2.15\nV4\nTable 2.15\nPercentage Distribution of Conventional Households by Main Source of Drinking Water, Area of Residence, County and Sub-County..\n\n\nV4_T2.16\nV4\nTable 2.16\nPercentage Distribution of Conventional Households by Main Mode of Human Waste Disposal, Area of Residence, County and Sub-County..\n\n\nV4_T2.17\nV4\nTable 2.17\nPercentage Distribution of Conventional Households by Main Mode of Solid Waste Disposal, Area of Residence, County and Sub-County..\n\n\nV4_T2.18\nV4\nTable 2.18\nPercentage Distribution of Conventional Households by Main Type of Cooking Fuel, Area of Residence, County and Sub-County….\n\n\nV4_T2.19\nV4\nTable 2.19\nPercentage Distribution of Conventional Households by Main Type of Lighting Fuel, Area of Residence, County and Sub-County….\n\n\nV4_T2.20\nV4\nTable 2.20\nDistribution of households practicing Agriculture, Fishing and Irrigation by County and Sub County.\n\n\nV4_T2.21\nV4\nTable 2.21\nDistribution of Households Growing Permanent Crops by Type and County.\n\n\nV4_T2.22\nV4\nTable 2.22\nDistribution of Households Growing Other Crops by Type, County and Sub County\n\n\nV4_T2.23\nV4\nTable 2.23\nDistribution of Households Rearing Livestock and Fish by County and Sub County.\n\n\nV4_T2.24\nV4\nTable 2.24\nDistribution of Livestock population by type, Fish Ponds and Fish Cages by County and Sub County..\n\n\nV4_T2.25\nV4\nTable 2.25\nDistribution of area (hectares) of Agricultural land and Farming Households by purpose of production, County and Sub-County..\n\n\nV4_T2.26\nV4\nTable 2.26\nDistribution of Population aged 5 years and above by Disability Status, Sex1, Area of Residence, County and Sub-County….\n\n\nV4_T2.27\nV4\nTable 2.27\nDistribution of Persons with Disability by Type of Disability, Sex1, Area of Residence, County and Sub County…\n\n\nV4_T2.28\nV4\nTable 2.28\nDistribution of Persons with Albinism by Sex1, Area of Residence, County and Sub County..\n\n\nV4_T2.29\nV4\nTable 2.29\nPopulation of Street Persons/Outdoor Sleepers by Sex1, Area of Residence and County.\n\n\nV4_T2.30\nV4\nTable 2.30\nDistribution of Population by Religious Affiliation and County\n\n\nV4_T2.31\nV4\nTable 2.31\nDistribution of Population by Ethnicity/Nationality\n\n\nV4_T2.32\nV4\nTable 2.32\nDistribution of Population Age 3 years and Above Owning a Mobile Phone by Area of Residence, Sex, County and Sub County\n\n\nV4_T2.33\nV4\nTable 2.33\nDistribution of Population Age 3 Years and Above Using Internet and Computer/Laptop/Tablet by Area of Residence, Sex, County and Sub-County…\n\n\nV4_T2.34\nV4\nTable 2.34\nDistribution of Population age 15 years and above who Searched and Bought Goods and Services Online by Area of Residence, Sex, County and Sub-County.\n\n\nV4_T2.35\nV4\nTable 2.35\nDistribution of Population Age 3 years and Above who owned and used Selected ICT Equipment and Service by Age, Area of Residence and County..\n\n\nV4_T2.36\nV4\nTable 2.36\nPercentage Distribution of Conventional Households by Ownership of Selected Household Assets by Area of Residence, County and Sub County…\n\n\nV4_T2.37\nV4\nTable 2.37\nBirths in the Last 12 months* by place of Occurrence and County…\n\n\nV4_T2.38\nV4\nTable 2.38\nBirths in the Last 5 Years* by place of Occurrence and County\n\n\nV4_T2.39\nV4\nTable 2.39\nNotified Births in the Last 12 months by County\n\n\nV4_T2.40\nV4\nTable 2.40\nNotified Births in the Last 5 Years by County\n\n\n\n\nCleaning Script\nNo cleaning script today, enjoy the data!\nThis is how I got the three datasets for today.\nlibrary(rKenyaCensus)\nlibrary(tidyverse)\n\n# create a markdown table for the readme\nrKenyaCensus::DataCatalogue %&gt;% \n  knitr::kable()\n\n# grab 3 tables of interest\ncrops &lt;- rKenyaCensus::V4_T2.21\ngender &lt;- rKenyaCensus::V1_T2.2\nhouseholds &lt;- rKenyaCensus::V1_T2.3\n\n# write them out\nhouseholds %&gt;% \n  write_csv(\"2021/2021-01-19/households.csv\")\n\ngender %&gt;% \n  write_csv(\"2021/2021-01-19/gender.csv\")\n\ncrops %&gt;% \n  write_csv(\"2021/2021-01-19/crops.csv\")"
  },
  {
    "objectID": "data/2021/2021-01-05/readme.html",
    "href": "data/2021/2021-01-05/readme.html",
    "title": "Transit Costs Project",
    "section": "",
    "text": "Transit Costs Project\nThe data this week comes from Transit Costs Project.\n\nWhy do transit-infrastructure projects in New York cost 20 times more on a per kilometer basis than in Seoul? We investigate this question across hundreds of transit projects from around the world. We have created a database that spans more than 50 countries and totals more than 11,000 km of urban rail built since the late 1990s. We will also examine this question in greater detail by carrying out six in-depth case studies that take a closer look at unique considerations and variables that aren’t easily quantified, like project management, governance, and site conditions.\nThe goal of this work is to figure out how to deliver more high-capacity transit projects for a fraction of the cost in countries like the United States. Additionally, we hope that our site will be a useful resource for elected officials, planners, researchers, journalists, advocates, and others interested in contextualizing transit-infrastructure costs and fighting for better projects.\n\nThe first completed Case Study can be found on Boston’s Green Line, although there is data from around the world!\nThe raw data is available as a Google Sheet, although I’ve downloaded and provided it as a .csv.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-01-05')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 2)\n\ntransit_cost &lt;- tuesdata$transit_cost\n\n# Or read in the data manually\n\ntransit_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-01-05/transit_cost.csv')\n\n\nData Dictionary\n\n\n\ntransit_cost.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ne\ndouble\nID\n\n\ncountry\ncharacter\nCountry Code - can be joined against countrycode via ecb or iso2c\n\n\ncity\ncharacter\nCity where transit tunnel is being created\n\n\nline\ncharacter\nLine name or path\n\n\nstart_year\ncharacter\nYear started\n\n\nend_year\ncharacter\nYear ended (predicted or actual)\n\n\nrr\ndouble\nI think this is Railroad (0 or 1), where 1 == Railroad?\n\n\nlength\ndouble\nLength of proposed line in km\n\n\ntunnel_per\ncharacter\nPercent of line length completed\n\n\ntunnel\ndouble\nTunnel length of line completed in km (can take this divided by length to get tunnel_per)\n\n\nstations\ndouble\nNumber of stations where passengers can board/leave\n\n\nsource1\ncharacter\nWhere was data sourced\n\n\ncost\ndouble\nCost in millions of local currency\n\n\ncurrency\ncharacter\nCurrency type\n\n\nyear\ndouble\nMidpoint year of construction\n\n\nppp_rate\ndouble\npurchasing power parity (PPP), based on the midpoint of construction\n\n\nreal_cost\ncharacter\nReal cost in Millions of USD\n\n\ncost_km_millions\ndouble\nCost/km in millions of USD\n\n\nsource2\ncharacter\nWhere was data sourced for cost\n\n\nreference\ncharacter\nReference URL for source\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nraw_df &lt;- read_csv(\"2021/2021-01-05/Merged Costs (1.0) - Sheet1.csv\") %&gt;% \n  janitor::clean_names() %&gt;% \n  filter(real_cost != \"MAX\")\n\nraw_df %&gt;% \n  arrange(desc(cost_km_millions))\n\nraw_df %&gt;% \n  write_csv(\"2021/2021-01-05/transit_cost.csv\")"
  },
  {
    "objectID": "data/2020/readme.html",
    "href": "data/2020/readme.html",
    "title": "2020 Data",
    "section": "",
    "text": "2020 Data\nArchive of datasets and articles from the 2020 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2019-12-31\nBring your own data from 2019!\nNA\nNA\n\n\n2\n2020-01-07\nAustralian Fires\nBureau of Meteorology\nNY Times & BBC\n\n\n3\n2020-01-14\nPasswords\nKnowledge is Beautiful\nInformation is Beautiful\n\n\n4\n2020-01-21\nSong Genres\nspotifyr\nKaylin Pavlik\n\n\n5\n2020-01-28\nSan Francisco Trees\ndata.sfgov.org\nSF Weekly\n\n\n6\n2020-02-04\nNFL Attendance\nPro Football Reference\nCasino.org\n\n\n7\n2020-02-11\nHotel Bookings\nAntonio, Almeida, and Nunes, 2019\ntidyverts\n\n\n8\n2020-02-18\nFood’s Carbon Footprint\nnu3\nr-tastic by Kasia Kulma\n\n\n9\n2020-02-25\nMeasles Vaccination\nThe Wallstreet Journal\nThe Wall Street Journal\n\n\n10\n2020-03-03\nNHL Goals\nHockeyReference.com\nWashington Post\n\n\n11\n2020-03-10\nCollege Tuition, Diversity, and Pay\nTuitionTracker.org\nTuitionTracker.org\n\n\n12\n2020-03-17\nThe Office\nschrute\nThe Pudding\n\n\n13\n2020-03-24\nTraumatic Brain Injury\nCDC\nCDC Traumatic Brain Injury Report\n\n\n14\n2020-03-31\nBeer Production\nTTB\nBrewers Association\n\n\n15\n2020-04-07\nTour de France\ntdf package\nAlastair Rushworth’s blog\n\n\n16\n2020-04-14\nBest Rap Artists\nBBC Music\nSimon Jockers at Datawrapper\n\n\n17\n2020-04-21\nGDPR Violations\nPrivacy Affairs\nRoel Hogervorst\n\n\n18\n2020-04-28\nBroadway Musicals\nPlaybill\nAlex Cookson\n\n\n19\n2020-05-05\nAnimal Crossing\nVillager DB\nPolygon\n\n\n20\n2020-05-12\nVolcano Eruptions\nSmithsonian\nAxios & Wikipedia\n\n\n21\n2020-05-19\nBeach Volleyball\nBigTimeStats\nFiveThirtyEight & Wikipedia\n\n\n22\n2020-05-26\nCocktails\nKaggle & Kaggle\nFiveThirtyEight\n\n\n23\n2020-06-02\nMarble Races\nJelle’s Marble Runs\nRandy Olson\n\n\n24\n2020-06-09\nAfrican-American Achievements\nWikipedia & Wikipedia\nDavid Blackwell & Petition for David Blackwell\n\n\n25\n2020-06-16\nAfrican-American History\nBlack Past & Census & Slave Voyages\nThe Guardian\n\n\n26\n2020-06-23\nCaribou Locations\nMovebank\nB.C. Ministry of Environment\n\n\n27\n2020-06-30\nClaremont Run of X-Men\nClaremont Run\nWikipedia - Uncanny X-Men\n\n\n28\n2020-07-07\nCoffee Ratings\nJames LeDoux & Coffee Quality Database\nYorgos Askalidis - TWD\n\n\n29\n2020-07-14\nAstronaut Database\nCorlett, Stavnichuk & Komarova article\nCorlett, Stavnichuk & Komarova article\n\n\n30\n2020-07-21\nAustralian Animal Outcomes\nRSPCA\nRSPCA Report\n\n\n31\n2020-07-28\nPalmer Penguins\nGorman, Williams and Fraser, 2014\nPalmer Penguins\n\n\n32\n2020-08-04\nEuropean Energy\nEurostat Energy\nWashington Post Energy\n\n\n33\n2020-08-11\nAvatar: The Last Airbender\nappa\nExploring Avatar: The Last Airbender transcript data\n\n\n34\n2020-08-18\nExtinct Plants\nIUCN Red List\nFlorent Lavergne infographic\n\n\n35\n2020-08-25\nChopped\nKaggle & IMDB\nVice\n\n\n36\n2020-09-01\nGlobal Crop Yields\nOur World in Data\nOur World in Data\n\n\n37\n2020-09-08\nFriends\nfriends R package\nceros interactive article\n\n\n38\n2020-09-15\nGov Spending on Kids\nUrban Institute\nJoshua Rosenberg’s tidykids package\n\n\n39\n2020-09-22\nHimalayan Climbers\nThe Himalayan Database\nAlex Cookson blog post\n\n\n40\n2020-09-29\nBeyonce & Taylor Swift Lyrics\nRosie Baillie and Dr. Sara Stoudt\nTaylor Swift lyrics\n\n\n41\n2020-10-06\nNCAA Women’s Basketball\nFiveThirtyEight\nFiveThirtyEight\n\n\n42\n2020-10-13\ndatasauRus dozen\nAlberto Cairo\ndatasauRus R package\n\n\n43\n2020-10-20\nGreat American Beer Festival Data\nGreat American Beer Festival\n2019 GABF Medal Winner Analysis\n\n\n44\n2020-10-27\nCanadian Wind Turbines\nopen.canada.ca\nCanada’s National Observer\n\n\n45\n2020-11-03\nIkea Furniture\nKaggle\nFiveThirtyEight\n\n\n46\n2020-11-10\nHistorical Phones\nMobile vs Landline subscriptions\nPew Research Smartphone Adoption\n\n\n47\n2020-11-17\nBlack in Data\nBlack in Data Week\nBlackInData #DataViz\n\n\n48\n2020-11-24\nWashington Trails\nWTA\nTidyX\n\n\n49\n2020-12-01\nToronto Shelters\nopendatatoronto\nrabble.ca\n\n\n50\n2020-12-08\nWomen of 2020\nBBC\nBBC\n\n\n51\n2020-12-15\nNinja Warrior\nData.World\nsasukepedia\n\n\n52\n2020-12-22\nBig Mac Index\nTheEconomist\nTheEconomist",
    "crumbs": [
      "Datasets",
      "2020"
    ]
  },
  {
    "objectID": "data/2020/2020-12-15/readme.html",
    "href": "data/2020/2020-12-15/readme.html",
    "title": "Ninja Warrior",
    "section": "",
    "text": "Ninja Warrior\nThe data this week comes from Data.World and originally from sasukepedia. Ninja Warrior and the continuation as American Ninja Warrior:\n\nis an American sports entertainment competition based on the Japanese television series Sasuke. It features hundreds of competitors attempting to complete series of obstacle courses of increasing difficulty in various cities across the United States, in hopes of advancing to the national finals on the Las Vegas Strip and becoming the season’s “American Ninja Warrior.”\n\nA description of each obstacle and pictures of them can be found at: sasukepedia.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-12-15')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 51)\n\nninja_warrior &lt;- tuesdata$ninja_warrior\n\n# Or read in the data manually\n\nninja_warrior &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-12-15/ninja_warrior.csv')\n\n\nData Dictionary\n\n\n\nninja_warrior.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ndouble\nSeason Number\n\n\nlocation\ncharacter\nSeason location\n\n\nround_stage\ncharacter\nRound stage\n\n\nobstacle_name\ncharacter\nName of obstacle\n\n\nobstacle_order\ndouble\nObstacle Order (number)\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(\"httr\")\nlibrary(\"readxl\")\n\nGET(\"https://query.data.world/s/satoodylrno77fbjmxxenx7nrehbkd\", write_disk(tf &lt;- tempfile(fileext = \".xlsx\")))\ndf &lt;- read_excel(tf)\n\nout_df &lt;- df %&gt;% \n  janitor::clean_names()\n\nwrite_csv(out_df, \"2020/2020-12-15/ninja_warrior.csv\")"
  },
  {
    "objectID": "data/2020/2020-12-01/readme.html",
    "href": "data/2020/2020-12-01/readme.html",
    "title": "Toronto Shelters",
    "section": "",
    "text": "Toronto Shelters\nThe data this week comes from Sharla Gelfand’s opendatatoronto R package. The site website has additional details and all sorts of great datasets.\nopendatatoronto package website. Quick intro/vignette can also be found here.\nData originally sourced from open.toronto.ca.\nArticle around Homeless Shelters in Toronto.\n\nIn his response to “where will homeless people go (when the respite closes)?” Ross tweeted:\n“Other respites will also work to accommodate. As respites, typically, see their highest usage in the winter months, we anticipate an easing of demand for respites with the warmer weather. Will be keeping a close watch, of course.”\n\nA list of places to donate to in Toronto:\n- Foodbanks\n\nEncampment Support Network Toronto - ESN is an ad-hoc, volunteer-run network of neighbours building community and supporting people living in encampments in 6 locations throughout Toronto. Every day since the beginning of June 2020, outreach volunteers from the network visit encampments to deliver basic supplies, such as water and tents, ice, sleeping bags, fire safety equipment, and snacks, which the City of Toronto has consistently refused to provide (despite recommendations from Ontario’s Chief Coroner to do so).\nToronto Tiny Shelters - Khaleel Seivwright is a carpenter and raising money to build durable insulated tiny shelters for homeless people across Toronto who might be living outside this winter.\n\nShelters from Reddit post\n\nCovenant House - They’re an incredible foundation that works with homeless youth. http://www.covenanthousetoronto.ca/homeless-youth/Donate\nEva’s Initiatives for Homeless Youth - Same as above. Eva’s is fantastic and runs shelters across the city. http://www.evas.ca\nRed Door Family Shelter - They have two shelters, one for women fleeing domestic violence and also one for families. https://www.reddoorshelter.ca\nNa-Me-Res - Provides temporary, transitional and permanent housing to Aboriginal men experiencing homelessness. https://www.canadahelps.org/en/charities/na-me-res-native-mens-residence/\nSojourn House - They provide short-term emergency shelter to refugees. http://www.sojournhouse.org/get-involved/donate/\nBethlehem United Shelter - The only shelter in Toronto that allows for pets to accompany their owners into the shelter. The parent organization, Fred Victor, also runs a women’s hostel. http://www.fredvictor.org/bethlehem_united_shelter\nChristie-Ossington Neighbourhood Shelter - Provides overnight shelter and supports the well-being of 68 men each night in Toronto’s West End who are homeless, street-oriented and facing mental health challenges, substance use issues, unemployment and other barriers to maintaining adequate housing. https://www.conccommunity.org/shelter-housing-lansdowne/\nHorizons for Youth - Provides emergency accommodation for up to 45 youth each night. They also have a lot of additional programs for homeless youth. https://horizonsforyouth.org\nYouth Without Shelter - Offers emergency accommodation for up to 33 youth each night and, like above, also provide additional support to homeless youth. http://www.yws.on.ca\nYMCA Housing - Like the two above, it provides support for homeless youth. https://ymcagta.org/youth-programs/youth-housing\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-12-01')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 49)\n\nshelters &lt;- tuesdata$shelters\n\n# Or read in the data manually\n\nshelters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-12-01/shelters.csv')\n\n\nData Dictionary\n\n\n\nshelters.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\n.\n\n\noccupancy_date\ncharacter\n.\n\n\norganization_name\ncharacter\n.\n\n\nshelter_name\ncharacter\n.\n\n\nshelter_address\ncharacter\n.\n\n\nshelter_city\ncharacter\n.\n\n\nshelter_province\ncharacter\n.\n\n\nshelter_postal_code\ncharacter\n.\n\n\nfacility_name\ncharacter\n.\n\n\nprogram_name\ncharacter\n.\n\n\nsector\ncharacter\n.\n\n\noccupancy\ninteger\n.\n\n\ncapacity\ninteger\n.\n\n\n\n\nCleaning Script\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(lubridate)\n\npackages &lt;- list_packages(limit = 10)\npackages$excerpt\n\nall_data &lt;- search_packages(\"Daily Shelter Occupancy\") %&gt;% \n  list_package_resources() %&gt;% \n  slice(2,4,6) %&gt;%\n  group_split(name) %&gt;% \n  map_dfr(get_resource)\n\nall_data %&gt;% \n  janitor::clean_names() %&gt;% \n  write_csv(\"2020/2020-12-01/shelters.csv\")"
  },
  {
    "objectID": "data/2020/2020-11-17/readme.html",
    "href": "data/2020/2020-11-17/readme.html",
    "title": "#BlackInDataWeek",
    "section": "",
    "text": "#BlackInDataWeek\n\nAmplifying Black folx in #informatics, #datascience, & #coding around the world\n\nFollow along via the #BlackInDataWeek hashtag.\nA week-long celebration #BlackInDataWeek to:\n\n\nhighlight the valuable work and experiences of Black people in the field of Data\n\n\n\nprovide community\n\n\n\neducational and professional resources.\n\n\nTo sign up for events, click here!\nTo learn more about #BlackInDataWeek, please see their website.\nThe #TidyTuesday community will be taking a break from our own data this week and asking you to engage with, promote, and support Black people in data.\nYou’re welcome to revisit older datasets from #TidyTuesday this week, but especially on Thursday Nov 19th, please create space and engage with the #BlackInDataViz hashtag on Twitter rather than promoting your own work.\n\nFrom the #BlackInDataViz website:\n\nCreating space for Black people in data to share their work in the form of favourite data visualisation images. Also hosting a competition for data visualisation using a shared dataset, further reinforcing community. We will also host a ‘Data Viz in a Day’ skills workshop, focusing on instructing beginners on creating data visualizations.\n\nWe don’t have our own dataset this week, but the data below returns the purpose, date, hashtag, and link to each day’s events to follow along with.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-11-17')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 47)\n\nblack_in_data &lt;- tuesdata$black_in_data\n\n# Or read in the data manually\n\nblack_in_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-11-17/black_in_data.csv')\n\n\nData Dictionary\n\n\n\nblack_in_data.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of Event\n\n\nhashtag\ncharacter\nHashtag for specific event\n\n\npurpose\ncharacter\nPurpose of event\n\n\nlink\ncharacter\nURL/Link to the event page\n\n\n\n\nCleaning Script\nblack_in_data &lt;- data.frame(\n  date = seq(as.Date(\"2020-11-16\"), as.Date(\"2020-11-21\"), 1),\n  hashtag = c(\"#BlackInDataRollCall\",\n              \"#BlackInDataJourney\",\n              \"#BlackInDataSkills\",\n              \"#BlackInDataViz\",\n              \"#BlackInDataJustice\", \n              \"#BlackInDataMentorship\"),\n  purpose = c(\n    \"Giving Black people in data a space to introduce themselves and their work. Introducing and valuing intersecting parts of their identities. We welcome contributions from a wide spectrum of Data Fields including but not limited to Informatics, Technology, Data Science, Coding, Social Science and Data Analytics.\",\n    \"Further fostering community for Black people in data, by encouraging them to share their varied journeys in data.\",\n    \"Discussing the skills Black people in data have learned, communal sharing of resources and advice for skills development.\",\n    \"Creating space for Black people in data to share their work in the form of favourite data visualisation images.\",\n    \"Hosting forums for learning and discussion of bias in the data field (and possible paths to address biases in data).\",\n    \"Join us for career development and mentorship events!\"\n  ),\n  link = c(\n    \"https://blkindata.github.io/project/blackindatarollcall/\",\n    \"https://blkindata.github.io/project/blackindatajourney/\",\n    \"https://blkindata.github.io/project/blackindataskills/\",\n    \"https://blkindata.github.io/project/blackindataviz/\",\n    \"https://blkindata.github.io/project/blackindatajustice/\",\n    \"https://blkindata.github.io/project/blackindatacommunity/\"\n  )\n)"
  },
  {
    "objectID": "data/2020/2020-11-03/readme.html",
    "href": "data/2020/2020-11-03/readme.html",
    "title": "IKEA Furniture",
    "section": "",
    "text": "IKEA logo\n\n\n\nIKEA Furniture\nThe data this week comes from Kaggle. Original source on IKEA\nArticle about Ikea from FiveThirtyEight\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-11-03')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 45)\n\nikea.csv &lt;- tuesdata$ikea.csv\n\n# Or read in the data manually\n\nikea &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-11-03/ikea.csv')\n\n\nData Dictionary\n\n\n\nikea.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nitem_id\ndouble\nitem id wich can be used later to merge with other IKEA dataframes\n\n\nname\ncharacter\nthe commercial name of items\n\n\ncategory\ncharacter\nthe furniture category that the item belongs to (Sofas, beds, chairs, Trolleys,…)\n\n\nprice\ndouble\nthe current price in Saudi Riyals as it is shown in the website by 4/20/2020\n\n\nold_price\ncharacter\nthe price of item in Saudi Riyals before discount\n\n\nsellable_online\nlogical\nSellable online TRUE or FALSE\n\n\nlink\ncharacter\nthe web link of the item\n\n\nother_colors\ncharacter\nif other colors are available for the item, or just one color as displayed in the website (Boolean)\n\n\nshort_description\ncharacter\na brief description of the item\n\n\ndesigner\ncharacter\nThe name of the designer who designed the item. this is extracted from the full_description column.\n\n\ndepth\ndouble\nDepth of the item in Centimeter\n\n\nheight\ndouble\nHeight of the item in Centimeter\n\n\nwidth\ndouble\nWidth of the item in Centimeter\n\n\n\n\nCleaning Script\nNo cleaning today!"
  },
  {
    "objectID": "data/2020/2020-10-20/readme.html",
    "href": "data/2020/2020-10-20/readme.html",
    "title": "Beer Awards",
    "section": "",
    "text": "GABF Logo\n\n\n\nBeer Awards\nThe data this week comes from Great American Beer Festival.\n\nThe Professional Judge Panel awards gold, silver or bronze medals that are recognized around the world as symbols of brewing excellence. These awards are among the most coveted in the industry and heralded by the winning brewers in their national advertising.\nFive different three-hour judging sessions take place over the three-day period during the week of the festival. Judges are assigned beers to evaluate in their specific area of expertise and never judge their own product or any product in which they have a concern.\n\nBest States for Beer\n2018 GABF Medal Winner Analysis 2019 GABF Medal Winner Analysis\nNOTE - There are a few missing data points unfortunately\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-10-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 43)\n\nbeer_awards &lt;- tuesdata$beer_awards\n\n# Or read in the data manually\n\nbeer_awards &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-10-20/beer_awards.csv')\n\n\nData Dictionary\n\n\n\nbeer_awards.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmedal\ncharacter\nMedal awarded (Gold, Silver, Bronze)\n\n\nbeer_name\ncharacter\nBeer Name\n\n\nbrewery\ncharacter\nBrewery Name\n\n\ncity\ncharacter\nCity of Brewery\n\n\nstate\ncharacter\nState of Brewery\n\n\ncategory\ncharacter\nCategory for beer award\n\n\nyear\ncharacter\nYear of Competition\n\n\n\n\nCleaning Script\nNo cleaning script listed today."
  },
  {
    "objectID": "data/2020/2020-10-06/readme.html",
    "href": "data/2020/2020-10-06/readme.html",
    "title": "NCAA Women’s Basketball Tournament",
    "section": "",
    "text": "Louisiana Tech was a women’s basketball powerhouse in the 1980s and ’90s. The Techsters lost the 1998 title game to Tennessee. GARY CASKEY / REUTERS\n\n\n\nNCAA Women’s Basketball Tournament\nThe data this week comes from FiveThirtyEight. The original raw data is on their [GitHub]. More details about the NCAA Women’s Basketball Tournament which expended to 64 teams in 1994. There are some additional data points at that Wikipedia link if you’re curious!\nNote that for their dataviz, they converted seed to a 100 point scale based off the average wins/seed. Note that FiveThirtyEight used Simple Rating System scores from Sports-Reference, but I’ve simplified it into simply the average per seed.\nA quick table of this as seen below:\n\n\nTo measure this, we awarded “seed points” in proportion to a given seed number’s expected wins in the tournament, calibrated to a 100-point scale where the No. 1 seed gets 100 points, No. 2 gets 70 points, and so forth.\n\nThis aligns to (based off the averages):\n\n\n\nSeed\nPoints\n\n\n\n\n1st\n100\n\n\n2nd\n72.7\n\n\n3rd\n54.5\n\n\n4th\n48.5\n\n\n5th\n33.3\n\n\n6th\n33.3\n\n\n7th\n27.3\n\n\n8th\n21.2\n\n\n9th\n18.2\n\n\n10th\n18.2\n\n\n11th\n18.2\n\n\n12th\n15.2\n\n\n13th\n9.09\n\n\n14th\n6.06\n\n\n15th\n3.03\n\n\n16th\n0\n\n\n\nTheir modeled fit:\n\nYou could see the quick plot of these points, but again note that this will vary a bit from the FiveThirtyEight table as they included SRS score in their equation.\ntibble(\n  seed = c(1:16),\n  exp_wins = c(3.3, 2.4, 1.8, 1.6, 1.1, 1.1, 0.9, 0.7, 0.6, 0.6, 0.6, 0.5, 0.3, 0.2, 0.1, 0)\n  \n) %&gt;% \n  mutate(\n    points = exp_wins/3.3 * 100\n  ) %&gt;% \n  ggplot(aes(x = seed, y = points)) +\n  geom_point() +\n  geom_smooth() +\n  geom_hline(yintercept = 0, color = \"black\") +\n  scale_y_continuous(breaks = seq(0, 100, by = 20)) +\n  coord_cartesian(ylim = c(0, 100)) +\n  scale_x_continuous(breaks = seq(1, 16, by = 3)) +\n  theme_minimal() +\n  labs(\n    x = \"Tournament Seed\", y = \"Seed Points\",\n    title = \"How much is that seed worth?\"\n  )\n\nThus, to get the points for each team/tournament season you can multiply the teams initial seed (1 - 16) by the assigned points as defined in the above table of 100 - 0.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-10-06')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 41)\n\ntournament &lt;- tuesdata$tournament\n\n# Or read in the data manually\n\ntournament &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-10-06/tournament.csv')\n\n\nData Dictionary\n\n\n\nSeed Point Value as defined in a separate FiveThirtyEight Article\n\n\n\nSeed\nPoints\n\n\n\n\n1st\n100\n\n\n2nd\n72.7\n\n\n3rd\n54.5\n\n\n4th\n48.5\n\n\n5th\n33.3\n\n\n6th\n33.3\n\n\n7th\n27.3\n\n\n8th\n21.2\n\n\n9th\n18.2\n\n\n10th\n18.2\n\n\n11th\n18.2\n\n\n12th\n15.2\n\n\n13th\n9.09\n\n\n14th\n6.06\n\n\n15th\n3.03\n\n\n16th\n0\n\n\n\n\n\ntournament.csv\nTo get the points for each team/tournament season you can multiply the teams initial seed (1 - 16) by the assigned points as defined in the above table of 100 - 0.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nTournament year\n\n\nschool\ncharacter\nSchool name\n\n\nseed\ndouble\nSeed rank\n\n\nconference\ncharacter\nConference name\n\n\nconf_w\ndouble\nConference wins\n\n\nconf_l\ndouble\nConference losses\n\n\nconf_percent\ndouble\nConference win/loss percent\n\n\nconf_place\ncharacter\nConference placement (ie, 1st, 2nd, etc)\n\n\nreg_w\ndouble\nRegular season wins\n\n\nreg_l\ndouble\nRegular season losses\n\n\nreg_percent\ndouble\nRegular season win/loss percent\n\n\nhow_qual\ncharacter\nHow qualified - Whether the school qualified with an automatic bid (by winning its conference or conference tournament) or an at-large bid\n\n\nx1st_game_at_home\ncharacter\nWhether the school played its first-round tournament games on its home court.\n\n\ntourney_w\ndouble\nTournament wins\n\n\ntourney_l\ndouble\nTournament games losses\n\n\ntourney_finish\ncharacter\nTournament finish - The round of the final game for each team. OR=opening-round loss (1983 only); 1st=first-round loss; 2nd=second-round loss; RSF=loss in the Sweet 16; RF=loss in the Elite Eight; NSF=loss in the national semifinals; N2nd=national runner-up; Champ=national champions\n\n\nfull_w\ndouble\nTotal sum of wins\n\n\nfull_l\ndouble\nTotal sum of losses\n\n\nfull_percent\ndouble\nTotal sum win/loss percent\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nraw_df &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/ncaa-womens-basketball-tournament/ncaa-womens-basketball-tournament-history.csv\")\n\nclean_tourn &lt;- raw_df %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(across(c(seed, conf_w:conf_percent, full_percent), parse_number))\n\nclean_tourn %&gt;% \n  write_csv(\"2020/2020-10-06/tournament.csv\")"
  },
  {
    "objectID": "data/2020/2020-09-22/readme.html",
    "href": "data/2020/2020-09-22/readme.html",
    "title": "Himalayan Climbing Expeditions",
    "section": "",
    "text": "Picture of the Himalayas from Wikipedia\n\n\n\nHimalayan Climbing Expeditions\nThe data this week comes from The Himalayan Database.\n\nThe Himalayan Database is a compilation of records for all expeditions that have climbed in the Nepal Himalaya. The database is based on the expedition archives of Elizabeth Hawley, a longtime journalist based in Kathmandu, and it is supplemented by information gathered from books, alpine journals and correspondence with Himalayan climbers.\nThe data cover all expeditions from 1905 through Spring 2019 to more than 465 significant peaks in Nepal. Also included are expeditions to both sides of border peaks such as Everest, Cho Oyu, Makalu and Kangchenjunga as well as to some smaller border peaks. Data on expeditions to trekking peaks are included for early attempts, first ascents and major accidents.\n\nh/t to Alex Cookson for sharing and cleaning this data!\nThis blog post by Alex Cookson explores the data in greater detail.\nI don’t want to underplay that there are some positives and some awful negatives for native Sherpa climbers. One-third of Everest deaths are Sherpa Climbers.\nAlso National Geographic has 5 Ways to help the Sherpas of Everest.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-09-22')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 39)\n\nclimbers &lt;- tuesdata$climbers\n\n# Or read in the data manually\n\nmembers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-22/members.csv')\nexpeditions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-22/expeditions.csv')\npeaks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-22/peaks.csv')\n\n\n\nData Dictionary\n\npeaks.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npeak_id\ncharacter\nUnique identifier for peak\n\n\npeak_name\ncharacter\nCommon name of peak\n\n\npeak_alternative_name\ncharacter\nAlternative name of peak (for example, the “Mount Everest” is “Sagarmatha” in Nepalese)\n\n\nheight_metres\ndouble\nHeight of peak in metres\n\n\nclimbing_status\ncharacter\nWhether the peak has been climbed\n\n\nfirst_ascent_year\ndouble\nYear of first successful ascent, if applicable\n\n\nfirst_ascent_country\ncharacter\nCountry name(s) of expedition members part of the first ascent. Can have multiple values if members were from different countries. Country name is as of date of ascent (for example, “W Germany” for ascents before 1990).\n\n\nfirst_ascent_expedition_id\ncharacter\nUnique identifier for expedition. Can be linked to expeditions or members tables.\n\n\n\n\n\nexpeditions.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nexpedition_id\ncharacter\nUnique identifier for expedition. Can be linked to peaks or members tables.\n\n\npeak_id\ncharacter\nUnique identifier for peak. Can be linked to peaks table.\n\n\npeak_name\ncharacter\nCommon name for peak\n\n\nyear\ndouble\nYear of expedition\n\n\nseason\ncharacter\nSeason of expedition (Spring, Summer, etc.)\n\n\nbasecamp_date\ndate\nDate of expedition arrival at basecamp\n\n\nhighpoint_date\ndate\nDate of expedition summiting the peak for the first time or, if peak wasn’t reached, date of reaching its highpoint\n\n\ntermination_date\ndate\nDate the expedition was terminated\n\n\ntermination_reason\ncharacter\nPrimary reason the expedition was terminated. There are two possibilities for a successful expeditions, depending on whether the main peak or a sub-peak was summitted.\n\n\nhighpoint_metres\ndouble\nElevation highpoint of the expedition\n\n\nmembers\ndouble\nNumber of expedition members. For expeditions in Nepal, this is usually the number of foreigners listed on the expedition permit. For expeditions in China, this is usually the number of non-hired members.\n\n\nmember_deaths\ndouble\nNumber of expeditions members who died\n\n\nhired_staff\ndouble\nNumber of hired staff who went above basecamp\n\n\nhired_staff_deaths\ndouble\nNumber of hired staff who died\n\n\noxygen_used\nlogical\nWhether oxygen was used by at least one member of the expedition\n\n\ntrekking_agency\ncharacter\nName of the trekking agency\n\n\n\n\n\nmembers.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nexpedition_id\ncharacter\nUnique identifier for expedition. Can be linked to peaks or members tables.\n\n\nmember_id\ncharacter\nUnique identifier for the person. This is not consistent across expeditions, so you cannot use a single member_id to look up all expeditions a person was part of.\n\n\npeak_id\ncharacter\nUnique identifier for peak. Can be linked to peaks table.\n\n\npeak_name\ncharacter\nCommon name for peak\n\n\nyear\ndouble\nYear of expedition\n\n\nseason\ncharacter\nSeason of expedition (Spring, Summer, etc.)\n\n\nsex\ncharacter\nSex of the person\n\n\nage\ndouble\nAge of the person. Depending on the best available data, this could be as of the summit date, the date of death, or the date of arrival at basecamp.\n\n\ncitizenship\ncharacter\nCitizenship of the person\n\n\nexpedition_role\ncharacter\nRole of the person on the expedition\n\n\nhired\nlogical\nWhether the person was hired by the expedition\n\n\nhighpoint_metres\ndouble\nElevation highpoint of the person\n\n\nsuccess\nlogical\nWhether the person was successful in summitting a main peak or sub-peak, depending on the goal of expedition\n\n\nsolo\nlogical\nWhether the person attempted a solo ascent\n\n\noxygen_used\nlogical\nWhether the person used oxygen\n\n\ndied\nlogical\nWhether the person died\n\n\ndeath_cause\ncharacter\nPrimary cause of death\n\n\ndeath_height_metres\ndouble\nHeight at which the person died\n\n\ninjured\nlogical\nWhether the person was injured\n\n\ninjury_type\ncharacter\nPrimary cause of injury\n\n\ninjury_height_metres\ndouble\nHeight at which the injury occurred\n\n\n\n\n\nCleaning Script\n# Libraries\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n# Peaks\npeaks &lt;- read_csv(\"./himalayan-expeditions/raw/peaks.csv\") %&gt;%\n  transmute(\n    peak_id = PEAKID,\n    peak_name = PKNAME,\n    peak_alternative_name = PKNAME2,\n    height_metres = HEIGHTM,\n    climbing_status = PSTATUS,\n    first_ascent_year = PYEAR,\n    first_ascent_country = PCOUNTRY,\n    first_ascent_expedition_id = PEXPID\n  ) %&gt;%\n  mutate(\n    climbing_status = case_when(\n      climbing_status == 0 ~ \"Unknown\",\n      climbing_status == 1 ~ \"Unclimbed\",\n      climbing_status == 2 ~ \"Climbed\"\n    )\n  )\n\n# Create small dataframe of peak names to join to other dataframes\npeak_names &lt;- peaks %&gt;%\n  select(peak_id, peak_name)\n\n# Expeditions\nexpeditions &lt;- read_csv(\"./himalayan-expeditions/raw/exped.csv\") %&gt;%\n  left_join(peak_names, by = c(\"PEAKID\" = \"peak_id\")) %&gt;%\n  transmute(\n    expedition_id = EXPID,\n    peak_id = PEAKID,\n    peak_name,\n    year = YEAR,\n    season = SEASON,\n    basecamp_date = BCDATE,\n    highpoint_date = SMTDATE,\n    termination_date = TERMDATE,\n    termination_reason = TERMREASON,\n    # Highpoint of 0 is most likely missing value\n    highpoint_metres = ifelse(HIGHPOINT == 0, NA, HIGHPOINT),\n    members = TOTMEMBERS,\n    member_deaths = MDEATHS,\n    hired_staff = TOTHIRED,\n    hired_staff_deaths = HDEATHS,\n    oxygen_used = O2USED,\n    trekking_agency = AGENCY\n  ) %&gt;%\n  mutate(\n    termination_reason = case_when(\n      termination_reason == 0 ~ \"Unknown\",\n      termination_reason == 1 ~ \"Success (main peak)\",\n      termination_reason == 2 ~ \"Success (subpeak)\",\n      termination_reason == 3 ~ \"Success (claimed)\",\n      termination_reason == 4 ~ \"Bad weather (storms, high winds)\",\n      termination_reason == 5 ~ \"Bad conditions (deep snow, avalanching, falling ice, or rock)\",\n      termination_reason == 6 ~ \"Accident (death or serious injury)\",\n      termination_reason == 7 ~ \"Illness, AMS, exhaustion, or frostbite\",\n      termination_reason == 8 ~ \"Lack (or loss) of supplies or equipment\",\n      termination_reason == 9 ~ \"Lack of time\",\n      termination_reason == 10 ~ \"Route technically too difficult, lack of experience, strength, or motivation\",\n      termination_reason == 11 ~ \"Did not reach base camp\",\n      termination_reason == 12 ~ \"Did not attempt climb\",\n      termination_reason == 13 ~ \"Attempt rumoured\",\n      termination_reason == 14 ~ \"Other\"\n    ),\n    season = case_when(\n      season == 0 ~ \"Unknown\",\n      season == 1 ~ \"Spring\",\n      season == 2 ~ \"Summer\",\n      season == 3 ~ \"Autumn\",\n      season == 4 ~ \"Winter\"\n    )\n  )\n\nmembers &lt;-\n  read_csv(\"./himalayan-expeditions/raw/members.csv\", guess_max = 100000) %&gt;%\n  left_join(peak_names, by = c(\"PEAKID\" = \"peak_id\")) %&gt;%\n  transmute(\n    expedition_id = EXPID,\n    member_id = paste(EXPID, MEMBID, sep = \"-\"),\n    peak_id = PEAKID,\n    peak_name,\n    year = MYEAR,\n    season = MSEASON,\n    sex = SEX,\n    age = CALCAGE,\n    citizenship = CITIZEN,\n    expedition_role = STATUS,\n    hired = HIRED,\n    # Highpoint of 0 is most likely missing value\n    highpoint_metres = ifelse(MPERHIGHPT == 0, NA, MPERHIGHPT),\n    success = MSUCCESS,\n    solo = MSOLO,\n    oxygen_used = MO2USED,\n    died = DEATH,\n    death_cause = DEATHTYPE,\n    # Height of 0 is most likely missing value\n    death_height_metres = ifelse(DEATHHGTM == 0, NA, DEATHHGTM),\n    injured = INJURY,\n    injury_type = INJURYTYPE,\n    # Height of 0 is most likely missing value\n    injury_height_metres = ifelse(INJURYHGTM == 0, NA, INJURYHGTM)\n  ) %&gt;%\n  mutate(\n    season = case_when(\n      season == 0 ~ \"Unknown\",\n      season == 1 ~ \"Spring\",\n      season == 2 ~ \"Summer\",\n      season == 3 ~ \"Autumn\",\n      season == 4 ~ \"Winter\"\n    ),\n    age = ifelse(age == 0, NA, age),\n    death_cause = case_when(\n      death_cause == 0 ~ \"Unspecified\",\n      death_cause == 1 ~ \"AMS\",\n      death_cause == 2 ~ \"Exhaustion\",\n      death_cause == 3 ~ \"Exposure / frostbite\",\n      death_cause == 4 ~ \"Fall\",\n      death_cause == 5 ~ \"Crevasse\",\n      death_cause == 6 ~ \"Icefall collapse\",\n      death_cause == 7 ~ \"Avalanche\",\n      death_cause == 8 ~ \"Falling rock / ice\",\n      death_cause == 9 ~ \"Disappearance (unexplained)\",\n      death_cause == 10 ~ \"Illness (non-AMS)\",\n      death_cause == 11 ~ \"Other\",\n      death_cause == 12 ~ \"Unknown\"\n    ),\n    injury_type = case_when(\n      injury_type == 0 ~ \"Unspecified\",\n      injury_type == 1 ~ \"AMS\",\n      injury_type == 2 ~ \"Exhaustion\",\n      injury_type == 3 ~ \"Exposure / frostbite\",\n      injury_type == 4 ~ \"Fall\",\n      injury_type == 5 ~ \"Crevasse\",\n      injury_type == 6 ~ \"Icefall collapse\",\n      injury_type == 7 ~ \"Avalanche\",\n      injury_type == 8 ~ \"Falling rock / ice\",\n      injury_type == 9 ~ \"Disappearance (unexplained)\",\n      injury_type == 10 ~ \"Illness (non-AMS)\",\n      injury_type == 11 ~ \"Other\",\n      injury_type == 12 ~ \"Unknown\"\n    ),\n    death_cause = ifelse(died, death_cause, NA_character_),\n    death_height_metres = ifelse(died, death_height_metres, NA),\n    injury_type = ifelse(injured, injury_type, NA_character_),\n    injury_height_metres = ifelse(injured, injury_height_metres, NA)\n  )\n\n\n### Write to CSV\nwrite_csv(expeditions, \"./himalayan-expeditions/expeditions.csv\")\nwrite_csv(members, \"./himalayan-expeditions/members.csv\")\nwrite_csv(peaks, \"./himalayan-expeditions/peaks.csv\")"
  },
  {
    "objectID": "data/2020/2020-09-08/readme.html",
    "href": "data/2020/2020-09-08/readme.html",
    "title": "Friends",
    "section": "",
    "text": "Friends logo - credit to TurboLogo\n\n\n\nFriends\nThe data this week comes from the friends R package for the Friends transcripts and additional information.\nh/t to Emil Hvitfeldt for aggregating, packaging and sharing this data with us!\nFriends Wikipedia:\n\nFriends is an American television sitcom, created by David Crane and Marta Kauffman, which aired on NBC from September 22, 1994, to May 6, 2004, lasting ten seasons. With an ensemble cast starring Jennifer Aniston, Courteney Cox, Lisa Kudrow, Matt LeBlanc, Matthew Perry and David Schwimmer, the show revolves around six friends in their 20s and 30s who live in Manhattan, New York City. The series was produced by Bright/Kauffman/Crane Productions, in association with Warner Bros. Television. The original executive producers were Kevin S. Bright, Kauffman, and Crane.\n\nThe friends package can be installed from CRAN with install.packages(\"friends\").\nThis ceros interactive article looks at which characters appear together.\nThere’s text, appearance, ratings, and many other datasets here - if you’re trying out Text Analysis, check out the tidytext mining book/package or the newly released Supervised Machine Learning for Text Analysis in R book, both which are freely available online at the respective links.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-09-08')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 37)\n\nfriends &lt;- tuesdata$friends\n\n# Or read in the data manually\n\nfriends &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-08/friends.csv')\nfriends_emotions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-08/friends_emotions.csv')\nfriends_info &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-08/friends_info.csv')\n\n\nData Dictionary\n\n\n\nfriends.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntext\ncharacter\nDialogue as text\n\n\nspeaker\ncharacter\nName of the speaker\n\n\nseason\ndouble\nSeason Number\n\n\nepisode\ndouble\nEpisode Number\n\n\nscene\ndouble\nScene Number\n\n\nutterance\ndouble\nUtterance Number\n\n\n\n\n\nfriends_emotions.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ninteger\nSeason Number\n\n\nepisode\ninteger\nEpisode Number\n\n\nscene\ninteger\nScene Number\n\n\nutterance\ninteger\nUtterance Number\n\n\nemotion\ncharacter\nOne of 7 emotions\n\n\n\n\n\nfriends_entities data (within package)\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ninteger\nSeason Number\n\n\nepisode\ninteger\nEpisode Number\n\n\nscene\ninteger\nScene Number\n\n\nutterance\ninteger\nUtterance Number\n\n\nentities\nlist\nCharacter entities\n\n\n\n\n\nfriends_info.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ninteger\nSeason Number\n\n\nepisode\ninteger\nEpisode Number\n\n\ntitle\ncharacter\nTitle\n\n\ndirected_by\ncharacter\nName of director(s)\n\n\nwritten_by\ncharacter\nName of writer(s)\n\n\nair_date\ndate\nOriginal Airing date in USA\n\n\nus_views_millions\ndouble\nViewers in USA in millions\n\n\nimdb_rating\ndouble\nIMDB Rating (10 is best)\n\n\n\n\nCleaning Script\nNo cleaning this week!"
  },
  {
    "objectID": "data/2020/2020-08-25/readme.html",
    "href": "data/2020/2020-08-25/readme.html",
    "title": "Chopped",
    "section": "",
    "text": "Chopped\nThe data this week comes from Kaggle courtesy of Jeffrey Braun with a h/t to: Nick Wan.\nChopped season ratings from IMDB and meta-info on Wikipedia.\n\nChopped is an American reality-based cooking television game show series. It is hosted by Ted Allen. The series pits four chefs against each other as they compete for a chance to win $10,000.\nIn each episode, four chefs compete in a three-round contest, where they attempt to incorporate unusual combinations of ingredients into dishes that are later evaluated by a panel of three judges. At the beginning of each round (typically “Appetizer”, “Entrée”, and “Dessert”, but with occasional exceptions), the chefs are each given a basket containing four mystery ingredients and are expected to create dishes that use all of them in some way. Although failing to use an ingredient is not an automatic disqualification, the judges do take such omissions into account when making their decisions. The ingredients are often not commonly prepared together. The chefs are given unlimited access to a pantry and refrigerator stocked with a wide variety of other ingredients, and each chef has his/her own stations for preparing and cooking food. The kitchen also includes a variety of specialized tools and equipment for the chefs’ use, such as a deep fryer, a blast chiller, and an ice cream machine.\nEach round has a time limit, typically 20 minutes for Appetizer, and 30 minutes each for Entrée and Dessert. These limits have been extended on occasion for special-format episodes and for rounds in which one or more mystery ingredients require additional preparation/cooking time. The chefs must cook their dishes and complete four platings (three for the judges and one “beauty plate”) before time runs out. Once time has expired, the judges critique the dishes based on presentation, taste and creativity and select one chef to be “chopped” - eliminated from the competition with no winnings. Allen reveals the judges’ decision by lifting a cloche on their table to show the losing chef’s dish, and one of the judges comments on the reason for their choice to the eliminated chef. In the Dessert round, the judges consider not only the dishes created by the two chefs during that round, but also their overall performance throughout the competition. The winner receives $10,000, although in special competitions, winners can earn anywhere between $20,000 to $50,000.\n\nNote the data was joined from two different sources, and there are episodes where the ratings are missing from IMBD.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-08-25')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 35)\n\nchopped &lt;- tuesdata$chopped\n\n# Or read in the data manually\n\nchopped &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-25/chopped.tsv')\n\n\nData Dictionary\n\n\n\nchopped.tsv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ndouble\nSeason Number\n\n\nseason_episode\ndouble\nEpisode number within a season\n\n\nseries_episode\ndouble\nEpisode number as part of the entire series\n\n\nepisode_rating\ndouble\nIMDB sourced episode rating 0-10 scale\n\n\nepisode_name\ncharacter\nEpisode Name\n\n\nepisode_notes\ncharacter\nEpisode notes\n\n\nair_date\ncharacter\nEpisode air date\n\n\njudge1\ncharacter\nJudge 1 Name\n\n\njudge2\ncharacter\nJudge 2 Name\n\n\njudge3\ncharacter\nJudge 3 Name\n\n\nappetizer\ncharacter\nAppetizer ingredients\n\n\nentree\ncharacter\nEntree ingredients\n\n\ndessert\ncharacter\nDessert ingredients\n\n\ncontestant1\ncharacter\nContestant 1 name\n\n\ncontestant1_info\ncharacter\nContestant 1 Info\n\n\ncontestant2\ncharacter\nContestant 2 name\n\n\ncontestant2_info\ncharacter\nContestant 2 Info\n\n\ncontestant3\ncharacter\nContestant 3 name\n\n\ncontestant3_info\ncharacter\nContestant 3 Info\n\n\ncontestant4\ncharacter\nContestant 4 name\n\n\ncontestant4_info\ncharacter\nContestant 4 Info\n\n\n\n\nCleaning Script\nlibrary(glue)\nlibrary(tidyverse)\nlibrary(rvest)\n\ncreate_tidytuesday_folder()\n\nraw_df &lt;- read_csv(\"2020/2020-08-25/chopped-raw.csv\") %&gt;% \n  mutate(episode_name = str_remove_all(episode_name, '\"'))\n\nraw_df\n\n# Scrape IMDB -------------------------------------------------------------\n\n\ntest_url &lt;- \"https://www.imdb.com/title/tt1353281/episodes?season=1\"\n\n\nget_imdb_data &lt;- function(season){\n  \n  # be nice\n  Sys.sleep(1)\n  cat(\n    glue::glue(\"Scraping S{season}!\"),\n    \"\\n\"\n    )\n  \n  raw_url &lt;- glue::glue(\"https://www.imdb.com/title/tt1353281/episodes?season={season}&ref_=ttep_ep_sn_nx\")\n  \n  raw_html &lt;- raw_url %&gt;% \n    read_html()\n  \n  raw_eps &lt;- raw_html %&gt;% \n    html_nodes(\"#episodes_content &gt; div.clear &gt; div.list.detail.eplist\") %&gt;% \n    html_nodes(\"div.list_item\")\n  \n  ep_ct &lt;- if (season != 1) {\n    1:(length(raw_eps))\n  } else {\n    2:(length(raw_eps))\n  } \n  \n  get_airdate &lt;- function(scrape_number){\n    raw_eps[[scrape_number]] %&gt;% \n      html_node(\"div.airdate\") %&gt;% \n      html_text() %&gt;% \n      str_squish() %&gt;% \n      str_remove_all(\"\\n\")\n  }\n  \n  get_title &lt;- function(scrape_number){\n    raw_eps[[scrape_number]] %&gt;% \n      html_node(\"div.info &gt; strong &gt; a\") %&gt;%\n      html_attr(\"title\")\n  }\n  \n  get_rating &lt;- function(scrape_number){\n    raw_eps[[scrape_number]] %&gt;% \n      html_node(\"span.ipl-rating-star__rating\") %&gt;% \n      html_text() %&gt;% \n      as.double()\n  }\n  \n  get_description &lt;- function(scrape_number){\n    raw_eps[[scrape_number]] %&gt;% \n      html_node(\"div.item_description\") %&gt;% \n      html_text() %&gt;% \n      str_remove_all(\"\\n\") %&gt;% \n      str_squish()\n  }\n  \n  get_episode &lt;- function(scrape_number){\n    raw_eps[[scrape_number]] %&gt;% \n      html_node(\"div.image\") %&gt;% \n      html_text() %&gt;% \n      str_remove_all(\"\\n\") %&gt;% \n      str_remove(\"Add Image \") %&gt;% \n      str_squish()\n  }\n  \n  tibble(\n    scrape_number = ep_ct\n  ) %&gt;% \n    mutate(\n      air_date = map_chr(scrape_number, get_airdate),\n      episode_title = map_chr(scrape_number, get_title),\n      episode_rating = map_dbl(scrape_number, get_rating),\n      episode_description = map_chr(scrape_number, get_description),\n      ep_num = map_chr(scrape_number, get_episode)\n    ) %&gt;% \n    separate(ep_num, into = c(\"season_num\", \"episode_num\"), sep = \", Ep\") %&gt;% \n    mutate(season = str_remove(season_num, \"S\") %&gt;% as.integer(),\n           episode = as.integer(episode_num),\n           air_date = lubridate::dmy(air_date)) %&gt;% \n    select(season, episode, air_date:episode_description)\n\n}\n\n# scrape all the IMDB data\nall_ep_ratings &lt;- map_dfr(1:45, get_imdb_data)\n\njoined_df &lt;- raw_df %&gt;% \n  left_join(all_ep_ratings %&gt;% \n              select(season, season_episode = episode, episode_rating),\n            by = c(\"season\", \"season_episode\")) %&gt;% \n  mutate(episode_rating = if_else(episode_rating == 0, NA_real_, episode_rating)) %&gt;% \n  select(season:series_episode, episode_rating, everything())\n\njoined_df %&gt;% \n  write_tsv(\"2020/2020-08-25/chopped.tsv\")\n\nglimpse(joined_df)\n\njoined_df %&gt;% \n  ggplot(aes(x = series_episode, y = season_episode)) +\n  geom_point()\n\njoined_df %&gt;% \n  ggplot(aes(x = series_episode, y = episode_rating)) +\n  geom_point() +\n  geom_smooth()"
  },
  {
    "objectID": "data/2020/2020-08-11/readme.html",
    "href": "data/2020/2020-08-11/readme.html",
    "title": "Avatar: The last airbender",
    "section": "",
    "text": "Hero image of the main characters\n\n\n\nAvatar: The last airbender\nThe data this week comes from the appa R package created by Avery Robbins. H/t to Kelsey Gonzalez for recommending this data package.\nThe original data came from the Avatar Wiki, and the example code used to scrape this dataset is also covered on Avery’s blog along with a quick exploration of the available data in a separate blog post.\nThere is also a “Avatar” themed palette from the tvthemes R package courtesy of Ryo Nakagawara\nFor people who have not seen the show or read this series of books, Wikipedia covers the high level details.\n\nAvatar: The Last Airbender is set in a world where human civilization consists of four nations, named after the four classical elements: the Water Tribes, the Earth Kingdom, the Fire Nation, and the Air Nomads. In each nation, certain people, known as “benders” (waterbenders, earthbenders, firebenders and airbenders), have the ability to telekinetically manipulate and control the element corresponding to their nation, using gestures based on Chinese martial arts. The Avatar is the only person with the ability to bend all four elements.\nThe series is centered around the journey of 12-year-old Aang, the current Avatar and last survivor of his nation, the Air Nomads, along with his friends Sokka, Katara, and later Toph Beifong, as they strive to end the Fire Nation’s war against the other nations of the world. It also follows the story of Zuko—the exiled prince of the Fire Nation, seeking to restore his lost honor by capturing Aang, accompanied by his wise uncle Iroh—and later, that of his ambitious sister Azula.\nAvatar: The Last Airbender was commercially successful and was acclaimed by audiences and critics, who praised its art direction, soundtrack, cultural references, humor, characters, and themes. These include concepts rarely touched on in youth entertainment, such as war, genocide, imperialism, colonialism, totalitarianism, and free choice.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-08-11')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 33)\n\navatar &lt;- tuesdata$avatar\n\n# Or read in the data manually\n\navatar &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-11/avatar.csv')\nscene_description &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-11/scene_description.csv')\n\n\nData Dictionary\n\n\n\navatar.csv\nThis is the core dataset (scene description text moved to alternative dataset as it was a list column).\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nUnique Row identifier\n\n\nbook\ncharacter\nBook name\n\n\nbook_num\ninteger\nBook number\n\n\nchapter\ncharacter\nChapter name\n\n\nchapter_num\ninteger\nChapter Name\n\n\ncharacter\ncharacter\nCharacter speaking\n\n\nfull_text\ncharacter\nFull text (scene description, character text)\n\n\ncharacter_words\ncharacter\nText coming from characters\n\n\nwriter\ncharacter\nWriter of book\n\n\ndirector\ncharacter\nDirector of episode\n\n\nimdb_rating\ndouble\nIMDB rating for episode\n\n\n\n\n\nscene_description.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nUnique row identifier\n\n\nscene_description\ncharacter\nScene description text\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(appa)\n\navatar &lt;- appa::appa\n\nscene_description &lt;- avatar %&gt;% \n  select(id, scene_description) %&gt;% \n  unnest_longer(scene_description) %&gt;% \n  filter(!is.na(scene_description))\n\nscene_description %&gt;% \n  write_csv(here::here(\"2020\", \"2020-08-11\", \"scene_description.csv\"))\n\navatar %&gt;% \n  select(-scene_description) %&gt;% \n  write_csv(here::here(\"2020\", \"2020-08-11\", \"avatar.csv\"))"
  },
  {
    "objectID": "data/2020/2020-07-28/readme.html",
    "href": "data/2020/2020-07-28/readme.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The palmer penguins\n\n\n\nPalmer Penguins\nThe data this week comes from Dr. Kristen Gorman by way of the palmerpenguins R package by Dr. Kristen Gorman, Dr. Allison Horst, and Dr. Alison Hill.\nTheir palmerpenguins packagedown site and corresponding GitHub Repo has all the details, which I will duplicate some of below.\nYou can install their package for reproducible use via install.packages(\"palmerpenguins\").\n\nThe goal of palmerpenguins is to provide a great dataset for data exploration & visualization, as an alternative to iris.\nWe gratefully acknowledge Palmer Station LTER and the US LTER Network. Special thanks to Marty Downs (Director, LTER Network Office) for help regarding the data license & use.\n\nThey’ve bundled both the raw data and the cleaned data together, which I have also included here.\nThe main measurements are body mass, culmen (bill) length, bill depth, and flipper length.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-28')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 31)\n\npenguins &lt;- tuesdata$penguins\n\n# Or read in the data manually\n\npenguins.csv &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-07-28/penguins.csv')\n\npenguins_raw.csv &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-07-28/penguins_raw.csv')\n\n\nData Dictionary\n\n\n\npenguins.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nisland\ninteger\nIsland where recorded (Biscoe, Dream, Torgersen)\n\n\nbill_length_mm\ndouble\nBill length in millimeters (also known as culmen length)\n\n\nbill_depth_mm\ndouble\nBill depth in millimeters (also known as culmen depth)\n\n\nflipper_length_mm\ninteger\nFlipper length in mm\n\n\nbody_mass_g\ninteger\nBody mass in grams\n\n\nsex\ninteger\nsex of the animal\n\n\nyear\ninteger\nyear recorded\n\n\n\n\n\npenguins_raw.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstudyName\ncharacter\nStudy name\n\n\nSample Number\ndouble\nSample id\n\n\nSpecies\ncharacter\nSpecies of penguin\n\n\nRegion\ncharacter\nRegion where recorded\n\n\nIsland\ncharacter\nIsland where recorded\n\n\nStage\ncharacter\nStage of egg\n\n\nIndividual ID\ncharacter\nIndividual penguin ID\n\n\nClutch Completion\ncharacter\nEgg clutch completion\n\n\nDate Egg\ndouble\nDate of egg\n\n\nCulmen Length (mm)\ndouble\nculmen length in mm (beak length)\n\n\nCulmen Depth (mm)\ndouble\nculmen depth in mm (beak depth)\n\n\nFlipper Length (mm)\ndouble\nFlipper length in mm\n\n\nBody Mass (g)\ndouble\nBody mass in g\n\n\nSex\ncharacter\nSex of the penguin\n\n\nDelta 15 N (o/oo)\ndouble\nBlood isotopic Nitrogen - used for dietary comparison\n\n\nDelta 13 C (o/oo)\ndouble\nBlood isotopic Carbon - used for dietary comparison\n\n\nComments\ncharacter\nMiscellaneous comments\n\n\n\n\nCleaning Script\nNo cleaning script today, feel free to work with the pre-cleaned data or try your hand at the raw data!"
  },
  {
    "objectID": "data/2020/2020-07-14/readme.html",
    "href": "data/2020/2020-07-14/readme.html",
    "title": "Astronaut database",
    "section": "",
    "text": "NASA Astronaut at the International Space Station\n\n\n\nAstronaut database\nThe data this week comes from Mariya Stavnichuk and Tatsuya Corlett.\nThis article talks about the data set in greater detail.\n\nThis database contains publically available information about all astronauts who participated in space missions before 15 January 2020 collected from NASA, Roscosmos, and fun-made websites. The provided information includes full astronaut name, sex, date of birth, nationality, military status, a title and year of a selection program, and information about each mission completed by a particular astronaut such as a year, ascend and descend shuttle names, mission and extravehicular activity (EVAs) durations.\n\nCredit for preparing the dataset: Georgios Karamanis\nIt may be interesting to also use the Space Launches dataset from TidyTuesday 2019, week 3.\nThere is also a Wikipedia Article on cumulative spacewalk records - you should be able to create the same dataset with the astronaut database and the eva_hrs_mission column.\nTo get that data in, use tidytuesdayR::tt_load(\"2019\", week = 3).\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-14')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 29)\n\nastronauts &lt;- tuesdata$astronauts\n\n# Or read in the data manually\n\nastronauts &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-07-14/astronauts.csv')\n\n\nData Dictionary\n\n\n\nastronauts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nID\n\n\nnumber\ndouble\nNumber\n\n\nnationwide_number\ndouble\nNumber within country\n\n\nname\ncharacter\nFull name\n\n\noriginal_name\ncharacter\nName in original language\n\n\nsex\ncharacter\nSex\n\n\nyear_of_birth\ndouble\nYear of birth\n\n\nnationality\ncharacter\nNationality\n\n\nmilitary_civilian\ncharacter\nMilitary status\n\n\nselection\ncharacter\nName of selection program\n\n\nyear_of_selection\ndouble\nYear of selection program\n\n\nmission_number\ndouble\nMission number\n\n\ntotal_number_of_missions\ndouble\nTotal number of missions\n\n\noccupation\ncharacter\nOccupation\n\n\nyear_of_mission\ndouble\nMission year\n\n\nmission_title\ncharacter\nMission title\n\n\nascend_shuttle\ncharacter\nName of ascent shuttle\n\n\nin_orbit\ncharacter\nName of spacecraft used in orbit\n\n\ndescend_shuttle\ncharacter\nName of descent shuttle\n\n\nhours_mission\ndouble\nDuration of mission in hours\n\n\ntotal_hrs_sum\ndouble\nTotal duration of all missions in hours\n\n\nfield21\ndouble\nInstances of EVA by mission\n\n\neva_hrs_mission\ndouble\nDuration of extravehicular activities during the mission\n\n\ntotal_eva_hrs\ndouble\nTotal duration of all extravehicular activities in hours\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\n\nastronauts &lt;- read_csv(\"data/astronauts.csv\") %&gt;% \n  clean_names() %&gt;% \n  filter(!is.na(number)) %&gt;%  # remove last row (all values are NA)\n  mutate(\n    sex = if_else(sex == \"M\", \"male\", \"female\"),\n    military_civilian = if_else(military_civilian == \"Mil\", \"military\", \"civilian\")\n  )"
  },
  {
    "objectID": "data/2020/2020-06-30/readme.html",
    "href": "data/2020/2020-06-30/readme.html",
    "title": "Uncanny X-men",
    "section": "",
    "text": "Uncanny X-men\nThe data this week comes from the Claremont Run Project and Malcom Barret who put these datasets into a R data package. The Claremont Run project has a nice Foreward capturing some of the reasoning behind this dataset. “The Claremont Run is a SSHRC-funded academic initiative micro-publishing data-based analysis of Chris Claremont’s 16 year run on Uncanny X-Men #97-278.”\n\nFrom 1975-1991, Chris Claremont wrote X-men, forming the longest stint of any mainstream superhero writer on a single title. During his tenure, X-men went from a B-list title on the verge of cancellation, to the best-selling comic book in the world, and Claremont holds the Guinness World Record to this day for the bestselling single issue comic of all-time.\n\n\nClaremont’s work is too culturally important to lose touch with. A generation of writers, filmmakers, and artists were all but weaned on the stories he told, and Claremont’s fingerprints are all over the media landscape that we have today – structures and strategies and dynamics. But exposing a new generation of readers and scholars to the Claremont run (in all its scope) is challenging to say the least.\n\n\nAnd that’s where our humble website comes in. By building an expansive data set on the Claremont run, this project hopes to open new doors of exploration and consideration for the next generation of comics scholars. In this sense, this project is looking both to the past (in order to deconstruct and chronicle the landmark contribution of a comics artist to the field of popular culture as a whole) and to the future (in order to facilitate yet-to-come discussions of the author’s work, enabling and empowering future breakthroughs).\n\nThe Claremont Project has a Twitter handle - please reference them when using this data: @ClaremontRun\nMalcom Barret put these datasets into a R data package: claremontrun, which is where we got the data for this week.\nTo Install the claremontrun package:\n- remotes::install_github(\"malcolmbarrett/claremontrun\")\nOr use the raw CSVs from this repo with tidytuesdayR.\n\nclaremontrun is an R data package that provides data from the Claremont Run project. This project collects data on Chris Claremont’s iconic run on Uncanny X-Men.\n\nNote that while the claremontrun has information about the Bechdel test, it doesn’t include gender as a measure. You may therefore want to also explore the 2018 TidyTuesday dataset that had more metadata about specific comic book characters:\n2018 - Week 9\nLoad that data with: tidytuesdayR::tt_load(2018, week = 9)\nThese datasets could be joined by character names.\nclaremontrun includes 7 data sets relevant to the Claremont run:\n\ncharacter_visualization, counts of character speech, thought, narrative, or visual depictions\n\ncharacters, descriptions of character actions\n\ncomic_bechdel, whether or not an issue of another (non-X-Men) comic series met the Bechdel test\n\ncovers, data on covers of issues of Uncanny X-Men\n\nissue_collaborators, data about other collaborators on each issue, such as editors\n\nlocation, locations that appear in each issue\n\nxmen_bechdel, whether or not an issue of Uncanny X-Men met the Bechdel test\n\nThe Bechdel Test according to Wikipedia:\n\nThe Bechdel Test is a measure of the representation of women in fiction. It asks whether a work features at least two women who talk to each other about something other than a man. The requirement that the two women must be named is sometimes added.\nAbout half of all films meet these criteria, according to user-edited databases and the media industry press. Passing or failing the test is not necessarily indicative of how well women are represented in any specific work. Rather, the test is used as an indicator for the active presence of women in the entire field of film and other fiction, and to call attention to gender inequality in fiction. Media industry studies indicate that films that pass the test perform better financially than those that do not.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest!\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-06-30')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 27)\n\ncomic_bechdel &lt;- tuesdata$comic_bechdel\ncharacters &lt;- tuesdata$characters\n\n# Or read in manually with read_csv()\n\ncomic_bechdel &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-30/comic_bechdel.csv')\n\ncharacter_visualization &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-30/character_visualization.csv')\n\ncharacters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-30/characters.csv')\n\nxmen_bechdel &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-30/xmen_bechdel.csv')\n\ncovers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-30/covers.csv')\n\nissue_collaborators &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-30/issue_collaborators.csv')\n\nlocations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-30/locations.csv')\n\n\nData Dictionary\n\n\n\ncomic_bechdel.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseries\ncharacter\nSeries title\n\n\nissue\ninteger\nIssue number\n\n\ntitle\ncharacter\nTitle of the comic issue\n\n\nwriter\ncharacter\nWriter\n\n\nartist\ncharacter\nComic artist\n\n\ncover_artist\ncharacter\nCover artist\n\n\npass_bechdel\ncharacter\nDoes it pass the bechdel test?\n\n\npage_number\ncharacter\nPage Number\n\n\nnotes\ncharacter\nNotes\n\n\n\n\n\nxmen_bechdel.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nissue\ndouble\nIssue number\n\n\npass_bechdel\ncharacter\nDoes it pass the bechdel test?\n\n\nnotes\ncharacter\nNotes\n\n\nreprint\nlogical\nReprint?\n\n\n\n\n\ncharacter_visualization.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nissue\ndouble\nIssue number\n\n\ncostume\ninteger\nIn costume or not in costume\n\n\ncharacter\ncharacter\nCharacter name (secret identity and their superhero name)\n\n\nspeech\ndouble\nSpeech bubble in that issue\n\n\nthought\ndouble\nThought bubble in that issue\n\n\nnarrative\ndouble\nNarrative statements in that issue\n\n\ndepicted\ndouble\nNumber of depictions in that issue\n\n\n\n\n\ncharacters.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nissue\ndouble\nIssue number\n\n\ncharacter\ncharacter\nCharacter name\n\n\nrendered_unconcious\ndouble\nNumber of times rendered unconscious\n\n\ncaptured\ndouble\nNumber of times captured\n\n\ndeclared_dead\ndouble\nNumber of times declared dead\n\n\nredressed\ndouble\nNumber of times re-dressed\n\n\ndepowered\ndouble\nNumber of times depowered\n\n\nclothing_torn\ndouble\nNumber of times clothing torn\n\n\nsubject_to_torture\ndouble\nNumber of times tortured\n\n\nquits_team\ndouble\nNumber of times quits team\n\n\nsurrenders\ndouble\nNumber of times surrenders\n\n\nnumber_of_kills_humans\ndouble\nNumber of humans killed\n\n\nnumber_of_kills_non_humans\ndouble\nNumber of non-humans killed\n\n\ninitiates_physical_conflict\ncharacter\nNumber of times initiates physical conflict\n\n\nexpresses_reluctance_to_fight\ndouble\nNumber of times expresses reluctance to fight\n\n\non_a_date_with_which_character\ncharacter\nNumber of times on a data with specific character\n\n\nkiss_with_which_character\ncharacter\nNumber of times kissing with a character\n\n\nhand_holding_with_which_character\ncharacter\nNumber of times holding hands with character\n\n\ndancing_with_which_character\ncharacter\nNumber of times dancing with a character\n\n\nflying_with_another_character\ncharacter\nNumber of times flying with a character\n\n\narm_in_arm_with_which_character\ncharacter\nNumber of times arm in arm with character\n\n\nhugging_with_which_character\ncharacter\nNumber of times hugging with a character\n\n\nphysical_contact_other\ncharacter\nNumber of times with physical contact with a character\n\n\ncarrying_with_which_character\ncharacter\nNumber of times carrying a character\n\n\nshared_bed_with_which_character\nlogical\nNumber of times sharing bed with character\n\n\nshared_room_domestically_with_which_character\nlogical\nNumber of times sharing room domestically with character\n\n\nexplicitly_states_i_love_you_to_whom\ncharacter\nNumber of times saying I love you to whom\n\n\nshared_undress\ncharacter\nNumber of times sharing undress\n\n\nshower_number_of_panels_shower_lasts\ndouble\nNumber of times showering number of panels\n\n\nbath_number_of_panels_bath_lasts\ndouble\nNumber of times/panels bathing\n\n\ndepicted_eating_food\ndouble\nNumber of times eating food\n\n\nvisible_tears_number_of_panels\ndouble\nNumber of panels with tears (crying)\n\n\nvisible_tears_number_of_intances\ndouble\nVisible tears number of instances\n\n\nspecial_notes\ncharacter\nSpecial notes\n\n\n\n\n\ncovers.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nissue\ndouble\nIsse number\n\n\ncover_artist\ncharacter\nCover artist\n\n\nnarrative_captions\ncharacter\nNarrative captions\n\n\ncharacters_visualized\ncharacter\nWhich characters visualized\n\n\ncharacters_speaking\ncharacter\nWhich characters speaking\n\n\ndialog_text\ncharacter\nDialog text on cover\n\n\n\n\n\nissue_collaborators.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nissue\ndouble\nIssue number\n\n\neditor_in_chief\ncharacter\nEditor in chief of issue\n\n\neditor\ncharacter\nEditor of issue\n\n\npenciller\ncharacter\nPenciller of issue\n\n\n\n\n\nlocations.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nissue\ndouble\nIssue number\n\n\nlocation\ncharacter\nLocation\n\n\ncontext\ncharacter\nContext - what’s going on\n\n\nnotes\ncharacter\nNotes\n\n\n\n\nCleaning Script\nNo cleaning script today - this data came pre-cleaned."
  },
  {
    "objectID": "data/2020/2020-06-16/readme.html",
    "href": "data/2020/2020-06-16/readme.html",
    "title": "American Slavery and Juneteenth",
    "section": "",
    "text": "Abolition of slavery celebration, Washington DC April 19 1866"
  },
  {
    "objectID": "data/2020/2020-06-16/readme.html#suggested-reading-material",
    "href": "data/2020/2020-06-16/readme.html#suggested-reading-material",
    "title": "American Slavery and Juneteenth",
    "section": "Suggested Reading Material",
    "text": "Suggested Reading Material\nPlease use your best judgement when working with this data. There is a lot of pain and suffering that we cannot fully capture in simple numbers and charts. We believe that it is important to understand how wide-spread slavery was, how many people were affected then, and how this continually impacts the world.\nTeaching Hard History - American Slavery &gt; The consequences of slavery continue to distort and stunt lives in America, so it’s quite right that we should engage in what can be an agonizing conversation about this history. Only when our history is faced squarely can removing Confederate monuments be properly understood, as a small but significant step toward ending the celebration of treason and white supremacy, if not toward ameliorating their effects.\nWe are influenced by the “Teaching Hard History” resources. Please take the time to review these materials, examine some of the quick Summary Videos\nThe full article around these education materials can be found here."
  },
  {
    "objectID": "data/2020/2020-06-16/readme.html#juneteenth-details",
    "href": "data/2020/2020-06-16/readme.html#juneteenth-details",
    "title": "American Slavery and Juneteenth",
    "section": "Juneteenth details",
    "text": "Juneteenth details\n\n\n\nJuneteenth Inscription\n\n\nTexas Historical Commission Juneteenth Inscription\n\nCOMMEMORATED ANNUALLY ON JUNE 19TH, JUNETEENTH IS THE OLDEST KNOWN CELEBRATION OF THE END OF SLAVERY IN THE U.S. THE EMANCIPATION PROCLAMATION, ISSUED BY PRESIDENT ABRAHAM LINCOLN ON SEP. 22, 1862, ANNOUNCED, “THAT ON THE 1ST DAY OF JANUARY, A.D. 1863, ALL PERSONS HELD AS SLAVES WITHIN ANY STATE…IN REBELLION AGAINST THE U.S. SHALL BE THEN, THENCEFORWARD AND FOREVER FREE.” HOWEVER, IT WOULD TAKE THE CIVIL WAR AND PASSAGE OF THE 13TH AMENDMENT TO THE CONSTITUTION TO END THE BRUTAL INSTITUTION OF AFRICAN AMERICAN SLAVERY. AFTER THE CIVIL WAR ENDED IN APRIL 1865 MOST SLAVES IN TEXAS WERE STILL UNAWARE OF THEIR FREEDOM. THIS BEGAN TO CHANGE WHEN UNION TROOPS ARRIVED IN GALVESTON. MAJ. GEN. GORDON GRANGER, COMMANDING OFFICER, DISTRICT OF TEXAS, FROM HIS HEADQUARTERS IN THE OSTERMAN BUILDING (STRAND AND 22ND ST.), READ ‘GENERAL ORDER NO. 3’ ON JUNE 19, 1865. THE ORDER STATED “THE PEOPLE OF TEXAS ARE INFORMED THAT, IN ACCORDANCE WITH A PROCLAMATION FROM THE EXECUTIVE OF THE UNITED STATES, ALL SLAVES ARE FREE. THIS INVOLVES AN ABSOLUTE EQUALITY OF PERSONAL RIGHTS AND RIGHTS OF PROPERTY BETWEEN FORMER MASTERS AND SLAVES.” WITH THIS NOTICE, RECONSTRUCTION ERA TEXAS BEGAN. FREED AFRICAN AMERICANS OBSERVED “EMANCIPATION DAY,” AS IT WAS FIRST KNOWN, AS EARLY AS 1866 IN GALVESTON. AS COMMUNITY GATHERINGS GREW ACROSS TEXAS, CELEBRATIONS INCLUDED PARADES, PRAYER, SINGING, AND READINGS OF THE PROCLAMATION. IN THE MID-20TH CENTURY, COMMUNITY CELEBRATIONS GAVE WAY TO MORE PRIVATE COMMEMORATIONS. A RE-EMERGENCE OF PUBLIC OBSERVANCE HELPED JUNETEENTH BECOME A STATE HOLIDAY IN 1979. INITIALLY OBSERVED IN TEXAS, THIS LANDMARK EVENT’S LEGACY IS EVIDENT TODAY BY WORLDWIDE COMMEMORATIONS THAT CELEBRATE FREEDOM AND THE TRIUMPH OF THE HUMAN SPIRIT."
  },
  {
    "objectID": "data/2020/2020-06-02/readme.html",
    "href": "data/2020/2020-06-02/readme.html",
    "title": "Marble Racing",
    "section": "",
    "text": "Marble Racing\nThe data this week comes from Jelle’s Marble Runs courtesy of Randy Olson.\nRandy’s blogpost covers some additional analysis.\n\nJelle’s Marble Runs started as a quirky YouTube channel back in 2006 and has refined the art of marble racing to the point that many — including sponsor John Oliver from Last Week Tonight — consider marble racing a legitimate contender for the national sports spotlight. Given that Jelle’s Marble Runs just completed their popular Marbula One competition last month, I was curious to look at the race results to see if these races were anything more than chaos.\nDo some marbles race better than others? Who would I put my money on in season 2 of Marbula One? … If any of these questions interest you, read on and I’ll answer some of them.\nThe first step to answering these questions was to get some data. Thankfully, all of the Marbula One videos are organized in a YouTube playlist available here. From every race, my marble racing analytics team recorded each marble racer’s qualifier performance, total race time, average lap time, final rank, and some other statistics. That dataset is available for download on my website here.\n\nSome additional context from the fandom Wiki for Jelle’s Marble Runs and a link to Season 1 courtesy of Georgios Karamanis.\n\nSpotlight from John Oliver on Last Week Tonight\n\ncourtesy of Dennis Hammerschmidt\n\n\nGet the data here\n# Get the Data\n\nmarbles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-02/marbles.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-06-02')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 23)\n\n\nmarbles &lt;- tuesdata$marbles\n\n\nData Dictionary\n\n\n\nmarbles.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ncharacter\ndate of race\n\n\nrace\ncharacter\nrace id\n\n\nsite\ncharacter\nsite of race\n\n\nsource\ncharacter\nyoutube url\n\n\nmarble_name\ncharacter\nname of marble\n\n\nteam_name\ncharacter\nteam name\n\n\ntime_s\ndouble\nTime in seconds\n\n\npole\ncharacter\npole position\n\n\npoints\ndouble\nPoints gained\n\n\ntrack_length_m\ndouble\ntrack length in meters\n\n\nnumber_laps\ndouble\nnumber of laps\n\n\navg_time_lap\ndouble\naverage lap time\n\n\nhost\ncharacter\nHost of race\n\n\nnotes\ncharacter\nNotes (very few, but some notes about potential errors)\n\n\n\n\nskimr\n── Data Summary ────────────────────────\n                           Values \nName                       marbles\nNumber of rows             256    \nNumber of columns          14     \n_______________________           \nColumn type frequency:            \n  character                9      \n  numeric                  5      \n________________________          \nGroup variables                   \n\n── Variable type: character ────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate   min   max empty n_unique whitespace\n1 date                  0        1          8     9     0       16          0\n2 race                  0        1          4     4     0       16          0\n3 site                  0        1          7    15     0        8          0\n4 source                0        1         34    34     0       16          0\n5 marble_name           0        1          4     9     0       32          0\n6 team_name             0        1          6    16     0       16          0\n7 pole                128        0.5        2     3     0       16          0\n8 host                  0        1          2     3     0        2          0\n9 notes               249        0.0273    37   100     0        7          0\n\n── Variable type: numeric ──────────────────────────────────────────────────────────\n  skim_variable  n_missing complete_rate   mean      sd hist \n1 time_s                 3         0.988 191.   169.    ▇▁▁▇▁\n2 points               128         0.5     6.45   7.74  ▇▂▂▁▁\n3 track_length_m         0         1      13.2    0.952 ▅▅▂▁▇\n4 number_laps            0         1       6.25   5.53  ▇▁▃▂▂\n5 avg_time_lap           3         0.988  29.7    5.55  ▃▆▇▇▂\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(janitor)\n\n\nmarbles &lt;- read_csv(\"2020/2020-06-02/Jelles-Marble-Racing-Marbula-One.csv\") %&gt;% \n janitor::clean_names() %&gt;% \n  select(-x14) %&gt;% \n  rename(notes = x15)\n\nskimr::skim(marbles)\n\nmarbles %&gt;% \n  write_csv(\"2020/2020-06-02/marbles.csv\")"
  },
  {
    "objectID": "data/2020/2020-05-19/readme.html",
    "href": "data/2020/2020-05-19/readme.html",
    "title": "Beach Volleyball",
    "section": "",
    "text": "Beach Volleyball\n\n\n\nBeach Volleyball\nThe data this week comes from Adam Vagnar who also blogged about this dataset. There’s a LOT of data here - match-level results, player details, and match-level statistics for some matches. For all this dataset all the matches are played 2 vs 2, so there are columns for 2 winners (1 team) and 2 losers (1 team). The data is relatively ready for analysis and clean, although there are some duplicated columns and the data is wide due to the 2-players per team.\nCheck out the data dictionary, or Wikipedia for some longer-form details around what the various match statistics mean.\nMost of the data is from the international FIVB tournaments but about 1/3 is from the US-centric AVP.\n\nThe FIVB Beach Volleyball World Tour (known between 2003 and 2012 as the FIVB Beach Volleyball Swatch World Tour for sponsorship reasons) is the worldwide professional beach volleyball tour for both men and women organized by the Fédération Internationale de Volleyball (FIVB). The World Tour was introduced for men in 1989 while the women first competed in 1992.\nWinning the World Tour is considered to be one of the highest honours in international beach volleyball, being surpassed only by the World Championships, and the Beach Volleyball tournament at the Summer Olympic Games.\n\nFiveThirtyEight examined the disadvantage of serving in beach volleyball, although they used Olympic-level data. Again, Adam Vagnar also covered this data on his blog.\n\nGet the data here\n# Get the Data\n\nvb_matches &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-05-19')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 21)\n\n\nvb_matches &lt;- tuesdata$vb_matches\n\n\nData Dictionary\n\n\n\nvb_matches.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncircuit\ncharacter\nEither AVP (USA) or FIVB (International)\n\n\ntournament\ncharacter\nTournament City or Name\n\n\ncountry\ncharacter\nCountry where tournament played\n\n\nyear\ndouble\nYear of tournament\n\n\ndate\ndouble\nDate of match\n\n\ngender\ncharacter\nGender of team\n\n\nmatch_num\ndouble\nMatch Number\n\n\nw_player1\ncharacter\nWinner player 1 Name\n\n\nw_p1_birthdate\ndouble\nWinner player 1 birth date\n\n\nw_p1_age\ndouble\nWinner player 1 age\n\n\nw_p1_hgt\ndouble\nWinner player 1 height in inches\n\n\nw_p1_country\ncharacter\nWinner player country\n\n\nw_player2\ncharacter\nWinner player 2 name\n\n\nw_p2_birthdate\ndouble\nWinner player 2 birthdate\n\n\nw_p2_age\ndouble\nWinner player 2 age\n\n\nw_p2_hgt\ndouble\nWinner player 2 height in inches\n\n\nw_p2_country\ncharacter\nWinner player 2 country\n\n\nw_rank\ncharacter\nWinner team rank\n\n\nl_player1\ncharacter\nLosing player 1 name\n\n\nl_p1_birthdate\ndouble\nLosing player 1 birthdate\n\n\nl_p1_age\ndouble\nLosing player 1 age\n\n\nl_p1_hgt\ndouble\nLosing player 1 height in inches\n\n\nl_p1_country\ncharacter\nLosing player 1 country\n\n\nl_player2\ncharacter\nLosing player 2 name\n\n\nl_p2_birthdate\ndouble\nLosing player 2 birthdate\n\n\nl_p2_age\ndouble\nLosing player 2 age\n\n\nl_p2_hgt\ndouble\nLosing player 2 height in inches\n\n\nl_p2_country\ncharacter\nLosing player 2 country\n\n\nl_rank\ncharacter\nLosing team rank\n\n\nscore\ncharacter\nMatch score separated by a dash and matches separated by a comma, eg 21 points to 12 points is 21-12\n\n\nduration\ndouble\nDuration of match in minutes\n\n\nbracket\ncharacter\nTournament bracket\n\n\nround\ncharacter\nTournament round\n\n\nw_p1_tot_attacks\ndouble\nWinner player 1 number of attacks (attacking swings over the net)\n\n\nw_p1_tot_kills\ndouble\nWinner player 1 number of kills (point ending attacks)\n\n\nw_p1_tot_errors\ndouble\nWinner player 1 mistakes\n\n\nw_p1_tot_hitpct\ndouble\nWinner player 1 hitting percentage - calculated as (kills-errors)/attacks - this is the player’s effectiveness at scoring\n\n\nw_p1_tot_aces\ndouble\nWinner player 1 total aces - point ending serves\n\n\nw_p1_tot_serve_errors\ndouble\nWinner player 1 total serving errors - mistakes made on serve\n\n\nw_p1_tot_blocks\ndouble\nWinner player 1 total blocks - point ending blocks\n\n\nw_p1_tot_digs\ndouble\nWinner player 1 total digs - successful defense of an attack\n\n\nw_p2_tot_attacks\ndouble\nWinner player 2 number of attacks (attacking swings over the net)\n\n\nw_p2_tot_kills\ndouble\nWinner player 2 number of kills (point ending attacks)\n\n\nw_p2_tot_errors\ndouble\nWinner player 2 mistakes\n\n\nw_p2_tot_hitpct\ndouble\nWinner player 2 hitting percentage - calculated as (kills-errors)/\n\n\nw_p2_tot_aces\ndouble\nWinner player 2 total aces - point ending serves\n\n\nw_p2_tot_serve_errors\ndouble\nWinner player 2 total serving errors - mistakes made on serve\n\n\nw_p2_tot_blocks\ndouble\nWinner player 2 total blocks - point ending blocks\n\n\nw_p2_tot_digs\ndouble\nWinner player 2 total digs - successful defense of an attack\n\n\nl_p1_tot_attacks\ndouble\nLosing player 1 number of attacks (attacking swings over the net)\n\n\nl_p1_tot_kills\ndouble\nLosing player 1 number of kills (point ending attacks)\n\n\nl_p1_tot_errors\ndouble\nLosing player 1 mistakes\n\n\nl_p1_tot_hitpct\ndouble\nLosing player 1 hitting percentage - calculated as (kills-errors)/\n\n\nl_p1_tot_aces\ndouble\nLosing player 1 total aces - point ending serves\n\n\nl_p1_tot_serve_errors\ndouble\nLosing player 1 total serving errors - mistakes made on serve\n\n\nl_p1_tot_blocks\ndouble\nLosing player 1 total blocks - point ending blocks\n\n\nl_p1_tot_digs\ndouble\nLosing player 1 total digs - successful defense of an attack\n\n\nl_p2_tot_attacks\ndouble\nLosing player 2 number of attacks (attacking swings over the net)\n\n\nl_p2_tot_kills\ndouble\nLosing player 2 number of kills (point ending attacks)\n\n\nl_p2_tot_errors\ndouble\nLosing player 2 mistakes\n\n\nl_p2_tot_hitpct\ndouble\nLosing player 2 hitting percentage - calculated as (kills-errors)/\n\n\nl_p2_tot_aces\ndouble\nLosing player 2 total aces - point ending serves\n\n\nl_p2_tot_serve_errors\ndouble\nLosing player 2 total serving errors - mistakes made on serve\n\n\nl_p2_tot_blocks\ndouble\nLosing player 2 total blocks - point ending blocks\n\n\nl_p2_tot_digs\ndouble\nLosing player 2 total digs - successful defense of an attack\n\n\n\n\nskimr\n── Data Summary ────────────────────────\n                           Values  \nName                       vb_matches\nNumber of rows             76756   \nNumber of columns          65      \n_______________________            \nColumn type frequency:             \n  character                17      \n  Date                     5       \n  difftime                 1       \n  numeric                  42      \n________________________           \nGroup variables            None    \n\n── Variable type: character ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   skim_variable n_missing complete_rate   min   max empty n_unique whitespace\n 1 circuit               0         1         3     4     0        2          0\n 2 tournament            0         1         3    22     0      177          0\n 3 country               0         1         4    22     0       51          0\n 4 gender                0         1         1     1     0        2          0\n 5 w_player1             0         1         6    29     0     3388          0\n 6 w_p1_country         12         1.00      4    20     0       85          0\n 7 w_player2             0         1         5    30     0     3431          0\n 8 w_p2_country          5         1.00      4    20     0       87          0\n 9 w_rank              148         0.998     1     7     0      812          0\n10 l_player1             0         1         5    29     0     5713          0\n11 l_p1_country         18         1.00      4    20     0      109          0\n12 l_player2             0         1         5    30     0     5689          0\n13 l_p2_country         10         1.00      4    20     0      111          0\n14 l_rank             1240         0.984     1     7     0      837          0\n15 score                22         1.00      4    25     0     6624          0\n16 bracket               0         1         6    21     0       36          0\n17 round              4939         0.936     7     8     0       10          0\n\n── Variable type: Date ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n  skim_variable  n_missing complete_rate min        max        median     n_unique\n1 date                   0         1     2000-09-16 2019-08-29 2009-08-25      658\n2 w_p1_birthdate       383         0.995 1953-06-13 2004-07-15 1981-10-30     2805\n3 w_p2_birthdate       408         0.995 1952-10-11 2004-06-08 1981-10-15     2847\n4 l_p1_birthdate      1059         0.986 1953-06-13 2004-12-01 1982-03-28     4236\n5 l_p2_birthdate       959         0.988 1949-12-04 2004-08-12 1982-03-20     4282\n\n── Variable type: difftime ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min      max       median n_unique\n1 duration           2249         0.971 120 secs 8040 secs 42'00\"      108\n\n── Variable type: numeric ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   skim_variable         n_missing complete_rate     mean     sd  hist \n 1 year                          0         1     2010.     5.48   ▃▇▆▅▇\n 2 match_num                     0         1       31.8   23.5    ▇▅▂▁▁\n 3 w_p1_age                    383         0.995   28.7    5.05   ▂▇▃▁▁\n 4 w_p1_hgt                   3966         0.948   73.7    3.64   ▁▅▇▃▁\n 5 w_p2_age                    408         0.995   28.8    4.85   ▁▇▆▁▁\n 6 w_p2_hgt                   4016         0.948   73.7    3.69   ▁▃▇▆▁\n 7 l_p1_age                   1059         0.986   28.3    5.26   ▂▇▂▁▁\n 8 l_p1_hgt                   6988         0.909   73.4    3.62   ▁▃▇▅▁\n 9 l_p2_age                    959         0.988   28.4    5.12   ▂▇▁▁▁\n10 l_p2_hgt                   6983         0.909   73.5    3.65   ▁▃▇▅▁\n11 w_p1_tot_attacks          62178         0.190   25.9   10.0    ▇▅▁▁▁\n12 w_p1_tot_kills            62178         0.190   14.7    5.34   ▂▇▅▁▁\n13 w_p1_tot_errors           62413         0.187    2.90   2.27   ▇▁▁▁▁\n14 w_p1_tot_hitpct           62185         0.190    0.480  0.230  ▇▁▁▁▁\n15 w_p1_tot_aces             60560         0.211    1.32   1.45   ▇▂▁▁▁\n16 w_p1_tot_serve_errors     62417         0.187    2.03   1.65   ▇▃▁▁▁\n17 w_p1_tot_blocks           60560         0.211    1.70   2.15   ▇▂▁▁▁\n18 w_p1_tot_digs             62178         0.190    8.35   5.48   ▇▅▁▁▁\n19 w_p2_tot_attacks          62174         0.190   26.1   10.1    ▇▇▁▁▁\n20 w_p2_tot_kills            62174         0.190   14.8    5.33   ▂▇▅▁▁\n21 w_p2_tot_errors           62413         0.187    2.92   2.29   ▇▁▁▁▁\n22 w_p2_tot_hitpct           62181         0.190    0.477  0.164  ▁▇▁▁▁\n23 w_p2_tot_aces             60556         0.211    1.19   1.36   ▇▁▁▁▁\n24 w_p2_tot_serve_errors     62413         0.187    1.93   1.62   ▇▃▁▁▁\n25 w_p2_tot_blocks           60556         0.211    1.69   2.19   ▇▂▁▁▁\n26 w_p2_tot_digs             62174         0.190    8.54   5.56   ▇▃▁▁▁\n27 l_p1_tot_attacks          62179         0.190   27.1   11.1    ▇▁▁▁▁\n28 l_p1_tot_kills            62179         0.190   12.8    5.76   ▃▇▃▁▁\n29 l_p1_tot_errors           62413         0.187    4.38   2.76   ▇▂▁▁▁\n30 l_p1_tot_hitpct           62189         0.190    0.313  0.176  ▃▇▁▁▁\n31 l_p1_tot_aces             60561         0.211    0.776  1.04   ▇▂▁▁▁\n32 l_p1_tot_serve_errors     62418         0.187    2.10   1.66   ▇▃▁▁▁\n33 l_p1_tot_blocks           60561         0.211    0.997  1.53   ▇▁▁▁▁\n34 l_p1_tot_digs             62179         0.190    7.19   5.17   ▇▂▁▁▁\n35 l_p2_tot_attacks          62178         0.190   26.7   10.8    ▃▇▁▁▁\n36 l_p2_tot_kills            62178         0.190   12.6    5.66   ▃▇▃▁▁\n37 l_p2_tot_errors           62413         0.187    4.32   2.71   ▇▃▁▁▁\n38 l_p2_tot_hitpct           62189         0.190    0.313  0.176  ▂▇▁▁▁\n39 l_p2_tot_aces             60560         0.211    0.775  1.06   ▇▁▁▁▁\n40 l_p2_tot_serve_errors     62417         0.187    2.05   1.66   ▇▂▁▁▁\n41 l_p2_tot_blocks           60560         0.211    1.06   1.56   ▇▁▁▁▁\n42 l_p2_tot_digs             62178         0.190    7.14   5.18   ▇▃▁▁▁\n\n\nCleaning Script\nData is already pretty clean! You may want to pivot the data by team or optionally separate out the winning/losing scores by match.\nlibrary(tidyverse)\n\ncol_types_vb &lt;- cols(\n  circuit = col_character(),\n  tournament = col_character(),\n  country = col_character(),\n  year = col_double(),\n  date = col_date(format = \"\"),\n  gender = col_character(),\n  match_num = col_double(),\n  w_player1 = col_character(),\n  w_p1_birthdate = col_date(format = \"\"),\n  w_p1_age = col_double(),\n  w_p1_hgt = col_double(),\n  w_p1_country = col_character(),\n  w_player2 = col_character(),\n  w_p2_birthdate = col_date(format = \"\"),\n  w_p2_age = col_double(),\n  w_p2_hgt = col_double(),\n  w_p2_country = col_character(),\n  w_rank = col_character(),\n  l_player1 = col_character(),\n  l_p1_birthdate = col_date(format = \"\"),\n  l_p1_age = col_double(),\n  l_p1_hgt = col_double(),\n  l_p1_country = col_character(),\n  l_player2 = col_character(),\n  l_p2_birthdate = col_date(format = \"\"),\n  l_p2_age = col_double(),\n  l_p2_hgt = col_double(),\n  l_p2_country = col_character(),\n  l_rank = col_character(),\n  score = col_character(),\n  duration = col_time(format = \"\"),\n  bracket = col_character(),\n  round = col_character(),\n  w_p1_tot_attacks = col_double(),\n  w_p1_tot_kills = col_double(),\n  w_p1_tot_errors = col_double(),\n  w_p1_tot_hitpct = col_double(),\n  w_p1_tot_aces = col_double(),\n  w_p1_tot_serve_errors = col_double(),\n  w_p1_tot_blocks = col_double(),\n  w_p1_tot_digs = col_double(),\n  w_p2_tot_attacks = col_double(),\n  w_p2_tot_kills = col_double(),\n  w_p2_tot_errors = col_double(),\n  w_p2_tot_hitpct = col_double(),\n  w_p2_tot_aces = col_double(),\n  w_p2_tot_serve_errors = col_double(),\n  w_p2_tot_blocks = col_double(),\n  w_p2_tot_digs = col_double(),\n  l_p1_tot_attacks = col_double(),\n  l_p1_tot_kills = col_double(),\n  l_p1_tot_errors = col_double(),\n  l_p1_tot_hitpct = col_double(),\n  l_p1_tot_aces = col_double(),\n  l_p1_tot_serve_errors = col_double(),\n  l_p1_tot_blocks = col_double(),\n  l_p1_tot_digs = col_double(),\n  l_p2_tot_attacks = col_double(),\n  l_p2_tot_kills = col_double(),\n  l_p2_tot_errors = col_double(),\n  l_p2_tot_hitpct = col_double(),\n  l_p2_tot_aces = col_double(),\n  l_p2_tot_serve_errors = col_double(),\n  l_p2_tot_blocks = col_double(),\n  l_p2_tot_digs = col_double()\n)\n\nraw_df &lt;- c(\"https://raw.githubusercontent.com/BigTimeStats/beach-volleyball/master/data/match_archive_2000_to_2017_v2.csv\",\n            \"https://raw.githubusercontent.com/BigTimeStats/beach-volleyball/master/data/match_update_20170729_to_20170912.csv\",\n            \"https://raw.githubusercontent.com/BigTimeStats/beach-volleyball/master/data/match_update_20170913_to_20180314.csv\",\n            \"https://raw.githubusercontent.com/BigTimeStats/beach-volleyball/master/data/match_update_20180315_to_20180821.csv\",\n            \"https://raw.githubusercontent.com/BigTimeStats/beach-volleyball/master/data/match_update_20180822_to_20190409.csv\",\n            \"https://raw.githubusercontent.com/BigTimeStats/beach-volleyball/master/data/match_update_20190410_to_20190818.csv\",\n            \"https://raw.githubusercontent.com/BigTimeStats/beach-volleyball/master/data/match_update_20190818_to_20190902.csv\") %&gt;% \n  map_dfr(read_csv, col_types = col_types_vb)\n  \nraw_df %&gt;% \n  skimr::skim()\n  \n# Georgios Karamanis noticed that the birthdates are \n# incorrect for anyone born before 1970 (off by 100 years)\nclean_df &lt;- mutate_at(\n  raw_df,\n  vars(contains(\"birthdate\")),\n  list(~ if_else(. &gt;= as.Date(\"2020-01-01\"),\n    . - lubridate::years(100),\n    .\n  ))\n)\n\nwrite_csv(clean_df, \"2020/2020-05-19/vb_matches.csv\")"
  },
  {
    "objectID": "data/2020/2020-05-05/readme.html",
    "href": "data/2020/2020-05-05/readme.html",
    "title": "Animal Crossing - New Horizons",
    "section": "",
    "text": "Animal crossing villagers on a bridge with Tom Nook\n\n\n\nAnimal Crossing - New Horizons\nThe data this week comes from the VillagerDB and Metacritic. VillagerDB brings info about villagers, items, crafting, accessories, including links to their images. Metacritic brings user and critic reviews of the game (scores and raw text).\nPer Wikipedia:\n\nAnimal Crossing: New Horizons is a 2020 life simulation video game developed and published by Nintendo for the Nintendo Switch. It is the fifth main series title in the Animal Crossing series. New Horizons was released in all regions on March 20, 2020.\n\n\nNew Horizons sees the player assuming the role of a customizable character who moves to a deserted island after purchasing a package from Tom Nook, a tanuki character who has appeared in every entry in the Animal Crossing series. Taking place in real-time, the player can explore the island in a nonlinear fashion, gathering and crafting items, catching insects and fish, and developing the island into a community of anthropomorphic animals.\n\nAnimal Crossing as explained by a Polygon opinion piece.\n\nWith just a few design twists, the work behind collecting hundreds or even thousands of items over weeks anpd months becomes an exercise of mindfulness, predictability, and agency that many players find soothing instead of annoying.\n\n\nGames that feature gentle progression give us a sense of progress and achievability, teaching us that putting in a little work consistently while taking things one step at a time can give us some fantastic results. It’s a good life lesson, as well as a way to calm yourself and others, and it’s all achieved through game design.\n\nPotential Analyses: * Reviews: Sentiment analysis, text analysis, scores, date effect * Villagers/Items: Gender, species, sayings, personality, price, recipe, what about a star sign based off the birthday column?\nSome potential context for user_reviews.tsv from 538 and a point of potential strife via Animal Crossing World, and lastly a spoiler article analyzing the reviews in R by Boon Tan.\nPS there is an easter egg somewhere in the readme - something to do with… turnips.\n\nGet the data here\n# Get the Data\n\ncritic &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-05/critic.tsv')\nuser_reviews &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-05/user_reviews.tsv')\nitems &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-05/items.csv')\nvillagers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-05/villagers.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-05-05')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 19)\n\n\ncritic &lt;- tuesdata$critic\n\n\nData Dictionary\n\n\n\ncritic.tsv\nSource\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngrade\ninteger\n0-100 score given by the critic (missing for some) where higher score = better.\n\n\npublication\ncharacter\nThe source of the review\n\n\ntext\ncharacter\nRaw text describing the review.\n\n\ndate\ndouble\nDate review published\n\n\n\n\n\nuser_reviews.tsv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngrade\ninteger\nRaw score (0-10) where higher score = better.\n\n\nuser_name\ncharacter\nUser name of reviewer\n\n\ntext\ncharacter\nRaw text of the review\n\n\ndate\ndouble\nDate review published.\n\n\n\n\n\nvillagers.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrow_n\ninteger\nrow_n is a numerical ID\n\n\nid\ncharacter\nid is a short text identifier\n\n\nname\ncharacter\nname of the villager\n\n\ngender\ncharacter\ngender of the villager\n\n\nspecies\ncharacter\nspecies of the villager\n\n\nbirthday\ncharacter\nbirthday of the villager (month-day)\n\n\npersonality\ncharacter\nPersonality\n\n\nsong\ncharacter\nSong associated with the villager\n\n\nphrase\ncharacter\nCatchphraase of the villager\n\n\nfull_id\ncharacter\nFull text id of villager\n\n\nurl\ncharacter\nLink to image of the villager\n\n\n\n\n\nitems.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nnum_id\ninteger\nNumerical id - note that some items have multiple rows as they have multiple recipe items\n\n\nid\ncharacter\nCharacter id\n\n\nname\ncharacter\nName of the item\n\n\ncategory\ncharacter\nCategory of item (eg furniture, clothing, etc\n\n\norderable\nlogical\nOrderable from catalogue\n\n\nsell_value\ninteger\nsell value\n\n\nsell_currency\ncharacter\nsell currency\n\n\nbuy_value\ninteger\nbuy value\n\n\nbuy_currency\ncharacter\nbuy currency\n\n\nsources\ncharacter\nway to acquire or person/place to acquire from\n\n\ncustomizable\ncharacter\nIs it customizable?\n\n\nrecipe\ninteger\nRecipe number\n\n\nrecipe_id\ncharacter\nRecipe ID\n\n\ngames_id\ncharacter\ngame id\n\n\nid_full\ncharacter\nFull character id\n\n\nimage_url\ncharacter\nLink to image of item\n\n\n\n\nCleaning Script\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(listviewer)\n\nurl &lt;- \"https://github.com/jefflomacy/villagerdb/tree/master/data/items\"\n\nall_villagers &lt;- list.files(\"villagerdb-master/data/villagers\")\n\nvillage_read &lt;- function(file_name){\n  fromJSON(here::here(\"villagerdb-master/data/villagers\", file_name))\n}\n\nitem_read &lt;- function(file_name){\n  fromJSON(here::here(\"villagerdb-master/data/items\", file_name))\n}\n\n\njson_list &lt;- all_villagers %&gt;% \n  map(village_read)\n\nlistviewer::jsonedit(json_com)\n\nclean_villagers &lt;- json_list %&gt;% \n  enframe() %&gt;% \n  rename(row_n = name) %&gt;% \n  unnest_wider(value) %&gt;% \n  unnest_longer(games) %&gt;% \n  unnest_wider(games) %&gt;% \n  unnest_wider(coffee) %&gt;% \n  select(-...1) %&gt;% \n  rename(coffee_beans = beans, coffee_milk = milk, coffee_sugar = sugar) %&gt;% janitor::clean_names() %&gt;% \n  filter(games_id == \"nh\") %&gt;% \n  select(row_n, id, name, gender:personality, song, phrase)\n\nfinal_villagers &lt;- left_join(clean_villagers, villager_db_villagers_images %&gt;% \n  mutate(full_id = id, \n         id = str_remove(id, \"villager-\")) %&gt;% \n  select(full_id, id, name, url),\n  by = c(\"name\", \"id\"))\n\nfinal_villagers %&gt;% \n  write_csv(\"2020/2020-05-05/villagers.csv\")\n\n# Read and clean item JSON ------------------------------------------------\n\n\n\nall_items &lt;- list.files(\"villagerdb-master/data/items\")\n\nitems_list &lt;- all_items %&gt;% \n  map(item_read)\n\njsonedit(items_list)\n\nitems_nh &lt;- items_list %&gt;% \n  enframe() %&gt;% \n  rename(row_n = name) %&gt;% \n  unnest_wider(value) \n\nitems_price &lt;- items_nh %&gt;% \n  unnest_longer(games) %&gt;% \n  unnest_wider(games) %&gt;% \n  unnest_wider(sellPrice) %&gt;% \n  rename(sell_value = value, sell_currency = currency) %&gt;% \n  select(-...1) %&gt;% \n  unnest_wider(buyPrices) %&gt;% \n  select(-...1)\n\nitems_long &lt;- items_price %&gt;% \n  unnest_longer(recipe) %&gt;% \n  mutate(customizable = unlist(customizable)) %&gt;% \n  unnest_longer(sources) %&gt;% \n  unnest_longer(interiorThemes)\n\nbuy_long &lt;- items_long %&gt;% \n  unnest_wider(currency) %&gt;% \n  rename(buy_price_1 = ...1,\n         buy_price_2 = ...2)\n\nbuy_df_wide &lt;- buy_long %&gt;% \n  unnest_wider(value) %&gt;% \n  rename(buy_currency_1 = ...1,\n         buy_currency_2 = ...2)\n\ncurrency_2 &lt;- buy_df_wide %&gt;% \n  filter(!is.na(buy_currency_2)) %&gt;% \n  select(-buy_price_1, -buy_currency_1) %&gt;% \n  rename(buy_value = buy_price_2, buy_currency = buy_currency_2)\n\nitem_df_final &lt;- buy_df_wide %&gt;% \n  select(-buy_currency_2, -buy_price_2) %&gt;%\n  rename(buy_value = buy_price_1, buy_currency = buy_currency_1) %&gt;% \n  bind_rows(currency_2) %&gt;% \n  arrange(row_n, id) %&gt;% \n  rename(buy_cur = buy_currency, buy_val = buy_value) %&gt;% \n  rename(buy_value = buy_cur, buy_currency = buy_val) %&gt;% \n  unnest_longer(rvs) %&gt;% \n  filter(games_id == \"nh\")\n\nitem_df_final\n\njoined_img_df &lt;- item_df_final %&gt;% \n  left_join(all_items, by = c(\"id\", \"name\")) %&gt;% \n  select(num_id = row_n, id:orderable, sell_value, sell_currency, buy_value, buy_currency, sources, customizable, recipe:id_full, image_url = url, -xSize, -ySize)\n\njoined_img_df %&gt;% \n  write_csv(\"2020/2020-05-05/items.csv\")\n                                                                                                       \n                         \n                         \n                         \n                                                                                                       \n\n\nBONUS SUPER SECRET DATASET\nKeep going if you wanna learn about the turnip market.\n\n                         \n                         \n                         \n                         \n                         \n                         \n\n\n\ntornps - image of Tom Nook imitating the stonks meme\n\n\n\n\n\nWARNING\nPlease note that this may be bordering on making the game a type of “work” - so feel free to skip if you don’t want to think about the game THIS hard.\n                         \n                         \n                         \n                         \n                         \n                         \n\nSERIOUSLY ENJOY THE GAME HOWEVER YOU WANT\nIf you want to continue please see the below for context and some scraping code for an example plot in R.\n                         \n                         \n                         \n\n\nOk here is the easter egg\nThis is an example dataset from GameWith of example turnip price graphs and additional info from Polygon. Lastly - The Verge also dives into Turnip price watch groups - links to The Stalk Market.\nThere appear to be 3-4 types of turnip price trends. * Random: Price fluctuates without clear pattern * Spike: Price declines for a few days and then jumps up 3x before quickly declining * Crash: Price increases early and then crashes * Decline: Price constantly decreases across week\n# Turnip price graphs examples\n\nraw_turnip &lt;- read_html(turnip)\n\ncooked_turnips &lt;- raw_turnip %&gt;% \n  html_nodes(\"div.acnh_kabu &gt; table\") %&gt;% \n  html_table() %&gt;% \n  bind_rows() %&gt;% \n  as_tibble() %&gt;% \n  rename(\"time\" = ...1) %&gt;% \n  slice(3:10) %&gt;% \n  group_by(time) %&gt;% \n  mutate(week = row_number()) %&gt;% \n  ungroup() %&gt;% \n  pivot_longer(cols = Mon:Sat, names_to = \"day\", values_to = \"turnip_price\")\n\n\nturnip_levels &lt;- cooked_turnips %&gt;% \n  distinct(day) %&gt;% \n  pull()\n\ncooked_turnips %&gt;% \n  mutate(day_time = paste(day, time, sep = \"-\"),\n         day_time = factor(day_time, \n                           levels = c(\"Mon-AM\", \"Mon-PM\", \"Tue-AM\",\"Tue-PM\", \n                                      \"Wed-AM\", \"Wed-PM\", \"Thu-AM\", \"Thu-PM\", \n                                      \"Fri-AM\", \"Fri-PM\", \"Sat-AM\" , \"Sat-PM\")),\n         week = factor(week, labels = c(\"Random\", \"Spike\", \"Crash\", \"Declining\"))\n  ) %&gt;% \n  ggplot(aes(x = day_time, y = turnip_price, color = week, group = week)) +\n  geom_line()"
  },
  {
    "objectID": "data/2020/2020-04-21/readme.html",
    "href": "data/2020/2020-04-21/readme.html",
    "title": "GDPR Fines",
    "section": "",
    "text": "Keep calm and comply with GDPR\n\n\n\nGDPR Fines\nh/t to Bob Rudis for sharing the data source, and to Roel Hogervorst for the guide to scraping this data. He provided the bulk of the scraping code, and I added bit of additional data cleaning. The data this week comes from Privacy Affairs.\nI have also included all the raw text (gdpr_text.tsv) for the actual GDPR legal documents, in case someone was interested in parsing through them or using them along with the violations.\nPer Wikipedia GDPR is:\n\nThe General Data Protection Regulation (EU) 2016/679 (GDPR) is a regulation in EU law on data protection and privacy in the European Union (EU) and the European Economic Area (EEA). It also addresses the transfer of personal data outside the EU and EEA areas. The GDPR aims primarily to give control to individuals over their personal data and to simplify the regulatory environment for international business by unifying the regulation within the EU.[1] Superseding the Data Protection Directive 95/46/EC, the regulation contains provisions and requirements related to the processing of personal data of individuals (formally called data subjects in the GDPR) who reside in the EEA, and applies to any enterprise—regardless of its location and the data subjects’ citizenship or residence—that is processing the personal information of data subjects inside the EEA.\n\n\nGet the data here\n# Get the Data\n\ngdpr_violations &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-21/gdpr_violations.tsv')\ngdpr_text &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-21/gdpr_text.tsv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE the tidytuesdayR version after Jan 2020.\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-04-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 17)\n\n\ngdpr_violations &lt;- tuesdata$gdpr_violations\n\n\nData Dictionary\n\n\n\ngdpr_violations.tsv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nIdetifier for fine/violation\n\n\npicture\ncharacter\nSVG image of violation country flag\n\n\nname\ncharacter\nName of country where violation was enforced\n\n\nprice\ninteger\nFine price in Euros (€)\n\n\nauthority\ncharacter\nAuthority that enacted the violation\n\n\ndate\ncharacter\nDate of violation\n\n\ncontroller\ncharacter\nController of data - the violator\n\n\narticle_violated\ncharacter\nSpecific GDPR Article violated (see the gdpr_text.tsv data for specifics)\n\n\ntype\ncharacter\nType of violation\n\n\nsource\ncharacter\nOriginal source (URL) of fine data\n\n\nsummary\ncharacter\nSummary of violation\n\n\n\n\n\ngdpr_text.tsv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nchapter\ndouble\nGDPR Chapter Number\n\n\nchapter_title\ncharacter\nChapter title\n\n\narticle\ndouble\nGDPR Article number\n\n\narticle_title\ncharacter\nArticle title\n\n\nsub_article\ndouble\nSub article number\n\n\ngdpr_text\ncharacter\nRaw text of article/subarticle\n\n\nhref\ncharacter\nURL to the raw text itself\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(rvest)\n\n# Note the following code was adapted from\n# https://blog.rmhogervorst.nl/blog/2020/04/08/scraping-gdpr-fines/\n\nlink &lt;- \"https://www.privacyaffairs.com/gdpr-fines/\"\npage &lt;- read_html(link)\n\n\ntemp &lt;- page %&gt;% html_nodes(\"script\") %&gt;% \n  .[9] %&gt;% \n  rvest::html_text() \n\nends &lt;- str_locate_all(temp, \"\\\\]\")\nstarts &lt;- str_locate_all(temp, \"\\\\[\")\n\ntable1 &lt;- temp %&gt;% \n  stringr::str_sub(start = starts[[1]][1,2], end = ends[[1]][1,1]) %&gt;% \n  str_remove_all(\"\\\\\\n\") %&gt;% \n  str_remove_all(\"\\\\\\r\") %&gt;%\n  jsonlite::fromJSON() %&gt;% \n  as_tibble() %&gt;% \n  mutate(summary = str_remove_all(summary,\"&lt;p&gt;|&lt;/p&gt;|\\n\"))\n\n\ntable2 &lt;- temp %&gt;% \n  stringr::str_sub(start = starts[[1]][2,2], end = ends[[1]][2,1]) %&gt;% \n  str_remove_all(\"\\\\\\n\") %&gt;% \n  str_remove_all(\"\\\\\\r\") %&gt;%  \n  jsonlite::fromJSON() %&gt;% \n  as_tibble() %&gt;% \n  mutate(summary = str_remove_all(summary,\"&lt;p&gt;|&lt;/p&gt;|\\n\"))\n\n\nall_df &lt;- bind_rows(table1, table2) %&gt;% \n  janitor::clean_names() %&gt;%\n  mutate(\n    authority = str_remove(authority, \"\\t\"),\n    article_violated = str_remove(article_violated, '&lt;a href=\"https://www.privacy-regulation.eu/en/32.htm\"&gt;') %&gt;% \n           str_remove('&lt;/a&gt;'),\n    article_violated = str_replace_all(article_violated, \", Art\", \"|Art\"),\n    type = str_remove(type, '&lt;a href=\"https://www.privacy-regulation.eu/en/32.htm\"&gt;') %&gt;% \n      str_remove('&lt;/a&gt;')\n           )\n\n# most frequent articles violated\nall_df %&gt;% \n  separate_rows(article_violated, sep = \"\\\\|\") %&gt;% \n  count(article_violated, sort = T)\n\nall_df %&gt;% \n  write_tsv(\"2020/2020-04-21/gdpr_violations.tsv\")\n\n\n# Getting the actual article text -----------------------------------------\n\nraw_article &lt;- \"https://gdpr-info.eu/\" %&gt;% \n  read_html()\n\n# Get all the urls for specific articles/chapters\ngdpr_href &lt;- raw_article %&gt;% \n  html_node(xpath = '//*[@id=\"tablepress-12\"]') %&gt;% \n  html_nodes(\"a\") %&gt;% \n  html_attr(\"href\")\n\n# pull the titles as well\ngdpr_titles &lt;- raw_article %&gt;% \n  html_node(xpath = '//*[@id=\"tablepress-12\"]') %&gt;% \n  html_nodes(\"a\") %&gt;% \n  html_attr(\"data-title\")\n\n# pull the numbers of article/chapters\ngdpr_numbers &lt;- raw_article %&gt;% \n  html_node(xpath = '//*[@id=\"tablepress-12\"]') %&gt;% \n  html_nodes(\"a\") %&gt;% \n  html_text()\n\n# put it all into a df\ngdpr_df &lt;- tibble(\n  article = gdpr_numbers,\n  title = str_trim(gdpr_titles),\n  href = gdpr_href\n) \n\n# Tidy up the data, create chapters vs articles\nclean_gdpr &lt;- gdpr_df %&gt;% \n  mutate(chapter = if_else(str_length(article) &gt; 3, article, NA_character_),\n         chapter_title = if_else(str_length(article) &gt; 3, title, NA_character_)) %&gt;% \n  fill(chapter, chapter_title) %&gt;% \n  filter(!str_detect(article, \"Chapter\")) %&gt;% \n  mutate(article = as.double(article)) %&gt;% \n  filter(!is.na(article)) %&gt;% \n  select(starts_with(\"chapter\"), article, article_title = title, href)\n\nclean_gdpr\n\n# LONG running outcome\n# Get all the raw html from each of the urls for each article\nall_articles &lt;- clean_gdpr %&gt;% \n  mutate(raw_html = map(href, read_html))\n\n# function to take raw html and turn it into text for that specific article\nget_gdpr_text &lt;- function(html_in){\n  \n  test_var &lt;- html_in %&gt;% \n    html_node(\".entry-content\") %&gt;% \n    html_nodes(\"ol\") %&gt;% \n    html_text()\n  \n  if (length(test_var) == 0){\n   text &lt;- html_in %&gt;%\n     html_node(\".entry-content &gt; p\") %&gt;% \n     html_text() %&gt;% \n     str_remove(\"^[:digit:]\") \n  } else {\n    text &lt;- html_in %&gt;% \n      html_node(\".entry-content\") %&gt;% \n      html_nodes(\"ol\") %&gt;% \n      html_text() %&gt;% \n      .[[1]] %&gt;% \n      str_replace_all(\";\\n\", \"\\t\") %&gt;% \n      str_replace_all(\":\\n\", \"\\t\") %&gt;% \n      str_split(\"\\n\") %&gt;% \n      .[[1]] %&gt;% \n      .[. != \"\"] %&gt;% \n      str_replace_all(\"\\t\", \"\\n\") %&gt;% \n      str_remove(\"^[:digit:]\")\n  }\n  \n  \n  text\n    \n}\n\n# Test\nget_gdpr_text(read_html(\"http://gdpr-info.eu/art-2-gdpr/\"))\n\n# unnest the list column of text\nclean_articles &lt;- all_articles %&gt;% \n  mutate(gdpr_text = map(raw_html, get_gdpr_text)) %&gt;% \n  unnest_longer(gdpr_text)\n\n# final dataframe\nfinal_articles &lt;- clean_articles %&gt;% \n  group_by(article) %&gt;% \n  mutate(sub_article = row_number()) %&gt;% \n  relocate(sub_article, .after = \"article_title\") %&gt;% \n  relocate(gdpr_text, .after = \"sub_article\") %&gt;% \n  ungroup() %&gt;% \n  mutate(chapter = str_extract(chapter, \"[:digit:]+\")) %&gt;% \n  mutate_at(vars(chapter, article, sub_article), as.double) %&gt;% \n  select(-raw_html)\n\nfinal_articles %&gt;% view()\n\nwrite_tsv(final_articles, \"2020/2020-04-21/gdpr_text.tsv\")"
  },
  {
    "objectID": "data/2020/2020-04-07/readme.html",
    "href": "data/2020/2020-04-07/readme.html",
    "title": "tdf_winners.csv",
    "section": "",
    "text": "# Tour de France\n\nThe Tour de France is an annual men’s multiple stage bicycle race primarily held in France, while also occasionally passing through nearby countries. Like the other Grand Tours (the Giro d’Italia and the Vuelta a España), it consists of 21 day-long stages over the course of 23 days. It has been described as “the world’s most prestigious and most difficult bicycle race”.\n\nThe data this week comes from Alastair Rushworth’s Data Package tdf and Kaggle.\nAlastair has a very nice walkthrough of his data package at his blog!\nI’ve added the Kaggle data which goes through 2017 for some additional stage-specific data not captured in his dataset. Please note that for the most part these datasets COULD be joined by year/edition.\nSome other stats and records can be found on Wikipedia.\n\nGet the data here\n# Get the Data\n\ntdf_winners &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-07/tdf_winners.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-04-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 15)\n\n\ntdf_winners &lt;- tuesdata$tdf_winners\n\n\nData Dictionary\n\n\ntdf_winners.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nedition\ninteger\nEdition of the Tour de France\n\n\nstart_date\ndouble\nStart date of the Tour\n\n\nwinner_name\ncharacter\nWinner’s name\n\n\nwinner_team\ncharacter\nWinner’s team (NA if not on a team)\n\n\ndistance\ndouble\nDistance traveled in KM across the entire race\n\n\ntime_overall\ndouble\nTime in hours taken by the winner to complete the race\n\n\ntime_margin\ndouble\nDifference in finishing time between the race winner and the runner up\n\n\nstage_wins\ndouble\nNumber of stage wins (note that it is possible to win the GC without winning any stages at all)\n\n\nstages_led\ndouble\nStages led is the number of stages spent as the race leader (wearing the yellow jersey) by the eventual winner\n\n\nheight\ndouble\nHeight in meters\n\n\nweight\ndouble\nWeight in kg\n\n\nage\ninteger\nAge as winner\n\n\nborn\ndouble\nyear born\n\n\ndied\ndouble\nYear died\n\n\nfull_name\ncharacter\nFull name\n\n\nnickname\ncharacter\nNickname\n\n\nbirth_town\ncharacter\nBirth town\n\n\nbirth_country\ncharacter\nBirth country\n\n\nnationality\ncharacter\nNationality\n\n\n\n\n\nstage_data.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nedition\ninteger\nRace edition\n\n\nyear\ndouble\nYear of race\n\n\nstage_results_id\ncharacter\nStage ID\n\n\nrank\ncharacter\nRank of racer for stage\n\n\ntime\ndouble\nTime of racer\n\n\nrider\ncharacter\nRider name\n\n\nage\ninteger\nAge of racer\n\n\nteam\ncharacter\nTeam (NA if not on team)\n\n\npoints\ninteger\nPoints for the stage\n\n\nelapsed\ndouble\nTime elapsed stored as lubridate::period\n\n\nbib_number\ninteger\nBib number\n\n\n\n\n\ntdf_stages.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nStage\ncharacter\nStage Number\n\n\nDate\ndouble\nDate of stage\n\n\nDistance\ndouble\nDistance in KM\n\n\nOrigin\ncharacter\nOrigin city\n\n\nDestination\ncharacter\nDestination city\n\n\nType\ncharacter\nStage Type\n\n\nWinner\ncharacter\nWinner of the stage\n\n\nWinner_Country\ncharacter\nWinner’s nationality\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(tdf) # install at: https://github.com/alastairrushworth/tdf\n\nwinners &lt;- tdf::editions %&gt;% \n  select(-stage_results)\n\nall_years &lt;- tdf::editions %&gt;% \n  unnest_longer(stage_results) %&gt;% \n  mutate(stage_results = map(stage_results, ~ mutate(.x, rank = as.character(rank)))) %&gt;% \n  unnest_longer(stage_results) \n\nstage_all &lt;- all_years %&gt;% \n  select(stage_results) %&gt;% \n  flatten_df()\n\ncombo_df &lt;- bind_cols(all_years, stage_all) %&gt;% \n  select(-stage_results)\n\nstage_clean &lt;- combo_df %&gt;% \n  select(edition, start_date,stage_results_id:last_col()) %&gt;% \n  mutate(year = lubridate::year(start_date)) %&gt;% \n  rename(age = age...25) %&gt;% \n  select(edition, year, everything(), -start_date)\n\nwinners %&gt;% \n  write_csv(here::here(\"2020\", \"2020-04-07\", \"tdf_winners.csv\"))\n\nstage_clean %&gt;% \n  write_csv(here::here(\"2020\", \"2020-04-07\", \"stage_data.csv\"))"
  },
  {
    "objectID": "data/2020/2020-03-24/readme.html",
    "href": "data/2020/2020-03-24/readme.html",
    "title": "Traumatic Brain Injury (TBI)",
    "section": "",
    "text": "Traumatic Brain Injury (TBI)\n\nBrain Injury Awareness Month, observed each March, was established 3 decades ago to educate the public about the incidence of brain injury and the needs of persons with brain injuries and their families (1). Caused by a bump, blow, or jolt to the head, or penetrating head injury, a traumatic brain injury (TBI) can lead to short- or long-term changes affecting thinking, sensation, language, or emotion. - CDC\n\nThe goal of this week’s #TidyTuesday is to spread awareness for just how common TBIs are - both in civilian and military populations.\n\nOne of every 60 people in the U.S. lives with a TBI related disability. Moderate and severe traumatic brain injury (TBI) can lead to a lifetime of physical, cognitive, emotional, and behavioral changes.\n\nIf you want to share an infographic or summary graphic from this data - please consider using the awareness hashtag: #ChangeYourMind, #braininjuryawarenessmonth, or tagging the Brain Injury Association. More details can be found at the Brain Injury Association Website.\nThe data this week comes from the CDC and Veterans Brain Injury Center. Additional stats can be found at CDC.gov.\nThis data and cleaning script are primarily from scraping tables out of a PDF. This would be a good example of trying to clean and organize tables from PDFs, using the pdftools package from ropensci. I have included the PDFs this data was scraped from, and there are lots of examples of potential graphs to recreate or improve upon. Try your hand at improving or otherwise learning how to use packages like stringr or tidyr to extract data from messy PDF tables.\n\nGet the data here\n# Get the Data\n\ntbi_age &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-24/tbi_age.csv')\ntbi_year &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-24/tbi_year.csv')\ntbi_military &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-24/tbi_military.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-03-24')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 13)\n\n\ntbi_age &lt;- tuesdata$tbi_age\n\n\nData Dictionary\n\n\n\ntbi_age.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nage_group\ncharacter\nAge group\n\n\ntype\ncharacter\nType of measure\n\n\ninjury_mechanism\ncharacter\nInjury mechanism\n\n\nnumber_est\ndouble\nEstimated observed cases in 2014\n\n\nrate_est\ndouble\nRate/100,000 in 2014\n\n\n\n\n\ntbi_year.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ninjury_mechanism\ncharacter\nInjury mechanism\n\n\ntype\ncharacter\nType of measure\n\n\nyear\ncharacter\nYear\n\n\nrate_est\ndouble\nRate/100,000 in 2014\n\n\nnumber_est\ninteger\nEstimated observed cases in each year\n\n\n\n\n\ntbi_military.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nservice\ncharacter\nMilitary branch\n\n\ncomponent\ncharacter\nMilitary component (active, guard, reserve)\n\n\nseverity\ncharacter\nSeverity/type of TBI\n\n\ndiagnosed\ndouble\nNumber diagnosed\n\n\nyear\ninteger\nYear for observation\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(rvest)\n\n\n# Hospital TBI data -------------------------------------------------------\n\nall_years &lt;- read_html(\"https://www.cdc.gov/traumaticbraininjury/data/tbi-edhd.html\") %&gt;% \n  html_node(xpath = '//*[@id=\"acc-panel-1\"]/div/div/table/thead/tr') %&gt;% \n  html_text() %&gt;% \n  str_split(\"\\\\\\n\") %&gt;% \n  str_extract_all(\"[0-9]+\") %&gt;% \n  simplify()\n\nhospital_url &lt;- \"https://www.cdc.gov/traumaticbraininjury/data/tbi-edhd.html\"\n  \nraw_html_hospital &lt;- read_html(url) \n\nget_table_row &lt;- function(input_html, tab_num){\n  \n  node &lt;- html_node(input_html, xpath = glue::glue('//*[@id=\"acc-panel-1\"]/div/div/table/tbody/tr[{tab_num}]'))\n  \n  html_text(node)\n}\n\ntbi_hospital &lt;- tibble(\n  input_html = list(raw_html_hospital),\n  tab_num = 1:4\n) %&gt;% \n  mutate(raw_data = map2_chr(input_html, tab_num, get_table_row)) %&gt;% \n  separate(raw_data, into = c(\"type\", all_years), sep = \"\\\\\\n\") %&gt;% \n  select(-tab_num,-input_html) %&gt;% \n  pivot_longer(cols = -type, values_to = \"value\", names_to = \"year\") %&gt;% \n  mutate(value = parse_number(value),\n         year = as.double(year))\n\n\n\n# TBI Deaths --------------------------------------------------------------\n\n\n\nurl_deaths &lt;- \"https://www.cdc.gov/traumaticbraininjury/data/tbi-deaths.html\"\n\nraw_html_deaths &lt;- read_html(url_deaths)\n\ntbi_deaths &lt;- tibble(\n  tab_num = 1:7,\n  input_html = list(raw_html_deaths)\n) %&gt;% \n  mutate(raw_data = map2_chr(input_html, tab_num, get_table_row)) %&gt;% \n  separate(raw_data, into = c(\"type\", all_years), sep = \"\\\\\\n\") %&gt;% \n  select(-tab_num,-input_html) %&gt;% \n  mutate(type = str_remove(type, \"††|‡‡|§§\")) %&gt;% \n  pivot_longer(cols = -type, values_to = \"value\", names_to = \"year\") %&gt;% \n  mutate(value = parse_number(value),\n         year = as.double(year))\n\n\n\n# ED Visits ---------------------------------------------------------------\n\nurl_ed &lt;- \"https://www.cdc.gov/traumaticbraininjury/data/tbi-ed-visits.html\"\n\nraw_html_ed &lt;- read_html(url_ed)\n\ntbi_ed_visits &lt;- tibble(\n  tab_num = 1:7,\n  input_html = list(raw_html_ed)\n) %&gt;% \n  mutate(raw_data = map2_chr(input_html, tab_num, get_table_row)) %&gt;% \n  separate(raw_data, into = c(\"type\", all_years), sep = \"\\\\\\n\") %&gt;% \n  select(-tab_num,-input_html) %&gt;% \n  mutate(type = str_remove(type, \"††|‡‡|§§\")) %&gt;% \n  pivot_longer(cols = -type, values_to = \"value\", names_to = \"year\") %&gt;% \n  mutate(value = parse_number(value),\n         year = as.double(year))\n\n\n# Hospitalizations --------------------------------------------------------\n\nurl_hosp &lt;- \"https://www.cdc.gov/traumaticbraininjury/data/tbi-hospitalizations.html\"\n\nraw_html_hosp &lt;- read_html(url_hosp)\n\ntbi_hosp &lt;- tibble(\n  tab_num = 1:7,\n  input_html = list(raw_html_hosp)\n) %&gt;% \n  mutate(raw_data = map2_chr(input_html, tab_num, get_table_row)) %&gt;% \n  separate(raw_data, into = c(\"type\", all_years), sep = \"\\\\\\n\") %&gt;% \n  select(-tab_num,-input_html) %&gt;% \n  mutate(type = str_remove(type, \"††|‡‡|§§\")) %&gt;% \n  pivot_longer(cols = -type, values_to = \"value\", names_to = \"year\") %&gt;% \n  mutate(value = parse_number(value),\n         year = as.double(year))\n\n\n# PDF report --------------------------------------------------------------\n\npdf_url &lt;- \"https://www.cdc.gov/traumaticbraininjury/pdf/TBI-Surveillance-Report-FINAL_508.pdf\"\n\nraw_pdf_text &lt;- pdftools::pdf_text(pdf_url)\n\n\n# General table scraping function -----------------------------------------\n\nclean_pdf_table &lt;- function(table_number){\n  \n  input_text &lt;- chuck(raw_pdf_text, table_number)\n  \n  raw_table_text &lt;- str_split(raw_pdf_text[[table_number]], \n                              \"\\\\)\\\\\\n|\\\\*\\\\\\n\", simplify = TRUE)\n  \n  vec_length &lt;- c(\"0-1\", \"0-4\", \"5-1\", \"15-\", \"25-\", \"35-\", \"45-\", \"55-\", \"65-\",\n                  \"75+\", \"Tot\") %&gt;% paste(collapse = \"|\")\n  \n  vec_subset &lt;- str_detect(str_sub(raw_table_text, 1, 4), vec_length)\n  \n  small_tab &lt;- raw_table_text[vec_subset]\n  \n  if(length(small_tab) &lt;= 2) stop(\"No table found!\")\n  \n  pre_tab &lt;- small_tab %&gt;% \n    str_squish() %&gt;% \n    str_replace_all(\" \\\\(\", \"_(\") %&gt;% \n    str_replace_all(\"\\\\s\", \"|\") %&gt;% \n    str_replace_all(\",\", \"\")\n  \n  text_con &lt;- textConnection(pre_tab)\n  \n  output_df &lt;- read.csv(text_con, header = FALSE, sep = \"|\", stringsAsFactors = FALSE)\n  \n  tibble_df &lt;- output_df %&gt;% \n    as_tibble() %&gt;% \n    mutate_all(str_replace, \"##|¶¶\", \"\")\n  \n  tibble_fixed &lt;- tibble_df %&gt;% \n    mutate_at(.vars = vars(2:last_col()),\n              str_extract, \"[^_]+\")\n  \n  tibble_typed &lt;- suppressMessages(type_convert(tibble_fixed))\n  \n  tibble_typed\n}\n\n# PDF Tab 1 ---------------------------------------------------------------\n\n# This was a first attempt before moving to a function\n\nraw_pdf_text[[9]]\n\nraw_table_text &lt;- str_split(raw_pdf_text[[9]], \"\\\\\\n\", simplify = TRUE)\n\npre_table &lt;- raw_table %&gt;% \n  .[9:19] %&gt;% \n  str_squish() %&gt;% \n  str_replace_all(\" \\\\(\", \"_(\") %&gt;% \n  str_replace_all(\"\\\\s\", \"|\")\n\ntext_con &lt;- textConnection(pre_table)\ndata_table &lt;- read.csv(text_con, sep = \"|\", header = FALSE, stringsAsFactors = FALSE)\n\ntab_1 &lt;- data_table %&gt;% \n  set_names(nm = c(\"age_group\", \"ed_visit\", \"ed_visit_rate\", \"hospitilizations\",\n                   \"hospitilization_rate\", \"death_no\", \"death_rate\", \"total_edhd\", \n                   \"total_edhd_rate\")) %&gt;%\n  as_tibble() %&gt;% \n  mutate_all(as.character) %&gt;% \n  mutate_at(vars(contains(\"rate\")), str_extract, \"[^_]+\") %&gt;% \n  mutate_at(vars(ed_visit:total_edhd_rate), parse_number) %&gt;% \n  add_column(type = \"ED Visits, Hospitalizations, and deaths\", .after = \"age_group\")\n\ntab_1\n\n\n# PDF Tab 2 ---------------------------------------------------------------\n\nclean_pdf_table(10)\n\ntab_2 &lt;- clean_pdf_table(10) %&gt;% \n  set_names(nm = c(\"age_group\", \"num_crash\", \"rate_crash\", \"num_falls\",\n                   \"rate_falls\", \"num_struck\", \"rate_struck\", \"num_inj_unspecified\", \n                   \"rate_inj_unspecified\", \"num_self_harm\", \"rate_self_harm\",\n                   \"num_assault\", \"rate_assault\", \"num_other\", \"rate_other\")) %&gt;% \n  add_column(type = \"Emergency Department Visit\", .after = \"age_group\")\n\ntab_2\n  \n\n# PDF Tab 3 ---------------------------------------------------------------\n\ntab_3 &lt;- clean_pdf_table(11) %&gt;% \n  set_names(nm = c(\"age_group\", \"num_crash\", \"rate_crash\", \"num_falls\",\n                   \"rate_falls\", \"num_struck\", \"rate_struck\", \"num_inj_unspecified\", \n                   \"rate_inj_unspecified\", \"num_self_harm\", \"rate_self_harm\",\n                   \"num_assault\", \"rate_assault\", \"num_other\", \"rate_other\")) %&gt;% \n  add_column(type = \"Hospitalizations\", .after = \"age_group\")\n\ntab_3\n\n\n# PDF Tab 4 ---------------------------------------------------------------\n\ntab_4 &lt;- clean_pdf_table(12) %&gt;% \n  set_names(nm = c(\"age_group\", \"num_crash\", \"rate_crash\", \"num_falls\",\n                   \"rate_falls\", \"num_struck\", \"rate_struck\", \"num_inj_unspecified\", \n                   \"rate_inj_unspecified\", \"num_self_harm\", \"rate_self_harm\",\n                   \"num_assault\", \"rate_assault\", \"num_other\", \"rate_other\")) %&gt;% \n  add_column(type = \"Deaths\", .after = \"age_group\")\n\ntab_4\n\n\n# PDF Combo Table ---------------------------------------------------------\n\ncombo_tab &lt;- bind_rows(tab_2, tab_3, tab_4)\n\nview(combo_tab)\n\ncombo_tab\n\nnum_df &lt;- combo_tab %&gt;% \n  pivot_longer(cols = contains(\"num\"), names_to = \"injury_mechanism\",\n               values_to = \"number_est\") %&gt;% \n  select(-contains(\"rate\")) %&gt;% \n  separate(injury_mechanism, into = c(\"count_type\", \"injury_mechanism\"), \n           sep = \"_\", extra = \"merge\")\n\nrate_df &lt;- combo_tab %&gt;% \n  pivot_longer(cols = contains(\"rate\"), names_to = \"injury_mechanism\",\n               values_to = \"rate_est\") %&gt;% \n  select(-contains(\"num\")) %&gt;% \n  separate(injury_mechanism, into = c(\"count_type\", \"injury_mechanism\"), \n           sep = \"_\", extra = \"merge\")\n\nlong_combo_df &lt;- left_join(num_df, rate_df, \n                           by = c(\"age_group\", \"type\", \"injury_mechanism\")) %&gt;% \n  select(-contains(\"count_type\")) %&gt;% \n  mutate(\n    injury_mechanism = factor(\n      injury_mechanism,\n      levels = c( \"crash\", \"falls\", \"struck\", \"inj_unspecified\",\"self_harm\",\n                  \"assault\", \"other\"),\n      labels = c(\"Motor Vehicle Crashes\", \"Unintentional Falls\", \n                 \"Unintentionally struck by or against an object\",\n                 \"Other unintentional injury, mechanism unspecified\",\n                 \"Intentional self-harm\", \"Assault\", \"Other or no mechanism specified\"\n                 )\n      ),\n    injury_mechanism = as.character(injury_mechanism)\n    )\n\n\n# Other PDF Tables --------------------------------------------------------\n\n\nget_graph_table &lt;- function(page_number){\n  \n  titles &lt;- c(\n    \"Motor vehicle crashes\", \"Unintentional falls\", \"Unintentionally struck by or against an object\",\n    \"Other unintentional injury, mechanism unspecified\",\"Intentional self-harm\",\n    \"Assault\", \"Other or no mechanism specified\", \"Total\"\n  ) \n  \n  titles_collapse &lt;- titles %&gt;% paste0(collapse = \"|\")\n\n  \n  raw_table_text &lt;- raw_pdf_text[[page_number]] %&gt;% \n    str_remove_all(\"††\\\\\\n|§§|‡‡\\\\\\n|††|‡‡\") %&gt;% \n    str_split(\"\\\\\\n\", simplify = TRUE) %&gt;% \n    str_squish() %&gt;% \n    str_replace_all(titles_collapse, \"type\") %&gt;% \n    str_remove_all(\",\") %&gt;% \n    str_squish() %&gt;% \n    str_replace_all(\"\\\\s\", \"|\")\n  \n  vec_subset &lt;- str_detect(str_sub(raw_table_text, 1, 4), \"type\")\n  \n  \n  small_tab &lt;- raw_table_text[vec_subset]\n  \n  if(length(small_tab) &lt;= 2) stop(\"No table found!\")\n  \n  text_con &lt;- textConnection(small_tab)\n  \n  output_df &lt;- read.csv(text_con, header = FALSE, sep = \"|\", stringsAsFactors = FALSE)\n  \n  tibble_df &lt;- output_df %&gt;% \n    as_tibble()\n  \n  tibble_typed &lt;- suppressMessages(type_convert(tibble_df))\n  \n  type_measure &lt;- raw_table_text[str_detect(str_sub(raw_table_text, 1, 6), \"FIGURE\")] %&gt;% \n    str_detect(\"RATE\")\n  \n  measure_label &lt;- if_else(type_measure == TRUE, \"rate_est\", \"number_est\")\n  \n  tibble_typed %&gt;% \n    set_names(nm = c(\"injury_mechanism\", as.character(2006:2014))) %&gt;% \n    mutate(injury_mechanism = titles[1:nrow(tibble_typed)],\n           type = case_when(\n             page_number %in% c(15,16) ~ \"Emergency Department Visit\",\n             page_number %in% c(17,18) ~ \"Hospitalizations\",\n             page_number %in% c(19,20) ~ \"Deaths\",\n           )) %&gt;% \n    pivot_longer(cols = c(-type, -injury_mechanism), \n                 names_to = \"year\", values_to = measure_label)\n}\n\ned_time &lt;- 15:16 %&gt;% \n  map(get_graph_table) %&gt;% \n  reduce(left_join, by = c(\"injury_mechanism\", \"type\", \"year\"))\n\nhosp_time &lt;- 17:18 %&gt;% \n  map(get_graph_table) %&gt;% \n  reduce(left_join, by = c(\"injury_mechanism\", \"type\", \"year\"))\n\ndeath_time &lt;- 19:20 %&gt;% \n  map(get_graph_table) %&gt;% \n  reduce(left_join, by = c(\"injury_mechanism\", \"type\", \"year\"))\n\nlong_time_df &lt;- bind_rows(ed_time, hosp_time, death_time)\n\n\n# Saving Tables -----------------------------------------------------------\n\nlong_combo_df %&gt;% write_csv(here::here(\"2020\", \"2020-03-24\", \"tbi_age.csv\"))\n\nlong_time_df %&gt;% write_csv(here::here(\"2020\", \"2020-03-24\", \"tbi_year.csv\"))\n\n\n\n# Military TBI ------------------------------------------------------------\n\nurl &lt;- \"https://dvbic.dcoe.mil/dod-worldwide-numbers-tbi\"\n\ndownload_pdf &lt;- function(year){\n  url &lt;- glue::glue(\"https://dvbic.dcoe.mil/sites/default/files/tbi-numbers/worldwide-totals-{year}_jun-21-2018_v1.0_2018-07-26_0.pdf\")\n  download.file(url = url, destfile = here::here(\"2020\",\"2020-03-24\", glue::glue(\"dod_tbi_{year}.pdf\")))\n}\n\n2006:2014 %&gt;% \n  walk(download_pdf)\n\n\nget_dod_tbi &lt;- function(year, page_number){\n  \n  dod_file &lt;- here::here(\"2020\",\"2020-03-24\", glue::glue(\"dod_tbi_{year}.pdf\"))\n  \n  raw_dod_text &lt;- pdftools::pdf_text(dod_file)\n  \n  text_df &lt;- raw_dod_text[[page_number]] %&gt;% \n    str_remove_all(\",\") %&gt;% \n    str_split(\"\\\\\\n\", simplify = F) %&gt;% \n    as.data.frame()\n  \n  service_lab &lt;- case_when(\n    page_number == 2 ~ \"Army\",\n    page_number == 3 ~ \"Navy\",\n    page_number == 4 ~ \"Air Force\",\n    page_number == 5 ~ \"Marines\"\n    \n  )\n  \n  text_df %&gt;% \n    rename(\"col1\" = 1) %&gt;% \n    as_tibble() %&gt;% \n    separate(col1, sep = \"              \", into = c(\"service\", \"component\", \"severity\", \"diagnosed\", \"count\", \"count2\")) %&gt;% \n    mutate_all(str_squish) %&gt;% \n    slice(-1) %&gt;% \n    mutate(count = if_else(str_length(count) &lt; 2, count2, count),\n           severity = if_else(str_length(severity) &lt; 2, diagnosed, severity),\n           diagnosed = as.double(count),\n           service = if_else(str_length(service) == 0, NA_character_, service),\n           component = if_else(str_length(component) == 0, NA_character_, component),\n           year = year,\n           service = if_else(is.na(service), service_lab, service)) %&gt;% \n    select(-count2, -count) %&gt;% \n    filter(!is.na(diagnosed)) %&gt;% \n    fill(service, .direction = \"downup\") %&gt;% \n    fill(component, .direction = \"downup\")\n  \n  \n}\n\ntest_df &lt;- get_dod_tbi(2006, 2)\n\ntest_df\n\nall_years &lt;- crossing(\n  year = 2006:2014, \n  page_number = 2:5\n) %&gt;% \n  mutate(data = map2(.x = year, .y = page_number, .f = get_dod_tbi)) %&gt;% \n  select(data) %&gt;% \n  unnest()\n   \nall_years %&gt;% \n  ggplot(aes(x = year, y = diagnosed, color = severity)) +\n  geom_point() +\n  geom_line() + \n  facet_grid(service ~ component)\n\nall_years %&gt;% \n  write_csv(here::here(\"2020\", \"2020-03-24\", \"tbi_military.csv\"))"
  },
  {
    "objectID": "data/2020/2020-03-10/readme.html",
    "href": "data/2020/2020-03-10/readme.html",
    "title": "College tuition, diversity, and pay",
    "section": "",
    "text": "College tuition, diversity, and pay\nCollege tuition data is somewhat difficult to find - with many sites limiting it to online tools.\nThe data this week comes from many different sources but originally came from the US Department of Education. The most comprehensive and easily accessible data cames from Tuitiontracker.org who allows for a .csv download! Unfortunately it’s in a very wide format that is not ready for analysis, but tidyr can make quick work of that with pivot_longer(). It has a massive amount of data, I have filtered it down to a few tables as seen in the attached .csv files. Tuition and diversity data can be quickly joined by dplyr::left_join(tuition_cost, diversity_school, by = c(\"name\", \"state\")). Some of the other tables can also be joined but there may be some fuzzy matching needed.\nHistorical averages from the NCES - cover 1985-2016.\nTuition and fees by college/university for 2018-2019, along with school type, degree length, state, in-state vs out-of-state from the Chronicle of Higher Education.\nDiversity by college/university for 2014, along with school type, degree length, state, in-state vs out-of-state from the Chronicle of Higher Education.\nExample diversity graphics from Priceonomics.\nAverage net cost by income bracket from TuitionTracker.org.\nExample price trend and graduation rates from TuitionTracker.org\nSalary potential data comes from payscale.com.\n\nGet the data here\n# Get the Data\n\ntuition_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-10/tuition_cost.csv')\n\ntuition_income &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-10/tuition_income.csv') \n\nsalary_potential &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-10/salary_potential.csv')\n\nhistorical_tuition &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-10/historical_tuition.csv')\n\ndiversity_school &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-10/diversity_school.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-03-10')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 11)\n\n\ntuition_cost &lt;- tuesdata$tuition_cost\n\n\nData Dictionary\n\n\n\ntuition_cost.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nSchool name\n\n\nstate\ncharacter\nState name\n\n\nstate_code\ncharacter\nState Abbreviation\n\n\ntype\ncharacter\nType: Public, private, for-profit\n\n\ndegree_length\ncharacter\n4 year or 2 year degree\n\n\nroom_and_board\ndouble\nRoom and board in USD\n\n\nin_state_tuition\ndouble\nTuition for in-state residents in USD\n\n\nin_state_total\ndouble\nTotal cost for in-state residents in USD (sum of room & board + in state tuition)\n\n\nout_of_state_tuition\ndouble\nTuition for out-of-state residents in USD\n\n\nout_of_state_total\ndouble\nTotal cost for in-state residents in USD (sum of room & board + out of state tuition)\n\n\n\n\n\ntuition_income.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nSchool name\n\n\nstate\ncharacter\nState Name\n\n\ntotal_price\ndouble\nTotal price in USD\n\n\nyear\ndouble\nyear\n\n\ncampus\ncharacter\nOn or off-campus\n\n\nnet_cost\ndouble\nNet-cost - average actually paid after scholarship/award\n\n\nincome_lvl\ncharacter\nIncome bracket\n\n\n\n\n\nsalary_potential.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrank\ndouble\nPotential salary rank within state\n\n\nname\ncharacter\nName of school\n\n\nstate_name\ncharacter\nstate name\n\n\nearly_career_pay\ndouble\nEstimated early career pay in USD\n\n\nmid_career_pay\ndouble\nEstimated mid career pay in USD\n\n\nmake_world_better_percent\ndouble\nPercent of alumni who think they are making the world a better place\n\n\nstem_percent\ndouble\nPercent of student body in STEM\n\n\n\n\n\nhistorical_tuition.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntype\ncharacter\nType of school (All, Public, Private)\n\n\nyear\ncharacter\nAcademic year\n\n\ntuition_type\ncharacter\nTuition Type All Constant (dollar inflation adjusted), 4 year degree constant, 2 year constant, Current to year, 4 year current, 2 year current\n\n\ntuition_cost\ndouble\nTuition cost in USD\n\n\n\n\n\ndiversity_school.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nSchool name\n\n\ntotal_enrollment\ndouble\nTotal enrollment of students\n\n\nstate\ncharacter\nState name\n\n\ncategory\ncharacter\nGroup/Racial/Gender category\n\n\nenrollment\ndouble\nenrollment by category\n\n\n\n\nCleaning Script\nPlease see the various .R files."
  },
  {
    "objectID": "data/2020/2020-02-25/readme.html",
    "href": "data/2020/2020-02-25/readme.html",
    "title": "Measles",
    "section": "",
    "text": "Measles\nThe data this week comes from The Wallstreet Journal. They recently published an article around 46,412 schools across 32 US States.\n\n“This repository contains immunization rate data for schools across the U.S., as compiled by The Wall Street Journal. The dataset includes the overall and MMR-specific vaccination rates for 46,412 schools in 32 states. As used in”What’s the Measles Vaccination Rate at Your Child’s School?“.\n\n\nVaccination rates are for the 2017-18 school year for Colorado, Connecticut, Minnesota, Montana, New Jersey, New York, North Dakota, Pennsylvania, South Dakota, Utah and Washington. Rates for other states are 2018-19.”\n\nAdditional data sources are available at: * CDC Child immunization\n* World Bank\n* CDC General Immunization\n* WSJ Measles\n\nGet the data here\n# Get the Data\n\nmeasles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-02-25/measles.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-02-25')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 9)\n\n\nmeasles &lt;- tuesdata$measles\n\n\nData Dictionary\n\n\n\nmeasles.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nindex\ndouble\nIndex ID\n\n\nstate\ncharacter\nSchool’s state\n\n\nyear\ncharacter\nSchool academic year\n\n\nname\ncharacter\nSchool name\n\n\ntype\ncharacter\nWhether a school is public, private, charter\n\n\ncity\ncharacter\nCity\n\n\ncounty\ncharacter\nCounty\n\n\ndistrict\nlogical\nSchool district\n\n\nenroll\ndouble\nEnrollment\n\n\nmmr\ndouble\nSchool’s Measles, Mumps, and Rubella (MMR) vaccination rate\n\n\noverall\ndouble\nSchool’s overall vaccination rate\n\n\nxrel\nlogical\nPercentage of students exempted from vaccination for religious reasons\n\n\nxmed\ndouble\nPercentage of students exempted from vaccination for medical reasons\n\n\nxper\ndouble\nPercentage of students exempted from vaccination for personal reasons\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(rvest)\n\nurl_wsj &lt;- \"https://raw.githubusercontent.com/WSJ/measles-data/master/all-measles-rates.csv\"\n\nwsj &lt;- read_csv(url_wsj)\n\nlist_of_urls &lt;- \"https://github.com/WSJ/measles-data/tree/master/individual-states\"\n\nraw_states &lt;- list_of_urls %&gt;% \n  read_html() %&gt;% \n  html_table() %&gt;% \n  .[[1]] %&gt;% \n  select(Name) %&gt;% \n  mutate(Name = str_remove(Name, \"\\\\.csv\")) %&gt;% \n  filter(str_length(Name) &gt; 3, str_length(Name) &lt; 20) %&gt;% \n  pull(Name)\n\nall_states &lt;- glue::glue(\"https://raw.githubusercontent.com/WSJ/measles-data/master/individual-states/{raw_states}.csv\") %&gt;% \n  map(read_csv)\n\nclean_states &lt;- all_states %&gt;% \n  map(~select(., state, name, lat, lng)) %&gt;% \n  map(~mutate_at(., vars(lat, lng), as.numeric)) %&gt;% \n  bind_rows() %&gt;% \n  filter(!is.na(lat))\n\nwsj %&gt;% \n  left_join(clean_states, by = c(\"name\", \"state\")) %&gt;% \n  write_csv(here::here(\"2020\",\"2020-02-25\",\"measles.csv\"))"
  },
  {
    "objectID": "data/2020/2020-02-11/readme.html",
    "href": "data/2020/2020-02-11/readme.html",
    "title": "Hotels",
    "section": "",
    "text": "Picture of hotel room bed overlooking window\n\n\n\nHotels\nThe data this week comes from an open hotel booking demand dataset from Antonio, Almeida and Nunes, 2019.\nAlso shoutout to a series of packages for time-series analysis and plotting - tidyverts!\n\ntsibble - a time-series tibble\n\nfeasts - Feature Extraction And Statistics for Time Series.\nfable - Commonly used time-series forecasting\n\n\nGet the data here\n# Get the Data\n\nhotels &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-02-11/hotels.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-02-11')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 7)\n\n\nhotels &lt;- tuesdata$hotels\n\n\nData Dictionary\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nhotel\ncharacter\nHotel (H1 = Resort Hotel or H2 = City Hotel)\n\n\nis_canceled\ndouble\nValue indicating if the booking was canceled (1) or not (0)\n\n\nlead_time\ndouble\nNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date\n\n\narrival_date_year\ndouble\nYear of arrival date\n\n\narrival_date_month\ncharacter\nMonth of arrival date\n\n\narrival_date_week_number\ndouble\nWeek number of year for arrival date\n\n\narrival_date_day_of_month\ndouble\nDay of arrival date\n\n\nstays_in_weekend_nights\ndouble\nNumber of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n\n\nstays_in_week_nights\ndouble\nNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n\n\nadults\ndouble\nNumber of adults\n\n\nchildren\ndouble\nNumber of children\n\n\nbabies\ndouble\nNumber of babies\n\n\nmeal\ncharacter\nType of meal booked. Categories are presented in standard hospitality meal packages:  Undefined/SC – no meal package;BB – Bed & Breakfast;  HB – Half board (breakfast and one other meal – usually dinner);  FB – Full board (breakfast, lunch and dinner)\n\n\ncountry\ncharacter\nCountry of origin. Categories are represented in the ISO 3155–3:2013 format\n\n\nmarket_segment\ncharacter\nMarket segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\ndistribution_channel\ncharacter\nBooking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\nis_repeated_guest\ndouble\nValue indicating if the booking name was from a repeated guest (1) or not (0)\n\n\nprevious_cancellations\ndouble\nNumber of previous bookings that were cancelled by the customer prior to the current booking\n\n\nprevious_bookings_not_canceled\ndouble\nNumber of previous bookings not cancelled by the customer prior to the current booking\n\n\nreserved_room_type\ncharacter\nCode of room type reserved. Code is presented instead of designation for anonymity reasons\n\n\nassigned_room_type\ncharacter\nCode for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons\n\n\nbooking_changes\ndouble\nNumber of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n\n\ndeposit_type\ncharacter\nIndication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:No Deposit – no deposit was made;Non Refund – a deposit was made in the value of the total stay cost;Refundable – a deposit was made with a value under the total cost of stay.\n\n\nagent\ncharacter\nID of the travel agency that made the booking\n\n\ncompany\ncharacter\nID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons\n\n\ndays_in_waiting_list\ndouble\nNumber of days the booking was in the waiting list before it was confirmed to the customer\n\n\ncustomer_type\ncharacter\nType of booking, assuming one of four categories:Contract - when the booking has an allotment or other type of contract associated to it;Group – when the booking is associated to a group;Transient – when the booking is not part of a group or contract, and is not associated to other transient booking;Transient-party – when the booking is transient, but is associated to at least other transient booking\n\n\nadr\ndouble\nAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n\n\nrequired_car_parking_spaces\ndouble\nNumber of car parking spaces required by the customer\n\n\ntotal_of_special_requests\ndouble\nNumber of special requests made by the customer (e.g. twin bed or high floor)\n\n\nreservation_status\ncharacter\nReservation last status, assuming one of three categories:Canceled – booking was canceled by the customer;Check-Out – customer has checked in but already departed;No-Show – customer did not check-in and did inform the hotel of the reason why\n\n\nreservation_status_date\ndouble\nDate at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel\n\n\n\n\n\n\nhotels.csv\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(feasts)\n\n# resort hotel\nh1 &lt;- read_csv(here::here(\"2020\", \"2020-02-11\", \"H1.csv\")) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(hotel = \"Resort Hotel\") %&gt;% \n  select(hotel, everything())\n\n# city hotel\nh2 &lt;- read_csv(here::here(\"2020\", \"2020-02-11\", \"H2.csv\")) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(hotel = \"City Hotel\") %&gt;% \n  select(hotel, everything())\n\nhotel_df &lt;- bind_rows(h1, h2)\n\nhotel_plot &lt;- hotel_df %&gt;% \n  filter(hotel == \"City Hotel\") %&gt;%\n  mutate(date = glue::glue(\"{arrival_date_year}-{arrival_date_month}-{arrival_date_day_of_month}\"),\n         date = parse_date(date, format = \"%Y-%B-%d\")) %&gt;% \n  select(date, everything()) %&gt;% \n  arrange(date) %&gt;% \n  count(date) %&gt;% \n  rename(daily_bookings = n) %&gt;% \n  tsibble::as_tsibble() %&gt;% \n  model(STL(daily_bookings ~ season(window = Inf))) %&gt;% \n  components() %&gt;% autoplot()\n\nhotel_plot\n\nggsave(\"hotel_bookings.png\", hotel_plot, path = here::here(\"2020\", \"2020-02-11\"), dpi = \"retina\")"
  },
  {
    "objectID": "data/2020/2020-01-28/readme.html",
    "href": "data/2020/2020-01-28/readme.html",
    "title": "San Francisco Trees",
    "section": "",
    "text": "Southern Magnolia tree in urban San Francisco\n\n\n\nSan Francisco Trees\nThe data this week comes from San Francisco’s open data portal.\nThere are dozens of tree species, and many other intresting features to explore in this dataset! I did drop a few columns that were either &gt; 75% missing or redundant, feel free to check out the source for the fully original dataset.\nAlso - make sure to follow @tidypod - they’ll have some interesting #TidyTuesday updates to come this week!\nSome interesting articles: - Trees of Life in SF - Landmark trees - Non-native trees - Friends of the urban forest - SF Tree Guide\n\nGet the data here\n# Get the Data\n\nsf_trees &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-28/sf_trees.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO UPDATE tidytuesdayR from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-01-28') \ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 5)\n\n\nsf_trees &lt;- tuesdata$sf_trees\n\n\nData Dictionary\n\n\n\nsf_trees.csv\nA full data dictionary is available at: the source but it’s fairly sparse.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntree_id\ndouble\nUnique ID\n\n\nlegal_status\ncharacter\nLegalLegal staus: Permitted or DPW maintained\n\n\nspecies\ncharacter\nTree species includes common name after the :: separator\n\n\naddress\ncharacter\nStreet Address\n\n\nsite_order\ndouble\nOrder of tree at address where multiple trees are at same address. Trees are ordered in ascending\n\n\naddress order\n\n\n\n\nsite_info\ncharacter\nSite Info - Where the tree resides\n\n\ncaretaker\ncharacter\nAgency or person that is primary caregiver to tree – Owner of Tree\n\n\ndate\ndouble\nDate Planted (NA if before 1955)\n\n\ndbh\ndouble\nDiameter at breast height\n\n\nplot_size\ncharacter\nDimension of plot - typically in feet\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\n\n\nCleaning Script\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tidytuesdaymeta)\nlibrary(pryr)\nlibrary(visdat)\nlibrary(skimr)\nlibrary(lubridate)\nlibrary(leaflet)\n\ncreate_tidytuesday_folder()\n\nraw_df &lt;- read_csv(here::here(\"2020\", \"2020-01-28\", \"Street_Tree_Map.csv\"),\n                   col_types = \n                   cols(\n                     TreeID = col_double(),\n                     qLegalStatus = col_character(),\n                     qSpecies = col_character(),\n                     qAddress = col_character(),\n                     SiteOrder = col_double(),\n                     qSiteInfo = col_character(),\n                     PlantType = col_character(),\n                     qCaretaker = col_character(),\n                     qCareAssistant = col_character(),\n                     PlantDate = col_character(),\n                     DBH = col_double(),\n                     PlotSize = col_character(),\n                     PermitNotes = col_character(),\n                     XCoord = col_double(),\n                     YCoord = col_double(),\n                     Latitude = col_double(),\n                     Longitude = col_double(),\n                     Location = col_character()\n                   )) %&gt;% \n  janitor::clean_names()\n\nsmall_df &lt;- raw_df %&gt;% \n  select(-x_coord,-y_coord,-q_care_assistant, -permit_notes) %&gt;% \n  filter(plant_type != \"Landscaping\") %&gt;% \n  select(-plant_type) %&gt;% \n  separate(plant_date, into = c(\"date\", \"time\"), sep = \" \") %&gt;% \n  mutate(date = parse_date(date, \"%m/%d/%Y\")) %&gt;% \n  select(-time, -location) %&gt;% \n  arrange(date) %&gt;% \n  rename(legal_status = q_legal_status,\n         species = q_species,\n         address = q_address,\n         site_info = q_site_info,\n         caretaker = q_caretaker)\n\nsmall_df %&gt;% skimr::skim()\n\nsmall_df %&gt;% \n  write_csv(here::here(\"2020\", \"2020-01-28\", \"sf_trees.csv\"))"
  },
  {
    "objectID": "data/2020/2020-01-14/readme.html",
    "href": "data/2020/2020-01-14/readme.html",
    "title": "Passwords",
    "section": "",
    "text": "### XKCD Source for Comic\n\nPasswords\nThis week’s data is all about passwords. Data is sourced from Information is Beautiful, with the graphic coming from the same group here.\nThere’s lots of additional information about password quality & strength in the source Doc. Please note that the “strength” column in this dataset is relative to these common aka “bad” passwords and YOU SHOULDN’T USE ANY OF THEM!\nWikipedia has a nice article on password strength as well.\n\nGet the data here\n# Get the Data\n\npasswords &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-14/passwords.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO UPDATE tidytuesdayR from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-01-14') \ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 3)\n\n\npasswords &lt;- tuesdata$passwords\n\n\nData Dictionary\n\n\n\npasswords.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrank\ndouble\npopularity in their database of released passwords\n\n\npassword\ncharacter\nActual text of the password\n\n\ncategory\ncharacter\nWhat category does the password fall in to?\n\n\nvalue\ndouble\nTime to crack by online guessing\n\n\ntime_unit\ncharacter\nTime unit to match with value\n\n\noffline_crack_sec\ndouble\nTime to crack offline in seconds\n\n\nrank_alt\ndouble\nRank 2\n\n\nstrength\ndouble\nStrength = quality of password where 10 is highest, 1 is lowest, please note that these are relative to these generally bad passwords\n\n\nfont_size\ndouble\nUsed to create the graphic for KIB"
  },
  {
    "objectID": "data/2019/readme.html",
    "href": "data/2019/readme.html",
    "title": "2019 Data",
    "section": "",
    "text": "2019 Data\nArchive of datasets and articles from the 2019 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2019-01-01\n#Rstats & #TidyTuesday Tweets\nrtweet\nstackoverflow.blog\n\n\n2\n2019-01-08\nTV’s Golden Age\nIMDb\nThe Economist\n\n\n3\n2019-01-15\nSpace Launches\nJSR Launch Vehicle Database\nThe Economist\n\n\n4\n2019-01-22\nIncarceration Trends\nVera Institute\nVera Institute\n\n\n5\n2019-01-29\nDairy production & Consumption\nUSDA\nNPR\n\n\n6\n2019-02-05\nHouse Price Index & Mortgage Rates\nFreddieMac & FreddieMac\nFortune\n\n\n7\n2019-02-12\nFederal R&D Spending\nAAAS\nNew York Times\n\n\n8\n2019-02-19\nUS PhD’s Awarded\nNSF\n#epibookclub\n\n\n9\n2019-02-26\nFrench Train Delays\nSNCF\nRTL - Today\n\n\n10\n2019-03-05\nWomen in the Workplace\nCensus Bureau & Bureau of Labor\nCensus Bureau\n\n\n11\n2019-03-12\nBoard Games\nBoard Game Geeks\nfivethirtyeight\n\n\n12\n2019-03-19\nStanford Open Policing Project\nStanford Open Policing Project  SOPP - arXiv:1706.05678\nSOPP - arXiv:1706.05678\n\n\n13\n2019-03-26\nSeattle Pet Names\nseattle.gov\nCurbed Seattle\n\n\n14\n2019-04-02\nSeattle Bike Traffic\nseattle.gov\nSeattle Times\n\n\n15\n2019-04-09\nTennis Grand Slam Champions\nWikipedia\nFinancial Times\n\n\n16\n2019-04-16\nThe Economist Data Viz Mistakes\nThe Economist\nThe Economist\n\n\n17\n2019-04-23\nAnime Data\nMyAnimeList\nMyAnimeList\n\n\n18\n2019-04-30\nChicago Bird Collisions\nWinger et al, 2019\nWinger et al, 2019\n\n\n19\n2019-05-07\nGlobal Student to Teacher Ratios\nUNESCO\nCenter for Public Education\n\n\n20\n2019-05-14\nNobel Prize Winners\nKaggle\nThe Economist\n\n\n21\n2019-05-21\nGlobal Plastic Waste\nOur World In Data\nOur World in Data\n\n\n22\n2019-05-28\nWine Ratings\nKaggle\nVivino\n\n\n23\n2019-06-04\nRamen Ratings\nTheRamenRater.com\nFood Republic\n\n\n24\n2019-06-11\nMeteorites\nNASA\nThe Guardian - Meteorite map\n\n\n25\n2019-06-18\nChristmas Bird Counts\nBird Studies Canada\nHamilton Christmas Bird Count\n\n\n26\n2019-06-25\nGlobal UFO Sightings\nNUFORC\nExample Plots\n\n\n27\n2019-07-02\nMedia Franchise Revenues\nWikipedia\nreddit/dataisbeautiful post\n\n\n28\n2019-07-09\nWomen’s World Cup\ndata.world\nWikipedia\n\n\n29\n2019-07-16\nR4DS Membership\nR4DS Slack\nR4DS useR Presentation\n\n\n30\n2019-07-23\nWildlife Strikes\nFAA\nFAA\n\n\n31\n2019-07-30\nVideo Games\nSteam Spy\nLiza Wood\n\n\n32\n2019-08-06\nBob Ross paintings\nFiveThirtyEight\nFiveThirtyEight\n\n\n33\n2019-08-13\nRoman Emperors\nWikipedia / Zonination\nreddit.com/r/dataisbeautiful\n\n\n34\n2019-08-20\nNuclear Explosions\nSIPRI\nOur World in Data\n\n\n35\n2019-08-27\nSimpsons Guest Stars\nWikipedia\nWikipedia\n\n\n36\n2019-09-03\nMoore’s Law\nWikipedia\nWikipedia\n\n\n37\n2019-09-10\nAmusement Park Injuries\nData.world & Saferparks\nSaferparks\n\n\n38\n2019-09-17\nNational Park Visits\nData.world\nfivethirtyeight article\n\n\n39\n2019-09-24\nSchool Diversity\nNCES\nWashington Post article\n\n\n40\n2019-10-01\nAll the Pizza\nJared Lander & Ludmila Janda, Tyler Richards, DataFiniti\nTyler Richards on TWD\n\n\n41\n2019-10-08\nPowerlifting\nOpenPowerlifting.org\nElias Oziolor\n\n\n42\n2019-10-15\nCar Fuel Economy\nEPA\nEllis Hughes\n\n\n43\n2019-10-22\nHorror movie ratings\nIMDB\nStephen Follows\n\n\n44\n2019-10-29\nNYC Squirrel Census\nSquirrel Census\nCityLab\n\n\n45\n2019-11-05\nBike & Walk Commutes\nACS\nACS\n\n\n46\n2019-11-12\nCRAN Code\nCRAN\nPhillip Massicotte\n\n\n47\n2019-11-19\nNZ Bird of the Year\nNew Zealand Forest and Bird Org\nDragonfly Data Science & Nathan Moore\n\n\n48\n2019-11-26\nStudent Loan Debt\nDepartment of Education\nDignity and Debt\n\n\n49\n2019-12-03\nPhilly Parking Tickets\nOpen Data Philly\nNBC Philadelphia\n\n\n50\n2019-12-10\nReplicating plots in R\nSimply Statistics\nRafael Irizarry\n\n\n51\n2019-12-17\nAdoptable dogs\nPetfinder\nThe Pudding\n\n\n52\n2019-12-24\nChristmas Songs\nBillboard Top 100\nA Dash of Data",
    "crumbs": [
      "Datasets",
      "2019"
    ]
  },
  {
    "objectID": "data/2019/2019-12-17/readme.html",
    "href": "data/2019/2019-12-17/readme.html",
    "title": "Adoptable dogs",
    "section": "",
    "text": "Adoptable dogs\nThis week’s data is from The Pudding. The data was cleaned, collected and story written by Amber Thomas, with design by Sacha Maxim.\nTheir article Finding Forever Homes examines data on all adoptable dogs from Petfinder.com in the USA on 2019-09-20.\nThere are a number of datasets, where the dog_travel and dog_descriptions datasets can be joined via the common id column.\nThe premise of the story in Amber’s own words:\n\n“If you’re looking to add a new furry friend to your family, you may be encouraged to”adopt not shop”. That is, to find a new dog at a local shelter or rescue organization rather than a pet store or breeder.\nBut where do adoptable dogs come from?”\n\n# Get the Data\n\ndog_moves &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-17/dog_moves.csv')\ndog_travel &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-17/dog_travel.csv')\ndog_descriptions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-17/dog_descriptions.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2019-12-17') \ntuesdata &lt;- tidytuesdayR::tt_load(2019, week = 51)\n\n\ndog_moves &lt;- tuesdata$dog_moves\n\n\nDictionary\n\ndog_moves.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlocation\ncharacter\nThe full name of the US state or country\n\n\nexported\ndouble\nThe number of adoptable dogs available in the US that originated in this location but were available for adoption in another location\n\n\nimported\ndouble\nThe number of adoptable dogs available in this state that originated in a different location\n\n\ntotal\ndouble\nThe total number of adoptable dogs availabe in a given state.\n\n\ninUS\nlogical\nWhether or not a location is in the US or not. Here, US territories will return FALSE\n\n\n\n\n\ndog_travel.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nThe unique PetFinder identification number for each animal\n\n\ncontact_city\ncharacter\nThe rescue/shelter’s listed city\n\n\ncontact_state\ncharacter\nThe rescue/shelter’s listed State\n\n\ndescription\ncharacter\nThe full description of each animal as entered by the rescue/shelter\n\n\nfound\ncharacter\nWhere the animal was found.\n\n\nmanual\ncharacter\n.\n\n\nremove\nlogical\nAnimal removed from location\n\n\nstill_there\nlogical\nTRUE/FALSE - Whether the animal is still located in their origin location and will be transported to their final destination after adoption.\n\n\n\n\n\ndog_descriptions.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nThe unique PetFinder identification number for each animal.\n\n\norg_id\ncharacter\nThe unique identification number for each shelter or rescue.\n\n\nurl\ncharacter\nThe URL for each animal’s listing.\n\n\nspecies\ncharacter\nSpecies of animal.\n\n\nbreed_primary\ncharacter\nThe primary (assumed) breed assigned by the shelter or rescue.\n\n\nbreed_secondary\ncharacter\nThe secondary (assumed) breed assigned by the shelter or rescue.\n\n\nbreed_mixed\nlogical\nWhether or not an animal is presumed to be mixed breed.\n\n\nbreed_unknown\nlogical\nWhether or not the animal’s breed is completely unknown.\n\n\ncolor_primary\ncharacter\nThe most prevalent color of an animal.\n\n\ncolor_secondary\ncharacter\nThe second most prevalent color of an animal.\n\n\ncolor_tertiary\ncharacter\nThe third most prevalent color of an animal.\n\n\nage\ncharacter\nThe assumed age class of an animal (Baby, Young, Adult, or Senior).\n\n\nsex\ncharacter\nThe sex of an animal (Female, Male, or Unknown).\n\n\nsize\ncharacter\nThe general size class of an animal (Small, Medium, Large, Extra Large).\n\n\ncoat\ncharacter\nCoat Length for each animal (Curly, Hairless, Long, Medium, Short, Wire).\n\n\nfixed\nlogical\nWhether or not an animal has been spayed/neutered.\n\n\nhouse_trained\nlogical\nWhether or not an animal is trained to not go to the bathroom in the house.\n\n\ndeclawed\nlogical\nWhether or not the animal has had its dewclaws removed.\n\n\nspecial_needs\nlogical\nWhether or not the animal is considered to have special needs (this can be a long-term medical condition or particular temperament that requires extra care).\n\n\nshots_current\nlogical\nWhether or not the animal is up to date on all of their vaccines and other shots.\n\n\nenv_children\nlogical\nWhether or not the animal is recommended for a home with children.\n\n\nenv_dogs\nlogical\nWhether or not the animal is recommended for a home with other dogs.\n\n\nenv_cats\nlogical\nWhether or not the animal is recommended for a home with cats.\n\n\nname\ncharacter\nThe animal’s name (as given by the shelter/rescue).\n\n\ntags\ncharacter\nAny tags given to the dog by the shelter rescue (pipe \\| separated).\n\n\nphoto\ncharacter\nThe URL to the animal’s primary photo.\n\n\nstatus\ncharacter\nWhether the animal is adoptable or not.\n\n\nposted\ncharacter\nThe date that this animal was first listed on PetFinder .\n\n\ncontact_city\ncharacter\nThe rescue/shelter’s listed city.\n\n\ncontact_state\ncharacter\nThe rescue/shelter’s listed state.\n\n\ncontact_zip\ncharacter\nThe rescue/shelter’s listed zip code.\n\n\ncontact_country\ncharacter\nThe rescue/shelter’s listed country.\n\n\nstateQ\ncharacter\nThe state abbreviation queried in the API to return this result .\n\n\naccessed\ndouble\nThe date that this data was acquired from the PetFinder API.\n\n\ntype\ncharacter\nThe type of animal.\n\n\ndescription\ncharacter\nThe full description of an animal, as entered by the rescue or shelter. This is the only field returned by the V1 API.\n\n\n\n\n\n\nCleaning\nAll the code/cleaning can be found on The Pudding Github."
  },
  {
    "objectID": "data/2019/2019-12-03/readme.html",
    "href": "data/2019/2019-12-03/readme.html",
    "title": "Philadelphia Parking Violations",
    "section": "",
    "text": "Philadelphia Parking Violations\nThis week’s data is from Open Data Philly - there is over 1 GB of data, but I have filtered it down to &lt;100 MB due to GitHub restrictions. I accomplished this mainly by filtering to data for only 2017 in Pennsylvania that had lat/long data. If you would like to use the entire dataset, please see the link above.\nH/t to Jess Streeter for sharing this week’s data!\nSome visualizations from Philly Open Data and a news article by NBC Philadelphia.\n\n\nGet the Data\ntickets &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-03/tickets.csv\")\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# Either ISO-8601 date or year/week works!\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load(\"2019-12-03\")\ntuesdata &lt;- tidytuesdayR::tt_load(2019, week = 49)\n\ntickets &lt;- tuesdata$tickets\n\n\nCleaning\n# Load Libraries ----------------------------------------------------------\n\nlibrary(here)\nlibrary(tidyverse)\n\n\n# Read in raw Data --------------------------------------------------------\n\ndf &lt;- read_csv(here(\"2019\", \"2019-12-03\", \"parking_violations.csv\"))\n\nsmall_df &lt;- df %&gt;% \n  mutate(date = lubridate::date(issue_datetime),\n         year = lubridate::year(date)) %&gt;% \n  filter(year == 2017, state == \"PA\") %&gt;% \n  \n  # removing date/year as duplicative\n  # removing state as all PA\n  # gps as filtering only lat/long present\n  # division is &gt; 60% missing\n  # location as a very large amount of metadata without as much use\n  \n  select(-date, -year, -gps, -location, -state, -division) %&gt;% \n  filter(!is.na(lat))\n\npryr::object_size(small_df)\n\n# Write to csv ------------------------------------------------------------\n\nwrite_csv(small_df, here(\"2019\", \"2019-12-03\", \"tickets.csv\"))"
  },
  {
    "objectID": "data/2019/2019-11-19/readme.html",
    "href": "data/2019/2019-11-19/readme.html",
    "title": "New Zealand Bird of the Year",
    "section": "",
    "text": "This week’s data is from the New Zealand Forest and Bird Orginization courtesy of Dragonfly Data Science by way of Nathan Moore.\nFull details around voting can be found at the Bird of the Year site.\nI have uploaded the raw data and the clean data, was a quick dplyr::pivot_longer() call!\nFull details around voting are below - please note that votes are ranked 1-5 (1 is best, 5 is worst), and the voters do not need to submit all 5 votes.\n\nThis year, voting is based on the instant runoff (IRV) voting system, which is similar to the system you might have seen in local elections. When you vote, you can rank up to five of your favourite birds, with #1 indicating your favourite bird, #2 indicating your second favourite bird, and so on. It’s no problem if you want to vote for less than five birds.\n\n\nHow the winner is decided\n\n\nIn the IRV voting system, the first preferences of all the votes cast are tallied in a first round of counting. If no bird has more than half of the votes, new rounds of counting are held until one bird has a majority.\n\n\nIn each of these rounds the bird with the lowest number of votes is eliminated and the next ranked choice of those who voted for that bird are added to the totals of the remaining birds.\n\n\nThis process continues until one bird has a majority of votes and is crowned Bird of the Year."
  },
  {
    "objectID": "data/2019/2019-11-19/readme.html#nz_bird",
    "href": "data/2019/2019-11-19/readme.html#nz_bird",
    "title": "New Zealand Bird of the Year",
    "section": "nz_bird",
    "text": "nz_bird\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of vote (ISO 8601)\n\n\nhour\ndouble\nHour of vote (numeric)\n\n\nvote_rank\ncharacter\nVote rank, 1-5, where 1 is highest, and 5 is lowest\n\n\nbird_breed\ncharacter\nBird breed"
  },
  {
    "objectID": "data/2019/2019-11-05/readme.html",
    "href": "data/2019/2019-11-05/readme.html",
    "title": "Modes Less Traveled - Bicycling and Walking to Work in the United States: 2008-2012",
    "section": "",
    "text": "This week’s data is from the ACS Survey. The article and underlying data can be found at the Census Website. The PDF report is also available for download if you’d like to try reading in some of the embedded tables.\nPlease note that the raw excel files are uploaded (6 total), along with the cleaned/tidy data (commute.csv). There is also a cleaned up version of Table 3 from the article, which incorporates summary data around age, gender, race, children, income, and education for modes of travel (bike, walk, other). If you work with the ACS table 3 I’d suggest dplyr::slice() to grab the specific sub-tables from within it!"
  },
  {
    "objectID": "data/2019/2019-11-05/readme.html#commute.csv",
    "href": "data/2019/2019-11-05/readme.html#commute.csv",
    "title": "Modes Less Traveled - Bicycling and Walking to Work in the United States: 2008-2012",
    "section": "commute.csv",
    "text": "commute.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncity\ncharacter\nCity\n\n\nstate\ncharacter\nState\n\n\ncity_size\ncharacter\nCity Size * Small = 20K to 99,999 * Medium = 100K to 199,999 * Large = &gt;= 200K\n\n\nmode\ncharacter\nMode of transport, either walk or bike\n\n\nn\ndouble\nN of individuals\n\n\npercent\ndouble\nPercent of total individuals\n\n\nmoe\ndouble\nMargin of Error (percent)\n\n\nstate_abb\ncharacter\nAbbreviated state name\n\n\nstate_region\ncharacter\nACS State region"
  },
  {
    "objectID": "data/2019/2019-10-22/readme.html",
    "href": "data/2019/2019-10-22/readme.html",
    "title": "Horror movie metadata",
    "section": "",
    "text": "This week’s data is from the IMDB by way of Kaggle.\nH/t to Georgios Karamanis for sharing the data this week.\nThrillist did a 75 Best Horror Movies of all Time article. There’s also a Stephen Follows article about horror movies exploring data around profit, popularity and ratings.\nLast year for Halloween we focused on Horror Movie Profit - feel free to take a peek at that data as well on our GitHub."
  },
  {
    "objectID": "data/2019/2019-10-22/readme.html#horror_movies.csv",
    "href": "data/2019/2019-10-22/readme.html#horror_movies.csv",
    "title": "Horror movie metadata",
    "section": "horror_movies.csv",
    "text": "horror_movies.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntitle\ncharacter\nTitle of the movie\n\n\ngenres\ncharacter\nMovie Generes\n\n\nrelease_date\ncharacter\nMovie release date - day-month-year\n\n\nrelease_country\ncharacter\nRelease country\n\n\nmovie_rating\ncharacter\nMPAA Rating\n\n\nreview_rating\ndouble\nMovie rating (0 - 10)\n\n\nmovie_run_time\ncharacter\nMovie run time (minutes)\n\n\nplot\ncharacter\nShort plot description (raw text)\n\n\ncast\ncharacter\nCast\n\n\nlanguage\ncharacter\nLanguage\n\n\nfilming_locations\ncharacter\nFilming location\n\n\nbudget\ncharacter\nBudget (US Dollars)"
  },
  {
    "objectID": "data/2019/2019-10-08/readme.html",
    "href": "data/2019/2019-10-08/readme.html",
    "title": "International Powerlifting",
    "section": "",
    "text": "This week’s data is from Open Powerlifting.\nWikipedia has many details around the sport itself, as well as more details around the 3 lifts (squat, bench, and deadlift).\nCredit to Nichole Monhait for sharing this fantastic open dataset. Please note this is a small subset of the data limited to IPF (International Powerlifting Federation) events, the full dataset with many more columns and alternative events can be found as a .csv at https://openpowerlifting.org/data. The full dataset has many more federations, ages, and meet types but is &gt;250 MB.\nA nice analysis of this dataset for age-effects in R can be found at Elias Oziolor’s Blog"
  },
  {
    "objectID": "data/2019/2019-10-08/readme.html#ipf_lifts.csv",
    "href": "data/2019/2019-10-08/readme.html#ipf_lifts.csv",
    "title": "International Powerlifting",
    "section": "ipf_lifts.csv",
    "text": "ipf_lifts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nIndividual lifter name\n\n\nsex\ncharacter\nBinary gender (M/F)\n\n\nevent\ncharacter\nThe type of competition that the lifter entered.Values are as follows:- SBD: Squat-Bench-Deadlift, also commonly called “Full Power”.- BD: Bench-Deadlift, also commonly called “Ironman” or “Push-Pull”- SD: Squat-Deadlift, very uncommon.- SB: Squat-Bench, very uncommon.- S: Squat-only.- B: Bench-only.- D: Deadlift-only.\n\n\nequipment\ncharacter\nThe equipment category under which the lifts were performed.Values are as follows:- Raw: Bare knees or knee sleeves.- Wraps: Knee wraps were allowed.- Single-ply: Equipped, single-ply suits.- Multi-ply: Equipped, multi-ply suits (includes Double-ply).- Straps: Allowed straps on the deadlift (used mostly for exhibitions, not real meets).\n\n\nage\ndouble\nThe age of the lifter on the start date of the meet, if known.\n\n\nage_class\ncharacter\nThe age class in which the filter falls, for example 40-45\n\n\ndivision\ncharacter\nFree-form UTF-8 text describing the division of competition, like Open or Juniors 20-23 or Professional.\n\n\nbodyweight_kg\ndouble\nThe recorded bodyweight of the lifter at the time of competition, to two decimal places.\n\n\nweight_class_kg\ncharacter\nThe weight class in which the lifter competed, to two decimal places.Weight classes can be specified as a maximum or as a minimum. Maximums are specified by just the number, for example 90 means “up to (and including) 90kg.” minimums are specified by a + to the right of the number, for example 90+ means “above (and excluding) 90kg.”\n\n\nbest3squat_kg\ndouble\nMaximum of the first three successful attempts for the lift.Rarely may be negative: that is used by some federations to report the lowest weight the lifter attempted and failed.\n\n\nbest3bench_kg\ndouble\nMaximum of the first three successful attempts for the lift.Rarely may be negative: that is used by some federations to report the lowest weight the lifter attempted and failed.\n\n\nbest3deadlift_kg\ndouble\nMaximum of the first three successful attempts for the lift.Rarely may be negative: that is used by some federations to report the lowest weight the lifter attempted and failed.\n\n\nplace\ncharacter\nThe recorded place of the lifter in the given division at the end of the meet.Values are as follows:- Positive number: the place the lifter came in.- G: Guest lifter. The lifter succeeded, but wasn’t eligible for awards.- DQ: Disqualified. Note that DQ could be for procedural reasons, not just failed attempts.- DD: Doping Disqualification. The lifter failed a drug test.- NS: No-Show. The lifter did not show up on the meet day.\n\n\ndate\ndouble\nISO 8601 Date of the event\n\n\nfederation\ncharacter\nThe federation that hosted the meet. (limited to IPF for this data subset)\n\n\nmeet_name\ncharacter\nThe name of the meet.The name is defined to never include the year or the federation. For example, the meet officially called 2019 USAPL Raw National Championships would have the MeetName Raw National Championshps."
  },
  {
    "objectID": "data/2019/2019-10-01/readme.html",
    "href": "data/2019/2019-10-01/readme.html",
    "title": "Pizza Party!",
    "section": "",
    "text": "This week’s data is from Jared Lander and Barstool Sports via Tyler Richards.\nCredit for this week’s concept goes to Ludmila who did a recent dataviz presentation and gave shoutouts to both #tidytuesday and a pizza dataset!\nCheck out her DataViz video and slides at her GitHub\nJared’s data is from top NY pizza restaurants, with a 6 point likert scale survey on ratings. The Barstool sports dataset has critic, public, and the Barstool Staff’s rating as well as pricing, location, and geo-location. There are 22 pizza places that overlap between the two datasets.\nIf you want to look more at geo-location of pizza places, checkout this one from DataFiniti. This includes 10000 pizza places, their price ranges and geo-locations."
  },
  {
    "objectID": "data/2019/2019-10-01/readme.html#pizza_jared.csv",
    "href": "data/2019/2019-10-01/readme.html#pizza_jared.csv",
    "title": "Pizza Party!",
    "section": "pizza_jared.csv",
    "text": "pizza_jared.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npolla_qid\ninteger\nQuiz ID\n\n\nanswer\ncharacter\nAnswer (likert scale)\n\n\nvotes\ninteger\nNumber of votes for that question/answer combo\n\n\npollq_id\ninteger\nPoll Question ID\n\n\nquestion\ncharacter\nQuestion\n\n\nplace\ncharacter\nPizza Place\n\n\ntime\ninteger\nTime of quiz\n\n\ntotal_votes\ninteger\nTotal number of votes for that pizza place\n\n\npercent\ndouble\nVote percent of total for that pizza place"
  },
  {
    "objectID": "data/2019/2019-10-01/readme.html#pizza_barstool.csv",
    "href": "data/2019/2019-10-01/readme.html#pizza_barstool.csv",
    "title": "Pizza Party!",
    "section": "pizza_barstool.csv",
    "text": "pizza_barstool.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nPizza place name\n\n\naddress1\ncharacter\nPizza place address\n\n\ncity\ncharacter\nCity\n\n\nzip\ndouble\nZip\n\n\ncountry\ncharacter\nCountry\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\nprice_level\ndouble\nPrice rating (fewer $ = cheaper, more $$$ = expensive)\n\n\nprovider_rating\ndouble\nProvider review score\n\n\nprovider_review_count\ndouble\nProvider review count\n\n\nreview_stats_all_average_score\ndouble\nAverage Score\n\n\nreview_stats_all_count\ndouble\nCount of all reviews\n\n\nreview_stats_all_total_score\ndouble\nReview total score\n\n\nreview_stats_community_average_score\ndouble\nCommunity average score\n\n\nreview_stats_community_count\ndouble\ncommunity review count\n\n\nreview_stats_community_total_score\ndouble\ncommunity review total score\n\n\nreview_stats_critic_average_score\ndouble\nCritic average score\n\n\nreview_stats_critic_count\ndouble\nCritic review count\n\n\nreview_stats_critic_total_score\ndouble\nCritic total score\n\n\nreview_stats_dave_average_score\ndouble\nDave (Barstool) average score\n\n\nreview_stats_dave_count\ndouble\nDave review count\n\n\nreview_stats_dave_total_score\ndouble\nDave total score"
  },
  {
    "objectID": "data/2019/2019-10-01/readme.html#pizza_datafiniti.csv",
    "href": "data/2019/2019-10-01/readme.html#pizza_datafiniti.csv",
    "title": "Pizza Party!",
    "section": "pizza_datafiniti.csv",
    "text": "pizza_datafiniti.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nPizza place\n\n\naddress\ncharacter\nAddress\n\n\ncity\ncharacter\nCity\n\n\ncountry\ncharacter\nCountry\n\n\nprovince\ncharacter\nState\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\ncategories\ncharacter\nRestaurant category\n\n\nprice_range_min\ndouble\nPrice range min\n\n\nprice_range_max\ndouble\nPrice range max"
  },
  {
    "objectID": "data/2019/2019-09-17/readme.html",
    "href": "data/2019/2019-09-17/readme.html",
    "title": "National Park Visits",
    "section": "",
    "text": "This week’s data is from dataisplural/data.world.\nThere are some additional datasets that may be interesting in relation to this data - namely US population and gas prices over time.\nThere’s also some shapefiles you might be able to use with the sf package.\n\ngeojson\n\nRegional Boundaries\n\nCentroids\n\nBoundaries\n\nLastly, a fivethirtyeight article covered this dataset with lots of great visualizations to copy or build off of."
  },
  {
    "objectID": "data/2019/2019-09-17/readme.html#national_parks.csv",
    "href": "data/2019/2019-09-17/readme.html#national_parks.csv",
    "title": "National Park Visits",
    "section": "national_parks.csv",
    "text": "national_parks.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear_raw\ninteger\nYear of record\n\n\ngnis_id\ncharacter\nID for shapefile and long-lat lookup\n\n\ngeometry\ncharacter\nGeometric shape for shapefile\n\n\nmetadata\ncharacter\nURL to metadata about the park\n\n\nnumber_of_records\ndouble\nNumber of records\n\n\nparkname\ncharacter\nFull park name\n\n\nregion\ncharacter\nUS Region where park is located\n\n\nstate\ncharacter\nState abbreviation\n\n\nunit_code\ncharacter\nPark code abbreviation\n\n\nunit_name\ncharacter\nPark Unit name\n\n\nunit_type\ncharacter\nPark unit type\n\n\nvisitors\ndouble\nNumber of visitors"
  },
  {
    "objectID": "data/2019/2019-09-17/readme.html#state_pop.csv",
    "href": "data/2019/2019-09-17/readme.html#state_pop.csv",
    "title": "National Park Visits",
    "section": "state_pop.csv",
    "text": "state_pop.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nJan 1st of year\n\n\nstate\ncharacter\nState abbreviation\n\n\npop\ndouble\nPopulation"
  },
  {
    "objectID": "data/2019/2019-09-17/readme.html#gas_price.csv",
    "href": "data/2019/2019-09-17/readme.html#gas_price.csv",
    "title": "National Park Visits",
    "section": "gas_price.csv",
    "text": "gas_price.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear (Jan 1st)\n\n\ngas_current\ndouble\nGas price in that year (dollars/gallon)\n\n\ngas_constant\ndouble\nGas price (constant 2015 dollars/gallon)"
  },
  {
    "objectID": "data/2019/2019-09-17/readme.html#locations.csv",
    "href": "data/2019/2019-09-17/readme.html#locations.csv",
    "title": "National Park Visits",
    "section": "locations.csv",
    "text": "locations.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlon\ndouble\nlongitude\n\n\nlat\ndouble\nlatitude\n\n\ngnis_id\ncharacter\nID for shapefile and long-lat lookup"
  },
  {
    "objectID": "data/2019/2019-09-03/readme.html",
    "href": "data/2019/2019-09-03/readme.html",
    "title": "Moore’s Law",
    "section": "",
    "text": "This week’s data is from Wikipedia - by way of the Data Is Beautiful Subreddit.\nAdditional info and graphics can be found at Our World in Data.\nMoore’s Law: Transistors per microprocessor\n\nThe observation that the number of transistors in a dense integrated circuit doubles approximately every two years."
  },
  {
    "objectID": "data/2019/2019-09-03/readme.html#cpu.csv",
    "href": "data/2019/2019-09-03/readme.html#cpu.csv",
    "title": "Moore’s Law",
    "section": "cpu.csv",
    "text": "cpu.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nprocessor\ncharacter\nProcessor name\n\n\ntransistor_count\ndouble\nNumber of transitors\n\n\ndate_of_introduction\ndouble\nyear introduced\n\n\ndesigner\ncharacter\nDesigner\n\n\nprocess\ndouble\nSize of manufacturing process (in nanometers)\n\n\narea\ndouble\nArea of chip in square millimeters"
  },
  {
    "objectID": "data/2019/2019-09-03/readme.html#gpu.csv",
    "href": "data/2019/2019-09-03/readme.html#gpu.csv",
    "title": "Moore’s Law",
    "section": "gpu.csv",
    "text": "gpu.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nprocessor\ncharacter\nProcessor name\n\n\ntransistor_count\ndouble\nTransistor count\n\n\ndate_of_introduction\ndouble\nyear introduced\n\n\ndesigner_s\ncharacter\ndesigner\n\n\nmanufacturer_s\ncharacter\nmanufacturer\n\n\nprocess\ndouble\nsize of manufacturing process (nanometers)\n\n\narea\ndouble\narea of chip in square millimeters\n\n\nref\ncharacter\nreference"
  },
  {
    "objectID": "data/2019/2019-09-03/readme.html#ram.csv",
    "href": "data/2019/2019-09-03/readme.html#ram.csv",
    "title": "Moore’s Law",
    "section": "ram.csv",
    "text": "ram.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nchip_name\ncharacter\nChip name\n\n\ncapacity_bits\ncharacter\ncapacity bits - essentially how many units of information it can work on, units in next column\n\n\nbit_units\ncharacter\nUnits for bit capacity (bits &lt; kb &lt; Mb &lt; Gb)\n\n\nram_type\ncharacter\nRam type\n\n\ntransistor_count\ndouble\nTransistor count\n\n\ndate_of_introduction\ndouble\nYear introduced\n\n\nmanufacturer_s\ncharacter\nManufactured\n\n\nprocess\ndouble\nSize of manufacturing process (nanometers)\n\n\narea\ndouble\nArea of chip in square millimeters\n\n\nref\ncharacter\nreference"
  },
  {
    "objectID": "data/2019/2019-08-20/readme.html",
    "href": "data/2019/2019-08-20/readme.html",
    "title": "Nuclear Explosions",
    "section": "",
    "text": "This week’s data is from Stockholm International Peace Research Institute, by way of data is plural with credit to Jesus Castagnetto for sharing the dataset.\nAdditional information can be found on Wikipedia or via the original report PDF.\nAdditional related datasets can be found at Our World in Data.\nFor details around units for yield/magnitude, please see the Nuclear Yield formulas."
  },
  {
    "objectID": "data/2019/2019-08-20/readme.html#nuclear_explosions.csv",
    "href": "data/2019/2019-08-20/readme.html#nuclear_explosions.csv",
    "title": "Nuclear Explosions",
    "section": "nuclear_explosions.csv",
    "text": "nuclear_explosions.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate_long\ndate\nymd date\n\n\nyear\ndouble\nyear of explosion\n\n\nid_no\ndouble\nunique ID\n\n\ncountry\ncharacter\nCountry deploying the nuclear device\n\n\nregion\ncharacter\nRegion where nuclear device was deployed\n\n\nsource\ncharacter\nSource the reported the explosion event\n\n\nlatitude\ndouble\nLatitude position\n\n\nlongitude\ndouble\nLongitude position\n\n\nmagnitude_body\ndouble\nBody wave magnitude of explosion (mb)\n\n\nmagnitude_surface\ndouble\nSurface wave magnitude of explosion (Ms)\n\n\ndepth\ndouble\nDepth at detonation in Km (could be underground or above ground) – please note that positive = depth (below ground), while negative = height (above ground)\n\n\nyield_lower\ndouble\nExplosion yield lower estimate in kilotons of TNT\n\n\nyield_upper\ndouble\nExplosion yield upper estimate in kilotons of TNT\n\n\npurpose\ncharacter\nPurpose of detonation: COMBAT (WWII bombs dropped over Japan), FMS (Soviet test, study phenomenon of nuclear explosion), ME (Military Exercise), PNE (Peaceful nuclear explosion), SAM (Soviet test, accidental mode/emergency), SSE (French/US tests - testing safety of nuclear weapons in case of accident), TRANSP (Transportation-storage purposes), WE (British, French, US, evaluate effects of nuclear detonation on various targets), WR (Weapons development program)\n\n\nname\ncharacter\nName of event or bomb\n\n\ntype\ncharacter\ntype - method of deployment – ATMOSPH (Atmospheric), UG (underground), BALLOON (Balloon drop), AIRDROP (Airplane deployed), ROCKET (Rocket deployed), TOWER (deplyed at top of constructed tower), WATERSURFACE (on surface of body of water), BARGE (on barge boat), SURFACE (on surface or in shallow crater), UW (Underwater), SHAFT (Vertical Shaft underground), TUNNEL/GALLERY (Horizontal tunnel)"
  },
  {
    "objectID": "data/2019/2019-08-06/readme.html",
    "href": "data/2019/2019-08-06/readme.html",
    "title": "Bob Ross - painting by the numbers",
    "section": "",
    "text": "This week’s DATA is from 538, and can either be read in directly from this repo or from the 538 R package which was recently updated to include more data from 538!\nThe 538 article can be found here."
  },
  {
    "objectID": "data/2019/2019-08-06/readme.html#bob-ross.csv",
    "href": "data/2019/2019-08-06/readme.html#bob-ross.csv",
    "title": "Bob Ross - painting by the numbers",
    "section": "bob-ross.csv",
    "text": "bob-ross.csv\n\nEach of the columns after episode/title correspond to the binary presence (0 or 1) of that element in the painting.\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEPISODE\ncharacter\nEpisode Number\n\n\nTITLE\ncharacter\nEpisode title\n\n\nAPPLE_FRAME\ndouble\n.\n\n\nAURORA_BOREALIS\ndouble\n.\n\n\nBARN\ndouble\n.\n\n\nBEACH\ndouble\n.\n\n\nBOAT\ndouble\n.\n\n\nBRIDGE\ndouble\n.\n\n\nBUILDING\ndouble\n.\n\n\nBUSHES\ndouble\n.\n\n\nCABIN\ndouble\n.\n\n\nCACTUS\ndouble\n.\n\n\nCIRCLE_FRAME\ndouble\n.\n\n\nCIRRUS\ndouble\n.\n\n\nCLIFF\ndouble\n.\n\n\nCLOUDS\ndouble\n.\n\n\nCONIFER\ndouble\n.\n\n\nCUMULUS\ndouble\n.\n\n\nDECIDUOUS\ndouble\n.\n\n\nDIANE_ANDRE\ndouble\n.\n\n\nDOCK\ndouble\n.\n\n\nDOUBLE_OVAL_FRAME\ndouble\n.\n\n\nFARM\ndouble\n.\n\n\nFENCE\ndouble\n.\n\n\nFIRE\ndouble\n.\n\n\nFLORIDA_FRAME\ndouble\n.\n\n\nFLOWERS\ndouble\n.\n\n\nFOG\ndouble\n.\n\n\nFRAMED\ndouble\n.\n\n\nGRASS\ndouble\n.\n\n\nGUEST\ndouble\n.\n\n\nHALF_CIRCLE_FRAME\ndouble\n.\n\n\nHALF_OVAL_FRAME\ndouble\n.\n\n\nHILLS\ndouble\n.\n\n\nLAKE\ndouble\n.\n\n\nLAKES\ndouble\n.\n\n\nLIGHTHOUSE\ndouble\n.\n\n\nMILL\ndouble\n.\n\n\nMOON\ndouble\n.\n\n\nMOUNTAIN\ndouble\n.\n\n\nMOUNTAINS\ndouble\n.\n\n\nNIGHT\ndouble\n.\n\n\nOCEAN\ndouble\n.\n\n\nOVAL_FRAME\ndouble\n.\n\n\nPALM_TREES\ndouble\n.\n\n\nPATH\ndouble\n.\n\n\nPERSON\ndouble\n.\n\n\nPORTRAIT\ndouble\n.\n\n\nRECTANGLE_3D_FRAME\ndouble\n.\n\n\nRECTANGULAR_FRAME\ndouble\n.\n\n\nRIVER\ndouble\n.\n\n\nROCKS\ndouble\n.\n\n\nSEASHELL_FRAME\ndouble\n.\n\n\nSNOW\ndouble\n.\n\n\nSNOWY_MOUNTAIN\ndouble\n.\n\n\nSPLIT_FRAME\ndouble\n.\n\n\nSTEVE_ROSS\ndouble\n.\n\n\nSTRUCTURE\ndouble\n.\n\n\nSUN\ndouble\n.\n\n\nTOMB_FRAME\ndouble\n.\n\n\nTREE\ndouble\n.\n\n\nTREES\ndouble\n.\n\n\nTRIPLE_FRAME\ndouble\n.\n\n\nWATERFALL\ndouble\n.\n\n\nWAVES\ndouble\n.\n\n\nWINDMILL\ndouble\n.\n\n\nWINDOW_FRAME\ndouble\n.\n\n\nWINTER\ndouble\n.\n\n\nWOOD_FRAMED\ndouble\n."
  },
  {
    "objectID": "data/2019/2019-07-23/readme.html",
    "href": "data/2019/2019-07-23/readme.html",
    "title": "FAA Wildlife Strike Database",
    "section": "",
    "text": "FAA Wildlife Strike Database\n’The FAA Wildlife Strike Database contains records of reported wildlife strikes since 1990. Strike reporting is voluntary. Therefore, this database only represents the information we have received from airlines, airports, pilots, and other sources. Report on wildlife impacts”. To keep things simple I just took data from the big 4: American Airlines, Delta, Southwest, and United as they account for ~ 70% of passengers in the USA but there are many many more available airlines.\nA report on the full dataset.\nSource\nFull data dictionary\nh/t to my colleague Katie Masiello for sharing this dataset!\n\n\nGet the data!\nwildlife_impacts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-07-23/wildlife_impacts.csv\")\n\n\n\nData Dictionary\n\nwildlife_impacts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nincident_date\ndate\nDate of incident\n\n\nstate\ncharacter\nState\n\n\nairport_id\ncharacter\nAirport ID\n\n\nairport\ncharacter\nAirport Name\n\n\noperator\ncharacter\nOperator/Airline\n\n\natype\ncharacter\nAirline type\n\n\ntype_eng\ncharacter\nEngine type\n\n\nspecies_id\ncharacter\nSpecies ID\n\n\nspecies\ncharacter\nSpecies\n\n\ndamage\ncharacter\nDamage: N None M Minor, M Uncertain, S Substantial, D Destroyed\n\n\nnum_engs\ncharacter\nNumber of engines\n\n\nincident_month\ndouble\nIncident month\n\n\nincident_year\ndouble\nIncident year\n\n\ntime_of_day\ncharacter\nIncident Time of day\n\n\ntime\ndouble\nIncident time\n\n\nheight\ndouble\nPlane height at impact\n\n\nspeed\ndouble\nPlane speed at impact\n\n\nphase_of_flt\ncharacter\nPhase of flight at impact\n\n\nsky\ncharacter\nSky condition\n\n\nprecip\ncharacter\nPrecipitation\n\n\ncost_repairs_infl_adj\ndouble\nCost of repairs adjusted for inflation\n\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(rvest)\n\ndf_aa &lt;- readxl::read_excel(here(\"2019\", \"2019-07-23\", \"wildlife_aa.xlsx\"))\n\ndf_delta &lt;- readxl::read_excel(here(\"2019\", \"2019-07-23\", \"wildlife_delta.xlsx\"))\n\ndf_southwest &lt;- readxl::read_excel(here(\"2019\", \"2019-07-23\", \"wildlife_southwest.xlsx\"))\n\ndf_united &lt;- readxl::read_excel(here(\"2019\", \"2019-07-23\", \"wildlife_united.xlsx\"))\n\ndf_all &lt;- bind_rows(df_aa, df_delta) %&gt;% \n  bind_rows(df_southwest) %&gt;% \n  bind_rows(df_united) %&gt;% \n  janitor::clean_names() %&gt;% \n  select(incident_date:airport, operator:damage, num_engs,incident_month:time,\n         height, speed, phase_of_flt, sky, precip, cost_repairs_infl_adj)\n\ndf_all %&gt;% \n  write_csv(here(\"2019\" , \"2019-07-23\", \"wildlife_impacts.csv\"))\n\n# create dictionary\ntomtom::create_dictionary(df_all)\n\n# test plot\ndf_all %&gt;% \n  ggplot(aes(x = incident_date)) +\n  geom_histogram() +\n  facet_wrap(~operator)\n\n# scrape airline size\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_largest_airlines_in_North_America\"\n\nraw_html &lt;- url %&gt;% \n  read_html() %&gt;% \n  html_table(fill = TRUE)\n\nclean_pass &lt;- raw_html %&gt;% .[[1]] %&gt;% \n  as_tibble() %&gt;% \n  select(Rank:`2018`) %&gt;% \n  rename(\"passengers\" = `2018`) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(passengers = parse_number(passengers),\n         group = c(rep(\"big 4\", 4), rep(\"Other\", 13))) \n\ntotal_sum &lt;- clean_pass %&gt;% \n  pull(passengers) %&gt;% \n  sum()\n\nclean_pass %&gt;% \n  group_by(group) %&gt;% \n  summarize(sum = sum(passengers)) %&gt;% \n  mutate(percent_of_total = paste0(round(sum/total_sum * 100, 1), \"%\"))\n\n|group |       sum|percent_of_total |\n|:-----|---------:|:----------------|\n|big 4 | 718146104|70.3%            |\n|Other | 304031622|29.7%            |"
  },
  {
    "objectID": "data/2019/2019-07-09/readme.html",
    "href": "data/2019/2019-07-09/readme.html",
    "title": "Women’s World Cup Results",
    "section": "",
    "text": "This dataset seemed appropriate for a lot of reasons - useR2019 is in France this year, the Women’s World Cup was in France this year and the tournament just ended! Congrats to all the teams for qualifying and participating in the world stage!\nThis data comes from data.world and includes final score and win/loss status data from 1991-2019. Additionally the 2019 rosters for each team are included. The data was uploaded to data.world as part of the #SportsVizSunday Twitter project courtesy of Simon Beaumont. There’s a lot more data at Wikipedia, including Attendance, Host/Results, and conference-level data. Additionally, there are lots of other fun sports-related datasets from #SportsVizSundayas seen at data.world.\nI cleaned up the data and have attached the final and original dataset. I also added the 2019 match data by hand through the Finals!\nCountry codes are on Wikipedia.\nI have included my script in this repo so you can take a peek at how we got here.\nAdditional datasets of interest: - Free play-by-play data - 538 Predictions article - 538 Predictions data"
  },
  {
    "objectID": "data/2019/2019-07-09/readme.html#cleaning-script",
    "href": "data/2019/2019-07-09/readme.html#cleaning-script",
    "title": "Women’s World Cup Results",
    "section": "Cleaning script",
    "text": "Cleaning script\nlibrary(tidyverse)\nlibrary(here)\nlibrary(rvest)\n\n# read in the datasets\ndf &lt;- readxl::read_xlsx(here(\"2019\", \"2019-07-09\", \"wwc_results.xlsx\")) %&gt;% \n  mutate(Year = as.integer(Year))\n\ndf_2019 &lt;- read_csv(here(\"2019\", \"2019-07-09\", \"wwc_2019.csv\"))\n\nsquads &lt;- readxl::read_xlsx(here(\"2019\", \"2019-07-09\", \"Womens Squads.xlsx\")) %&gt;% \n  janitor::clean_names()\n\n# bind datasets to include 2019\ndf_all &lt;- bind_rows(df, df_2019) %&gt;% \n  janitor::clean_names()\n\n# add win, tie status\ndf_both &lt;- df_all %&gt;% \n  group_by(year) %&gt;% \n  mutate(yearly_game_id = row_number(),\n         winner = case_when(score_1 &gt; score_2 ~ \"Team 1 Win\",\n                   score_2 &gt; score_1 ~ \"Team 2 Win\",\n                   score_1 == score_2 ~ \"Tie\",\n                   TRUE ~ NA_character_)) \n\n# grab team 1/score 1\ndf_team_1 &lt;- df_both %&gt;% \n  select(year:score_1, round, yearly_game_id, winner) %&gt;% \n  set_names(nm = c(\"year\", \"team\", \"score\", \"round\", \"yearly_game_id\", \"winner\")) %&gt;% \n  mutate(team_num = 1)\n\n# grab team2/score 2\ndf_team_2 &lt;- df_both %&gt;% \n  select(year, team_2:yearly_game_id, winner) %&gt;% \n  set_names(nm = c(\"year\", \"team\", \"score\", \"round\", \"yearly_game_id\", \"winner\")) %&gt;% \n  mutate(team_num = 2)\n\n# attach team1/team2 datasets together\n# Assign winner, loser, tie,\n# Correct for shootout wins in knockout stages\n\ndf_tidy &lt;- bind_rows(df_team_1, df_team_2) %&gt;% \n  arrange(year, yearly_game_id) %&gt;% \n  mutate(win_status = case_when(team_num == as.integer(str_extract(winner, \"[:digit:]\")) ~ \"Won\",\n                            team == \"USA\" & round == \"Final\" & year == 1999 ~ \"Won\",\n                            team == \"NOR\" & round == \"Round of 16\" & year == 2019 ~ \"Won\",\n                            team == \"JPN\" & round == \"Final\" & year == 2011 ~ \"Won\",\n                            team == \"CHN\" & round == \"Quarter Final\" & year == 1995 ~ \"Won\",\n                            team == \"FRA\" & round == \"Quarter Final\" & year == 2011 ~ \"Won\",\n                            team == \"USA\" & round == \"Quarter Final\" & year == 2011 ~ \"Won\",\n                            team == \"GER\" & round == \"Quarter Final\" & year == 2015 ~ \"Won\",\n                            team == \"BRA\" & round == \"Third Place Playoff\" & year == 1999 ~ \"Won\",\n                            round == \"Group\" & winner == \"Tie\" ~ \"Tie\",\n                            TRUE ~ \"Lost\")) %&gt;% \n  select(-winner)\n\n# confirm no double winners/losers\ndf_tidy %&gt;% \n  filter(round != \"Group\") %&gt;% \n  group_by(year, round, yearly_game_id) %&gt;% \n  count(win_status, sort = TRUE) %&gt;% \n  filter(n &gt;1)\n\nurl &lt;- \"https://simple.wikipedia.org/wiki/List_of_FIFA_country_codes\"\n\nhtml_content &lt;- url %&gt;% \n  read_html()\n\ncode_df &lt;- html_content %&gt;% \n  html_table(fill = TRUE) %&gt;% .[[1]] %&gt;% \n  select(1,2) %&gt;% \n  slice(-1, -2) %&gt;% \n  set_names(nm = c(\"country\", \"team\"))\n\n# output to csv\ndf_tidy %&gt;% \n  write_csv(here(\"2019\", \"2019-07-09\", \"wwc_outcomes.csv\"))\n\nsquads %&gt;% \n  write_csv(here(\"2019\", \"2019-07-09\", \"squads.csv\"))\n\ncode_df %&gt;% \n  write_csv(here(\"2019\", \"2019-07-09\", \"codes.csv\"))\n\n# data dictionaries for TidyTuesday\ntomtom::create_dictionary(df_tidy)\ntomtom::create_dictionary(squads)"
  },
  {
    "objectID": "data/2019/2019-06-25/readme.html",
    "href": "data/2019/2019-06-25/readme.html",
    "title": "UFO Sightings around the world",
    "section": "",
    "text": "UFO Sightings around the world\nThis data includes &gt;80,000 recorded UFO “sightings” around the world, including the UFO shape, lat/long and state/country of where the sighting occurred, duration of the “event” and the data_time when it occurred.\nData comes originally from NUFORC, was cleaned and uploaded to Github by Sigmond Axel, and some exploratory plots were created by Jonathan Bouchet a few years back.\nH/t to Georgios Karamanis for sharing the data as an issue on Github. If you want to submit a dataset of your own interest, please do so as an Issue on our GitHub! If you do submit a dataset, please drop a link and some context as an issue. Thanks!\n\n\nGet the data!\nufo_sightings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-06-25/ufo_sightings.csv\")\n\n\nData Dictionary\n\nufo_sightings.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate_time\ndatetime (mdy h:m)\nDate time sighting occurred\n\n\ncity_area\ncharacter\nCity or area of sighting\n\n\nstate\ncharacter\nstate/region of sighting\n\n\ncountry\ncharacter\nCountry of sighting\n\n\nufo_shape\ncharacter\nUFO Shape\n\n\nencounter_length\ndouble\nEncounter length in seconds\n\n\ndescribed_encounter_length\ncharacter\nEncounter length as described (eg 1 hour, etc\n\n\ndescription\ncharacter\nDescription of encounter\n\n\ndate_documented\ncharacter\nDate documented\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude"
  },
  {
    "objectID": "data/2019/2019-06-11/readme.html",
    "href": "data/2019/2019-06-11/readme.html",
    "title": "Meteorite Impacts",
    "section": "",
    "text": "Meteorite Impacts\nThis week’s dataset is a dataset all about meteorites, where they fell and when they fell! Data comes from the Meteoritical Society by way of NASA. H/t to #TidyTuesday community member Malin Axelsson for sharing this data as an issue on GitHub!\nIf you want to find out more about meteorite classifications, Malin was kind enough to share a wikipedia article as well!\nIf you want to submit a dataset of your own interest, please do so as an Issue on our GitHub! If you do submit a dataset, please drop a link and some context as an issue. Thanks!\n\n\nGet the data!\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-06-11/meteorites.csv\")\n\n\nData Dictionary\n\nmeteorites.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nMeteorite name\n\n\nid\ndouble\nMeteorite numerical ID\n\n\nname_type\ncharacter\nName type either valid or relict, where relict = a meteorite that cannot be assigned easily to a class\n\n\nclass\ncharacter\nClass of the meteorite, please see Wikipedia for full context\n\n\nmass\ndouble\nMass in grams\n\n\nfall\ncharacter\nFell or Found meteorite\n\n\nyear\ninteger\nYear found\n\n\nlat\ndouble\nLatitude\n\n\nlong\ndouble\nLongitude\n\n\ngeolocation\ncharacter\nGeolocation"
  },
  {
    "objectID": "data/2019/2019-05-28/readme.html",
    "href": "data/2019/2019-05-28/readme.html",
    "title": "Wine ratings",
    "section": "",
    "text": "Wine ratings\nThis week’s dataset is a wine-enthusiast ratings dataset from Kaggle. H/t to Georgios Karamanis for sharing this week’s dataset as an Issue on our GitHub! If you want to submit a dataset, please drop a link and some context as an issue. Thanks!\nAn article with some interesting graphs (different dataset) can be found here. They found some clear relationships, but it also appears that cost is factored into their rating system. How does the Wine Enthusiast ratings scale compare?\n\n\nGet the data!\nwine_ratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-05-28/winemag-data-130k-v2.csv\")\n\n\nData Dictionary\n\nwinemag-data-130k-v2.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry of origin\n\n\ndescription\ncharacter\nFlavors and taste profile as written by reviewer\n\n\ndesignation\ncharacter\nThe vineyard within the winery where the grapes that made the wine are from\n\n\npoints\ndouble\nThe number of points WineEnthusiast rated the wine on a scale of 1-100 (though they say they only post reviews for wines that score &gt;=80)\n\n\nprice\ndouble\nThe cost for a bottle of the wine\n\n\nprovince\ncharacter\nThe province or state that the wine is from\n\n\nregion_1\ncharacter\nThe wine growing area in a province or state (ie Napa)\n\n\ntaster_name\ncharacter\nThe taster/reviewer\n\n\ntitle\ncharacter\nThe title of the wine review, which often contains the vintage (year)\n\n\nvariety\ncharacter\nGrape type\n\n\nwinery\ncharacter\nThe winery that made the wine"
  },
  {
    "objectID": "data/2019/2019-05-14/readme.html",
    "href": "data/2019/2019-05-14/readme.html",
    "title": "Nobel Laureate Publications",
    "section": "",
    "text": "Nobel Laureate Publications\n“The Nobel Prize is a set of annual international awards bestowed in several categories by Swedish and Norwegian institutions in recognition of academic, cultural, or scientific advances.” - Wikipedia.\nThis week’s h/t goes to Georgios Karamanis for sharing the Nobel data links on GitHub! The data covers metadata for all of the Nobel prize winners. An additional dataset looks at all the publications from each winner. From the data aggregators for the publications dataset, “We constructed the publication records for almost all Nobel laureates in physics, chemistry, and physiology or medicine from 1900 to 2016 (545 out of 590, 92.4%). We first collected information manually from Nobel Prize official websites, their university websites, and Wikipedia. We then match it algorithmically with big data, tracing publication records from the MAG database.”\nThe raw data for all the papers come from the Harvard dataverse as seen here and the information about the winners themselves comes from Kaggle as seen here.\nIf you want to dive deeper on some network analysis - Wagner et al, 2015 examined Do Nobel Laureates Create Prize-Winning Networks? and shared some data.\nThere is also a nice run through with tidyverse codepic on Kaggle here.\n\n\nGet the data!\nnobel_winners &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-05-14/nobel_winners.csv\")\nnobel_winner_all_pubs &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-05-14/nobel_winner_all_pubs.csv\")\n\n\nData Dictionary\n\nnobel_winners.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nprize_year\ndouble\nYear that Nobel Prize was awarded\n\n\ncategory\ncharacter\nField of study/category\n\n\nprize\ncharacter\nPrize Name\n\n\nmotivation\ncharacter\nMotivation of the award\n\n\nprize_share\ncharacter\nShare eg 1 of 1, 1 of 2, 1 of 4, etc\n\n\nlaureate_id\ndouble\nID assigned to each winner\n\n\nlaureate_type\ncharacter\nIndividual or organization\n\n\nfull_name\ncharacter\nname of the winner\n\n\nbirth_date\ndouble\nbirth date of winner\n\n\nbirth_city\ncharacter\nbirth city/state of winner\n\n\nbirth_country\ncharacter\nbirth country of winner\n\n\ngender\ncharacter\nbinary gender of the winner\n\n\norganization_name\ncharacter\norganization name\n\n\norganization_city\ncharacter\norganization city\n\n\norganization_country\ncharacter\norganization country\n\n\ndeath_date\ndouble\ndeath date of the winner (if dead)\n\n\ndeath_city\ncharacter\ndeath city (if dead)\n\n\ndeath_country\ncharacter\ndeath country (if dead)\n\n\n\n\n\nnobel_winner_all_pubs.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlaureate_id\ndouble\nAssigned Laureate ID (doesn’t match other dataset)\n\n\nlaureate_name\ncharacter\nAbbreviated name\n\n\nprize_year\ndouble\nPrize year\n\n\ntitle\ncharacter\nTitle of paper\n\n\npub_year\ndouble\nPublication year\n\n\npaper_id\ndouble\nPaper ID\n\n\ndoi\ncharacter\nDOI of paper\n\n\njournal\ncharacter\nJournal paper published in\n\n\naffiliation\ncharacter\nAffiliation of the author\n\n\nis_prize_winning_paper\ncharacter\nIs associated as the Nobel winning paper (yes or no)\n\n\ncategory\ncharacter\nCategory/field of study\n\n\n\n\n\n\nCleaning script\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\n# read in the specific category/field datasets and the overall winners\n\nnobel_winners &lt;- read_csv(here(\"2019\", \"2019-05-14\", \"archive.csv\")) %&gt;% \n  janitor::clean_names() %&gt;% \n  rename(\"prize_year\" = year,\n         \"gender\" = sex)\n\nchem_pubs &lt;- read_csv(here(\"2019\", \"2019-05-14\", \"Chemistry publication record.csv\")) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(category = \"chemistry\")\n\nmed_pubs &lt;- read_csv(here(\"2019\", \"2019-05-14\", \"Medicine publication record.csv\")) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(category = \"medicine\")\n\nphysics_pubs &lt;- read_csv(here(\"2019\", \"2019-05-14\", \"Physics publication record.csv\")) %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(category = \"physics\")\n\nall_pubs &lt;- bind_rows(chem_pubs, med_pubs, physics_pubs)\n\nall_pubs %&gt;% \n  write_csv(here(\"2019\", \"2019-05-14\", \"nobel_winner_all_pubs.csv\"))\n\nnobel_winners %&gt;% \n  write_csv(here(\"2019\", \"2019-05-14\", \"nobel_winners.csv\"))"
  },
  {
    "objectID": "data/2019/2019-04-30/readme.html",
    "href": "data/2019/2019-04-30/readme.html",
    "title": "Chicago Bird Collisions",
    "section": "",
    "text": "Winger et al, 2019 examined nocturnal flight-calling behavior and vulnerability to artificial light in migratory birds.\n\n“Understanding interactions between biota and the built environment is increasingly important as human modification of the landscape expands in extent and intensity. For migratory birds, collisions with lighted structures are a major cause of mortality, but the mechanisms behind these collisions are poorly understood. Using 40 years of collision records of passerine birds, we investigated the importance of species’ behavioural ecologies in predicting rates of building collisions during nocturnal migration through Chicago, IL and Cleveland, OH, USA.”\n\n\n“One of the few means to examine species-specific dynamics of social biology during nocturnal bird migration is through the study of short vocalizations made in flight by migrating birds. Many species of birds, especially passerines (order Passeriformes), produce such vocal signals during their nocturnal migrations. These calls (hereafter, ‘flight calls’) are hypothesized to function as important social cues for migrating birds that may aid in orientation, navigation and other decision-making behaviours.not all nocturnally migratory species make flight calls, raising the possibility that different lineages of migratory birds vary in the degree to which social cues and collective decisions are important for accomplishing migration.”\n\nI have only uploaded the raw and tamed Chicago dataset as it is the most complete, but you can access the full raw data here.\nEach row in the bird_collisions.csv dataset accounts for a single observation of a bird collision. You can aggregate by species/genus, time, or other factors.\nh/t to Data is Plural 2019/04/10\n\n\nAn important point but somewhat spoiler from the authors &gt; From 2000 to 2018, D.E.W. and M.H. recorded data on the status of night-time lighting at McCormick Place during pre-dawn walks to collect collisions by recording the proportion of the 17 window bays that were illuminated… We used this index to test whether building lighting influenced the number of collisions and whether the influence of light levels on collisions counts varied across the sets of species with different flight-calling behaviour or habitat preferences.\nThere is a factor data column (bird_collisions$locality) that indicates if the data was collected at McCormick Place (MP) or elsewhere in Chicago (CHI). If you dplyr::filter to only use MP you can dplyr::left_join the light data and the bird collision data to look at the effects of light on bird collisions from 2000 on."
  },
  {
    "objectID": "data/2019/2019-04-30/readme.html#citations",
    "href": "data/2019/2019-04-30/readme.html#citations",
    "title": "Chicago Bird Collisions",
    "section": "Citations",
    "text": "Citations\nWhen using this data, please cite the original publication:\n\nWinger BM, Weeks BC, Farnsworth A, Jones AW, Hennen M, Willard DE (2019) Nocturnal flight-calling behaviour predicts vulnerability to artificial light in migratory birds. Proceedings of the Royal Society B 286(1900): 20190364. https://doi.org/10.1098/rspb.2019.0364\n\nIf using the data alone, please cite the Dryad data package:\n\nWinger BM, Weeks BC, Farnsworth A, Jones AW, Hennen M, Willard DE (2019) Data from: Nocturnal flight-calling behaviour predicts vulnerability to artificial light in migratory birds. Dryad Digital Repository. https://doi.org/10.5061/dryad.8rr0498"
  },
  {
    "objectID": "data/2019/2019-04-23/readme.html",
    "href": "data/2019/2019-04-23/readme.html",
    "title": "Anime Dataset",
    "section": "",
    "text": "Anime Dataset\nThis week’s data comes from Tam Nguyen and MyAnimeList.net via Kaggle. According to Wikipedia - “MyAnimeList, often abbreviated as MAL, is an anime and manga social networking and social cataloging application website. The site provides its users with a list-like system to organize and score anime and manga. It facilitates finding users who share similar tastes and provides a large database on anime and manga. The site claims to have 4.4 million anime and 775,000 manga entries. In 2015, the site received 120 million visitors a month.”\nAnime without rankings or popularity scores were excluded. Producers, genre, and studio were converted from lists to tidy observations, so there will be repetitions of shows with multiple producers, genres, etc. The raw data is also uploaded.\nLots of interesting ways to explore the data this week!\n\n\nGet the Data\ntidy_anime &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-23/tidy_anime.csv\")\n\n\n\nData Dictionary\nHeads up the dataset is about 97 mb - if you want to free up some space, drop the synopsis and background, they are long strings, or broadcast, premiered, related as they are redundant or less useful.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nanimeID\ndouble\nAnime ID (as in https://myanimelist.net/anime/animeID)\n\n\nname\ncharacter\nanime title - extracted from the site.\n\n\ntitle_english\ncharacter\ntitle in English (sometimes is different, sometimes is missing)\n\n\ntitle_japanese\ncharacter\ntitle in Japanese (if Anime is Chinese or Korean, the title, if available, in the respective language)\n\n\ntitle_synonyms\ncharacter\nother variants of the title\n\n\ntype\ncharacter\nanime type (e.g. TV, Movie, OVA)\n\n\nsource\ncharacter\nsource of anime (i.e original, manga, game, music, visual novel etc.)\n\n\nproducers\ncharacter\nproducers\n\n\ngenre\ncharacter\ngenre\n\n\nstudio\ncharacter\nstudio\n\n\nepisodes\ndouble\nnumber of episodes\n\n\nstatus\ncharacter\nAired or not aired\n\n\nairing\nlogical\nTrue/False is still airing\n\n\nstart_date\ndouble\nStart date (ymd)\n\n\nend_date\ndouble\nEnd date (ymd)\n\n\nduration\ncharacter\nPer episode duration or entire duration, text string\n\n\nrating\ncharacter\nAge rating\n\n\nscore\ndouble\nScore (higher = better)\n\n\nscored_by\ndouble\nNumber of users that scored\n\n\nrank\ndouble\nRank - weight according to MyAnimeList formula\n\n\npopularity\ndouble\nbased on how many members/users have the respective anime in their list\n\n\nmembers\ndouble\nnumber members that added this anime in their list\n\n\nfavorites\ndouble\nnumber members that favorites these in their list\n\n\nsynopsis\ncharacter\nlong string with anime synopsis\n\n\nbackground\ncharacter\nlong string with production background and other things\n\n\npremiered\ncharacter\nanime premiered on season/year\n\n\nbroadcast\ncharacter\nwhen is (regularly) broadcasted\n\n\nrelated\ncharacter\ndictionary: related animes, series, games etc.\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(here)\n\nraw_df &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-23/raw_anime.csv\")\n\nclean_df &lt;- raw_df %&gt;% \n  # Producers\n  mutate(producers = str_remove(producers, \"\\\\[\"),\n         producers = str_remove(producers, \"\\\\]\")) %&gt;% \n  separate_rows(producers, sep = \",\") %&gt;% \n  mutate(producers = str_remove(producers, \"\\\\'\"),\n         producers = str_remove(producers, \"\\\\'\"),\n         producers = str_trim(producers)) %&gt;% \n  # Genre\n  mutate(genre = str_remove(genre, \"\\\\[\"),\n         genre = str_remove(genre, \"\\\\]\")) %&gt;% \n  separate_rows(genre, sep = \",\") %&gt;% \n  mutate(genre = str_remove(genre, \"\\\\'\"),\n         genre = str_remove(genre, \"\\\\'\"),\n         genre = str_trim(genre)) %&gt;% \n  # Studio\n  mutate(studio = str_remove(studio, \"\\\\[\"),\n         studio = str_remove(studio, \"\\\\]\")) %&gt;% \n  separate_rows(studio, sep = \",\") %&gt;% \n  mutate(studio = str_remove(studio, \"\\\\'\"),\n         studio = str_remove(studio, \"\\\\'\"),\n         studio = str_trim(studio)) %&gt;% \n  # Aired\n  mutate(aired = str_remove(aired, \"\\\\{\"),\n         aired = str_remove(aired, \"\\\\}\"),\n         aired = str_remove(aired, \"'from': \"),\n         aired = str_remove(aired, \"'to': \"),\n         aired = word(aired, start = 1, 2, sep = \",\")) %&gt;% \n  separate(aired, into = c(\"start_date\", \"end_date\"), sep = \",\") %&gt;% \n  mutate(start_date = str_remove_all(start_date, \"'\"),\n         start_date = str_sub(start_date, 1, 10),\n         end_date = str_remove_all(start_date, \"'\"),\n         end_date = str_sub(end_date, 1, 10)) %&gt;%\n  mutate(start_date = lubridate::ymd(start_date),\n         end_date = lubridate::ymd(end_date)) %&gt;% \n  # Drop unranked or unpopular series\n  filter(rank != 0,\n         popularity != 0)\n\nwrite_csv(clean_df, here(\"2019\", \"2019-04-22\", \"tidy_anime.csv\"))\nraw_df %&gt;% write_csv(here(\"2019\", \"2019-04-22\", \"raw_anime.csv\"))"
  },
  {
    "objectID": "data/2019/2019-04-09/readme.html",
    "href": "data/2019/2019-04-09/readme.html",
    "title": "Tennis Grand Slams",
    "section": "",
    "text": "A few exciting Tennis-related datasets this week, all of which come courtesy of Wikipedia! “All records are based on data from the Association of Tennis Professionals (ATP), the International Tennis Federation (ITF), and the official websites of the four Grand Slam tournaments.” - Wikipedia\n“The Open Era is the current era of professional tennis. It began in 1968 when the Grand Slam tournaments allowed professional players to compete with amateurs, ending the division that had persisted since the dawn of the sport in the 19th century.” - Wikipedia\n“The Grand Slam tournaments, also called majors, are the four most important annual tennis events. They offer the most ranking points, prize money, public and media attention, the greatest strength and size of field, and greater number of”best of” sets for men. The Grand Slam itinerary consists of the Australian Open in mid January, the French Open around late May through early June, Wimbledon in June-July, and the US Open in August-September. Each tournament is played over a two-week period. The Australian and United States tournaments are played on hard courts, the French on clay, and Wimbledon on grass.” - Wikipedia\nThe court surface could be an interesting additional piece of data that I have left out, you could add it in with some clever case_when() calls.\nI have tamed the datasets pretty thoroughly but there are several ways you could combine, summarize, or otherwise plot the data for this week. I have a spoiler/hint at the bottom if you get stuck with combining! I also left my relatively rough cleaning and data collection .rmd in, and have uploaded it.\n\n\nThe Grand Slam tournaments happen in a rough 2-week timeframe as mentioned above, however I was unable to find a nice dataset that covered the specific day of both the men’s and women’s finals. As such I have used a static date that estimates the date of the championship match, and likely has an error of a few days for each tournament. However, this is still useful for determining the approximate age of the player at each tournament.\n\n\n\nWomen’s Timeline\nMen’s Timeline\nDate of Birth/First Title\nWomen’s Singles Champs\nMen’s Singles Champs\n\n\n\nThe Financial Times Article has lots of great inspiration plots, but uses different datasets. Specifically the author used match wins vs tournament wins and tennis rankings over time rather than tournament placing. The author John Burn-Murdoch is a great follow for DataViz and visual storytelling resources, including R and D3.\nAdditionally a gist and Tweet from John Burn-Murdoch goes through the process of collecting some men’s tennis data, and then how he iterated across several plots. It’s worth taking a look at for either code-inspiration for this week or as a general example of plot iteration for publication."
  },
  {
    "objectID": "data/2019/2019-04-09/readme.html#data-sources",
    "href": "data/2019/2019-04-09/readme.html#data-sources",
    "title": "Tennis Grand Slams",
    "section": "",
    "text": "Women’s Timeline\nMen’s Timeline\nDate of Birth/First Title\nWomen’s Singles Champs\nMen’s Singles Champs"
  },
  {
    "objectID": "data/2019/2019-04-09/readme.html#article",
    "href": "data/2019/2019-04-09/readme.html#article",
    "title": "Tennis Grand Slams",
    "section": "",
    "text": "The Financial Times Article has lots of great inspiration plots, but uses different datasets. Specifically the author used match wins vs tournament wins and tennis rankings over time rather than tournament placing. The author John Burn-Murdoch is a great follow for DataViz and visual storytelling resources, including R and D3.\nAdditionally a gist and Tweet from John Burn-Murdoch goes through the process of collecting some men’s tennis data, and then how he iterated across several plots. It’s worth taking a look at for either code-inspiration for this week or as a general example of plot iteration for publication."
  },
  {
    "objectID": "data/2019/2019-04-02/raw/readme.html",
    "href": "data/2019/2019-04-02/raw/readme.html",
    "title": "Raw Data Files",
    "section": "",
    "text": "Raw Data Files"
  },
  {
    "objectID": "data/2019/2019-03-19/readme.html",
    "href": "data/2019/2019-03-19/readme.html",
    "title": "The Stanford Open Policing Project",
    "section": "",
    "text": "The Stanford Open Policing Project Summary Video can be seen below. \n \nQuotes below from the Stanford Open Policing Project website:\n“On a typical day in the United States, police officers make more than 50,000 traffic stops. Our team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Our goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.\nCurrently, a comprehensive, national repository detailing interactions between police and the public doesn’t exist. That’s why the Stanford Open Policing Project is collecting and standardizing data on vehicle and pedestrian stops from law enforcement departments across the country — and we’re making that information freely available. We’ve already gathered over 200 million records from dozens of state and local police departments across the country.\nWe, the Stanford Open Policing Project, are an interdisciplinary team of researchers and journalists at Stanford University. We are committed to combining the academic rigor of statistical analysis with the explanatory power of data journalism.”\nNBC News recently covered this dataset (March 13, 2019) here.\nH/T to both DJ Patil and Alex Chohlas-Wood @LX_CW for making us aware of the dataset, and credit to the 15 people who helped contribute to collecting/cleaning/etc this data."
  },
  {
    "objectID": "data/2019/2019-03-19/readme.html#summary-level-datasets",
    "href": "data/2019/2019-03-19/readme.html#summary-level-datasets",
    "title": "The Stanford Open Policing Project",
    "section": "Summary level datasets",
    "text": "Summary level datasets\nIf that is a bit too much data to dig into, consider checking out the summary-level datasets here and the included figures from the group’s recent arXiv paper. If you do use the summary data - please cite their working paper ( arXiv:1706.05678 ). They have been kind enough to include all the code, data, figures, and even a tutorial!\nThese are datasets from the working paper mentioned above - the parent folder with the full details can be found here. Go here to skip straight to the results folder to get all the specific .csv files.\n\nGet the summary data\nThere are additional data files on their github, but a file was “created for convenience which combines data from all the main analyses in the paper”.\ncombined_data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/5harad/openpolicing/master/results/data_for_figures/combined_data.csv\")\nNo cleaning scripts this week, the summary level data is in great shape!\n\n\nData Dictionary\nFor the Summary-level datasets - there are a few data-dictionaries, you can find them here. These can help with conversion of county or district codes to more meaningful data.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlocation\ncharacter\nCounty/District location for each incidence\n\n\nstate\ncharacter\nState for each incidence\n\n\ndriver_race\ncharacter\nDriver’s race\n\n\nstops_per_year\ndouble\nNumber of stops per year\n\n\nstop_rate\ndouble\nStop rate (stop = police stop of a vehicle) (%)\n\n\nsearch_rate\ndouble\nSearch rate (%)\n\n\nconsent_search_rate\ndouble\nConsent to search rate (%)\n\n\narrest_rate\ndouble\nArrest rate (%)\n\n\ncitation_rate_speeding_stops\ndouble\nCitation rate for speeding stops (%)\n\n\nhit_rate\ndouble\nHit rate (%): the proportion of searches that successfully turn up contraband\n\n\ninferred_threshold\ndouble\nInferred threshold - based off the threshold test - please see section 4.2 of the paper."
  },
  {
    "objectID": "data/2019/2019-03-05/readme.html",
    "href": "data/2019/2019-03-05/readme.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "March is Women’s History month, as such we’re exploring data from the Bureau of Labor Statistics and the Census Bureau about women in the workforce. There are historical data about women’s earnings and employment status, as well as detailed information about specific occupation and earnings from 2013-2016.\nAccording to the AAUW - “The gender pay gap is the gap between what men and women are paid. Most commonly, it refers to the median annual pay of all women who work full time and year-round, compared to the pay of a similar cohort of men.” These data can be nuanced so please use your best judgement when reporting on trends in the dataset.\nThe specific jobs data came from the Census Bureau and the historical data comes from the Bureau of Labor here and here. The data is provided as is, and you recognize the limitations and issues in defining gender as binary.\nData Scientist and Austin #Rladies co-organizer Caitlin Hudon has some great recommendations on how to be a better ally for underepresented groups in tech. If you are interested in trying to make tech a better place for ALL please check out or support the following organizations.\nRLadies\nWomen Who Code\nGirls Who Code\nBlack Girls Code\nNational Center for Women & IT"
  },
  {
    "objectID": "data/2019/2019-03-05/readme.html#data-dictionary-1",
    "href": "data/2019/2019-03-05/readme.html#data-dictionary-1",
    "title": "TidyTuesday",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nYear\n\n\noccupation\ncharacter\nSpecific job/career\n\n\nmajor_category\ncharacter\nBroad category of occupation\n\n\nminor_category\ncharacter\nFine category of occupation\n\n\ntotal_workers\ndouble\nTotal estimated full-time workers &gt; 16 years old\n\n\nworkers_male\ndouble\nEstimated MALE full-time workers &gt; 16 years old\n\n\nworkers_female\ndouble\nEstimated FEMALE full-time workers &gt; 16 years old\n\n\npercent_female\ndouble\nThe percent of females for specific occupation\n\n\ntotal_earnings\ndouble\nTotal estimated median earnings for full-time workers &gt; 16 years old\n\n\ntotal_earnings_male\ndouble\nEstimated MALE median earnings for full-time workers &gt; 16 years old\n\n\ntotal_earnings_female\ndouble\nEstimated FEMALE median earnings for full-time workers &gt; 16 years old\n\n\nwage_percent_of_male\ndouble\nFemale wages as percent of male wages - NA for occupations with small sample size\n\n\n\n\n\n\nearnings_female.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ninteger\nYear\n\n\ngroup\ncharacter\nAge group\n\n\npercent\ndouble\nFemale salary percent of male salary\n\n\n\n\n\nemployed_gender.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\ntotal_full_time\ndouble\nPercent of total employed people usually working full time\n\n\ntotal_part_time\ndouble\nPercent of total employed people usually working part time\n\n\nfull_time_female\ndouble\nPercent of employed women usually working full time\n\n\npart_time_female\ndouble\nPercent of employed women usually working part time\n\n\nfull_time_male\ndouble\nPercent of employed men usually working full time\n\n\npart_time_male\ndouble\nPercent of employed men usually working part time\n\n\n\n\nSpoilers - Cleaning Script\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(unpivotr)\nlibrary(tidyxl)\nlibrary(rvest)\n\n\nDataset 1\ncol_nm &lt;- c(\n  \"category\", \"total_estimate\", \"total_moe3\", \"men_estimate\", \"men_moe3\",\n  \"women_estimate\", \"women_moe3\", \"percent_women\", \"percent_women_moe3\",\n  \"total_earnings_estimate\", \"total_earnings_moe3\", \"total_earnings_men_estimate\", \n  \"total_earnings_men_moe3\", \"total_earnings_women_estimate\", \"total_earnings_women_moe3\", \n  \"wage_percent_of_mens_estimate\", \"wage_percent_of_mens_moe3\"\n)\n\ncategory_names &lt;- readr::read_rds(\"category_names.rds\") %&gt;% str_remove_all(., \":\")\n\nearnings_2013 &lt;- readxl::read_excel(\"median-earnings-2013-final.xlsx\", skip = 6) %&gt;%\n  janitor::clean_names() %&gt;%\n  set_names(nm = col_nm) %&gt;%\n  filter(!is.na(total_estimate)) %&gt;%\n  mutate(year = 2013L) %&gt;%\n  mutate(category = category_names)\n\nearnings_2014 &lt;- readxl::read_excel(\"median-earnings-2014-final.xlsx\", skip = 6) %&gt;%\n  janitor::clean_names() %&gt;%\n  set_names(nm = col_nm) %&gt;%\n  filter(!is.na(total_estimate)) %&gt;%\n  mutate(year = 2014L) %&gt;%\n  mutate(category = category_names)\n\nearnings_2015 &lt;- readxl::read_excel(\"median-earnings-2015-final.xlsx\", skip = 6) %&gt;%\n  janitor::clean_names() %&gt;%\n  set_names(nm = col_nm) %&gt;%\n  filter(!is.na(total_estimate)) %&gt;%\n  filter(!str_detect(category, \"Transportation and Material Moving Occupations\")) %&gt;%\n  mutate(year = 2015L) %&gt;%\n  mutate(category = category_names)\n\nearnings_2016 &lt;- readxl::read_excel(\"median-earnings-2016-final.xlsx\", skip = 6) %&gt;%\n  janitor::clean_names() %&gt;%\n  set_names(nm = col_nm) %&gt;%\n  filter(!is.na(total_estimate)) %&gt;%\n  filter(!str_detect(category, \"Transportation and Material Moving Occupations\")) %&gt;%\n  mutate(year = 2016L) %&gt;%\n  mutate(category = category_names)\n\nall_years &lt;- bind_rows(earnings_2013, earnings_2014, earnings_2015, earnings_2016) %&gt;%\n  filter(!str_detect(category, \"Total\"))\n\n\nCreate the major category\nGrabbed the major categories from the table by hand and did some basic checks to make sure everything came out ok.\n\ncat1 &lt;- c(\n  \"Management, Business, and Financial Occupations\",\n  \"Computer, Engineering, and Science Occupations\",\n  \"Education, Legal, Community Service, Arts, and Media Occupations\",\n  \"Healthcare Practitioners and Technical Occupations\",\n  \"Service Occupations\",\n  \"Sales and Office Occupations\",\n  \"Natural Resources, Construction, and Maintenance Occupations\",\n  \"Production, Transportation, and Material Moving Occupations\"\n)\n\nlength(cat1)\n\ntibble(category_names) %&gt;%\n  filter(category_names %in% cat1) %&gt;%\n  nrow()\n\nall.equal(cat1, tibble(category_names) %&gt;%\n  filter(category_names %in% cat1) %&gt;%\n  pull())\n\n\nCreate the minor category\nGrabbed the minor categories from the table by hand and did some basic checks to make sure everything came out ok.\n\ncat2 &lt;- c(\n  \"Management Occupations\",\n  \"Business and Financial Operations Occupations\",\n  \"Computer and mathematical occupations\",\n  \"Architecture and Engineering Occupations\",\n  \"Life, Physical, and Social Science Occupations\",\n  \"Community and Social Service Occupations\",\n  \"Legal Occupations\",\n  \"Education, Training, and Library Occupations\",\n  \"Arts, Design, Entertainment, Sports, and Media Occupations\",\n  \"Healthcare Practitioners and Technical Occupations\",\n  \"Healthcare Support Occupations\",\n  \"Protective Service Occupations\",\n  \"Food Preparation and Serving Related Occupations\",\n  \"Building and Grounds Cleaning and Maintenance Occupations\",\n  \"Personal Care and Service Occupations\",\n  \"Sales and Related Occupations\",\n  \"Office and Administrative Support Occupations\",\n  \"Farming, Fishing, and Forestry Occupations\",\n  \"Construction and Extraction Occupations\",\n  \"Installation, Maintenance, and Repair Occupations\",\n  \"Production Occupations\",\n  \"Transportation Occupations\",\n  \"Material Moving Occupations\"\n)\n\nlength(cat2)\n\ntibble(category_names) %&gt;%\n  filter(category_names %in% cat2) %&gt;%\n  nrow()\n\nall.equal(cat2, tibble(category_names) %&gt;%\n  filter(category_names %in% cat2) %&gt;%\n  pull())\n\n\nAdd new columns\nAdd the new columns and remove “occupations” from the text to shorten things out.\ncategory_added &lt;- all_years %&gt;%\n  mutate(\n    cat1 = case_when(\n      category %in% cat1 ~ category,\n      TRUE ~ NA_character_\n    ),\n    cat2 = case_when(\n      category %in% cat2 ~ category,\n      TRUE ~ NA_character_\n    )\n  ) %&gt;%\n  mutate(\n    cat1 = str_remove(cat1, \" Occupations\"),\n    cat1 = str_remove(cat1, \" occupations\"),\n    cat2 = str_remove(cat2, \" Occupations\"),\n    cat2 = str_remove(cat2, \" occupations\"),\n    category = str_remove(category, \" Occupations\"),\n    category = str_remove(category, \"occupations\")\n  )\n\nclean_all &lt;- category_added %&gt;%\n  fill(cat1) %&gt;%\n  fill(cat2)\n\n\nCreate final dataframe\nClean up the names, filter to remove the within-table summary stats and leave only the more discrete occupations.\n\nnm_final &lt;- c(\"year\", \"occupation\", \"major_category\", \"minor_category\", \"total_workers\", \"workers_male\", \"workers_female\", \"percent_female\", \"total_earnings\", \"total_earnings_male\", \"total_earnings_female\", \"wage_percent_of_male\")\n\nfinal_all &lt;- clean_all %&gt;%\n  filter(!str_detect(category, cat1)) %&gt;%\n  filter(!str_detect(category, cat2)) %&gt;%\n  filter(category != \"Management,  Business, Science, and Arts Occupations\") %&gt;%\n  filter(category != \"Management, Business, and Financial Occupations\") %&gt;%\n  filter(category != \"Management,  Business, Science, and Arts\") %&gt;%\n  filter(!is.na(cat1)) %&gt;%\n  filter(!is.na(cat2)) %&gt;%\n  select(year, occupation = category, major_category = cat1, minor_category = cat2, everything()) %&gt;%\n  select(-contains(\"moe\")) %&gt;%\n  mutate_at(c(\"total_earnings_estimate\", \"total_earnings_men_estimate\", \"total_earnings_women_estimate\", \"wage_percent_of_mens_estimate\"), as.numeric) %&gt;%\n  set_names(nm = nm_final)\n\n\nMental and code checks\nfinal_all %&gt;% \n  group_by(year) %&gt;% count()\n\nfinal_all %&gt;% \n  ggplot(aes(x = year, y = total_earnings, group = occupation)) +\n  geom_line() +\n  facet_wrap(~major_category, scales = \"free\")\n\n\nScrape the additional datasets\nurl &lt;- \"https://www.bls.gov/opub/ted/2012/ted_20121123.htm\"\n\nraw_html &lt;- url %&gt;%\n  read_html()\n\nwomen_earnings &lt;- raw_html %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  .[[1]] %&gt;%\n  gather(group, percent, 2:9)\nurl2 &lt;- \"https://www.bls.gov/opub/ted/2017/percentage-of-employed-women-working-full-time-little-changed-over-past-5-decades.htm\"\n\nraw_html2 &lt;- url2 %&gt;%\n  read_html()\n\nwomen_employed &lt;- raw_html2 %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  .[[1]] %&gt;%\n  janitor::clean_names() %&gt;%\n  slice(2:50) %&gt;%\n  set_names(nm = c(\n    \"year\", \"total_full_time\", \"total_part_time\", \"full_time_female\", \"part_time_female\",\n    \"full_time_male\", \"part_time_male\"\n  )) %&gt;%\n  mutate_all(parse_number)"
  },
  {
    "objectID": "data/2019/2019-02-19/readme.html",
    "href": "data/2019/2019-02-19/readme.html",
    "title": "PhDs Awarded by Field",
    "section": "",
    "text": "PhDs Awarded by Field\nDr. Ellie Murray proposed a DataViz challenge for the #epibookclub based around the number of PhD degrees awraded in the USA. As an epidemiology postdoc she was especially interested in how others would approach DataViz for Epidemiology PhDs. There are additional fields within this dataset, so take a crack at whatever looks interesting!\n\n\nHappy Saturday #datavizbook #epibookclub! For this week, since we've learned a bit about #Rstats, I thought we'd try something new – a #dataviz challenge! The US gov collects data on all doctoral degree graduates every year. Let's see what we can learn! https://t.co/fjIBhq8Hkr\n\n— Ellie Murray (@EpiEllie) February 16, 2019\n\nThe data comes from the NSF - where there are at least 72 different datasets if you wanted to approach the data from a different angle. They are primarily summary tables stored as .xlsx files. Cleaning these can be a bit awkward so if you are interested, it would be a cool project to try to do fully in R!\nAlternatively - I have cleaned the data for you and saved as a .csv for you to read directly into R. There was a lot of duplication, ie totals were represented within the broader table, and there wasn’t a nice separation of the fields (simply indentation in the Excel sheet).\nTo get at the details for broad or major fields, dplyr::summarize is your friend!\nphd_field &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-19/phd_by_field.csv\")\n\nData Dictionary\nphd_by_field.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nbroad_field\ncharacter\nThe parent field (highest delineator)\n\n\nmajor_field\ncharacter\nA sub-field below the broad field\n\n\nfield\ncharacter\nThe fine-field, most detailed\n\n\nyear\ninteger\nYear PhD awarded\n\n\nn_phds\ndouble\nTotal number of PhDs awarded\n\n\n\n\n\nSpoilers - Cleaning Script\n# Because all the things\nlibrary(tidyverse)\nlibrary(tidyverse)\n\n# Grab just the sub-major (major field) titles for separating the columns\nsub_major_fields &lt;- readxl::read_excel(\"sed17-sr-tab012.xlsx\", skip = 3) %&gt;%\n  rename(field = `Field of study`) %&gt;%\n  filter(!is.na(field)) %&gt;%\n  pull(field)\n\n# Manually grabbed the broad fields (based off indentation)\nmajor_fields &lt;- c(\n  \"Life sciences\",\n  \"Physical sciences and earth sciences\",\n  \"Mathematics and computer sciences\",\n  \"Psychology and social sciences\",\n  \"Engineering\",\n  \"Education\",\n  \"Humanities and arts\",\n  \"Other\"\n)\n\n# read in fiend field dataset\n# create new columns based off the matching of additional major or broad fields\n\ndf &lt;- readxl::read_excel(\"sed17-sr-tab013.xlsx\", skip = 3) %&gt;%\n  rename(field = `Fine field of study`) %&gt;%\n  mutate(\n    field = case_when(\n      field == \"Othero\" ~ \"Other\",\n      TRUE ~ field\n    ),\n    sub_major_field = case_when(\n      field %in% sub_major_fields ~ field,\n      TRUE ~ NA_character_\n    ),\n    major_field = case_when(\n      field %in% major_fields ~ field,\n      TRUE ~ NA_character_\n    )\n  )\n\n\n# Use tidyr::fill() to fill in the repeats of each major/broad field\ndf_field &lt;- df %&gt;%\n  fill(major_field, .direction = \"down\") %&gt;%\n  fill(sub_major_field, .direction = \"down\") %&gt;%\n  filter(!field %in% major_fields) %&gt;%\n  filter(!field %in% sub_major_fields)\n\n# gather the years, remove the commas, and rename to appropriate columns\ndf_clean &lt;- df_field %&gt;%\n  gather(year, n_phds, `2008.0`:`2017.0`) %&gt;%\n  mutate(\n    year = factor(as.integer(year)),\n    n_phds = parse_number(n_phds)\n  ) %&gt;%\n  rename(field = field) %&gt;%\n  select(broad_field = major_field, major_field = sub_major_field, field, year, n_phds)\n\ndf_clean %&gt;% View()\n\n# Check to confirm numbers match (they do!)\ndf_clean %&gt;% \n  group_by(major_field, year) %&gt;% \n  summarize(sum(n_phds, na.rm = TRUE))\n# Write to .csv for posting\ndf_clean  %&gt;% \n  write_csv(\"phd_by_field.csv\")"
  },
  {
    "objectID": "data/2019/2019-02-05/readme.html",
    "href": "data/2019/2019-02-05/readme.html",
    "title": "House and Mortgage data",
    "section": "",
    "text": "Data this week comes from the Freddie Mac House Price Index at both the State and National level. The original source can be found here.\nThe House Price Index (HPI) is a broad measure of the movement of single-family house prices. The HPI is a weighted, repeat-sales index, meaning that it measures average price changes in repeat sales or refinancings on the same properties. This information is obtained by reviewing repeat mortgage transactions on single-family properties whose mortgages have been purchased or securitized by Fannie Mae or Freddie Mac since January 1975. - Quote from Federal Housing Finance Agency\nIf the term House Price Index is unfamiliar to you - check out the technical documents or this Wikipedia article.\nHint: A useful summary statistic could be the 12-month (yearly) percent change in the Price Index (eg is the price index rising or falling?).\nWe also have mortgage rates for fixed 30, fixed 15, and adjustable 5-1 Hybrids across time. Some of the data was only introduced in the 1990s and late 2000s, and as such some information can only be pulled from more recent data.\nLastly, we have included some recession dates in the US - the Great Recession (2007) had some major effects across many industries and markets. You can read more about some of the recent changes here from Fortune.\n\n\nHouse Price Index\nMortgage Rates\nRecession Dates\nor read the data directly into R!\nstate_hpi &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-05/state_hpi.csv\")\nmortgage_rates &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-05/mortgage.csv\")\nrecession_dates &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-05/recessions.csv\")"
  },
  {
    "objectID": "data/2019/2019-02-05/readme.html#some-useful-state-level-built-in-r-datasets",
    "href": "data/2019/2019-02-05/readme.html#some-useful-state-level-built-in-r-datasets",
    "title": "House and Mortgage data",
    "section": "Some useful state-level built-in R datasets",
    "text": "Some useful state-level built-in R datasets\nsource\nR currently contains the following “state” data sets. Note that all data are arranged according to alphabetical order of the state names.\nstate.abb:\ncharacter vector of 2-letter abbreviations for the state names.\nstate.area:\nnumeric vector of state areas (in square miles).\nstate.center:\nlist with components named x and y giving the approximate geographic center of each state in negative longitude and latitude. Alaska and Hawaii are placed just off the West Coast.\nstate.division:\nfactor giving state divisions (New England, Middle Atlantic, South Atlantic, East South Central, West South Central, East North Central, West North Central, Mountain, and Pacific).\nstate.name:\ncharacter vector giving the full state names.\nstate.region:\nfactor giving the region (Northeast, South, North Central, West) that each state belongs to."
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html",
    "href": "data/2019/2019-01-22/readme.html",
    "title": "Data Info",
    "section": "",
    "text": "Data comes from The Vera Institute GitHub. The raw dataset was taken from their GitHub - it is in a wide format and if you are keen on really flexing your data munging skills it is a worthy adversary! The truly raw data is seen here. My full code to reproduce the summary level datasets seen below can be found here, you can adapt this minorly to get more data from the original wide dataset.\nAlternatively, if you don’t want to spend the bulk of your time tidying data - I provided summary level and a subset of some tidy county-level data for you to play around with and visualize."
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html#data-files",
    "href": "data/2019/2019-01-22/readme.html#data-files",
    "title": "Data Info",
    "section": "Data files",
    "text": "Data files\n\n\n\nFile\nDescription\n\n\n\n\nprison_summary.csv\nSummary of population in prison by year and county-type\n\n\npretrial_summary.csv\nSummary of pretrial incarceration by year and county-type\n\n\nprison_population.csv\nCounty-level data for prison population\n\n\npretrial_population.csv\nCounty-level data for pretrial incarceration\n\n\nincarceration_trends.csv\nFull raw data (from Vera Institute)"
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html#data-dictionary",
    "href": "data/2019/2019-01-22/readme.html#data-dictionary",
    "title": "Data Info",
    "section": "Data Dictionary",
    "text": "Data Dictionary\nIncarceration Trends\nThe full codebook from the Vera Institute can be found here.\nPrison Summary\n\n\n\n\n\n\n\n\nvariable\nclass\nDescription\n\n\n\n\nyear\ninteger (date)\nYear\n\n\nurbanicity\ncharacter\nCounty-type (urban, suburban, small/mid, rural)\n\n\npop_category\ncharacter\nCategory for population - either race, gender, or Total\n\n\nrate_per_100000\ndouble\nRate within a category for prison population per 100,000 people\n\n\n\n\nPretrial Summary\n\n\n\n\n\n\n\n\nvariable\nclass\nDescription\n\n\n\n\nyear\ninteger (date)\nYear\n\n\nurbanicity\ncharacter\nCounty-type (urban, suburban, small/mid, rural)\n\n\npop_category\ncharacter\nCategory for population - either race, gender, or Total\n\n\nrate_per_100000\ndouble\nRate within a category for pretrial incarceration per 100,000 people\n\n\n\n\nPrison Population\n\n\n\n\n\n\n\n\nvariable\nclass\nDescription\n\n\n\n\nyear\ninteger (date)\nYear\n\n\nstate\ncharacter\nState code\n\n\ncounty_name\ncharacter\nCounty Name\n\n\nurbanicity\ncharacter\nCounty-type (urban, suburban, small/mid, rural)\n\n\nregion\ncharacter\nRegion of US\n\n\ndivision\ncharacter\nDivision of US\n\n\npop_category\ncharacter\nCategory for population - either race, gender, or Total\n\n\npopulation\ninteger\nNumber of total individuals in each category aged 15-64\n\n\nprison_population\ninteger\nNumber of individuals in prison for each category\n\n\n\n\nPretrial Population\n\n\n\n\n\n\n\n\nvariable\nclass\nDescription\n\n\n\n\nyear\ninteger (date)\nYear\n\n\nstate\ncharacter\nState code\n\n\ncounty_name\ncharacter\nCounty Name\n\n\nurbanicity\ncharacter\nCounty-type (urban, suburban, small/mid, rural)\n\n\nregion\ncharacter\nRegion of US\n\n\ndivision\ncharacter\nDivision of US\n\n\npop_category\ncharacter\nCategory for population - either race, gender, or Total\n\n\npopulation\ninteger\nNumber of total individuals in each category aged 15-64\n\n\npretrial_population\ninteger\nNumber of individuals incarcerated for each category"
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html#project-history",
    "href": "data/2019/2019-01-22/readme.html#project-history",
    "title": "Data Info",
    "section": "Project History",
    "text": "Project History\nIn December 2015, Vera released the Incarceration Trends data tool (http://trends.vera.org) and the companion publication In Our Own Backyard: Confronting Growth and Disparities in American Jails. This work employed two Bureau of Justice Statistics (BJS) data collections: the Census of Jails (COJ), which covers all jails and is conducted every five to eight years since 1970, and the Annual Survey of Jails (ASJ), which covers about one-third of jails-and includes nearly all of the largest jails-that has been conducted in non-census years since 1982. This project was funded by the Robert W. Wilson Charitable Trust.\nIn 2016-2018, through a grant from the MacArthur Foundation Safety and Justice Challenge, Vera updated the data tool to include newly released data from the 2013 COJ and 2015 ASJ and developed four publications:\n\nOverlooked: Women and Jails in an Era of Reform\nOut of Sight: The Growth of Jails in Rural America\nDivided Justice: Trends in Black and White Incarceration 1990-2013\nThe New Dynamics of Mass Incarceration\n\nIn 2018, through the In Our Backyards grant from Google.org, Vera completed work on a companion county-level prison dataset, examined in The New Dynamics of Mass Incarceration, that drew on the BJS National Corrections Reporting Program (NCRP) data collection. Vera then merged this data with the original jails dataset to produce a first-in-kind national dataset that can examine both jail and prison incarceration at the county level.\nResearch on incarceration has traditionally centered on state-level data: specifically state prison populations or the statewide combined prison and jail population. Using the state as the unit of analysis is sufficient for understanding the broad contours of incarceration in the United States, but it does not provide the level of detail necessary to unpack its causes and consequences. This is because it is largely county officials-judges, prosecutors, people who manage jails-that decide how communities use incarceration (i.e., who is sent to jail and prison, and for how long). Therefore, county-level variability makes for more robust, theoretically-grounded studies of the high rates of incarceration seen across the United States.\nFor more information on In Our Backyards, see http://www.vera.org/backyards."
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html#data",
    "href": "data/2019/2019-01-22/readme.html#data",
    "title": "Data Info",
    "section": "Data",
    "text": "Data\nThe data is distributed as a single file, available in CSV or Excel format."
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html#documentation",
    "href": "data/2019/2019-01-22/readme.html#documentation",
    "title": "Data Info",
    "section": "Documentation",
    "text": "Documentation\nCodebook"
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html#methodology",
    "href": "data/2019/2019-01-22/readme.html#methodology",
    "title": "Data Info",
    "section": "Methodology",
    "text": "Methodology\n\nJail data methodology.\nPrison data methodology."
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html#license",
    "href": "data/2019/2019-01-22/readme.html#license",
    "title": "Data Info",
    "section": "License",
    "text": "License\nBy downloading the data, you hereby agree to all of the terms specified in this license."
  },
  {
    "objectID": "data/2019/2019-01-22/readme.html#endmatter",
    "href": "data/2019/2019-01-22/readme.html#endmatter",
    "title": "Data Info",
    "section": "Endmatter",
    "text": "Endmatter\nIf you have questions, please contact Vera at trends@vera.org or Jacob Kang-Brown by mail at 233 Broadway, 12th Floor, New York, NY 10279.\nThe Vera Institute of Justice works to build and create justice systems that ensure fairness, promote safety, and strengthen communities.\nThe development of the public-use county-level incarceration dataset is funded through the In Our Backyards project by Google.org as part of its Inclusion and Racial Justice work. In Our Backyards is a Vera’s research and communications agenda to inform the public dialogue, advance research, and guide change to justice policy and practice on mass incarceration.\nPhoto credit: Jack Norton. Huerfano County Correctional Facility, built in 1997 and operated as a private prison by Corrections Corporation of America. Closed in 2010. A funding request to re-open the facility by Colorado Department of Corrections was denied by the state in 2018."
  },
  {
    "objectID": "data/2019/2019-01-08/readme.html",
    "href": "data/2019/2019-01-08/readme.html",
    "title": "TV’s golden age is real",
    "section": "",
    "text": "TV’s golden age is real\nThis week’s DATA is curated courtesy of Sara Stoudt and comes from the recently created The Economist Data GitHub!\nTheir November 24th article on TV ratings covers ‘all TV dramas … via IMDb from 1990 to 2018’.\n\n\nData Dictionary\nData dictionary courtesy of skimr and kable, with credit to Phillip Knor.\n\n\n\n\n\n\n\n\n\n\n\n\ntype\nvariable\nmissing\ncomplete\nn\nmin\nmax\n\n\n\n\ncharacter\ngenres\n0\n2266\n2266\n5\n25\n\n\ncharacter\ntitle\n0\n2266\n2266\n1\n51\n\n\ncharacter\ntitleId\n0\n2266\n2266\n9\n9\n\n\nDate\ndate\n0\n2266\n2266\n1990-01-03\n2018-10-10\n\n\ninteger\nseasonNumber\n0\n2266\n2266\nNA\nNA\n\n\nnumeric\nav_rating\n0\n2266\n2266\nNA\nNA\n\n\nnumeric\nshare\n0\n2266\n2266\nNA\nNA"
  },
  {
    "objectID": "data/2018/readme.html",
    "href": "data/2018/readme.html",
    "title": "2018 Data",
    "section": "",
    "text": "2018 Data\nArchive of datasets and articles from the 2018 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2018-04-03\nUS Tuition Costs\nonlinembapage.com\nonlinembapage.com\n\n\n2\n2018-04-10\nNFL Positional Salaries\nSpotrac.com\nfivethirtyeight.com\n\n\n3\n2018-04-17\nGlobal Mortality\nourworldindata.org\nourworldindata.org\n\n\n4\n2018-04-24\nAustralian Salaries by Gender\ndata.gov.au\ndata.gov.au\n\n\n5\n2018-05-01\nACS Census Data (2015)\ncensus.gov , Kaggle\nNo article\n\n\n6\n2018-05-08\nGlobal Coffee Chains\nStarbucks: kaggle.com , Tim Horton: timhortons.com , Dunkin Donuts: odditysoftware.com\nflowingdata.com\n\n\n7\n2018-05-15\nStar Wars Survey\nfivethirtyeight package\nfivethirtyeight.com\n\n\n8\n2018-05-22\nUS Honey Production\nUSDA, Kaggle.com\nBee Culture\n\n\n9\n2018-05-29\nComic book characters\nFiveThirtyEight package\nFiveThirtyEight.com\n\n\n10\n2018-06-05\nBiketown Bikeshare\nBiketownPDX\nBiketown cascadiaRconf/cRaggy\n\n\n11\n2018-06-12\nFIFA World Cup Audience\nFiveThirtyEight package\nFiveThirtyEight.com\n\n\n12\n2018-06-19\nHurricanes & Puerto Rico\nFiveThirtyEight package\nFiveThirtyEight.com\n\n\n13\n2018-06-26\nAlcohol Consumption\nFiveThirtyEight package\nFiveThirtyEight.com\n\n\n14\n2018-07-03\nGlobal Life Expectancy\nourworldindata.org\nourworldindata.org\n\n\n15\n2018-07-10\nCraft Beer USA\ndata.world\nthrillist.com\n\n\n16\n2018-07-17\nExercise USA\nCDC\nCDC - National Health Statistics Reports\n\n\n17\n2018-07-24\np-hack-athon collaboration\nsimplystatistics.org\np-hack-athon\n\n\n18\n2018-07-31\nDallas Animal Shelter FY2017\nDallas OpenData\nDallas OpenData FY2017 Summary\n\n\n19\n2018-08-07\nAirline Safety\nFiveThirtyEight Package\n538 - Airline Safety\n\n\n20\n2018-08-14\nRussian Troll Tweets\nFiveThirtyEight.com\n538 - Russian Troll Tweets\n\n\n21\n2018-08-21\nCalifornia Fires\nBuzzFeed.com\nBuzzFeed News - California Fires, RMarkdown\n\n\n22\n2018-08-28\nNFL Stats\npro-football-reference.com\neldo.co\n\n\n23\n2018-09-04\nFast Food Calories\nfastfoodnutrition.org\nfranchiseopportunities.com\n\n\n24\n2018-09-11\nCats vs Dogs (USA)\ndata.world\nWashington Post\n\n\n25\n2018-09-18\nUS Flights or Hypoxia\nfaa.govSoaring Society of America\ntravelweekly.com SSA - Hypoxia\n\n\n26\n2018-09-25\nGlobal Invasive Species\nPaini et al, 2016griis.org\nPaini et al, 2016griis.org\n\n\n27\n2018-10-02\nUS Births\nfivethirtyeight package\n538 - Births\n\n\n28\n2018-10-09\nUS Voter Turnout\ndata.world\nStar Tribune\n\n\n29\n2018-10-16\nCollege Major & Income\nfivethirtyeight/ACS\nfivethirtyeight\n\n\n30\n2018-10-23\nHorror Movie Profit\nthe-numbers.com\nfivethirtyeight\n\n\n31\n2018-10-30\nR and R package downloads\ncran-logs.rstudio.com\nNo Article\n\n\n32\n2018-11-06\nUS Wind Farm locations\nusgs.gov\nWind Market Reports\n\n\n33\n2018-11-13\nMalaria Data\nourworldindata.orgMalaria Data Challenge\nourworldindata.org malariaAtlas\n\n\n34\n2018-11-20\nThanksgiving Dinner or Transgender Day of Remembrance\nfivethirtyeightTDoR\nfivethirtyeightTDoR\n\n\n35\n2018-11-27\nBaltimore Bridges\nFederal Highway Administration\nBaltimore Sun\n\n\n36\n2018-12-04\nMedium Article Metadata\nKaggle.com\nTidyText package\n\n\n37\n2018-12-11\nNYC Restaurant inspections\nNYC OpenData/NYC Health Department\nFiveThirtyEight\n\n\n38\n2018-12-18\nCetaceans Data\nThe Pudding\nThe Pudding",
    "crumbs": [
      "Datasets",
      "2018"
    ]
  },
  {
    "objectID": "data/2018/2018-12-11/readme.html",
    "href": "data/2018/2018-12-11/readme.html",
    "title": "NYC Restaurant Inspections",
    "section": "",
    "text": "This week’s data is from the New York Open Data Portal.\nAs the dataset is &gt;100 MB (GitHub only allows 100 MB), I uploaded a data selection of 300,000 records sampled at random from the original dataset with sample_n(). You could read in the original dataset by using read_csv on the link as seen below.\nlibrary(tidyverse)\n\nset.seed(20181209)\n\n# You can use this url to download the data directly into R (will take a few seconds)\ndf &lt;- read_csv(\"https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv\")\n\n# Cleaning names with janitor, sampling 300,000 records, and dropping some variables\nsampled_df &lt;- df %&gt;% \n        janitor::clean_names() %&gt;%\n        select(-phone, -grade_date, -record_date, -building, -street) %&gt;% \n        sample_n(size = 300000)\n\n# save the .csv\nwrite_csv(sampled_df, \"nyc_restaurants.csv\")\nThe original dataset can be found here.\n\n\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nType\n\n\n\n\ncamis\nThis is an unique identifier for the entity (restaurant); 10-digit integer, static per restaurant permit\nPlain Text\n\n\ndba\nThis field represents the name (doing business as) of the entity (restaurant); Public business name, may change at discretion of restaurant owner\nPlain Text\n\n\nboro\nBorough in which the entity (restaurant) is located.;• 1 = MANHATTAN • 2 = BRONX • 3 = BROOKLYN • 4 = QUEENS • 5 = STATEN ISLAND • Missing; NOTE: There may be discrepancies between zip code and listed boro due to differences in an establishment’s mailing address and physical location\nPlain Text\n\n\nbuilding\nBuilding number for establishment (restaurant) location\nPlain Text\n\n\nstreet\nStreet name for establishment (restaurant) location\nPlain Text\n\n\nzipcode\nZip code of establishment (restaurant) location\nPlain Text\n\n\nphone\nPhone Number; Phone number provided by restaurant owner/manager\nPlain Text\n\n\ncuisine_description\nThis field describes the entity (restaurant) cuisine. ; Optional field provided by provided by restaurant owner/manager\nPlain Text\n\n\ninspection_type\nThis field represents the date of inspection; NOTE: Inspection dates of 1/1/1900 mean an establishment has not yet had an inspection\nDate & Time\n\n\naction\nThis field represents the actions that is associated with each restaurant inspection. ; • Violations were cited in the following area(s). • No violations were recorded at the time of this inspection. • Establishment re-opened by DOHMH • Establishment re-closed by DOHMH • Establishment Closed by DOHMH. Violations were cited in the following area(s) and those requiring immediate action were addressed. • “Missing” = not yet inspected;\nPlain Text\n\n\nviolation_code\nViolation code associated with an establishment (restaurant) inspection\nPlain Text\n\n\nviolation_description\nViolation description associated with an establishment (restaurant) inspection\nPlain Text\n\n\ncritical_flag\nIndicator of critical violation; “• Critical • Not Critical • Not Applicable”; Critical violations are those most likely to contribute to food-borne illness\nPlain Text\n\n\nscore\nTotal score for a particular inspection; Scores are updated based on adjudication results\nNumber\n\n\ngrade\nGrade associated with the inspection; N = Not Yet Graded, A = Grade A, B = Grade B, C = Grade C, Z = Grade Pending, P= Grade Pending issued on re-opening following an initial inspection that resulted in a closure\nPlain Text\n\n\ngrade_date\nThe date when the current grade was issued to the entity (restaurant)\nDate & Time\n\n\nrecord_date\nThe date when the extract was run to produce this data set\nDate & Time\n\n\ninspection_type\nA combination of the inspection program and the type of inspection performed; See Data Dictionary for full list of expected values\nPlain Text\n\n\n\n\n\n\n“How Data Made Me A Believer In New York City’s Restaurant Grades”"
  },
  {
    "objectID": "data/2018/2018-12-11/readme.html#data-dictionary",
    "href": "data/2018/2018-12-11/readme.html#data-dictionary",
    "title": "NYC Restaurant Inspections",
    "section": "",
    "text": "Column Name\nDescription\nType\n\n\n\n\ncamis\nThis is an unique identifier for the entity (restaurant); 10-digit integer, static per restaurant permit\nPlain Text\n\n\ndba\nThis field represents the name (doing business as) of the entity (restaurant); Public business name, may change at discretion of restaurant owner\nPlain Text\n\n\nboro\nBorough in which the entity (restaurant) is located.;• 1 = MANHATTAN • 2 = BRONX • 3 = BROOKLYN • 4 = QUEENS • 5 = STATEN ISLAND • Missing; NOTE: There may be discrepancies between zip code and listed boro due to differences in an establishment’s mailing address and physical location\nPlain Text\n\n\nbuilding\nBuilding number for establishment (restaurant) location\nPlain Text\n\n\nstreet\nStreet name for establishment (restaurant) location\nPlain Text\n\n\nzipcode\nZip code of establishment (restaurant) location\nPlain Text\n\n\nphone\nPhone Number; Phone number provided by restaurant owner/manager\nPlain Text\n\n\ncuisine_description\nThis field describes the entity (restaurant) cuisine. ; Optional field provided by provided by restaurant owner/manager\nPlain Text\n\n\ninspection_type\nThis field represents the date of inspection; NOTE: Inspection dates of 1/1/1900 mean an establishment has not yet had an inspection\nDate & Time\n\n\naction\nThis field represents the actions that is associated with each restaurant inspection. ; • Violations were cited in the following area(s). • No violations were recorded at the time of this inspection. • Establishment re-opened by DOHMH • Establishment re-closed by DOHMH • Establishment Closed by DOHMH. Violations were cited in the following area(s) and those requiring immediate action were addressed. • “Missing” = not yet inspected;\nPlain Text\n\n\nviolation_code\nViolation code associated with an establishment (restaurant) inspection\nPlain Text\n\n\nviolation_description\nViolation description associated with an establishment (restaurant) inspection\nPlain Text\n\n\ncritical_flag\nIndicator of critical violation; “• Critical • Not Critical • Not Applicable”; Critical violations are those most likely to contribute to food-borne illness\nPlain Text\n\n\nscore\nTotal score for a particular inspection; Scores are updated based on adjudication results\nNumber\n\n\ngrade\nGrade associated with the inspection; N = Not Yet Graded, A = Grade A, B = Grade B, C = Grade C, Z = Grade Pending, P= Grade Pending issued on re-opening following an initial inspection that resulted in a closure\nPlain Text\n\n\ngrade_date\nThe date when the current grade was issued to the entity (restaurant)\nDate & Time\n\n\nrecord_date\nThe date when the extract was run to produce this data set\nDate & Time\n\n\ninspection_type\nA combination of the inspection program and the type of inspection performed; See Data Dictionary for full list of expected values\nPlain Text"
  },
  {
    "objectID": "data/2018/2018-12-11/readme.html#fivethirtyeight-article",
    "href": "data/2018/2018-12-11/readme.html#fivethirtyeight-article",
    "title": "NYC Restaurant Inspections",
    "section": "",
    "text": "“How Data Made Me A Believer In New York City’s Restaurant Grades”"
  },
  {
    "objectID": "data/2018/2018-11-27/readme.html",
    "href": "data/2018/2018-11-27/readme.html",
    "title": "Maryland Bridges",
    "section": "",
    "text": "Many thanks to Sara Stoudt for submitting this dataset as to our GitHub!\nIf you want to submit a dataset please open an Issue on our GitHub with some details about the dataset and links to relevant material.\n\n\n“The Baltimore Sun looked at which bridges in the region are listed in”poor” condition by the Federal Highway Administration — and in particular, which have been “poor” for several years.\nTo be clear, “poor” condition doesn’t mean a bridge is unsafe — officials and experts emphasize that a bridge found to be structurally unsound would be closed. Still, such bridges can be a nuisance, damaging to cars, particularly tires. And the costs for repairing long-deteriorating bridges increase with every year.”\nThe raw data for this article and others can be found on GitHub at the Baltimore Sun Data Desk.\nArticle Author: Christine Zhang\n\n\n\n\n\n\n\n\n\n\nTitle\nDescription\n\n\n\n\nlat\nLatitude\n\n\nlong\nLongitude\n\n\ncounty\nCounty within Maryland\n\n\ncarries\nWhat street is carried by the bridge\n\n\nyr_built\nYear bridge was built\n\n\nbridge_condition\nBridge condition (Poor, Fair, Good)\n\n\navg_daily_traffic\nAverage daily traffic in number of vehicles\n\n\ntotal_improve_cost_thousands\nTotal improvement costs in thousands of dollars\n\n\ninspection_mo\nMonth of Inspection\n\n\ninspection_yr\nYear of Inspection\n\n\nowner\nOwner of the bridge\n\n\nresponsibility\nResponsibility of bridge\n\n\nvehicles\nVehicle traffic"
  },
  {
    "objectID": "data/2018/2018-11-27/readme.html#article",
    "href": "data/2018/2018-11-27/readme.html#article",
    "title": "Maryland Bridges",
    "section": "",
    "text": "“The Baltimore Sun looked at which bridges in the region are listed in”poor” condition by the Federal Highway Administration — and in particular, which have been “poor” for several years.\nTo be clear, “poor” condition doesn’t mean a bridge is unsafe — officials and experts emphasize that a bridge found to be structurally unsound would be closed. Still, such bridges can be a nuisance, damaging to cars, particularly tires. And the costs for repairing long-deteriorating bridges increase with every year.”\nThe raw data for this article and others can be found on GitHub at the Baltimore Sun Data Desk.\nArticle Author: Christine Zhang"
  },
  {
    "objectID": "data/2018/2018-11-27/readme.html#data-dictionary",
    "href": "data/2018/2018-11-27/readme.html#data-dictionary",
    "title": "Maryland Bridges",
    "section": "",
    "text": "Title\nDescription\n\n\n\n\nlat\nLatitude\n\n\nlong\nLongitude\n\n\ncounty\nCounty within Maryland\n\n\ncarries\nWhat street is carried by the bridge\n\n\nyr_built\nYear bridge was built\n\n\nbridge_condition\nBridge condition (Poor, Fair, Good)\n\n\navg_daily_traffic\nAverage daily traffic in number of vehicles\n\n\ntotal_improve_cost_thousands\nTotal improvement costs in thousands of dollars\n\n\ninspection_mo\nMonth of Inspection\n\n\ninspection_yr\nYear of Inspection\n\n\nowner\nOwner of the bridge\n\n\nresponsibility\nResponsibility of bridge\n\n\nvehicles\nVehicle traffic"
  },
  {
    "objectID": "data/2018/2018-11-20/TDoR_readme.html",
    "href": "data/2018/2018-11-20/TDoR_readme.html",
    "title": "Datathon for the Transgender Day of Rememberance",
    "section": "",
    "text": "Datathon for the Transgender Day of Rememberance\nTDoR data from Cardiff RUser Group in package form or in the raw data. More details about the datathon challenge can be found here.\nThe Transgender Day of Remembrance (TDoR), observed annually on 20 November, memorializes those who were killed or lost to suicide as a result of anti-transgender hatred or prejudice in the past year.\nWe are holding a non-competitive datathon centred on the data collected by monitoring bodies and trans activists about relevant deaths, as collated for recent years on https://tdor.translivesmatter.info/. This repository will be used to share ideas and potentially host code and other documents generated as part of the datathon.\nThe main event will be hosted by the Cardiff R User Group on Saturday 17 November, 10:30am to 4:30pm, where we will welcome the maintainer of tdor.translivesmatter.info as an invited participant, see further details on Meetup.\nThe datathon is supported by Forwards (https://forwards.github.io/) and Rainbow R (who are hosting this repository!). We welcome remote participants: - watch this repo for updates - reply to https://twitter.com/plzbeemyfriend/status/1060598224438939648 if you would like to meetup in Washington DC to work on this - join the tdor-2018 channel in the RainbowR Slack group. You can join the group and collaborate with the global community by first completing RainbowR’s Code of Conduct - read the issues page of the event’s GitHub repository to find ways to contribute\n- contact @R_Forwards on Twitter if you would like to join a Skype call towards the end of the Cardiff datathon to discuss on-going tasks and how you might contribute - exact time TBD\nFor the story behind tdor.translivesmatter.info, see this Medium post.\nFor an intro the datathon, see these slides which will presented at the start of the Cardiff event to kick things off!"
  },
  {
    "objectID": "data/2018/2018-11-06/readme.html",
    "href": "data/2018/2018-11-06/readme.html",
    "title": "US Wind Turbine Data",
    "section": "",
    "text": "Wind turbine location and characteristic data across the USA can be found here.\nSome potential questions: - How do newer installations compare to older turbines? - Which states/regions have the most wind farms? - Spread of wind-turbines over time? - Where is the most missing data?\nDISCLAIMER: There is quite a bit of missing data, but still an interesting dataset!\nInteractive map from usgs.gov.\nGIS shapefile\n\n\nCheck out OpenEI for &gt;1600 datasets on energy production and use.\n\n\n\nI intentionally left some of the “missing” data qualifiers as it is accurate for lots of government-level open data. They often use things like “-9999” means missing data. Take a look, dig through the data, and we can’t wait to see what you make!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfield\ncategory\ndescription\ntype\nnon-missing\nLabels\nMissing\nMin\nMedian\nMax\n\n\n\n\ncase_id\nid\nunique uswtdb id\nlong\n58185\nn/a\nn/a\n3000001\n3038762\n3087216\n\n\nfaa_ors\nid\nfaa digital obstacle file (dof) for obstacle repository system (ors)\nstr9\n49780\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nfaa_asn\nid\nfaa obstruction evaluation - airport airspace analysis (oe-aaa) aeronautical study number (asn)\nstr17\n50090\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nusgs_pr_id\nid\nusgs id from prior turbine dataset\nlong\n42854\nn/a\n-9999\n1\n27334\n49135\n\n\nt_state\nlocation\nstate where turbine is located\nstr2\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_county\nlocation\ncounty where turbine is located\nstr31\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_fips\nlocation\nstate and county fips where turbine is located\nstr5\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\np_name\nproject characteristic\nproject name\nstr42\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\np_year\nproject characteristic\nyear project became operational\nint\n58123\nn/a\n-9999\n1981\n2010\n2018\n\n\np_tnum\nproject characteristic\nnumber of turbines in project\nint\n58185\nn/a\n-9999\n1\n83\n1831\n\n\np_cap\nproject characteristic\nproject capacity (MW)\ndouble\n54493\nn/a\n-9999\n0.05\n129\n495.01\n\n\nt_manu\nturbine characteristic\nturbine original equipment manufacturer\nstr31\n54309\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_model\nturbine characteristic\nturbine model\nstr18\n53716\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_cap\nturbine characteristic\nturbine capacity (kW)\nint\n54491\nn/a\n-9999\n40\n1650\n6000\n\n\nt_hh\nturbine characteristic\nturbine hub height (meters)\ndouble\n51560\nn/a\n-9999\n18.2\n80\n130\n\n\nt_rd\nturbine characteristic\nturbine rotor diameter (meters)\ndouble\n53236\nn/a\n-9999\n11.0\n87.0\n150\n\n\nt_rsa\nturbine characteristic\nturbine rotor swept area (meters^2)\ndouble\n53236\nn/a\n-9999\n95.03\n5944.68\n17671.46\n\n\nt_ttlh\nturbine characteristic\nturbine total height - calculated (meters)\ndouble\n53123\nn/a\n-9999\n9.1\n123.4\n200.3\n\n\nt_conf_atr\nturbine characteristics qa/qc\nturbine characteristic confidence (0-3)\nbyte\n58185\nsee labels tab\nn/a\n1\n3\n3\n\n\nt_conf_loc\nvisual inspection qa/qc\nlocation confidence (0 -3)\nbyte\n58185\nsee labels tab\nn/a\n1\n3\n3\n\n\nt_img_date\nvisual inspection info\ndate of image used to visually verify turbine location\nint\n34011\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_img_srce\nvisual inspection info\nsource of image used to visually verify turbine location\nstr16\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nxlong\nlocation\nlongitude (decimal degrees - NAD 83 datum)\ndouble\n58185\nn/a\nn/a\n-171.7131\n-100.2475\n144.7227\n\n\nylat\nlocation\nlatitude (decimal degrees - NAD 83 datum)\ndouble\n58185\nn/a\nn/a\n13.3894\n37.8747\n66.8399"
  },
  {
    "objectID": "data/2018/2018-11-06/readme.html#interested-in-even-more-wind-power-data",
    "href": "data/2018/2018-11-06/readme.html#interested-in-even-more-wind-power-data",
    "title": "US Wind Turbine Data",
    "section": "",
    "text": "Check out OpenEI for &gt;1600 datasets on energy production and use."
  },
  {
    "objectID": "data/2018/2018-11-06/readme.html#data-dictionary",
    "href": "data/2018/2018-11-06/readme.html#data-dictionary",
    "title": "US Wind Turbine Data",
    "section": "",
    "text": "I intentionally left some of the “missing” data qualifiers as it is accurate for lots of government-level open data. They often use things like “-9999” means missing data. Take a look, dig through the data, and we can’t wait to see what you make!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfield\ncategory\ndescription\ntype\nnon-missing\nLabels\nMissing\nMin\nMedian\nMax\n\n\n\n\ncase_id\nid\nunique uswtdb id\nlong\n58185\nn/a\nn/a\n3000001\n3038762\n3087216\n\n\nfaa_ors\nid\nfaa digital obstacle file (dof) for obstacle repository system (ors)\nstr9\n49780\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nfaa_asn\nid\nfaa obstruction evaluation - airport airspace analysis (oe-aaa) aeronautical study number (asn)\nstr17\n50090\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nusgs_pr_id\nid\nusgs id from prior turbine dataset\nlong\n42854\nn/a\n-9999\n1\n27334\n49135\n\n\nt_state\nlocation\nstate where turbine is located\nstr2\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_county\nlocation\ncounty where turbine is located\nstr31\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_fips\nlocation\nstate and county fips where turbine is located\nstr5\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\np_name\nproject characteristic\nproject name\nstr42\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\np_year\nproject characteristic\nyear project became operational\nint\n58123\nn/a\n-9999\n1981\n2010\n2018\n\n\np_tnum\nproject characteristic\nnumber of turbines in project\nint\n58185\nn/a\n-9999\n1\n83\n1831\n\n\np_cap\nproject characteristic\nproject capacity (MW)\ndouble\n54493\nn/a\n-9999\n0.05\n129\n495.01\n\n\nt_manu\nturbine characteristic\nturbine original equipment manufacturer\nstr31\n54309\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_model\nturbine characteristic\nturbine model\nstr18\n53716\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_cap\nturbine characteristic\nturbine capacity (kW)\nint\n54491\nn/a\n-9999\n40\n1650\n6000\n\n\nt_hh\nturbine characteristic\nturbine hub height (meters)\ndouble\n51560\nn/a\n-9999\n18.2\n80\n130\n\n\nt_rd\nturbine characteristic\nturbine rotor diameter (meters)\ndouble\n53236\nn/a\n-9999\n11.0\n87.0\n150\n\n\nt_rsa\nturbine characteristic\nturbine rotor swept area (meters^2)\ndouble\n53236\nn/a\n-9999\n95.03\n5944.68\n17671.46\n\n\nt_ttlh\nturbine characteristic\nturbine total height - calculated (meters)\ndouble\n53123\nn/a\n-9999\n9.1\n123.4\n200.3\n\n\nt_conf_atr\nturbine characteristics qa/qc\nturbine characteristic confidence (0-3)\nbyte\n58185\nsee labels tab\nn/a\n1\n3\n3\n\n\nt_conf_loc\nvisual inspection qa/qc\nlocation confidence (0 -3)\nbyte\n58185\nsee labels tab\nn/a\n1\n3\n3\n\n\nt_img_date\nvisual inspection info\ndate of image used to visually verify turbine location\nint\n34011\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nt_img_srce\nvisual inspection info\nsource of image used to visually verify turbine location\nstr16\n58185\nn/a\nmissing\nn/a\nn/a\nn/a\n\n\nxlong\nlocation\nlongitude (decimal degrees - NAD 83 datum)\ndouble\n58185\nn/a\nn/a\n-171.7131\n-100.2475\n144.7227\n\n\nylat\nlocation\nlatitude (decimal degrees - NAD 83 datum)\ndouble\n58185\nn/a\nn/a\n13.3894\n37.8747\n66.8399"
  },
  {
    "objectID": "data/2018/2018-10-23/readme.html",
    "href": "data/2018/2018-10-23/readme.html",
    "title": "Week 30 - Horror Movies and Profit",
    "section": "",
    "text": "raw data\n\n\n“Horror movies get nowhere near as much draw at the box office as the big-time summer blockbusters or action/adventure movies — the horror genre accounts for only 3.7 percent of the total box-office haul this year — but there’s a huge incentive for studios to continue pushing them out.\nThe return-on-investment potential for horror movies is absurd.\nFor example, “Paranormal Activity” was made for $450,000 and pulled in $194 million — 431 times the original budget. That’s an extreme, I-invested-in-Microsoft-when-Bill-Gates-was-working-in-a-garage case, but it’s not rare. And that’s what makes horror such a compelling genre to produce.”\nQuote from Walt Hickey for fivethirtyeight article.\n\n\n\nData from the-numbers.com\n\n\n\nHeader\nDescription\n\n\n\n\nrelease_date\nmonth-day-year\n\n\nmovie\nMovie title\n\n\nproduction_budget\nMoney spent to create the film\n\n\ndomestic_gross\nGross revenue from USA\n\n\nworldwide_gross\nGross worldwide revenue\n\n\ndistributor\nThe distribution company\n\n\nmpaa_rating\nAppropriate age rating by the US-based rating agency\n\n\ngenre\nFilm category\n\n\n\n\n\n\nCheck out the boxoffice package!\nboxoffice() is a simple package to get information about daily box office results of movies. It scrapes the webpages of either http://www.boxofficemojo.com or https://www.the-numbers.com/ for this information. The data it returns are the following:\n\nMovie name\nThe studio that produced that movie\nThe daily gross\nDaily percent change in gross\nNumber of theaters it is playing in\nAverage gross per theater (result of 4 / result of 5)\nGross-to-date\nHow many days the movie has been playing\nThe date of the data"
  },
  {
    "objectID": "data/2018/2018-10-23/readme.html#scary-movies-are-the-best-investment-in-hollywood---fivethirtyeight",
    "href": "data/2018/2018-10-23/readme.html#scary-movies-are-the-best-investment-in-hollywood---fivethirtyeight",
    "title": "Week 30 - Horror Movies and Profit",
    "section": "",
    "text": "“Horror movies get nowhere near as much draw at the box office as the big-time summer blockbusters or action/adventure movies — the horror genre accounts for only 3.7 percent of the total box-office haul this year — but there’s a huge incentive for studios to continue pushing them out.\nThe return-on-investment potential for horror movies is absurd.\nFor example, “Paranormal Activity” was made for $450,000 and pulled in $194 million — 431 times the original budget. That’s an extreme, I-invested-in-Microsoft-when-Bill-Gates-was-working-in-a-garage case, but it’s not rare. And that’s what makes horror such a compelling genre to produce.”\nQuote from Walt Hickey for fivethirtyeight article."
  },
  {
    "objectID": "data/2018/2018-10-23/readme.html#data-dictionary",
    "href": "data/2018/2018-10-23/readme.html#data-dictionary",
    "title": "Week 30 - Horror Movies and Profit",
    "section": "",
    "text": "Data from the-numbers.com\n\n\n\nHeader\nDescription\n\n\n\n\nrelease_date\nmonth-day-year\n\n\nmovie\nMovie title\n\n\nproduction_budget\nMoney spent to create the film\n\n\ndomestic_gross\nGross revenue from USA\n\n\nworldwide_gross\nGross worldwide revenue\n\n\ndistributor\nThe distribution company\n\n\nmpaa_rating\nAppropriate age rating by the US-based rating agency\n\n\ngenre\nFilm category"
  },
  {
    "objectID": "data/2018/2018-10-23/readme.html#want-to-dive-further",
    "href": "data/2018/2018-10-23/readme.html#want-to-dive-further",
    "title": "Week 30 - Horror Movies and Profit",
    "section": "",
    "text": "Check out the boxoffice package!\nboxoffice() is a simple package to get information about daily box office results of movies. It scrapes the webpages of either http://www.boxofficemojo.com or https://www.the-numbers.com/ for this information. The data it returns are the following:\n\nMovie name\nThe studio that produced that movie\nThe daily gross\nDaily percent change in gross\nNumber of theaters it is playing in\nAverage gross per theater (result of 4 / result of 5)\nGross-to-date\nHow many days the movie has been playing\nThe date of the data"
  },
  {
    "objectID": "data/2018/2018-10-09/readme.html",
    "href": "data/2018/2018-10-09/readme.html",
    "title": "Voter Turnout Data 1980 - 2014",
    "section": "",
    "text": "Voter Turnout Data 1980 - 2014\nNumber of age-eligible voters versus total ballots/votes counted by state and year.\n\n\nElection Type\npresident_years &lt;- seq(1980, 2012, 4)\nmidterm_years &lt;- seq(1982, 2014, 4)"
  },
  {
    "objectID": "data/2018/2018-09-25/readme.html",
    "href": "data/2018/2018-09-25/readme.html",
    "title": "Week 26 - Invasive Species in Africa",
    "section": "",
    "text": "Paini et al, 2016 report on “Global threat to agriculture from invasive species”. Developing countries in Africa are some of the most sensitive to invasive species damage to agriculture.\nTable data from tables in the Appendix.\nAll invasive species data for Africa from http://www.griis.org/.\n\n\n\nRaw data is what the “raw” data looks like from the paper’s tables. If you want to practice your tidy skills this would be a good starting place! An example tidying script is also posted if you get stuck.\n\n\n\nTidy and cleaned data for the tables from Paini et al, 2016.\n\nTable 1: Ranking of all threatened countries by overall invasion threat (invasion_threat).\nTable 2: Ranking of all threatened countries by total invasion cost (invasion_cost).\nTable 3: Ranking of all threatened countries by total invasion cost (invasion_cost) as a proportion (gdp_proportion) of mean GDP (gdp_mean).\nTable 4: Ranking of all source countries by total invasion cost (invasion_cost)\nTable 6: List of 140 species and their maximum recorded percentage impact on one of their known host crops (source: CABI Crop Protection Compendium)\n\n * Figure 1 World map representation of model outputs. (A) The overall invasion threat (Table 1) to each threatened country, (B) the total invasion cost (Table 2) (in millions of US dollars) to threatened countries; (C) the total invasion cost (Table 3) (in millions of US dollars) to threatened countries, as a proportion of GDP; and (D) the total invasion cost (Table 4) (in millions of US dollars) from source countries. Those countries without color were not included in the analysis.\n\n\n\nDeveloping countries in Africa appear to be most vulnerable to invasive species damage to agriculture. africa_species.csv contains the known invasive species for African countries from griis.org."
  },
  {
    "objectID": "data/2018/2018-09-25/readme.html#data-sources",
    "href": "data/2018/2018-09-25/readme.html#data-sources",
    "title": "Week 26 - Invasive Species in Africa",
    "section": "",
    "text": "Paini et al, 2016 report on “Global threat to agriculture from invasive species”. Developing countries in Africa are some of the most sensitive to invasive species damage to agriculture.\nTable data from tables in the Appendix.\nAll invasive species data for Africa from http://www.griis.org/."
  },
  {
    "objectID": "data/2018/2018-09-25/readme.html#raw-data",
    "href": "data/2018/2018-09-25/readme.html#raw-data",
    "title": "Week 26 - Invasive Species in Africa",
    "section": "",
    "text": "Raw data is what the “raw” data looks like from the paper’s tables. If you want to practice your tidy skills this would be a good starting place! An example tidying script is also posted if you get stuck."
  },
  {
    "objectID": "data/2018/2018-09-25/readme.html#table-data",
    "href": "data/2018/2018-09-25/readme.html#table-data",
    "title": "Week 26 - Invasive Species in Africa",
    "section": "",
    "text": "Tidy and cleaned data for the tables from Paini et al, 2016.\n\nTable 1: Ranking of all threatened countries by overall invasion threat (invasion_threat).\nTable 2: Ranking of all threatened countries by total invasion cost (invasion_cost).\nTable 3: Ranking of all threatened countries by total invasion cost (invasion_cost) as a proportion (gdp_proportion) of mean GDP (gdp_mean).\nTable 4: Ranking of all source countries by total invasion cost (invasion_cost)\nTable 6: List of 140 species and their maximum recorded percentage impact on one of their known host crops (source: CABI Crop Protection Compendium)\n\n * Figure 1 World map representation of model outputs. (A) The overall invasion threat (Table 1) to each threatened country, (B) the total invasion cost (Table 2) (in millions of US dollars) to threatened countries; (C) the total invasion cost (Table 3) (in millions of US dollars) to threatened countries, as a proportion of GDP; and (D) the total invasion cost (Table 4) (in millions of US dollars) from source countries. Those countries without color were not included in the analysis."
  },
  {
    "objectID": "data/2018/2018-09-25/readme.html#invasive-species-by-african-countries",
    "href": "data/2018/2018-09-25/readme.html#invasive-species-by-african-countries",
    "title": "Week 26 - Invasive Species in Africa",
    "section": "",
    "text": "Developing countries in Africa appear to be most vulnerable to invasive species damage to agriculture. africa_species.csv contains the known invasive species for African countries from griis.org."
  },
  {
    "objectID": "data/2018/2018-09-11/readme.html",
    "href": "data/2018/2018-09-11/readme.html",
    "title": "Week 24 Data",
    "section": "",
    "text": "Week 24 Data\nData sourced from American Veterinary Medical Association via data.world this week."
  },
  {
    "objectID": "data/2018/2018-08-21/readme.html",
    "href": "data/2018/2018-08-21/readme.html",
    "title": "Week 21 data",
    "section": "",
    "text": "Full data from the BuzzFeed news GitHub.\nData Dictionary\nFire Incidents from John Saraceno\n\n\nData and R code underlying this Jul. 28, 2018 BuzzFeed News article on wildfires in California.\n\n\nThe code to process the data and repeat the analysis can be found online here, and in the R Markdown document index.Rmd.\n\n\n\nEmail the author Peter Aldhous at peter.aldhous@buzzfeed.com."
  },
  {
    "objectID": "data/2018/2018-08-21/readme.html#how-a-booming-population-and-climate-change-made-californias-wildfires-worse-than-ever",
    "href": "data/2018/2018-08-21/readme.html#how-a-booming-population-and-climate-change-made-californias-wildfires-worse-than-ever",
    "title": "Week 21 data",
    "section": "",
    "text": "Data and R code underlying this Jul. 28, 2018 BuzzFeed News article on wildfires in California.\n\n\nThe code to process the data and repeat the analysis can be found online here, and in the R Markdown document index.Rmd.\n\n\n\nEmail the author Peter Aldhous at peter.aldhous@buzzfeed.com."
  },
  {
    "objectID": "alt_text.html",
    "href": "alt_text.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "Please add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nThe DataViz Society/Nightingale has an article on writing good alt text for plots/graphs. &gt; Here’s a simple formula for writing alt text for data visualization: &gt; ### Chart type &gt; It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. &gt; Example: Line graph &gt; ### Type of data &gt; What data is included in the chart? The x and y axis labels may help you figure this out. &gt; Example: number of bananas sold per day in the last year &gt; ### Reason for including the chart &gt; Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. &gt; Example: the winter months have more banana sales &gt; ### Link to data or source &gt; Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. &gt; Example: Data from the USDA\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtoot} package includes the ability to post to the Mastodon social network with alt text programmatically.\nNeed a reminder? Many social networks have settings to require alt text in your posts. We recommend turning them on when they are available!"
  },
  {
    "objectID": "alt_text.html#alternative-text",
    "href": "alt_text.html#alternative-text",
    "title": "TidyTuesday",
    "section": "",
    "text": "Please add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nThe DataViz Society/Nightingale has an article on writing good alt text for plots/graphs. &gt; Here’s a simple formula for writing alt text for data visualization: &gt; ### Chart type &gt; It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. &gt; Example: Line graph &gt; ### Type of data &gt; What data is included in the chart? The x and y axis labels may help you figure this out. &gt; Example: number of bananas sold per day in the last year &gt; ### Reason for including the chart &gt; Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. &gt; Example: the winter months have more banana sales &gt; ### Link to data or source &gt; Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. &gt; Example: Data from the USDA\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtoot} package includes the ability to post to the Mastodon social network with alt text programmatically.\nNeed a reminder? Many social networks have settings to require alt text in your posts. We recommend turning them on when they are available!"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at rfordatasci@gmail.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-responsibilities",
    "href": "CODE_OF_CONDUCT.html#our-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at rfordatasci@gmail.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About TidyTuesday",
    "section": "",
    "text": "TidyTuesday is a weekly social data project. All are welcome to participate! Please remember to share the code used to generate your results!\nTidyTuesday is organized by the Data Science Learning Community. Join our Slack for free online help with R, Python, and other data-related topics, or to participate in a data-related book club!",
    "crumbs": [
      "About TidyTuesday"
    ]
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About TidyTuesday",
    "section": "Goals",
    "text": "Goals\nOur over-arching goal for TidyTuesday is to provide real-world datasets so that people can learn to work with data.\n\nFor 2024, our goal was to be used in at least 10 courses. Our survey indicates that we are used in at least 30 courses!\nFor 2025, our goal was to crowdsource the curation of TidyTuesday datasets. We certainly didn’t hit 100%, but we are grateful for the 51 pull requests, which included dataset submissions, improvements to welcome participants using Python and Julia, and other improvements to the project. We look forward to more community contributions in 2026!\nFor 2026, our goal is to provide tools for easier and better dataset curation, leading to curation of 95% of 2027 datasets by the end of 2026.",
    "crumbs": [
      "About TidyTuesday"
    ]
  },
  {
    "objectID": "about.html#how-to-participate",
    "href": "about.html#how-to-participate",
    "title": "About TidyTuesday",
    "section": "How to Participate",
    "text": "How to Participate\n\nData is posted to social media every Monday morning. Follow the instructions in the new post for how to download the data in R, Python, or Julia, or download the data directly from GitHub for use in your favorite data exploration tool.\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, Julia, or another programming language.\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\n\nR; TidyTuesday originated in the #RStats community on social media. Add that hashtag if you explore TidyTuesday data in R!\nPython: Add the #PydyTuesday hashtag so that Posit has the chance to highlight your work, too!\nJulia: Add the #TidierTuesday hashtag if you want the Tidier Org to share your visuals!\n\n\nYou can also curate a dataset for a future TidyTuesday!",
    "crumbs": [
      "About TidyTuesday"
    ]
  },
  {
    "objectID": "about.html#datasets",
    "href": "about.html#datasets",
    "title": "About TidyTuesday",
    "section": "DataSets",
    "text": "DataSets\n\n2018 | 2019 | 2020 | 2021 | 2022 | 2023 | 2024 | 2025 | 2026\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2026-01-06\nBring your own data to start the year!\nNA\nNA\n\n\n2\n2026-01-13\nThe Languages of Africa\nLanguages of Africa\nLanguages of Africa\n\n\n3\n2026-01-20\nAstronomy Picture of the Day (APOD) Archive\nNASA API\nAstronomy Picture of the Day\n\n\n4\n2026-01-27\nBrazilian Companies\nOpen data CNPJ - December 2025\nWikipedia’s List of largest Brazilian companies",
    "crumbs": [
      "About TidyTuesday"
    ]
  },
  {
    "objectID": "about.html#citing-tidytuesday",
    "href": "about.html#citing-tidytuesday",
    "title": "About TidyTuesday",
    "section": "Citing TidyTuesday",
    "text": "Citing TidyTuesday\nTo cite the TidyTuesday repo/project in publications use:\nData Science Learning Community (2024). Tidy Tuesday: A weekly social data project. https://tidytues.day\nA BibTeX entry for LaTeX users is\n  @misc{tidytuesday, \n    title = {Tidy Tuesday: A weekly social data project}, \n    author = {Data Science Learning Community}, \n    url = {https://tidytues.day}, \n    year = {2024} \n  }",
    "crumbs": [
      "About TidyTuesday"
    ]
  },
  {
    "objectID": "about.html#contributing",
    "href": "about.html#contributing",
    "title": "About TidyTuesday",
    "section": "Contributing",
    "text": "Contributing\nPlease see our contributing guide for ways that you can help!",
    "crumbs": [
      "About TidyTuesday"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brazilian Companies",
    "section": "",
    "text": "This week we’re exploring Brazilian Companies, curated from Brazil’s open CNPJ (Cadastro Nacional da Pessoa Jurídica) records published by the Brazilian Ministry of Finance / Receita Federal on the national open-data portal (dados.gov.br).\n\nThe CNPJ open data is a large-scale public registry of Brazilian legal entities. For this dataset, the raw company records were cleaned and enriched with lookup tables (legal nature, owner qualification, and company size), then filtered to retain firms above a share-capital threshold so the analysis focuses on meaningful variation in capital stock.\n\n\nWhich legal nature categories concentrate the highest total and average capital stock?\nHow does company size relate to capital stock (and how skewed is it)?\nDo specific owner qualification groups dominate high-capital companies?\nWhat patterns emerge when comparing the top capital-stock tail across categories (legal nature, size, qualification)?\n\nThank you to Marcelo Silva for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2026-01-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 4)\n\ncompanies &lt;- tuesdata$companies\nlegal_nature &lt;- tuesdata$legal_nature\nqualifications &lt;- tuesdata$qualifications\nsize &lt;- tuesdata$size\n\n# Option 2: Read directly from GitHub\n\ncompanies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv')\nlegal_nature &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv')\nqualifications &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv')\nsize &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2026-01-27')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncompanies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv')\nlegal_nature = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv')\nqualifications = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv')\nsize = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download datasets for the week, and load them as a NamedTuple of DataFrames\ndata = tt_load(\"2026-01-27\")\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncompanies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv\")\nlegal_nature = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv\")\nqualifications = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv\")\nsize = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncompanies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv\", DataFrame)\nlegal_nature = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv\", DataFrame)\nqualifications = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv\", DataFrame)\nsize = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncompany_id\ninteger\nCompany identifier (8-digit ID used as the primary key in this dataset).\n\n\ncompany_name\ncharacter\nCompany legal name (as provided in the source registry).\n\n\nlegal_nature\ncharacter\nCompany legal nature (e.g., “Limited Liability Business Company (LLC)”).\n\n\nowner_qualification\ncharacter\nOwner/partner qualification label (e.g., “Managing Partner / Partner-Administrator”).\n\n\ncapital_stock\nnumeric\nDeclared share capital (BRL), numeric.\n\n\ncompany_size\ncharacter\nCompany size category (e.g.,micro-enterprise, small-enterprise, other).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nLegal nature code (source registry code).\n\n\nlegal_nature\ncharacter\nLegal nature label corresponding to id.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nOwner qualification code (source registry code).\n\n\nowner_qualification\ncharacter\nOwner qualification label corresponding to id.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nCompany size code (source registry code).\n\n\ncompany_size\ncharacter\nCompany size label corresponding to id (e.g., micro-enterprise, small-enterprise).\n\n\n\n\n\n\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\n\n# Input data\ncompanies0 = pd.read_csv(\"../raw_data/raw_companies.csv\", sep=\";\", encoding=\"cp1252\", header=None)\n\nlegal_nature = pd.read_csv('../raw_data/legal_nature.csv', sep=',')\n\nsizes = pd.read_csv(\"../raw_data/size.csv\", sep=\",\", encoding=\"cp1252\")\n\nqualifications = pd.read_csv(\"../raw_data/qualifications.csv\", sep = \",\", encoding=\"cp1252\")\n\n# Remove all private associations\ndef treat_companies_dataframe(dataframe):\n    companies_df_column_name = [\"company_id\", \"company_name\",\"legal_nature\", \"owner_qualification\",\"capital_stock\",\"company_size\",\"federal_owner\"]\n\n    dataframe.columns = companies_df_column_name\n\n    dataframe['capital_stock'] = dataframe['capital_stock'].str.replace(',', '.')\n    dataframe['capital_stock'] = pd.to_numeric(dataframe['capital_stock'], errors='coerce')\n    \n    dataframe_filtered = dataframe[dataframe['capital_stock'] &gt; 150000]\n    dataframe_filtered = dataframe_filtered.drop(columns=[\"federal_owner\"])\n    \n    return dataframe_filtered\n\ndef mapper(dataframe, dictionary: dict, column_name: str):\n    dataframe[column_name] = dataframe[column_name].map(dictionary)\n    return dataframe\n\ndef replace_info(dataframe):\n    legal_nature_dict = dict(zip(legal_nature['id'], legal_nature['legal_nature']))\n    qualification_dict = dict(zip(qualifications['id'], qualifications['owner_qualification']))\n    size_dict = dict(zip(sizes['id'], sizes['company_size']))\n\n    dataframe = mapper(dataframe, legal_nature_dict, 'legal_nature')\n    dataframe = mapper(dataframe, qualification_dict, \"owner_qualification\")\n    dataframe = mapper(dataframe, size_dict, 'company_size')\n\n    return dataframe\n\ndef merge_and_clean(df_top, df_bottom):\n\n    combined_df = pd.concat([df_top, df_bottom], ignore_index=True)\n    \n    cleaned_df = combined_df.drop_duplicates()\n    \n    return cleaned_df\n\nfiltered_df0 = treat_companies_dataframe(companies0)\n\nfiltered_df0 = replace_info(filtered_df0)\n\nwith open(\"../data/companies.csv\", mode=\"w\", newline='') as file:\n    write = csv.writer(file, delimiter=';')\n    write.writerow(filtered_df0.columns) \n    write.writerows(filtered_df0.values.tolist())"
  },
  {
    "objectID": "index.html#the-data",
    "href": "index.html#the-data",
    "title": "Brazilian Companies",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2026-01-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 4)\n\ncompanies &lt;- tuesdata$companies\nlegal_nature &lt;- tuesdata$legal_nature\nqualifications &lt;- tuesdata$qualifications\nsize &lt;- tuesdata$size\n\n# Option 2: Read directly from GitHub\n\ncompanies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv')\nlegal_nature &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv')\nqualifications &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv')\nsize &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2026-01-27')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncompanies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv')\nlegal_nature = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv')\nqualifications = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv')\nsize = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download datasets for the week, and load them as a NamedTuple of DataFrames\ndata = tt_load(\"2026-01-27\")\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncompanies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv\")\nlegal_nature = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv\")\nqualifications = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv\")\nsize = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncompanies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/companies.csv\", DataFrame)\nlegal_nature = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/legal_nature.csv\", DataFrame)\nqualifications = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/qualifications.csv\", DataFrame)\nsize = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-27/size.csv\", DataFrame)"
  },
  {
    "objectID": "index.html#how-to-participate",
    "href": "index.html#how-to-participate",
    "title": "Brazilian Companies",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "index.html#data-dictionary",
    "href": "index.html#data-dictionary",
    "title": "Brazilian Companies",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ncompany_id\ninteger\nCompany identifier (8-digit ID used as the primary key in this dataset).\n\n\ncompany_name\ncharacter\nCompany legal name (as provided in the source registry).\n\n\nlegal_nature\ncharacter\nCompany legal nature (e.g., “Limited Liability Business Company (LLC)”).\n\n\nowner_qualification\ncharacter\nOwner/partner qualification label (e.g., “Managing Partner / Partner-Administrator”).\n\n\ncapital_stock\nnumeric\nDeclared share capital (BRL), numeric.\n\n\ncompany_size\ncharacter\nCompany size category (e.g.,micro-enterprise, small-enterprise, other).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nLegal nature code (source registry code).\n\n\nlegal_nature\ncharacter\nLegal nature label corresponding to id.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nOwner qualification code (source registry code).\n\n\nowner_qualification\ncharacter\nOwner qualification label corresponding to id.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nCompany size code (source registry code).\n\n\ncompany_size\ncharacter\nCompany size label corresponding to id (e.g., micro-enterprise, small-enterprise)."
  },
  {
    "objectID": "index.html#cleaning-script",
    "href": "index.html#cleaning-script",
    "title": "Brazilian Companies",
    "section": "",
    "text": "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\n\n# Input data\ncompanies0 = pd.read_csv(\"../raw_data/raw_companies.csv\", sep=\";\", encoding=\"cp1252\", header=None)\n\nlegal_nature = pd.read_csv('../raw_data/legal_nature.csv', sep=',')\n\nsizes = pd.read_csv(\"../raw_data/size.csv\", sep=\",\", encoding=\"cp1252\")\n\nqualifications = pd.read_csv(\"../raw_data/qualifications.csv\", sep = \",\", encoding=\"cp1252\")\n\n# Remove all private associations\ndef treat_companies_dataframe(dataframe):\n    companies_df_column_name = [\"company_id\", \"company_name\",\"legal_nature\", \"owner_qualification\",\"capital_stock\",\"company_size\",\"federal_owner\"]\n\n    dataframe.columns = companies_df_column_name\n\n    dataframe['capital_stock'] = dataframe['capital_stock'].str.replace(',', '.')\n    dataframe['capital_stock'] = pd.to_numeric(dataframe['capital_stock'], errors='coerce')\n    \n    dataframe_filtered = dataframe[dataframe['capital_stock'] &gt; 150000]\n    dataframe_filtered = dataframe_filtered.drop(columns=[\"federal_owner\"])\n    \n    return dataframe_filtered\n\ndef mapper(dataframe, dictionary: dict, column_name: str):\n    dataframe[column_name] = dataframe[column_name].map(dictionary)\n    return dataframe\n\ndef replace_info(dataframe):\n    legal_nature_dict = dict(zip(legal_nature['id'], legal_nature['legal_nature']))\n    qualification_dict = dict(zip(qualifications['id'], qualifications['owner_qualification']))\n    size_dict = dict(zip(sizes['id'], sizes['company_size']))\n\n    dataframe = mapper(dataframe, legal_nature_dict, 'legal_nature')\n    dataframe = mapper(dataframe, qualification_dict, \"owner_qualification\")\n    dataframe = mapper(dataframe, size_dict, 'company_size')\n\n    return dataframe\n\ndef merge_and_clean(df_top, df_bottom):\n\n    combined_df = pd.concat([df_top, df_bottom], ignore_index=True)\n    \n    cleaned_df = combined_df.drop_duplicates()\n    \n    return cleaned_df\n\nfiltered_df0 = treat_companies_dataframe(companies0)\n\nfiltered_df0 = replace_info(filtered_df0)\n\nwith open(\"../data/companies.csv\", mode=\"w\", newline='') as file:\n    write = csv.writer(file, delimiter=';')\n    write.writerow(filtered_df0.columns) \n    write.writerows(filtered_df0.values.tolist())"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "CONTRIBUTING",
    "section": "",
    "text": "Thank you for contributing to this project, whether you have a dataset to share or a pull request to contribute! We’re excited for people to engage with the project, and aim to address issues aligned with the goals of the project and respond to people as we can.\nThis project is in Active mode, meaning we review this repository on a weekly basis, but we may not have time to address every issue. So, you might not hear from us right away, but we appreciate your contribution!\nThere are several ways you can help:\n\nSubmit a dataset pull request\nReview submitted dataset ideas\nSubmit a dataset idea\nSubmit a bug report or feature request\n\nThese guidelines are for first-time contributors to get you started. If you are a regular contributor, we may be working in a different mode, where the workflow is different.\n\n\nIf you found something wrong or confusing with the project, please submit an issue!\nHowever, before you submit an issue, please search through existing issues to see if it’s something someone already shared. You may be able to provide more information to an existing issue.\n\nProvide as much information as you can. We suggest submitting a reprex if it’s an issue or bug. Also, see this guide to writing good bug reports for more on how to write a great bug report.\nIf it’s an idea for a feature or change, please describe the change you’re proposing, how you would plan to implement it (if you have an idea), and why it’s important for the goals of the project. See this guide to writing feature requests for more on how to write a great feature request.\nUse kind language aligned with how we treat each other in this project.\n\n\n\n\nThis project is in Active mode, meaning we review this repository on a weekly basis, but we may not have time to address every issue. So, you might not hear from us right away, but we appreciate your contribution!\nWe may:\n\nask questions for more clarity\nadd some changes to your pull request and merge\nexplain why this issue or PR isn’t right for this project right now\n\n\n\n\nJon Harmon is the primary maintainer for this project. Decisions for the project will be made with the project goals in mind.\nIf you’d like to propose a new idea for the project, please share it an issue for discussion."
  },
  {
    "objectID": "CONTRIBUTING.html#submitting-general-non-dataset-issues",
    "href": "CONTRIBUTING.html#submitting-general-non-dataset-issues",
    "title": "CONTRIBUTING",
    "section": "",
    "text": "If you found something wrong or confusing with the project, please submit an issue!\nHowever, before you submit an issue, please search through existing issues to see if it’s something someone already shared. You may be able to provide more information to an existing issue.\n\nProvide as much information as you can. We suggest submitting a reprex if it’s an issue or bug. Also, see this guide to writing good bug reports for more on how to write a great bug report.\nIf it’s an idea for a feature or change, please describe the change you’re proposing, how you would plan to implement it (if you have an idea), and why it’s important for the goals of the project. See this guide to writing feature requests for more on how to write a great feature request.\nUse kind language aligned with how we treat each other in this project."
  },
  {
    "objectID": "CONTRIBUTING.html#expectations-for-responsereview",
    "href": "CONTRIBUTING.html#expectations-for-responsereview",
    "title": "CONTRIBUTING",
    "section": "",
    "text": "This project is in Active mode, meaning we review this repository on a weekly basis, but we may not have time to address every issue. So, you might not hear from us right away, but we appreciate your contribution!\nWe may:\n\nask questions for more clarity\nadd some changes to your pull request and merge\nexplain why this issue or PR isn’t right for this project right now"
  },
  {
    "objectID": "CONTRIBUTING.html#governance",
    "href": "CONTRIBUTING.html#governance",
    "title": "CONTRIBUTING",
    "section": "",
    "text": "Jon Harmon is the primary maintainer for this project. Decisions for the project will be made with the project goals in mind.\nIf you’d like to propose a new idea for the project, please share it an issue for discussion."
  },
  {
    "objectID": "data/2018/2018-05-14/readme.html",
    "href": "data/2018/2018-05-14/readme.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "Back in 2018, we didn’t always have README files for the datasets."
  },
  {
    "objectID": "data/2018/2018-09-04/readme.html",
    "href": "data/2018/2018-09-04/readme.html",
    "title": "Fast food entree data",
    "section": "",
    "text": "Fast food entree data\n\nData from fastfoodnutrition.com\nPlease notice that I really only took entrees - feel free to select ALL food, sides, drinks, desserts, etc.\n\nAt the request of the website owner - I have removed web-scraping guide."
  },
  {
    "objectID": "data/2018/2018-09-18/readme.html",
    "href": "data/2018/2018-09-18/readme.html",
    "title": "Week 25 Data",
    "section": "",
    "text": "We have a double data-set for this week!\n\nUS airport data from 2013-2017\nHypoxia dataset in pilots from community member Nathan Cook\n\n\n\n\nFull details here\nData collected from calendar year (CY) 2013 - 2017 (preliminary)\n\n\n\n\nNathan is a glider pilot, and when gliders go above 14,000 feet, the pilot is required to have a supplemental oxygen source. The magazine of the Soaring Society of America (SSA), Soaring, recently published an article about the lack of oxygen and/or carbon dioxide during flight, and the table caught his eye. He received permission from the author and editor to post the article and “crowdsource” different means of presenting the data, which could include alternative tabular representations or other more visual means.\nHere is the article in full. Table 1 has the useful information.\nThe author, the editor, and I are very interested in the products of everyone’s imaginations! SSA is a non-profit, the author was not paid for his work, and the table originated from Guyton & Hall: Textbook of Medical Physiology, 12th ed. Attribution is all that is requested."
  },
  {
    "objectID": "data/2018/2018-09-18/readme.html#us-airport-data-dictionary",
    "href": "data/2018/2018-09-18/readme.html#us-airport-data-dictionary",
    "title": "Week 25 Data",
    "section": "",
    "text": "Full details here\nData collected from calendar year (CY) 2013 - 2017 (preliminary)"
  },
  {
    "objectID": "data/2018/2018-09-18/readme.html#hypoxia-details",
    "href": "data/2018/2018-09-18/readme.html#hypoxia-details",
    "title": "Week 25 Data",
    "section": "",
    "text": "Nathan is a glider pilot, and when gliders go above 14,000 feet, the pilot is required to have a supplemental oxygen source. The magazine of the Soaring Society of America (SSA), Soaring, recently published an article about the lack of oxygen and/or carbon dioxide during flight, and the table caught his eye. He received permission from the author and editor to post the article and “crowdsource” different means of presenting the data, which could include alternative tabular representations or other more visual means.\nHere is the article in full. Table 1 has the useful information.\nThe author, the editor, and I are very interested in the products of everyone’s imaginations! SSA is a non-profit, the author was not paid for his work, and the table originated from Guyton & Hall: Textbook of Medical Physiology, 12th ed. Attribution is all that is requested."
  },
  {
    "objectID": "data/2018/2018-10-02/readme.html",
    "href": "data/2018/2018-10-02/readme.html",
    "title": "U.S. Births",
    "section": "",
    "text": "U.S. Births\nThis folder contains data behind the story Some People Are Too Superstitious To Have A Baby On Friday The 13th.\nus_births_2000-2014.csv contains U.S. births data for the years 2000 to 2014, as provided by the Social Security Administration.\nThe data have the following structure:\n\n\n\nHeader\nDefinition\n\n\n\n\nyear\nYear\n\n\nmonth\nMonth\n\n\ndate_of_month\nDay number of the month\n\n\nday_of_week\nDay of week, where 1 is Monday and 7 is Sunday\n\n\nbirths\nNumber of births"
  },
  {
    "objectID": "data/2018/2018-10-16/readme.html",
    "href": "data/2018/2018-10-16/readme.html",
    "title": "538 - Economic Guide to Picking a College Major",
    "section": "",
    "text": "The data behind the story The Economic Guide To Picking A College Major.\nRaw data\nAll data is from American Community Survey 2010-2012 Public Use Microdata Series.\nOriginal data and more: http://www.census.gov/programs-surveys/acs/data/pums.html\nDocumentation here: http://www.census.gov/programs-surveys/acs/technical-documentation/pums.html\n\n\n\n\n\n\n\n\n\nHeader\nDescription\n\n\n\n\nRank\nRank by median earnings\n\n\nMajor_code\nMajor code, FO1DP in ACS PUMS\n\n\nMajor\nMajor description\n\n\nMajor_category\nCategory of major from Carnevale et al\n\n\nTotal\nTotal number of people with major\n\n\nSample_size\nSample size (unweighted) of full-time, year-round ONLY (used for earnings)\n\n\nMen\nMale graduates\n\n\nWomen\nFemale graduates\n\n\nShareWomen\nWomen as share of total\n\n\nEmployed\nNumber employed (ESR == 1 or 2)\n\n\nFull_time\nEmployed 35 hours or more\n\n\nPart_time\nEmployed less than 35 hours\n\n\nFull_time_year_round\nEmployed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP &gt;= 35)\n\n\nUnemployed\nNumber unemployed (ESR == 3)\n\n\nUnemployment_rate\nUnemployed / (Unemployed + Employed)\n\n\nMedian\nMedian earnings of full-time, year-round workers\n\n\nP25th\n25th percentile of earnigns\n\n\nP75th\n75th percentile of earnings\n\n\nCollege_jobs\nNumber with job requiring a college degree\n\n\nNon_college_jobs\nNumber with job not requiring a college degree\n\n\nLow_wage_jobs\nNumber in low-wage service jobs"
  },
  {
    "objectID": "data/2018/2018-10-16/readme.html#data-dictionary",
    "href": "data/2018/2018-10-16/readme.html#data-dictionary",
    "title": "538 - Economic Guide to Picking a College Major",
    "section": "",
    "text": "Header\nDescription\n\n\n\n\nRank\nRank by median earnings\n\n\nMajor_code\nMajor code, FO1DP in ACS PUMS\n\n\nMajor\nMajor description\n\n\nMajor_category\nCategory of major from Carnevale et al\n\n\nTotal\nTotal number of people with major\n\n\nSample_size\nSample size (unweighted) of full-time, year-round ONLY (used for earnings)\n\n\nMen\nMale graduates\n\n\nWomen\nFemale graduates\n\n\nShareWomen\nWomen as share of total\n\n\nEmployed\nNumber employed (ESR == 1 or 2)\n\n\nFull_time\nEmployed 35 hours or more\n\n\nPart_time\nEmployed less than 35 hours\n\n\nFull_time_year_round\nEmployed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP &gt;= 35)\n\n\nUnemployed\nNumber unemployed (ESR == 3)\n\n\nUnemployment_rate\nUnemployed / (Unemployed + Employed)\n\n\nMedian\nMedian earnings of full-time, year-round workers\n\n\nP25th\n25th percentile of earnigns\n\n\nP75th\n75th percentile of earnings\n\n\nCollege_jobs\nNumber with job requiring a college degree\n\n\nNon_college_jobs\nNumber with job not requiring a college degree\n\n\nLow_wage_jobs\nNumber in low-wage service jobs"
  },
  {
    "objectID": "data/2018/2018-10-30/readme.html",
    "href": "data/2018/2018-10-30/readme.html",
    "title": "Week 31 - R and Package download stats",
    "section": "",
    "text": "Anonymized package and R language downloads from the RStudio CRAN mirror.\n\nr-downloads.csv - R language downloads from RStudio CRAN mirror on last TidyTuesday for October 23, 2018.\nr_downloads_year.csv - A year’s worth of R language downloads from RStudio CRAN mirror between October 20, 2017 and October 20, 2018.\n\n\n\n\n\n\nHeader\nDescription\n\n\n\n\ndate\ndate of download (y-m-d)\n\n\ntime\ntime of download (in UTC)\n\n\nsize\nsize (in bytes)\n\n\nversion\nR release version\n\n\nos\nOperating System\n\n\ncountry\nTwo letter ISO country code.\n\n\nip_id\nAnonymized daily ip code (unique identifier)\n\n\n\n\n\n\nPackage downloads end up with MUCH larger metadata files and are a bit unwieldy to work with, but if you want to play around with them you can use a few ways to easily check!\n\n\n\n.csv.gz files (can be read directly into R via readr::read_csv()\nData back to Oct 2012\nPackage and R-language downloads (anonymized)\n\n\n\n# Set your range of dates\nstart &lt;- as.Date('2012-10-01')\ntoday &lt;- as.Date('2018-10-27')\n\nall_days &lt;- seq(start, today, by = 'day')\n\nyear &lt;- as.POSIXlt(all_days)$year + 1900\n\n# combine dates into a character vector of dates\nurls &lt;- paste0('http://cran-logs.rstudio.com/', year, '/', all_days, '.csv.gz')\n\n# Read directly into memory as dataframe\nurls %&gt;%\n  map_dfr(read_csv)\n  \n# Can use download.file to download instead of read into memory\ncranlogs * Extremely easy to use and small data files * Let’s you download by specific packages or look over small time-frames to see what were the most popular packages. * Con: Aggregated data for the most part\ninstallr::download_RStudio_CRAN_data * Downloads data from RStudio CRAN anonymized logs via API call * Has download and read function built in for working with CRAN log data"
  },
  {
    "objectID": "data/2018/2018-10-30/readme.html#data-dictionary",
    "href": "data/2018/2018-10-30/readme.html#data-dictionary",
    "title": "Week 31 - R and Package download stats",
    "section": "",
    "text": "Header\nDescription\n\n\n\n\ndate\ndate of download (y-m-d)\n\n\ntime\ntime of download (in UTC)\n\n\nsize\nsize (in bytes)\n\n\nversion\nR release version\n\n\nos\nOperating System\n\n\ncountry\nTwo letter ISO country code.\n\n\nip_id\nAnonymized daily ip code (unique identifier)"
  },
  {
    "objectID": "data/2018/2018-10-30/readme.html#package-level-downloads",
    "href": "data/2018/2018-10-30/readme.html#package-level-downloads",
    "title": "Week 31 - R and Package download stats",
    "section": "",
    "text": "Package downloads end up with MUCH larger metadata files and are a bit unwieldy to work with, but if you want to play around with them you can use a few ways to easily check!\n\n\n\n.csv.gz files (can be read directly into R via readr::read_csv()\nData back to Oct 2012\nPackage and R-language downloads (anonymized)\n\n\n\n# Set your range of dates\nstart &lt;- as.Date('2012-10-01')\ntoday &lt;- as.Date('2018-10-27')\n\nall_days &lt;- seq(start, today, by = 'day')\n\nyear &lt;- as.POSIXlt(all_days)$year + 1900\n\n# combine dates into a character vector of dates\nurls &lt;- paste0('http://cran-logs.rstudio.com/', year, '/', all_days, '.csv.gz')\n\n# Read directly into memory as dataframe\nurls %&gt;%\n  map_dfr(read_csv)\n  \n# Can use download.file to download instead of read into memory\ncranlogs * Extremely easy to use and small data files * Let’s you download by specific packages or look over small time-frames to see what were the most popular packages. * Con: Aggregated data for the most part\ninstallr::download_RStudio_CRAN_data * Downloads data from RStudio CRAN anonymized logs via API call * Has download and read function built in for working with CRAN log data"
  },
  {
    "objectID": "data/2018/2018-11-13/readme.html",
    "href": "data/2018/2018-11-13/readme.html",
    "title": "Malaria Dataset",
    "section": "",
    "text": "A lot of different options this week - all related to malaria data.\n\n\nThere is a Data Challenge hosted by the Wellcome Trust and Sage Bionetworks using the malariaAtlas package of data and tools. It is officially open as of November 14th, 2018, with the potential to net a £15k (~$19,531) prize ! You can still access the package to use the mapping tools and access their dataset.\nA lot of it is related to mapping, feel free to dive in and participate in their challenge or just stay within #TidyTuesday!\n\n\n\nAlternatively, if you’d rather work with some simple aggregated malaria data from Our World in Data, you can see many different summary-level datasets related to malaria incidence by region, age, or time.\n3 Datasets: * malaria_inc.csv - Malaria incidence by country for all ages across the world across time * malaria_deaths.csv - Malaria deaths by country for all ages across the world and time. * malaria_deaths_age.csv - Malaria deaths by age across the world and time."
  },
  {
    "objectID": "data/2018/2018-11-13/readme.html#data-challenge",
    "href": "data/2018/2018-11-13/readme.html#data-challenge",
    "title": "Malaria Dataset",
    "section": "",
    "text": "There is a Data Challenge hosted by the Wellcome Trust and Sage Bionetworks using the malariaAtlas package of data and tools. It is officially open as of November 14th, 2018, with the potential to net a £15k (~$19,531) prize ! You can still access the package to use the mapping tools and access their dataset.\nA lot of it is related to mapping, feel free to dive in and participate in their challenge or just stay within #TidyTuesday!"
  },
  {
    "objectID": "data/2018/2018-11-13/readme.html#our-world-in-data",
    "href": "data/2018/2018-11-13/readme.html#our-world-in-data",
    "title": "Malaria Dataset",
    "section": "",
    "text": "Alternatively, if you’d rather work with some simple aggregated malaria data from Our World in Data, you can see many different summary-level datasets related to malaria incidence by region, age, or time.\n3 Datasets: * malaria_inc.csv - Malaria incidence by country for all ages across the world across time * malaria_deaths.csv - Malaria deaths by country for all ages across the world and time. * malaria_deaths_age.csv - Malaria deaths by age across the world and time."
  },
  {
    "objectID": "data/2018/2018-11-20/readme.html",
    "href": "data/2018/2018-11-20/readme.html",
    "title": "FiveThirtyEight Thanksgiving Dinner or Transgender Day of Rembrance (TDoR)",
    "section": "",
    "text": "FiveThirtyEight Thanksgiving Dinner or Transgender Day of Rembrance (TDoR)\nData originally from fivethirtyeight.\n\n\nThanksgiving 2015\nThis directory contains the data behind the story Here’s What Your Part of America Eats On Thanksgiving.\nUsing a SurveyMonkey poll, they asked 1,058 respondents on Nov. 17, 2015 the following questions about their Thanksgiving:\n\nDo you celebrate Thanksgiving?\nWhat is typically the main dish at your Thanksgiving dinner?\n\nOther (please specify)\n\nHow is the main dish typically cooked?\n\nOther (please specify)\n\nWhat kind of stuffing/dressing do you typically have?\n\nOther (please specify)\n\nWhat type of cranberry sauce do you typically have?\n\nOther (please specify)\n\nDo you typically have gravy?\nWhich of these side dishes are typically served at your Thanksgiving dinner? Please select all that apply.\n\nBrussel sprouts\nCarrots\nCauliflower\nCorn\nCornbread\nFruit salad\nGreen beans/green bean casserole\nMacaroni and cheese\nMashed potatoes\nRolls/biscuits\nVegetable salad\nYams/sweet potato casserole\nOther (please specify)\n\nWhich type of pie is typically served at your Thanksgiving dinner? Please select all that apply.\n\nApple\nButtermilk\nCherry\nChocolate\nCoconut cream\nKey lime\nPeach\nPecan\nPumpkin\nSweet Potato\nNone\nOther (please specify)\n\nWhich of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.\n\nApple cobbler\nBlondies\nBrownies\nCarrot cake\nCheesecake\nCookies\nFudge\nIce cream\nPeach cobbler\nNone\nOther (please specify)\n\nDo you typically pray before or after the Thanksgiving meal?\nHow far will you travel for Thanksgiving?\nWill you watch any of the following programs on Thanksgiving? Please select all that apply.\n\nMacy’s Parade\n\nWhat’s the age cutoff at your “kids’ table” at Thanksgiving?\nHave you ever tried to meet up with hometown friends on Thanksgiving night?\nHave you ever attended a “Friendsgiving?”\nWill you shop any Black Friday sales on Thanksgiving Day?\nDo you work in retail?\nWill you employer make you work on Black Friday?\nHow would you describe where you live?\nAge\nWhat is your gender?\nHow much total combined money did all members of your HOUSEHOLD earn last year?\nUS Region"
  },
  {
    "objectID": "data/2018/2018-12-04/readme.html",
    "href": "data/2018/2018-12-04/readme.html",
    "title": "Medium Data Science articles",
    "section": "",
    "text": "This week’s dataset was submitted by Matthew Hendrickson, thanks! Also credit to Kanishka Misra who wanted to work with some text-based data via the tidytext package.\n\n\nData was originally scraped by Harrison Jansma and submitted as a Kaggle dataset.\nThe data-set originally consisted of 1.4 million stories from 95 of Medium’s most popular story-tags. Every story was published between August 1st, 2017 and August 1st, 2018.\nFor each story, Harrison collected all of the information present on a Medium story-card.\nHere is the full list of the information he was able to collect for each story: Title, Sub-Title, Author, Publication, Date, Tags, Read-Time, Claps-Received, Story-URL, and Author-URL.\nGiven that this file was ~660 MB, I filtered the dataset to only articles with tags in: AI, Artificial Intelligience, big data, data, data science, data visualization, deep learning and machine learning. Feel free to use the original dataset if you want a deeper dive.\n\n\n\n\n\n\n\ncolumn\ndescription\n\n\n\n\ntitle\nTitle of the article\n\n\nsubtitle\nSubtitle of the article\n\n\nimage\nHeader image present\n\n\nauthor\nAuthor’s name\n\n\npublication\nPublication that the article was published under\n\n\nyear\nYear\n\n\nmonth\nMonth\n\n\nday\nDay\n\n\nreading_time\nEstimated reading time in minutes\n\n\nclaps\nNumber of claps (similar to likes, 1 person could clap many times)\n\n\nurl\nurl for the article\n\n\nauthor_url\nurl for the author’s Medium page\n\n\ntag_ai\ntag AI\n\n\ntag_artificial_intelligience\ntag artificial intelligience\n\n\ntag_big_data\ntag big data\n\n\ntag_data\ntag data\n\n\ntag_data_science\ntag data science\n\n\ntag_data_visualization\ntag data visualization\n\n\ntag_deep_learning\ntag deep learning\n\n\ntag_machine_learning\ntag machine learning"
  },
  {
    "objectID": "data/2018/2018-12-04/readme.html#data",
    "href": "data/2018/2018-12-04/readme.html#data",
    "title": "Medium Data Science articles",
    "section": "",
    "text": "Data was originally scraped by Harrison Jansma and submitted as a Kaggle dataset.\nThe data-set originally consisted of 1.4 million stories from 95 of Medium’s most popular story-tags. Every story was published between August 1st, 2017 and August 1st, 2018.\nFor each story, Harrison collected all of the information present on a Medium story-card.\nHere is the full list of the information he was able to collect for each story: Title, Sub-Title, Author, Publication, Date, Tags, Read-Time, Claps-Received, Story-URL, and Author-URL.\nGiven that this file was ~660 MB, I filtered the dataset to only articles with tags in: AI, Artificial Intelligience, big data, data, data science, data visualization, deep learning and machine learning. Feel free to use the original dataset if you want a deeper dive.\n\n\n\n\n\n\n\ncolumn\ndescription\n\n\n\n\ntitle\nTitle of the article\n\n\nsubtitle\nSubtitle of the article\n\n\nimage\nHeader image present\n\n\nauthor\nAuthor’s name\n\n\npublication\nPublication that the article was published under\n\n\nyear\nYear\n\n\nmonth\nMonth\n\n\nday\nDay\n\n\nreading_time\nEstimated reading time in minutes\n\n\nclaps\nNumber of claps (similar to likes, 1 person could clap many times)\n\n\nurl\nurl for the article\n\n\nauthor_url\nurl for the author’s Medium page\n\n\ntag_ai\ntag AI\n\n\ntag_artificial_intelligience\ntag artificial intelligience\n\n\ntag_big_data\ntag big data\n\n\ntag_data\ntag data\n\n\ntag_data_science\ntag data science\n\n\ntag_data_visualization\ntag data visualization\n\n\ntag_deep_learning\ntag deep learning\n\n\ntag_machine_learning\ntag machine learning"
  },
  {
    "objectID": "data/2018/2018-12-18/readme.html",
    "href": "data/2018/2018-12-18/readme.html",
    "title": "Cetacean Dataset",
    "section": "",
    "text": "Data for this week comes from a The Pudding article.\n\n\nThis folder contains all of the data used in The Pudding article Free Willy and Flipper by the Numbers published in July 2017.\nThe below metadata and descriptions were taken directly from the GitHub for The Pudding and were prepared by Amber Thomas.\n\n\n\n\nWhat is this?: A collection of all data that I could find on whales and dolphins that spent some period of time captive in the US between 1938 and May 7, 2017.\nSource(s): The data used to create this aggregated set came from the US National Marine Mammal Inventory (original data requested on June 15, 2015 via FOIA available here) and from Ceta-Base. Data from Ceta-Base were downloaded May 7, 2017.\nLast Modified: May 7, 2017\nContact Information: Amber Thomas\nSpatial Applicability: United States\nTemporal Applicability: March 1, 1938 - May 7, 2017\nVariables (Columns):\n\n\n\n\n\n\n\n\n\nHeader\nDescription\nData Type\n\n\n\n\nspecies\nThe common name for an individual animal’s species (e.g., Bottlenose, Killer whale; orca etc.)\ntext\n\n\nid\nOne or more individual identification numbers assigned to an animal. Note: Not all individual animals have an ID number. If an animal has no number, it is denoted as NA.\ntext\n\n\nname\nAn individual animal’s name. Some animals have been known by more than one name. Where possible, the additional name(s) are noted in Notes.\ntext\n\n\nsex\nAn individual animal’s sex. • M = male• F = female• U = unknown\ntext\n\n\naccuracy\nThe accuracy of an animal’s birth date (i.e., birth dates for wild-caught animals are estimated). • a = actual date  • e = estimated date • u = unknown\ntext\n\n\nbirthYear\nThe year that an individual animal was born (see Accuracy to determine if that birth year is known or estimated). If a birth year is unknown and not estimated, it is coded as NA.\nnumber\n\n\nacquisition\nThe method through which an animal was acquired and brought into the captive cetacean group.  • Born = animal was born in captivity  • Capture = animal was captured from the ocean  • Rescue = animal was rescued (e.g., had stranded or required medical intervention) and was deemed unreleasable.  Note: If the recorder was unsure how an animal was acquired, they may have included a ? with the above code (e.g., born?).\ntext\n\n\noriginDate\nThe year that an animal entered captivity. For a wild-caught or rescued animal, this will be the year that they were rescued/captured. For captive-born animals, this will be the same as the birth year. Unknown dates are encoded as NA.\ndate\n\n\noriginLocation\nThe location that an animal originated from. For captured/rescued animals this is an approximate location in the ocean (e.g., Atlantic Ocean or Gulf of Mexico, MS, US), but for animals born in captivity it will list the facility where the animal was born.\ntext\n\n\nmother\nThe name of an animal’s biological mother (if known). Note: Some names contain species scientific names (e.g., Cindy (T.t. gilli)). Unknown names are encoded as NA.\ntext\n\n\nfather\nThe name of an animal’s biological father (if known). Note: Some names contain species scientific names (e.g., Jethro (T.t. gilli)). Unknown names are encoded as NA.\ntext\n\n\ntransfers\nA list of facilities that an animal was transferred to, with the approximate dates that they were transferred (e.g., SeaWorld Orlando to New England Aquarium (22-Feb-1982) to Dolphin Research Center (23-Oct-1991)). If no transfers are known, this field is encoded as NA.\ntext\n\n\ncurrently\nThe location where a living animal currently resides (as of May 7, 2017) or the last location where an animal lived before it died. If an animal’s last location is unknown it is encoded as Unknown.\ntext\n\n\nregion\nThe region of the world where the animal either currently or most-recently lived.\ntext\n\n\nstatus\nThe current status of an animal.  • Alive = animal is still alive and living in captivity (as of May 7, 2017)  • Died = animal has been confirmed as dead.  • Stillbirth = Calf that died before birth. • Miscarriage = Calf that was miscarried before full-term gestation. • Released = Animal had lived in captivity for some period of time but has since been released to the ocean.  • Unknown = The animal’s current status is unknown.  Note: When the recorder of an animal’s status wasn’t sure exactly what happened to the animal, they may have included a ? with the above code (e.g., stillbirth?).\ntext\n\n\nstatusDate\nThe date that an animal’s status changed. For living animals, the status date is NA, but for animals that have died or been released, this is their date of death or release.\ndate\n\n\nCOD\nThe animal’s reported cause of death. For living animals, this is NA. Note: Use these data with caution. COD is reported differently between facilities and are not always reported by a pathologist that can properly identify the cause of death.\ntext\n\n\ntransferDate\nThe date an animal was transferred into the US. For animals that were born in the US, this is NA.\ndate\n\n\ntransfer\nThe types of transfers that an animal has been involved with. • US = The animal was only transferred between facilities in the US • Foreign = The animal was transferred from outside of the US into the US.\ntext\n\n\nentryDate\nThe date that an individual animal entered the US captive population. For captive-born animals, this is their birth date or the day they were transferred to a facility in the US. For captured or rescued animals, this is either the date that they were captured/rescued or the day that they were transferred from a foreign facility to a US one.\ndate"
  },
  {
    "objectID": "data/2018/2018-12-18/readme.html#dataset-information",
    "href": "data/2018/2018-12-18/readme.html#dataset-information",
    "title": "Cetacean Dataset",
    "section": "",
    "text": "This folder contains all of the data used in The Pudding article Free Willy and Flipper by the Numbers published in July 2017.\nThe below metadata and descriptions were taken directly from the GitHub for The Pudding and were prepared by Amber Thomas."
  },
  {
    "objectID": "data/2018/2018-12-18/readme.html#allcetaceandata.csv",
    "href": "data/2018/2018-12-18/readme.html#allcetaceandata.csv",
    "title": "Cetacean Dataset",
    "section": "",
    "text": "What is this?: A collection of all data that I could find on whales and dolphins that spent some period of time captive in the US between 1938 and May 7, 2017.\nSource(s): The data used to create this aggregated set came from the US National Marine Mammal Inventory (original data requested on June 15, 2015 via FOIA available here) and from Ceta-Base. Data from Ceta-Base were downloaded May 7, 2017.\nLast Modified: May 7, 2017\nContact Information: Amber Thomas\nSpatial Applicability: United States\nTemporal Applicability: March 1, 1938 - May 7, 2017\nVariables (Columns):\n\n\n\n\n\n\n\n\n\nHeader\nDescription\nData Type\n\n\n\n\nspecies\nThe common name for an individual animal’s species (e.g., Bottlenose, Killer whale; orca etc.)\ntext\n\n\nid\nOne or more individual identification numbers assigned to an animal. Note: Not all individual animals have an ID number. If an animal has no number, it is denoted as NA.\ntext\n\n\nname\nAn individual animal’s name. Some animals have been known by more than one name. Where possible, the additional name(s) are noted in Notes.\ntext\n\n\nsex\nAn individual animal’s sex. • M = male• F = female• U = unknown\ntext\n\n\naccuracy\nThe accuracy of an animal’s birth date (i.e., birth dates for wild-caught animals are estimated). • a = actual date  • e = estimated date • u = unknown\ntext\n\n\nbirthYear\nThe year that an individual animal was born (see Accuracy to determine if that birth year is known or estimated). If a birth year is unknown and not estimated, it is coded as NA.\nnumber\n\n\nacquisition\nThe method through which an animal was acquired and brought into the captive cetacean group.  • Born = animal was born in captivity  • Capture = animal was captured from the ocean  • Rescue = animal was rescued (e.g., had stranded or required medical intervention) and was deemed unreleasable.  Note: If the recorder was unsure how an animal was acquired, they may have included a ? with the above code (e.g., born?).\ntext\n\n\noriginDate\nThe year that an animal entered captivity. For a wild-caught or rescued animal, this will be the year that they were rescued/captured. For captive-born animals, this will be the same as the birth year. Unknown dates are encoded as NA.\ndate\n\n\noriginLocation\nThe location that an animal originated from. For captured/rescued animals this is an approximate location in the ocean (e.g., Atlantic Ocean or Gulf of Mexico, MS, US), but for animals born in captivity it will list the facility where the animal was born.\ntext\n\n\nmother\nThe name of an animal’s biological mother (if known). Note: Some names contain species scientific names (e.g., Cindy (T.t. gilli)). Unknown names are encoded as NA.\ntext\n\n\nfather\nThe name of an animal’s biological father (if known). Note: Some names contain species scientific names (e.g., Jethro (T.t. gilli)). Unknown names are encoded as NA.\ntext\n\n\ntransfers\nA list of facilities that an animal was transferred to, with the approximate dates that they were transferred (e.g., SeaWorld Orlando to New England Aquarium (22-Feb-1982) to Dolphin Research Center (23-Oct-1991)). If no transfers are known, this field is encoded as NA.\ntext\n\n\ncurrently\nThe location where a living animal currently resides (as of May 7, 2017) or the last location where an animal lived before it died. If an animal’s last location is unknown it is encoded as Unknown.\ntext\n\n\nregion\nThe region of the world where the animal either currently or most-recently lived.\ntext\n\n\nstatus\nThe current status of an animal.  • Alive = animal is still alive and living in captivity (as of May 7, 2017)  • Died = animal has been confirmed as dead.  • Stillbirth = Calf that died before birth. • Miscarriage = Calf that was miscarried before full-term gestation. • Released = Animal had lived in captivity for some period of time but has since been released to the ocean.  • Unknown = The animal’s current status is unknown.  Note: When the recorder of an animal’s status wasn’t sure exactly what happened to the animal, they may have included a ? with the above code (e.g., stillbirth?).\ntext\n\n\nstatusDate\nThe date that an animal’s status changed. For living animals, the status date is NA, but for animals that have died or been released, this is their date of death or release.\ndate\n\n\nCOD\nThe animal’s reported cause of death. For living animals, this is NA. Note: Use these data with caution. COD is reported differently between facilities and are not always reported by a pathologist that can properly identify the cause of death.\ntext\n\n\ntransferDate\nThe date an animal was transferred into the US. For animals that were born in the US, this is NA.\ndate\n\n\ntransfer\nThe types of transfers that an animal has been involved with. • US = The animal was only transferred between facilities in the US • Foreign = The animal was transferred from outside of the US into the US.\ntext\n\n\nentryDate\nThe date that an individual animal entered the US captive population. For captive-born animals, this is their birth date or the day they were transferred to a facility in the US. For captured or rescued animals, this is either the date that they were captured/rescued or the day that they were transferred from a foreign facility to a US one.\ndate"
  },
  {
    "objectID": "data/2019/2019-01-01/readme.html",
    "href": "data/2019/2019-01-01/readme.html",
    "title": "#rstats and #TidyTuesday Tweets from rtweet",
    "section": "",
    "text": "Data for this week comes from Mike Kearney - author of the awesome rtweet package, as well as several other packages!\n\n\n#rstats\n#TidyTuesday\nJust a heads up, there are A LOT of columns (88!) in this collection - feel free to select whichever are useful for your analysis or interest! Both datasets have the same column types which can be seen below.\n\n\n\nvariable\nclass\n\n\n\n\nuser_id\ncharacter\n\n\nstatus_id\ncharacter\n\n\ncreated_at\ndouble\n\n\nscreen_name\ncharacter\n\n\ntext\ncharacter\n\n\nsource\ncharacter\n\n\ndisplay_text_width\ndouble\n\n\nreply_to_status_id\ncharacter\n\n\nreply_to_user_id\ncharacter\n\n\nreply_to_screen_name\ncharacter\n\n\nis_quote\nlogical\n\n\nis_retweet\nlogical\n\n\nfavorite_count\ninteger\n\n\nretweet_count\ninteger\n\n\nhashtags\nlist\n\n\nsymbols\nlist\n\n\nurls_url\nlist\n\n\nurls_t.co\nlist\n\n\nurls_expanded_url\nlist\n\n\nmedia_url\nlist\n\n\nmedia_t.co\nlist\n\n\nmedia_expanded_url\nlist\n\n\nmedia_type\nlist\n\n\next_media_url\nlist\n\n\next_media_t.co\nlist\n\n\next_media_expanded_url\nlist\n\n\next_media_type\ncharacter\n\n\nmentions_user_id\nlist\n\n\nmentions_screen_name\nlist\n\n\nlang\ncharacter\n\n\nquoted_status_id\ncharacter\n\n\nquoted_text\ncharacter\n\n\nquoted_created_at\ndouble\n\n\nquoted_source\ncharacter\n\n\nquoted_favorite_count\ninteger\n\n\nquoted_retweet_count\ninteger\n\n\nquoted_user_id\ncharacter\n\n\nquoted_screen_name\ncharacter\n\n\nquoted_name\ncharacter\n\n\nquoted_followers_count\ninteger\n\n\nquoted_friends_count\ninteger\n\n\nquoted_statuses_count\ninteger\n\n\nquoted_location\ncharacter\n\n\nquoted_description\ncharacter\n\n\nquoted_verified\nlogical\n\n\nretweet_status_id\ncharacter\n\n\nretweet_text\ncharacter\n\n\nretweet_created_at\ndouble\n\n\nretweet_source\ncharacter\n\n\nretweet_favorite_count\ninteger\n\n\nretweet_retweet_count\ninteger\n\n\nretweet_user_id\ncharacter\n\n\nretweet_screen_name\ncharacter\n\n\nretweet_name\ncharacter\n\n\nretweet_followers_count\ninteger\n\n\nretweet_friends_count\ninteger\n\n\nretweet_statuses_count\ninteger\n\n\nretweet_location\ncharacter\n\n\nretweet_description\ncharacter\n\n\nretweet_verified\nlogical\n\n\nplace_url\ncharacter\n\n\nplace_name\ncharacter\n\n\nplace_full_name\ncharacter\n\n\nplace_type\ncharacter\n\n\ncountry\ncharacter\n\n\ncountry_code\ncharacter\n\n\ngeo_coords\nlist\n\n\ncoords_coords\nlist\n\n\nbbox_coords\nlist\n\n\nstatus_url\ncharacter\n\n\nname\ncharacter\n\n\nlocation\ncharacter\n\n\ndescription\ncharacter\n\n\nurl\ncharacter\n\n\nprotected\nlogical\n\n\nfollowers_count\ninteger\n\n\nfriends_count\ninteger\n\n\nlisted_count\ninteger\n\n\nstatuses_count\ninteger\n\n\nfavourites_count\ninteger\n\n\naccount_created_at\ndouble\n\n\nverified\nlogical\n\n\nprofile_url\ncharacter\n\n\nprofile_expanded_url\ncharacter\n\n\naccount_lang\ncharacter\n\n\nprofile_banner_url\ncharacter\n\n\nprofile_background_url\ncharacter\n\n\nprofile_image_url\ncharacter"
  },
  {
    "objectID": "data/2019/2019-01-01/readme.html#datasets",
    "href": "data/2019/2019-01-01/readme.html#datasets",
    "title": "#rstats and #TidyTuesday Tweets from rtweet",
    "section": "",
    "text": "#rstats\n#TidyTuesday\nJust a heads up, there are A LOT of columns (88!) in this collection - feel free to select whichever are useful for your analysis or interest! Both datasets have the same column types which can be seen below.\n\n\n\nvariable\nclass\n\n\n\n\nuser_id\ncharacter\n\n\nstatus_id\ncharacter\n\n\ncreated_at\ndouble\n\n\nscreen_name\ncharacter\n\n\ntext\ncharacter\n\n\nsource\ncharacter\n\n\ndisplay_text_width\ndouble\n\n\nreply_to_status_id\ncharacter\n\n\nreply_to_user_id\ncharacter\n\n\nreply_to_screen_name\ncharacter\n\n\nis_quote\nlogical\n\n\nis_retweet\nlogical\n\n\nfavorite_count\ninteger\n\n\nretweet_count\ninteger\n\n\nhashtags\nlist\n\n\nsymbols\nlist\n\n\nurls_url\nlist\n\n\nurls_t.co\nlist\n\n\nurls_expanded_url\nlist\n\n\nmedia_url\nlist\n\n\nmedia_t.co\nlist\n\n\nmedia_expanded_url\nlist\n\n\nmedia_type\nlist\n\n\next_media_url\nlist\n\n\next_media_t.co\nlist\n\n\next_media_expanded_url\nlist\n\n\next_media_type\ncharacter\n\n\nmentions_user_id\nlist\n\n\nmentions_screen_name\nlist\n\n\nlang\ncharacter\n\n\nquoted_status_id\ncharacter\n\n\nquoted_text\ncharacter\n\n\nquoted_created_at\ndouble\n\n\nquoted_source\ncharacter\n\n\nquoted_favorite_count\ninteger\n\n\nquoted_retweet_count\ninteger\n\n\nquoted_user_id\ncharacter\n\n\nquoted_screen_name\ncharacter\n\n\nquoted_name\ncharacter\n\n\nquoted_followers_count\ninteger\n\n\nquoted_friends_count\ninteger\n\n\nquoted_statuses_count\ninteger\n\n\nquoted_location\ncharacter\n\n\nquoted_description\ncharacter\n\n\nquoted_verified\nlogical\n\n\nretweet_status_id\ncharacter\n\n\nretweet_text\ncharacter\n\n\nretweet_created_at\ndouble\n\n\nretweet_source\ncharacter\n\n\nretweet_favorite_count\ninteger\n\n\nretweet_retweet_count\ninteger\n\n\nretweet_user_id\ncharacter\n\n\nretweet_screen_name\ncharacter\n\n\nretweet_name\ncharacter\n\n\nretweet_followers_count\ninteger\n\n\nretweet_friends_count\ninteger\n\n\nretweet_statuses_count\ninteger\n\n\nretweet_location\ncharacter\n\n\nretweet_description\ncharacter\n\n\nretweet_verified\nlogical\n\n\nplace_url\ncharacter\n\n\nplace_name\ncharacter\n\n\nplace_full_name\ncharacter\n\n\nplace_type\ncharacter\n\n\ncountry\ncharacter\n\n\ncountry_code\ncharacter\n\n\ngeo_coords\nlist\n\n\ncoords_coords\nlist\n\n\nbbox_coords\nlist\n\n\nstatus_url\ncharacter\n\n\nname\ncharacter\n\n\nlocation\ncharacter\n\n\ndescription\ncharacter\n\n\nurl\ncharacter\n\n\nprotected\nlogical\n\n\nfollowers_count\ninteger\n\n\nfriends_count\ninteger\n\n\nlisted_count\ninteger\n\n\nstatuses_count\ninteger\n\n\nfavourites_count\ninteger\n\n\naccount_created_at\ndouble\n\n\nverified\nlogical\n\n\nprofile_url\ncharacter\n\n\nprofile_expanded_url\ncharacter\n\n\naccount_lang\ncharacter\n\n\nprofile_banner_url\ncharacter\n\n\nprofile_background_url\ncharacter\n\n\nprofile_image_url\ncharacter"
  },
  {
    "objectID": "data/2019/2019-01-15/readme.html",
    "href": "data/2019/2019-01-15/readme.html",
    "title": "Data Info",
    "section": "",
    "text": "Data comes from The Economist GitHub. The following information was taken directly from their GitHub readme."
  },
  {
    "objectID": "data/2019/2019-01-15/readme.html#data-files",
    "href": "data/2019/2019-01-15/readme.html#data-files",
    "title": "Data Info",
    "section": "Data files",
    "text": "Data files\n\n\n\nFile\nContents\nSource\n\n\n\n\nagencies\nSpace launch providers\nJonathan McDowell; The Economist\n\n\nlaunches\nIndividual space launches\nJonathan McDowell; The Economist"
  },
  {
    "objectID": "data/2019/2019-01-15/readme.html#codebook",
    "href": "data/2019/2019-01-15/readme.html#codebook",
    "title": "Data Info",
    "section": "Codebook",
    "text": "Codebook\n\nlaunches\n\n\n\nvariable\ndefinition\n\n\n\n\ntag\nHarvard or COSPAR id of launch\n\n\nJD\nJulian Date of launch\n\n\nlaunch_date\ndate of launch\n\n\nlaunch_year\nyear of launch\n\n\ntype\ntype of launch vehicle\n\n\nvariant\nvariant of launch vehicle\n\n\nmission\n\n\n\nagency\nlaunching agency\n\n\nstate_code\nlaunching agency’s state\n\n\ncategory\nsuccess (O) or failure (F)\n\n\nagency_type\ntype of agency\n\n\n\n\n\nagencies\n\n\n\nvariable\ndefinition\n\n\n\n\nagency\norg phase code\n\n\ncount\nnumber of launches\n\n\nucode\norg Ucode\n\n\nstate_code\nresponsible state\n\n\ntype\ntype of org\n\n\nclass\nclass of org\n\n\ntstart\norg/phase founding date\n\n\ntstop\norg/phase ending date\n\n\nshort_name\nshort name\n\n\nname\nfull name\n\n\nlocation\nplain english location\n\n\nlongitude\n\n\n\nlatitude\n\n\n\nerror\nuncertainty in long/lat\n\n\nparent\nparent org\n\n\nshort_english_name\nenglish short name\n\n\nenglish_name\nenglish full name\n\n\nunicode_name\nunicode full name\n\n\nagency_type\ntype of agency"
  },
  {
    "objectID": "data/2019/2019-01-29/readme.html",
    "href": "data/2019/2019-01-29/readme.html",
    "title": "Datasets",
    "section": "",
    "text": "milkcow_facts.csv\nmilk_facts.csv\nmilk_products_facts.csv\nclean_cheese.csv\nstate_milk_production.csv\nData this week comes from the USDA (United States Department of Agriculture)! The raw datasets (Excel Sheets) can be found here. There is even more data at the source if you are interested, much of which requires a LOT of fun Excel sheet data munging. Enjoy!\nA related NPR article - “Nobody is Moving our Cheese” and Washington Post article - “America’s Cheese Stockpile hit an alltime high””.\nThis week’s data was found via the Data is Plural newsletter."
  },
  {
    "objectID": "data/2019/2019-01-29/readme.html#milkcow_facts",
    "href": "data/2019/2019-01-29/readme.html#milkcow_facts",
    "title": "Datasets",
    "section": "milkcow_facts",
    "text": "milkcow_facts\n\n\n\n\n\n\n\n\nvariable\nclass\nDescription\n\n\n\n\nyear\ndate\nYear\n\n\navg_milk_cow_number\ndouble\nAverage number of milk cows\n\n\nmilk_per_cow\ndouble\nAverage milk production/cow in pounds\n\n\nmilk_production_lbs\ndouble\nTotal Milk production in pounds\n\n\navg_price_milk\ndouble\nAverage price paid for milk (dollars per pound)\n\n\ndairy_ration\ndouble\nAverage price paid for dairy cow rations (dollars per pound)\n\n\nmilk_feed_price_ratio\ndouble\nRatio of average price of milk per dairy cow ration\n\n\nmilk_cow_cost_per_animal\ndouble\nAverage cost of milk cow per animal (dollars)\n\n\nmilk_volume_to_buy_cow_in_lbs\ndouble\nMilk volume required to purchase a cow (pounds)\n\n\nalfalfa_hay_price\ndouble\nAlfalfa hay price received by farmers (tons)\n\n\nslaughter_cow_price\ndouble\nSlaughter cow price (value of meat in dollars per pound)"
  },
  {
    "objectID": "data/2019/2019-01-29/readme.html#fluid_milk_sales",
    "href": "data/2019/2019-01-29/readme.html#fluid_milk_sales",
    "title": "Datasets",
    "section": "fluid_milk_sales",
    "text": "fluid_milk_sales\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndate\nYear\n\n\nmilk_type\ninteger\nCategory of Milk product\n\n\npounds\ndouble\nPounds of milk product per year"
  },
  {
    "objectID": "data/2019/2019-01-29/readme.html#milk_products_facts",
    "href": "data/2019/2019-01-29/readme.html#milk_products_facts",
    "title": "Datasets",
    "section": "milk_products_facts",
    "text": "milk_products_facts\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndate\nYear\n\n\nfluid_milk\ndouble\nAverage milk consumption in lbs per person\n\n\nfluid_yogurt\ndouble\nAverage yogurt consumption in lbs per person\n\n\nbutter\ndouble\nAverage butter consumption in lbs per person\n\n\ncheese_american\ndouble\nAverage American cheese consumption in lbs per person\n\n\ncheese_other\ndouble\nAverage other cheese consumption in lbs per person\n\n\ncheese_cottage\ndouble\nAverage cottage cheese consumption in lbs per person\n\n\nevap_cnd_canned_whole_milk\ndouble\nAverage evaporated and canned whole milk consumption in lbs per person\n\n\nevap_cnd_bulk_whole_milk\ndouble\nAverage evaporated and canned bulk whole milk consumption in lbs per person\n\n\nevap_cnd_bulk_and_can_skim_milk\ndouble\nAverage evaporated and canned bulk and can skim milk consumption in lbs per person\n\n\nfrozen_ice_cream_regular\ndouble\nAverage regular frozen ice cream consumption in lbs per person\n\n\nfrozen_ice_cream_reduced_fat\ndouble\nAverage reducated fat frozen ice cream consumption in lbs per person\n\n\nfrozen_sherbet\ndouble\nAverage frozen sherbet consumption in lbs per person\n\n\nfrozen_other\ndouble\nAverage other frozen milk product consumption in lbs per person\n\n\ndry_whole_milk\ndouble\nAverage dry whole milk consumption in lbs per person\n\n\ndry_nonfat_milk\ndouble\nAverage dry nonfat milk consumption in lbs per person\n\n\ndry_buttermilk\ndouble\nAverage dry buttermilk consumption in lbs per person\n\n\ndry_whey\ndouble\nAverage dry whey (milk protein) consumption in lbs per person"
  },
  {
    "objectID": "data/2019/2019-01-29/readme.html#clean_cheese",
    "href": "data/2019/2019-01-29/readme.html#clean_cheese",
    "title": "Datasets",
    "section": "clean_cheese",
    "text": "clean_cheese\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndate\nYear\n\n\nCheddar\ndouble\nCheddar consumption in lbs per person\n\n\nAmerican Other\ndouble\nAmerican Other consumption in lbs per person\n\n\nMozzarella\ndouble\nMozzarella consumption in lbs per person\n\n\nItalian other\ndouble\nItalian other consumption in lbs per person\n\n\nSwiss\ndouble\nSwiss consumption in lbs per person\n\n\nBrick\ndouble\nBrick consumption in lbs per person\n\n\nMuenster\ndouble\nMuenster consumption in lbs per person\n\n\nCream and Neufchatel\ndouble\nCream and Neufchatel consumption in lbs per person\n\n\nBlue\ndouble\nBlue consumption in lbs per person\n\n\nOther Dairy Cheese\ndouble\nOther Dairy Cheese consumption in lbs per person\n\n\nProcessed Cheese\ndouble\nProcessed Cheese consumption in lbs per person\n\n\nFoods and spreads\ndouble\nFoods and spreads consumption in lbs per person\n\n\nTotal American Chese\ndouble\nTotal American Chese consumption in lbs per person\n\n\nTotal Italian Cheese\ndouble\nTotal Italian Cheese consumption in lbs per person\n\n\nTotal Natural Cheese\ndouble\nTotal Natural Cheese consumption in lbs per person\n\n\nTotal Processed Cheese Products\ndouble\nTotal Processed Cheese Products consumption in lbs per person"
  },
  {
    "objectID": "data/2019/2019-01-29/readme.html#state_milk_production",
    "href": "data/2019/2019-01-29/readme.html#state_milk_production",
    "title": "Datasets",
    "section": "state_milk_production",
    "text": "state_milk_production\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nregion\ncharacter\nRegion of the US\n\n\nstate\ncharacter\nUS State\n\n\nyear\ndate\nYear\n\n\nmilk_produced\ndouble\nPounds of Milk Produced"
  },
  {
    "objectID": "data/2019/2019-02-12/readme.html",
    "href": "data/2019/2019-02-12/readme.html",
    "title": "Federal Research and Development Spending by Agency",
    "section": "",
    "text": "Federal Research and Development Spending by Agency\nThe NY Times has a recent article on federal Research and Development spending by agency. Cheers to Costa Samaras for making us aware of this data source and the article. Their take on the data is that Energy spending and Climate Change Research spending is lagging compared to other departments.\nData comes directly from the American Association for the Advancement of Science Historical Trends - there are dozens if not hundreds of ways to break this data down - we have presented a relatively high-level overview, but if you are interested take a look at the raw Excel sheets to get at even more data.\nEVEN MORE DATA\n\nGrab the Data Here\nFederal Research and Development Spending by Agency\nor read the data directly into R!\nfed_rd &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-12/fed_r_d_spending.csv\")\nenergy_spend &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-12/energy_spending.csv\")\nclimate_spend &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-12/climate_spending.csv\")\n\n\n\nData Dictionary\nFor reference: * DOD - Deparment of Defense * NASA - National Aeronautics and Space Administration * DOE - Department of Energy * HHS - Department of Health and Human Services * NIH - National Institute of Health * NSF - National Science Foundation * USDA - US Department of Agriculture * Interior - Department of Interior * DOT - Deparment of Transportation * EPA - Environmental Protection Agency * DOC - Department of Corrections * DHS - Department of Homeland Security * VA - Department of Veterands Affairs * Other - other research and development spending\n\nTotal Federal R&D Spending by agency/deparment\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndepartment\ncharacter\nUS agency/department\n\n\nyear\ndate/integer\nFiscal Year\n\n\nrd_budget\ndouble\nResearch and Development Dollars in inflation-adjusted (constant) dollars\n\n\ntotal_outlays\ndouble\nTotal Federal Government spending in inflation-adjusted (constant) dollars\n\n\ndiscretionary_outlays\ndouble\nTotal Discretionary Federal Government spending in inflation-adjusted (constant) dollars\n\n\ngdp\ndouble\nTotal US Gross Domestic Product in inflation-adjusted (constant) dollars\n\n\n\n\nEnergy Departments Data\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndepartment\ncharacter\nSub-agency of Energy Spending\n\n\nyear\ndate/integer\nFiscal Year\n\n\nenergy_spending\ndouble\nResearch and Development Dollars in inflation-adjusted (constant) dollars\n\n\n\n\n\nGlobal Climate Change Research Program Spending\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndepartment\ncharacter\nSub-agency of Global Climate Change Spending\n\n\nyear\ndate/integer\nFiscal Year\n\n\ngcc_spending\ndouble\nResearch and Development Dollars in inflation-adjusted (constant) dollars\n\n\n\n\n\n\n\n(SPOILERS) How I cleaned data\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Read in the by-agency spending data\nrd_raw &lt;- read_excel(here::here(\"2019\", \"2019-02-12\", \"Agencies%3B.xlsx\"), skip = 2)\n\nrd_clean &lt;- rd_raw %&gt;% \n  slice(1:14) %&gt;% \n  mutate(department = str_extract(`Fiscal Year`, \"[:alpha:]+\")) %&gt;% \n  select(department, everything(), -`Fiscal Year`) %&gt;% \n  gather(year, rd_budget, `1976`:`2018**`) %&gt;% \n  select(-`'08-'18`) %&gt;% \n  mutate(year = str_extract(year, \"[:digit:]+\"),\n         year = as.integer(year),\n         rd_budget = rd_budget * 1*10^6)\n\n# read in the spending as % of total federal budget\nrd_fed_budget_raw &lt;- read_excel(here::here(\"2019\", \"2019-02-12\", \"Budget%3B.xlsx\"), skip = 3)\n\nrd_fed_budget_clean &lt;- rd_fed_budget_raw %&gt;% \n  select(1:3) %&gt;% \n  set_names(nm = c(\"year\", \"total_outlays\", \"discretionary_outlays\")) %&gt;% \n  mutate(year = str_extract(year, \"[:digit:]+\")) %&gt;% \n  mutate_at(vars(2, 3), as.numeric) %&gt;% \n  mutate(year = as.integer(year)) %&gt;% \n  na.omit() %&gt;% \n  filter(year &gt;= 1976 & year &lt;= 2018) %&gt;% \n  mutate(total_outlays = total_outlays * 1*10^9,\n         discretionary_outlays = discretionary_outlays * 1*10^9)\n\n# read in the spending as % of GDP\nrd_gdp_raw &lt;- read_excel(here::here(\"2019\", \"2019-02-12\", \"RDGDP%3B.xlsx\"), skip = 3)\n\nrd_gdp_clean &lt;- rd_gdp_raw %&gt;% \n  slice(9) %&gt;% \n  gather(year, gdp, `1976`:`2017**`) %&gt;% \n  select(year, gdp) %&gt;% \n  mutate(year = str_extract(year, \"[:digit:]+\"),\n         year = as.integer(year),\n         gdp = gdp * 1*10^9)\n\n# combine by agency spending, federal budget and gdp data into one dataframe\nfinal_df &lt;- left_join(rd_clean, rd_fed_budget_clean, by = \"year\") %&gt;% \n  left_join(., rd_gdp_clean, by = \"year\") %&gt;% \n  filter(year &lt;= 2017)\n\n# write to csv\nfinal_df %&gt;% \n  write_csv(\"fed_r_d_spending.csv\")\n\nEnergy Data\nenergy_raw &lt;- read_excel(here::here(\"2019\", \"2019-02-12\", \"DOE%3B.xlsx\"), skip = 2)\n\nenergy_clean &lt;- energy_raw %&gt;% \n  slice(1:15) %&gt;% \n  na.omit() %&gt;% \n  gather(year, energy_spending, `1997`:`2018**`) %&gt;% \n  select(department = `Fiscal Years`, year, energy_spending) %&gt;% \n  mutate(year = str_extract(year, \"[:digit:]+\"),\n         year = as.integer(year),\n         energy_spending = as.numeric(energy_spending),\n         energy_spending = energy_spending * 1*10^6)\n\ngcc_raw &lt;- read_excel(here::here(\"2019\", \"2019-02-12\", \"USGCRP.xlsx\"), skip = 2)\n\ngcc_clean &lt;- gcc_raw %&gt;% \n  slice(2:8) %&gt;% \n  gather(year, gcc_spending, `FY 2000`:`FY 2017`) %&gt;% \n  mutate(year = str_extract(year, \"[:digit:]+\")) %&gt;% \n  select(department = X__1, year, gcc_spending) %&gt;% \n  mutate(year = as.integer(year),\n         gcc_spending = as.numeric(gcc_spending),\n         gcc_spending = gcc_spending * 1*10^6)"
  },
  {
    "objectID": "data/2019/2019-02-26/readme.html",
    "href": "data/2019/2019-02-26/readme.html",
    "title": "French Train stats",
    "section": "",
    "text": "French Train stats\nWe’ve focused a lot on US data over the past few weeks, so many thanks to Mathilda for helping curate this week’s French trains dataset! She posted an awesome visualization of this data - if you are interested in a heat-map style geom_tile(), take a look at her post https://twitter.com/noccaea/status/1095735292206739456.\nThe data comes from the SNCF open data portal - there are additional datasets there you can download, but fair to say most things are in French! :smile:\nThe SNCF (National Society of French Railways) is France’s national state-owned railway company. Founded in 1938, it operates the country’s national rail traffic along with Monaco, including the TGV, France’s high-speed rail network. This dataset covers 2015-2018 with a lot of different train stations. The dataset primarily covers aggregate trip times, delay times, cause for delay, etc for each station - lots of different ways to approach the full_trains.csv dataset with it’s 27 columns!\nIf you are interested in a shorter data journey, check out the small_trains.csv dataset - it is only 13 columns and has a gather() already performed.\nLastly, if for some reason you’d like to see the raw untranslated dataset it is also available here.\n\nGrab the raw data here\nfull_trains &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-26/full_trains.csv\")\nsmall_trains &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-02-26/small_trains.csv\") \n\n\n\nData Dictionary\nsmall_trains.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger/date\nYear of Observation\n\n\nmonth\ndouble/date\nMonth of Observation\n\n\nservice\nfactor\nType of train service (National, Internation, NA)\n\n\ndeparture_station\ncharacter\nDeparture Station (name)\n\n\narrival_station\ncharacter\nArrival Station (name)\n\n\njourney_time_avg\ndouble\nAverage Journey time (minutes)\n\n\ntotal_num_trips\ndouble\nTotal number of trains in the time period\n\n\navg_delay_all_departing\ndouble\nThe average delay (minutes) for all departing trains\n\n\navg_delay_all_arriving\ndouble\nAverage delay (minutes) for all arriving trains\n\n\nnum_late_at_departure\ndouble\nNumber of trains that were late at departure\n\n\nnum_arriving_late\ndouble\nNumber of trains arriving late\n\n\ndelay_cause\ncharacter\nCause for delay\n\n\ndelayed_number\ndouble\nPercent of trains delayed\n\n\n\n\n\n\nfull_trains.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger/date\nYear of Observation\n\n\nmonth\ndouble/date\nMonth of Observation\n\n\nservice\nfactor\nType of train service (National, Internation, NA)\n\n\ndeparture_station\ncharacter\nDeparture Station (name)\n\n\narrival_station\ncharacter\nArrival Station (name)\n\n\njourney_time_avg\ndouble\nAverage Journey time (minutes)\n\n\ntotal_num_trips\ndouble\nTotal number of trains in the time period\n\n\nnum_of_canceled_trains\ndouble\nNumber of canceled trains\n\n\ncomment_cancellations\ncharacter\nComment for Cancellations\n\n\nnum_late_at_departure\ndouble\nNumber of trains that were late at departure\n\n\navg_delay_late_at_departure\ndouble\nThe average delay (minutes) for trains late at departure\n\n\navg_delay_all_departing\ndouble\nThe average delay (minutes) for all departing trains\n\n\ncomment_delays_at_departure\ncharacter\nComment for trains delayed at departure\n\n\nnum_arriving_late\ndouble\nNumber of trains arriving late\n\n\navg_delay_late_on_arrival\ndouble\nAverage delay (minutes) for trains that were late on arrival\n\n\navg_delay_all_arriving\ndouble\nAverage delay (minutes) for all arriving trains\n\n\ncomment_delays_on_arrival\ncharacter\nComment for delayes on arrival\n\n\ndelay_cause_external_cause\ndouble\nCause for delay (%) - External Cause\n\n\ndelay_cause_rail_infrastructure\ndouble\nCause for delay (%) - Rail infrastructure\n\n\ndelay_cause_traffic_management\ndouble\nCause for delay (%) - Traffic management\n\n\ndelay_cause_rolling_stock\ndouble\nCause for delay (%) - Rolling stock\n\n\ndelay_cause_station_management\ndouble\nCause for delay (%) - Station management\n\n\ndelay_cause_travelers\ndouble\nCause for delay (%) - Travelers\n\n\nnum_greater_15_min_late\ndouble\nNumber of trains greater than 15 min late\n\n\navg_delay_late_greater_15_min\ndouble\nAverage delay of trains that were late more than 15 min\n\n\nnum_greater_30_min_late\ndouble\nNumber of trains greater than 30 min late\n\n\nnum_greater_60_min_late\ndouble\nNumber of trains greater than 60 min late\n\n\n\n\n\nSpoilers - Cleaning Script\nlibrary(tidyverse)\n\n# translated col names\nenglish_names &lt;- c(\n  \"year\", \"month\", \"service\", \"departure_station\", \"arrival_station\", \"journey_time_avg\",\n  \"total_num_trips\", \"num_of_canceled_trains\", \"comment_cancellations\", \n  \"num_late_at_departure\", \"avg_delay_late_at_departure\", \n  \"avg_delay_all_departing\", \"comment_delays_at_departure\", \"num_arriving_late\", \n  \"avg_delay_late_on_arrival\", \"avg_delay_all_arriving\", \n  \"comment_delays_on_arrival\", \"delay_cause_external_cause\", \"delay_cause_rail_infrastructure\", \n  \"delay_cause_traffic_management\", \"delay_cause_rolling_stock\",\n  \"delay_cause_station_management\", \"delay_cause_travelers\", \"num_greater_15_min_late\",\n  \"avg_delay_late_greater_15_min\", \"num_greater_30_min_late\", \n  \"num_greater_60_min_late\", \"period\", \"delay_for_external_cause\", \n  \"delay_for_railway_infrastructure\", \"delay_for_traffic_managment\", \"delay_for_rolling_stock\",\n  \"delay_for_station_management\", \"delay_for_passengers\"\n)\n\n# read in the dataset\ndf &lt;- read_delim(here::here(\"2019\", \"2019-02-19\", \"regularite-mensuelle-tgv-aqst.csv\"), \n                 delim = \";\") %&gt;% \n  set_names(nm = english_names)\n\n# select columns of interest and create dictionary\ntrains_df &lt;- df %&gt;% \n  select(year:num_greater_60_min_late)\n\ntomtom::create_dictionary(trains_df)\n\n# write to csv\ntrains_df %&gt;% \n  write_csv(\"full_trains.csv\")\n# gather cause for delay and create a more focused dataset\nsmall_df &lt;- df %&gt;% \n  gather(delay_cause, delayed_number, delay_cause_external_cause:delay_cause_travelers) %&gt;% \n  select(year:total_num_trips, avg_delay_all_departing, avg_delay_all_arriving, num_late_at_departure, num_arriving_late, delay_cause, delayed_number) \n\n# create data dictionary\nsmall_df %&gt;% \n  tomtom::create_dictionary()\n\n# write to csv\nsmall_df %&gt;% \n  write_csv(\"small_trains.csv\")"
  },
  {
    "objectID": "data/2019/2019-03-12/readme.html",
    "href": "data/2019/2019-03-12/readme.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "Board Games Database\nThis week’s data comes from the Board Game Geek database. The site’s database has more than 90,000 games, with crowd-sourced ratings. There is also an R package with the fulldataset (bggAnalysis) but it hasn’t been updated in ~2 years.\nTo follow along with a fivethirtyeight article, I limited to only games with at least 50 ratings and for games between 1950 and 2016. This still leaves us with 10,532 games!\nboard_games &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-03-12/board_games.csv\")\n\n\nData Dictionary\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngame_id\ncharacter\nUnique game identifier\n\n\ndescription\ncharacter\nA paragraph of text describing the game\n\n\nimage\ncharacter\nURL image of the game\n\n\nmax_players\ninteger\nMaximum recommended players\n\n\nmax_playtime\ninteger\nMaximum recommended playtime (min)\n\n\nmin_age\ninteger\nMinimum recommended age\n\n\nmin_players\ninteger\nMinimum recommended players\n\n\nmin_playtime\ninteger\nMinimum recommended playtime (min)\n\n\nname\ncharacter\nName of the game\n\n\nplaying_time\ninteger\nAverage playtime\n\n\nthumbnail\ncharacter\nURL thumbnail of the game\n\n\nyear_published\ninteger\nYear game was published\n\n\nartist\ncharacter\nArtist for game art\n\n\ncategory\ncharacter\nCategories for the game (separated by commas)\n\n\ncompilation\ncharacter\nIf part of a multi-compilation - name of compilation\n\n\ndesigner\ncharacter\nGame designer\n\n\nexpansion\ncharacter\nIf there is an expansion pack - name of expansion\n\n\nfamily\ncharacter\nFamily of game - equivalent to a publisher\n\n\nmechanic\ncharacter\nGame mechanic - how game is played, separated by comma\n\n\npublisher\ncharacter\nComoany/person who published the game, separated by comma\n\n\naverage_rating\ndouble\nAverage rating on Board Games Geek (1-10)\n\n\nusers_rated\ndouble\nNumber of users that rated the game\n\n\n\n\n\nData Cleaning\n```{r] library(bggAnalysis) library(tidyverse)\nnamed_games &lt;- BoardGames %&gt;% janitor::clean_names() %&gt;% set_names(~str_replace(.x, “details_”, ““)) %&gt;% set_names(~str_replace(.x,”attributes_boardgame”, ““)) %&gt;% set_names(~str_replace(.x,”stats_“,”“)) %&gt;% select(game_id:average, usersrated, averageweight) %&gt;% filter(!is.na(yearpublished)) %&gt;% filter(yearpublished &lt;=2016 & yearpublished &gt;= 1950) %&gt;% filter(usersrated &gt;= 50, game_type ==”boardgame”)\nnamed_games %&gt;% group_by(yearpublished) %&gt;% summarize(count = n()) %&gt;% ggplot(aes(x = yearpublished, y = count)) + geom_line()\ntidy_names &lt;- c(“game_id”, “game_type”, “description”, “image”, “max_players”, “max_playtime”, “min_age”, “min_players”, “min_playtime”, “name”, “playing_time”, “thumbnail”, “year_published”, “artist”, “category”, “compilation”, “designer”, “expansion”, “family”, “implementation”, “integration”, “mechanic”, “publisher”, “attributes_total”, “average_rating”, “users_rated”, “average_weight”)\ntidy_games &lt;- named_games %&gt;% set_names(nm = tidy_names) %&gt;% select(-attributes_total, -game_type, - implementation, -integration, - average_weight)\ntidy_games %&gt;% tomtom::create_dictionary()\ntidy_games %&gt;% write_csv(“board_games.csv”)\n```"
  },
  {
    "objectID": "data/2019/2019-03-26/readme.html",
    "href": "data/2019/2019-03-26/readme.html",
    "title": "Seattle Pet Names",
    "section": "",
    "text": "Seattle’s open data portal has a dataset of registered pets here. While they don’t include the sex or age of the animal, they were kind enough to leave in the license issue date, animal’s name, species, breed, and zip code. This should open up some fun explorations!\nh/t to Jacqueline Nolis for sharing this data!\nA few articles examined the most popular pet names in 2018, one from Seattle specifically, and another from Australia.\n\n\nseattle_pets &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-03-26/seattle_pets.csv\")\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlicense_issue_date\ndate\nDate the animal was registered with Seattle\n\n\nlicense_number\nnumeric\nUnique license number\n\n\nanimals_name\ncharacter\nAnimal’s name\n\n\nspecies\ncharacter\nAnimal’s species (dog, cat, goat, etc)\n\n\nprimary_breed\ncharacter\nPrimary breed of the animal\n\n\nsecondary_breed\ncharacter\nSecondary breed if mixed\n\n\nzip_code\nnumeric\nZip code animal registered under"
  },
  {
    "objectID": "data/2019/2019-03-26/readme.html#get-the-data",
    "href": "data/2019/2019-03-26/readme.html#get-the-data",
    "title": "Seattle Pet Names",
    "section": "",
    "text": "seattle_pets &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-03-26/seattle_pets.csv\")\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlicense_issue_date\ndate\nDate the animal was registered with Seattle\n\n\nlicense_number\nnumeric\nUnique license number\n\n\nanimals_name\ncharacter\nAnimal’s name\n\n\nspecies\ncharacter\nAnimal’s species (dog, cat, goat, etc)\n\n\nprimary_breed\ncharacter\nPrimary breed of the animal\n\n\nsecondary_breed\ncharacter\nSecondary breed if mixed\n\n\nzip_code\nnumeric\nZip code animal registered under"
  },
  {
    "objectID": "data/2019/2019-04-02/readme.html",
    "href": "data/2019/2019-04-02/readme.html",
    "title": "Seattle Bike Counters",
    "section": "",
    "text": "Seattle Bike Counters\n“Seattle Department of Transportation has 12 bike counters (four of which also count pedestrians) located on neighborhood greenways, multi-use trails, at the Fremont Bridge and on SW Spokane Street. The counters are helping us create a ridership baseline in 2014 that can be used to assess future years and make sure our investments are helping us to reach our goal of quadrupling ridership by 2030. Read our Bicycle Master Plan to learn more about what Seattle is doing to create a citywide bicycle network.”\nThe Seattle Times recently covered What we can learn from Seattle’s bike-counter data. They have some elegant data-visualizations there!\n\n\nGet the Data!\nbike_traffic &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-02/bike_traffic.csv\")\n\nData Dictionary\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate (mdy hms am/pm)\nDate of data upload\n\n\ncrossing\ncharacter\nThe Street crossing/intersection\n\n\ndirection\ncharacter\nNorth/South/East/West - varies by crossing\n\n\nbike_count\ndouble\nNumber of bikes counted for each hour window\n\n\nped_count\ndouble\nNumber of pedestrians counted for each hour window"
  },
  {
    "objectID": "data/2019/2019-04-16/readme.html",
    "href": "data/2019/2019-04-16/readme.html",
    "title": "Economist’s “Mistakes, we’ve drawn a few”",
    "section": "",
    "text": "Sarah Leo from The Economist went through the Economist’s archives and found 7 examples of charts that were in need of improvement.\n“I grouped our crimes against data visualisation into three categories: charts that are (1) misleading, (2) confusing and (3) failing to make a point. For each, I suggest an improved version that requires a similar amount of space — an important consideration when drawing charts to be published in print.”\nShe was nice enough to include the raw data as .csv files, where I have included both the raw and tidied formats for your graphing fun!\n\n\nbrexit &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/brexit.csv\")\n\ncorbyn &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/corbyn.csv\")\n\ndogs &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/dogs.csv\")\n\neu_balance &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/eu_balance.csv\")\n\npensions &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/pensions.csv\")\n\ntrade &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/trade.csv\")\n\nwomen_research &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/women_research.csv\")"
  },
  {
    "objectID": "data/2019/2019-04-16/readme.html#get-the-data",
    "href": "data/2019/2019-04-16/readme.html#get-the-data",
    "title": "Economist’s “Mistakes, we’ve drawn a few”",
    "section": "",
    "text": "brexit &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/brexit.csv\")\n\ncorbyn &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/corbyn.csv\")\n\ndogs &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/dogs.csv\")\n\neu_balance &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/eu_balance.csv\")\n\npensions &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/pensions.csv\")\n\ntrade &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/trade.csv\")\n\nwomen_research &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-04-16/women_research.csv\")"
  },
  {
    "objectID": "data/2019/2019-04-30/raw/readme.html",
    "href": "data/2019/2019-04-30/raw/readme.html",
    "title": "Raw Data",
    "section": "",
    "text": "Raw Data\nRaw data from the paper, includes additional bird data from the supplemental data."
  },
  {
    "objectID": "data/2019/2019-05-07/readme.html",
    "href": "data/2019/2019-05-07/readme.html",
    "title": "Global Student to Teacher Ratios",
    "section": "",
    "text": "Global Student to Teacher Ratios\n“The UNESCO Institute of Statistics collects country-level data on the number of teachers, teacher-to-student ratios, and related figures. You can download the data or explore it in UNESCO’s eAtlas of Teachers or their interactive visualization of teacher supply in Asia”\nh/t to Data is Plural 2019/04/03\nThere is even more education data at the country level available at UNESCO Institute of Statistics.\n“Reducing class size to increase student achievement is an approach that has been tried, debated, and analyzed for several decades. The premise seems logical: with fewer students to teach, teachers can coax better performance from each of them. But what does the research show?Some researchers have not found a connection between smaller classes and higher student achievement, but most of the research shows that when class size reduction programs are well-designed and implemented in the primary grades (K-3), student achievement rises as class size drops.” &gt; Center for Public Education\n\n\nGet the data!\nstudent_ratio &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-05-07/student_teacher_ratio.csv\")\n\n\nData Dictionary\n\nstudent_teacher_ratio.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nedulit_ind\ncharacter\nUnique ID\n\n\nindicator\ncharacter\nEducation level group (“Lower Secondary Education”, “Primary Education”, “Upper Secondary Education”, “Pre-Primary Education”, “Secondary Education”, “Tertiary Education”, “Post-Secondary Non-Tertiary Education”)\n\n\ncountry_code\ncharacter\nCountry code\n\n\ncountry\ncharacter\nCountry Full name\n\n\nyear\ninteger (date)\nYear\n\n\nstudent_ratio\ndouble\nStudent to teacher ratio (lower = fewer students/teacher)\n\n\nflag_codes\ncharacter\nCode to indicate some metadata about exceptions\n\n\nflags\ncharacter\nMetadata about exceptions\n\n\n\n\n\n\nCleaning script\nlibrary(tidyverse)\nlibrary(here)\n\nraw_df &lt;- read_csv(here(\"2019\", \"2019-05-07\", \"EDULIT_DS_06052019101747206.csv\"))\n\nclean_ed &lt;- raw_df %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(indicator = str_remove(indicator, \"Pupil-teacher ratio in\"),\n         indicator = str_remove(indicator, \"(headcount basis)\"),\n         indicator = str_remove(indicator, \"\\\\(\\\\)\"),\n         indicator = str_trim(indicator),\n         indicator = stringr::str_to_title(indicator)) %&gt;% \n  select(-time_2) %&gt;% \n  rename(\"country_code\" = location,\n         \"student_ratio\" = value,\n         \"year\" = time)\n\nclean_ed %&gt;% \n  write_csv(here(\"2019\", \"2019-05-07\", \"student_teacher_ratio.csv\"))"
  },
  {
    "objectID": "data/2019/2019-05-21/readme.html",
    "href": "data/2019/2019-05-21/readme.html",
    "title": "Global Plastic Waste",
    "section": "",
    "text": "Global Plastic Waste\nPlastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at the various levels including globally, per country, and over time.\nAdditionally, National Geographic is running a dataviz communication contest on plastic waste as seen here.\nI intentionally left the datasets “uncleaned” this week as they are already in good shape minus the column names. I would suggest trying the janitor package, where janitor::clean_names() or purrr::set_names() can come in handy to clean up the names quickly!\n\n\nGet the data!\ncoast_vs_waste &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-05-21/coastal-population-vs-mismanaged-plastic.csv\")\n\nmismanaged_vs_gdp &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-05-21/per-capita-mismanaged-plastic-waste-vs-gdp-per-capita.csv\")\n\nwaste_vs_gdp &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-05-21/per-capita-plastic-waste-vs-gdp-per-capita.csv\")\n\n\nData Dictionary\n\ncoast_vs_waste.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\nCharacter\nCountry Name\n\n\nCode\nCharacter\n3 Letter country code\n\n\nYear\nInteger (date)\nYear\n\n\nMismanaged plastic waste (tonnes)\ndouble\nTonnes of mismanaged plastic waste\n\n\nCoastal population\nDouble\nNumber of individuals living on/near coast\n\n\nTotal Population\ndouble\nTotal population according to Gapminder\n\n\n\n\n\nmismanaged_vs_gdp.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\nCharacter\nCountry Name\n\n\nCode\nCharacter\n3 Letter country code\n\n\nYear\nInteger (date)\nYear\n\n\nPer capita mismanaged plastic waste (kg per day)\ndouble\nAmount of mismanaged plastic waste per capita in kg/day\n\n\nGDP per capita\nDouble\nGDP per capita constant 2011 international $, rate\n\n\nTotal Population\ndouble\nTotal population according to Gapminder\n\n\n\n\n\nwaste_vs_gdp.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\nCharacter\nCountry Name\n\n\nCode\nCharacter\n3 Letter country code\n\n\nYear\nInteger (date)\nYear\n\n\nPer capita plastic waste (kg per person per day)\ndouble\nAmount of plastic waste per capita in kg/day\n\n\nGDP per capita\nDouble\nGDP per capita constant 2011 international $, rate\n\n\nTotal Population\ndouble\nTotal population according to Gapminder"
  },
  {
    "objectID": "data/2019/2019-06-04/readme.html",
    "href": "data/2019/2019-06-04/readme.html",
    "title": "Ramen ratings",
    "section": "",
    "text": "Ramen ratings\nThis week’s dataset is a ramen ratings dataset from The Ramen Rater. H/t to Data is Plural.\nIf you want to submit a dataset please do so as an Issue on our GitHub! If you do submit a dataset, please drop a link and some context as an issue. Thanks!\n\n\nGet the data!\nramen_ratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-06-04/ramen_ratings.csv\")\n\n\nData Dictionary\n\nramen_ratings.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nreview_number\ninteger\nRamen review number, increasing from 1\n\n\nbrand\ncharacter\nBrand of the ramen\n\n\nvariety\ncharacter\nThe ramen variety, eg a flavor, style, ingredient\n\n\nstyle\ncharacter\nStyle of container (cup, pack, tray,\n\n\ncountry\ncharacter\nOrigin country of the ramen brand\n\n\nstars\ndouble\n0-5 rating of the ramen, 5 is best, 0 is worst"
  },
  {
    "objectID": "data/2019/2019-06-18/readme.html",
    "href": "data/2019/2019-06-18/readme.html",
    "title": "Christmas Bird Counts",
    "section": "",
    "text": "Christmas Bird Counts\nEvery year around Christmas time, since 1921, birdwatchers in the Hamilton area of Ontario have gone out and counted all the birds they see or hear in a day.\nThey have been carefully recording this data, and the raw data is available through the website of Bird Studies Canada (twitter handle: @BirdsCanada).\nSharleen has been a part of this data collection for the past two years, and decided to do some citizen data science with it! She went through and cleaned this data and made it much more ready for analysis! She detailed her data journey in a 5 part blog series as seen here. Many thanks to her for cleaning, visualizing, and then sharing it!\n\n\nGet the data!\nbird_counts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-06-18/bird_counts.csv\")\n\n\nData Dictionary\n\nbird_counts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nyear\n\n\nspecies\ncharacter\nThe species name in English and the scientific name\n\n\nspecies_latin\ncharacter\nThe species name in latin\n\n\nhow_many_counted\ndouble\nActual raw bird count observed\n\n\ntotal_hours\ndouble\nTotal hours spent observing\n\n\nhow_many_counted_by_hour\ndouble\nHow many birds were counted divided by the number of person-hours that year"
  },
  {
    "objectID": "data/2019/2019-07-02/readme.html",
    "href": "data/2019/2019-07-02/readme.html",
    "title": "Media Franchise Powerhouses",
    "section": "",
    "text": "Media Franchise Powerhouses\nThis data comes from Wikipedia and includes franchises that have grossed at least $4 billion usd. How do different media franchises stack up with their revenue streams?\nI took a stab at cleaning up the data in R directly from the source - I made some opinionated decisions about how to group categories (there were &gt; 60 distinct categories), if you’d like a deeper dive on data cleaning try working with the data purely from the source.\nI have included my script in this repo so you can take a peek at how we got here.\nA popular reddit/dataisbeautiful post examined this data with ggplot2.\n\n\nGet the data!\nmedia_franchises &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-07-02/media_franchises.csv\")\n\n\nData Dictionary\n\nmedia_franchises.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nfranchise\ncharacter\nFranchise name\n\n\nrevenue_category\ncharacter\nRevenue category\n\n\nrevenue\ndouble\nRevenue generated per category (in billions)\n\n\nyear_created\ninteger/date\nYear created\n\n\noriginal_media\ncharacter\nOriginal source of the franchise\n\n\ncreators\ncharacter\nCreators of the franchise\n\n\nowners\ncharacter\nCurrent owners of the franchise"
  },
  {
    "objectID": "data/2019/2019-07-16/readme.html",
    "href": "data/2019/2019-07-16/readme.html",
    "title": "R for Data Science Online Learning Community Stats",
    "section": "",
    "text": "R for Data Science Online Learning Community Stats\nIn honor of the R4DS Online Learning Community team’s recent presentation at useR-2019, here are the stats for the R4DS community from it’s founding through the start of July. No user names or user information is included - it is aggregated by date.\nSee Jesse’s talk about founding this community here.\nJoin us on Slack here or check out our website.\n\n\nGet the data!\nr4ds_members &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-07-16/r4ds_members.csv\")\n\n\n\nData Dictionary\n\nr4ds_members.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\ndate\n\n\ntotal_membership\ndouble\ntotal Members\n\n\nfull_members\ndouble\nFull members\n\n\nguests\ndouble\nGuests\n\n\ndaily_active_members\ndouble\nDaily active members\n\n\ndaily_members_posting_messages\ndouble\nDaily members posting messages\n\n\nweekly_active_members\ndouble\nWeekly active members\n\n\nweekly_members_posting_messages\ndouble\nWeekly members posting messages\n\n\nmessages_in_public_channels\ndouble\nMessages in public channels\n\n\nmessages_in_private_channels\ndouble\nMessages in private channels\n\n\nmessages_in_shared_channels\ndouble\nMessages in shared channels\n\n\nmessages_in_d_ms\ndouble\nmessages in Direct Messages\n\n\npercent_of_messages_public_channels\ndouble\npercent of messages in public channels\n\n\npercent_of_messages_private_channels\ndouble\npercent of messages in private channels\n\n\npercent_of_messages_d_ms\ndouble\npercent of messages in Direct Messages\n\n\npercent_of_views_public_channels\ndouble\nPercent of Views public channels\n\n\npercent_of_views_private_channels\ndouble\nPercent of views private channels\n\n\npercent_of_views_d_ms\ndouble\nPercent of Views DMs\n\n\nname\ndouble\nRedacted\n\n\npublic_channels_single_workspace\ndouble\nPublic channels single workspace\n\n\nmessages_posted\ndouble\nMessages posted"
  },
  {
    "objectID": "data/2019/2019-07-30/readme.html",
    "href": "data/2019/2019-07-30/readme.html",
    "title": "Video Games Dataset",
    "section": "",
    "text": "Video Games Dataset\nThis week’s data comes courtesy of Liza Wood via Steam Spy. She recently published a blog post on her data analysis of this video game data.\nShe was kind enough to provide a fairly clean dataset, and I have done some small additional clean up seen below.\nThere is time played, ownership, release date, publishing information, and for some a metascore! Lots of ways to slice and dice this data!\n\nWarning\nPlease be advised that the average and median playtime is over the last two weeks, as such there are many many games where playtime is low or zero.\n\n\n\nGet the data!\nvideo_games &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-07-30/video_games.csv\")\n\n\n\nData Dictionary\n\nvideo_games.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nnumber\ndouble\nGame number\n\n\ngame\ncharacter\nGame Title\n\n\nrelease_date\ncharacter\nRelease date\n\n\nprice\ndouble\nUS Dollars + Cents\n\n\nowners\ncharacter\nEstimated number of people owning this game.\n\n\ndeveloper\ncharacter\nGroup that developed the game\n\n\npublisher\ncharacter\nGroup that published the game\n\n\naverage_playtime\ndouble\nAverage playtime in minutes\n\n\nmedian_playtime\ndouble\nMedian playtime in minutes\n\n\nmetascore\ndouble\nMetascore rating\n\n\n\nlibrary(tidyverse)\n\n# clean dataset from lizawood's github\nurl &lt;- \"https://raw.githubusercontent.com/lizawood/apps-and-games/master/PC_Games/PCgames_2004_2018_raw.csv\"\n\n# read in raw data\nraw_df &lt;- url %&gt;% \n  read_csv() %&gt;% \n  janitor::clean_names() \n\n# clean up some of the factors and playtime data\nclean_df &lt;- raw_df %&gt;% \n  mutate(price = as.numeric(price),\n         score_rank = word(score_rank_userscore_metascore, 1),\n         average_playtime = word(playtime_median, 1),\n         median_playtime = word(playtime_median, 2),\n         median_playtime = str_remove(median_playtime, \"\\\\(\"),\n         median_playtime = str_remove(median_playtime, \"\\\\)\"),\n         average_playtime = 60 * as.numeric(str_sub(average_playtime, 1, 2)) +\n           as.numeric(str_sub(average_playtime, 4, 5)),\n         median_playtime = 60 * as.numeric(str_sub(median_playtime, 1, 2)) +\n           as.numeric(str_sub(median_playtime, 4, 5)),\n         metascore = as.double(str_sub(score_rank_userscore_metascore, start = -4, end = -3))) %&gt;% \n  select(-score_rank_userscore_metascore, -score_rank, -playtime_median) %&gt;% \n  rename(publisher = publisher_s, developer = developer_s)"
  },
  {
    "objectID": "data/2019/2019-08-13/readme.html",
    "href": "data/2019/2019-08-13/readme.html",
    "title": "Roman Emperors",
    "section": "",
    "text": "This week’s data is from Wikipedia, with credit to Georgios Karamanis for sharing the dataset.\nA Reddit /r/dataisbeautiful post covers this data."
  },
  {
    "objectID": "data/2019/2019-08-13/readme.html#emperors.csv",
    "href": "data/2019/2019-08-13/readme.html#emperors.csv",
    "title": "Roman Emperors",
    "section": "emperors.csv",
    "text": "emperors.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nindex\ndouble\nNumerical Index\n\n\nname\ncharacter\nName\n\n\nname_full\ncharacter\nFull Name\n\n\nbirth\ndate\nBirth date\n\n\ndeath\ndate\nDeath date\n\n\nbirth_cty\ncharacter\nBirth city\n\n\nbirth_prv\ncharacter\nBirth Province\n\n\nrise\ncharacter\nHow did they come to power\n\n\nreign_start\ndate\nDate of start of reign\n\n\nreign_end\ndate\nDate of end of reign\n\n\ncause\ncharacter\nCause of death\n\n\nkiller\ncharacter\nKiller\n\n\ndynasty\ncharacter\nDynasty name\n\n\nera\ncharacter\nEra\n\n\nnotes\ncharacter\nNotes\n\n\nverif_who\ncharacter\nIf verified, by whom"
  },
  {
    "objectID": "data/2019/2019-08-27/readme.html",
    "href": "data/2019/2019-08-27/readme.html",
    "title": "Simpsons Guest Stars",
    "section": "",
    "text": "This week’s data is from Wikipedia, by way of Andrew Collier."
  },
  {
    "objectID": "data/2019/2019-08-27/readme.html#simpsons-guests.csv",
    "href": "data/2019/2019-08-27/readme.html#simpsons-guests.csv",
    "title": "Simpsons Guest Stars",
    "section": "simpsons-guests.csv",
    "text": "simpsons-guests.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ninteger\nSeason of the show\n\n\nnumber\ncharacter\nEpisode number\n\n\nproduction_code\ncharacter\nProduction code for the episode\n\n\nepisode_title\ncharacter\nEpisode Title\n\n\nguest_star\ncharacter\nGuest star (actual name)\n\n\nrole\ncharacter\nRole in the show, either a character or themself"
  },
  {
    "objectID": "data/2019/2019-09-10/readme.html",
    "href": "data/2019/2019-09-10/readme.html",
    "title": "Amusement Park injuries",
    "section": "",
    "text": "This week’s data is from data.world and the Safer Parks database.\nA lot of free text this week, some inconsistent NAs (n/a, N/A) and dates (ymd, dmy). A good chance to do some data cleaning and then take a look at frequency, type of injury, and analyze free text.\nAdditional data can be found at SaferParks Database"
  },
  {
    "objectID": "data/2019/2019-09-10/readme.html#tx_injuries.csv",
    "href": "data/2019/2019-09-10/readme.html#tx_injuries.csv",
    "title": "Amusement Park injuries",
    "section": "tx_injuries.csv",
    "text": "tx_injuries.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ninjury_report_rec\ndouble\nUnique Record ID\n\n\nname_of_operation\ncharacter\nCompany name\n\n\ncity\ncharacter\nCity\n\n\nst\ncharacter\nState (all TX)\n\n\ninjury_date\ncharacter\nInjury date - note there are some different formats\n\n\nride_name\ncharacter\nRide Name\n\n\nserial_no\ncharacter\nSerial number of ride\n\n\ngender\ncharacter\nGender of the injured individual\n\n\nage\ncharacter\nAge of the injured individual\n\n\nbody_part\ncharacter\nBody part injured\n\n\nalleged_injury\ncharacter\nAlleged injury - type of injury\n\n\ncause_of_injury\ncharacter\nApproximate cause of the injury (free text)\n\n\nother\ncharacter\nAnecdotal information in addition to cause of injury"
  },
  {
    "objectID": "data/2019/2019-09-10/readme.html#safer_parks.csv",
    "href": "data/2019/2019-09-10/readme.html#safer_parks.csv",
    "title": "Amusement Park injuries",
    "section": "safer_parks.csv",
    "text": "safer_parks.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nacc_id\ndouble\nUnique ID\n\n\nacc_date\ncharacter\nAccident Date\n\n\nacc_state\ncharacter\nAccident State\n\n\nacc_city\ncharacter\nAccident City\n\n\nfix_port\ncharacter\n.\n\n\nsource\ncharacter\nSource of injury report\n\n\nbus_type\ncharacter\nBusiness type\n\n\nindustry_sector\ncharacter\nIndustry sector\n\n\ndevice_category\ncharacter\nDevice category\n\n\ndevice_type\ncharacter\nDevice type\n\n\ntradename_or_generic\ncharacter\nCommon name of the device\n\n\nmanufacturer\ncharacter\nManufacturer of device\n\n\nnum_injured\ndouble\nNum injured\n\n\nage_youngest\ndouble\nYoungest individual injured\n\n\ngender\ncharacter\nGender of individual injured\n\n\nacc_desc\ncharacter\nDescription of accident\n\n\ninjury_desc\ncharacter\nInjury description\n\n\nreport\ncharacter\nReport URL\n\n\ncategory\ncharacter\nCategory of accident\n\n\nmechanical\ndouble\nMechanical failure (binary NA/1)\n\n\nop_error\ndouble\nOperator error (binary NA/1)\n\n\nemployee\ndouble\nEmployee error (binary NA/1)\n\n\nnotes\ncharacter\nAdditional notes"
  },
  {
    "objectID": "data/2019/2019-09-24/readme.html",
    "href": "data/2019/2019-09-24/readme.html",
    "title": "School Diversity",
    "section": "",
    "text": "This week’s data is from The Washington Post courtesy of Kate Rabinowitz, Laura Meckler, and Armand Emamdjomeh.\nA lot of the visualizations were written in a scrollytelling format with JavaScript. If you want to play around with a similar format you could try out the experimental package rolldown by Yihui. There is geospatial and shapefile data linked below in the methodology.\nA methodology section taken verbatim from the article is below:\n“This analysis used the Common Core of Data from the National Center for Education Statistics (NCES). Charter and private schools were excluded because the government has limited control over them. Virtual schools were also excluded.\nThe Washington Post used data from the 1994-1995 school year, the earliest near-comprehensive data, and from 2016-2017, the latest available data. Findings were checked against interim years at a five-year interval.\nDiversity was defined by the proportion of students in the dominant racial group. Diverse districts are places where fewer than 75 percent of students are of the same race. Undiverse districts are where 75 to 90 percent of students are the same race. In extremely undiverse districts one racial group constitutes more than 90 percent of students.\nBlack, Asian, Native American and white data excludes anyone with Hispanic ethnicity. Asian includes Asians, Native Hawaiians and other Pacific Islanders. Multiracial was not a racial category in 1995.\nThe Post measured integration for diverse school districts that have at least six schools, more than 1,000 students and where the sum total of black and hispanic students was at least 5 percent and no more than 95 percent of students.\nThe variance or correlation ratio, also referred to as eta-squared, was used to measure integration. The ratio calculates how isolated a racial group or groups are while controlling for the demographics of the district. The variance ratio was computed for black and Hispanic students because of the history of exclusion and achievement gaps faced by these groups.\nIntegration groupings were defined by calculating Jenks breaks, a classification method for optimally determining data groupings, for the most recent data and applying it to earlier data.\nThe Post confirmed findings against an analysis that looked only at elementary schools, a method some researchers use to better control for differences in race across age groups and the typically smaller number of upper-level schools.\nGeographic classifications are from NCES. Geospatial data is courtesy of the U.S. Census Bureau.\nThe code for this analysis and the output data can be found here.”"
  },
  {
    "objectID": "data/2019/2019-09-24/readme.html#school_diversity.csv",
    "href": "data/2019/2019-09-24/readme.html#school_diversity.csv",
    "title": "School Diversity",
    "section": "school_diversity.csv",
    "text": "school_diversity.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nLEAID\ncharacter\nUnique school id\n\n\nLEA_NAME\ncharacter\nSchool District Name\n\n\nST\ncharacter\nState of school district\n\n\nd_Locale_Txt\ncharacter\nType of school district, town, rural, city, suburban combined with distant, remote, fringe, small, midsize, large\n\n\nSCHOOL_YEAR\ncharacter\nSchool year (either 1994-1995 or 2016-2017)\n\n\nAIAN\ndouble\nAmerican indian and alaskan native proportion of student population\n\n\nAsian\ndouble\nAsian proportion of student population\n\n\nBlack\ndouble\nBlack proportion of student population\n\n\nHispanic\ndouble\nHispanic proportion of student population\n\n\nWhite\ndouble\nWhite proportion of student population\n\n\nMulti\ndouble\nMulti-ethnic proportion of student population\n\n\nTotal\ndouble\nTotal student body count\n\n\ndiverse\ncharacter\nDiverse rating (Diverse, undiverse, extremely undiverse)\n\n\nvariance\ndouble\nthe variance ratio\n\n\nint_group\ncharacter\nthe level of integration, defined as “Highly integated”, “Somewhat integrated” and “Not integrated”"
  },
  {
    "objectID": "data/2019/2019-10-08/full_readme.html",
    "href": "data/2019/2019-10-08/full_readme.html",
    "title": "OpenPowerlifting Data Distribution README",
    "section": "",
    "text": "For a rendered version of this document, view the README on GitLab.\n\n\nThe OpenPowerlifting database is distributed as a CSV file for your convenience.\nThe CSV format used is simple: double-quotes and in-field commas are disallowed.\n\n\nMandatory. The name of the lifter in UTF-8 encoding.\nLifters who share the same name are distinguished by use of a # symbol followed by a unique number. For example, two lifters both named John Doe would have Name values John Doe #1 and John Doe #2 respectively.\n\n\n\nMandatory. The sex category in which the lifter competed, M or F.\nThe Sex column is defined by modules/opltypes/src/sex.rs.\n\n\n\nMandatory. The type of competition that the lifter entered.\nValues are as follows:\n\nSBD: Squat-Bench-Deadlift, also commonly called “Full Power”.\nBD: Bench-Deadlift, also commonly called “Ironman” or “Push-Pull”.\nSD: Squat-Deadlift, very uncommon.\nSB: Squat-Bench, very uncommon.\nS: Squat-only.\nB: Bench-only.\nD: Deadlift-only.\n\nThe Event column is defined by modules/opltypes/src/event.rs.\n\n\n\nMandatory. The equipment category under which the lifts were performed.\nNote that this does not mean that the lifter was actually wearing that equipment! For example, GPC-affiliated federations do not have a category that disallows knee wraps. Therefore, all lifters, even if they only wore knee sleeves, nevertheless competed in the Wraps equipment category, because they were allowed to wear wraps.\nValues are as follows:\n\nRaw: Bare knees or knee sleeves.\nWraps: Knee wraps were allowed.\nSingle-ply: Equipped, single-ply suits.\nMulti-ply: Equipped, multi-ply suits (includes Double-ply).\nStraps: Allowed straps on the deadlift (used mostly for exhibitions, not real meets).\n\nThe Equipment column is defined by modules/opltypes/src/equipment.rs.\n\n\n\nOptional. The age of the lifter on the start date of the meet, if known.\nAges can be one of two types: exact or approximate. Exact ages are given as integer numbers, for example 23. Approximate ages are given as an integer plus 0.5, for example 23.5.\nApproximate ages mean that the lifter could be either of two possible ages. For an approximate age of n + 0.5, the possible ages are n or n+1. For example, a lifter with the given age 23.5 could be either 23 or 24 – we don’t have enough information to know.\nApproximate ages occur because some federations only provide us with birth year information. So another way to think about approximate ages is that 23.5 implies that the lifter turns 24 that year.\nThe Age column is defined by modules/opltypes/src/age.rs.\n\n\n\nOptional. The age class in which the filter falls, for example 40-45.\nAgeClass is mostly useful because sometimes a federation will report that a lifter competed in the 50-54 divison without providing any further age information. This way, we can still tag them as 50-54, even if the Age column is empty.\nThe full range available to AgeClass is defined by modules/opltypes/src/ageclass.rs.\n\n\n\nOptional. Free-form UTF-8 text describing the division of competition, like Open or Juniors 20-23 or Professional.\nSome federations are configured in our database, which means that we have agreed on a limited set of division options for that federation, and we have rewritten their results to only use that set, and tests enforce that. Even still, divisions are not standardized between configured federations: it really is free-form text, just to provide context.\nInformation about age should not be extracted from the Division, but from the AgeClass column.\n\n\n\nOptional. The recorded bodyweight of the lifter at the time of competition, to two decimal places.\n\n\n\nOptional. The weight class in which the lifter competed, to two decimal places.\nWeight classes can be specified as a maximum or as a minimum. Maximums are specified by just the number, for example 90 means “up to (and including) 90kg.” minimums are specified by a + to the right of the number, for example 90+ means “above (and excluding) 90kg.”\nWeightClassKg is defined by modules/opltypes/src/weightclasskg.rs.\n\n\n\nOptional. First attempts for each of squat, bench, and deadlift, respectively. Maximum of two decimal places.\nNegative values indicate failed attempts.\nNot all federations report attempt information. Some federations only report Best attempts.\n\n\n\nOptional. Second attempts for each of squat, bench, and deadlift, respectively. Maximum of two decimal places.\nNegative values indicate failed attempts.\nNot all federations report attempt information. Some federations only report Best attempts.\n\n\n\nOptional. Third attempts for each of squat, bench, and deadlift, respectively. Maximum of two decimal places.\nNegative values indicate failed attempts.\nNot all federations report attempt information. Some federations only report Best attempts.\n\n\n\nOptional. Fourth attempts for each of squat, bench, and deadlift, respectively. Maximum of two decimal places.\nNegative values indicate failed attempts.\nFourth attempts are special, in that they do not count toward the Best3TotalKg. They are used for recording single-lift records.\n\n\n\nOptional. Maximum of the first three successful attempts for the lift.\nRarely may be negative: that is used by some federations to report the lowest weight the lifter attempted and failed.\n\n\n\nOptional. Sum of Best3SquatKg, Best3BenchKg, and Best3DeadliftKg, if all three lifts were a success. If one of the lifts was failed, or the lifter was disqualified for some other reason, the TotalKg is empty.\nRarely, mostly for older meets, a federation will report the total but not any lift information.\n\n\n\nMandatory. The recorded place of the lifter in the given division at the end of the meet.\nValues are as follows:\n\nPositive number: the place the lifter came in.\nG: Guest lifter. The lifter succeeded, but wasn’t eligible for awards.\nDQ: Disqualified. Note that DQ could be for procedural reasons, not just failed attempts.\nDD: Doping Disqualification. The lifter failed a drug test.\nNS: No-Show. The lifter did not show up on the meet day.\n\nThe Place column is defined by modules/opltypes/src/place.rs.\n\n\n\nOptional. A positive number if Wilks points could be calculated, empty if the lifter was disqualified.\nWilks is the most common formula used for determining Best Lifter in a powerlifting meet.\nThe calculation of Wilks points is defined by modules/coefficients/src/wilks.rs.\n\n\n\nOptional. A positive number if McCulloch points could be calculated, empty if the lifter was disqualified.\nMcCulloch is the name used by the USPA/IPL for Wilks points multiplied by an age-adjustment factor. McCulloch is technically just the coefficients for Masters lifters – coefficients for Junior and Sub-Junior lifters are called “Foster Coefficients.” Our implementation of McCulloch contains both.\nThe calculation of McCulloch points is defined by modules/coefficients/src/mcculloch.rs.\n\n\n\nOptional. A positive number if Glossbrenner points could be calculated, empty if the lifter was disqualified.\nGlossbrenner was created by Herb Glossbrenner as an update of the Wilks formula. It is most commonly used by GPC-affiliated federations.\nThe calculation of Glossbrenner points is defined by modules/coefficients/src/glossbrenner.rs.\n\n\n\nOptional. A positive number if IPF Points could be calculated, empty if the lifter was disqualified or IPF Points were undefined for the Event type.\nThe IPF formula is a normal distribution with a mean of 500 and a standard deviation of 100. The IPF adopted it beginning in 2019 to replace the Wilks formula.\nThe calculation of IPF points is defined by modules/coefficients/src/ipf.rs.\n\n\n\nOptional. Yes if the lifter entered a drug-tested category, empty otherwise.\nNote that this records whether the results count as drug-tested, which does not imply that the lifter actually took a drug test. Federations do not report which lifters, if any, were subject to drug testing.\n\n\n\nOptional. The home country of the lifter, if known.\nThe full list of valid Country values is defined by modules/opltypes/src/country.rs.\n\n\n\nMandatory. The federation that hosted the meet.\nNote that this may be different than the international federation that provided sanction to the meet. For example, USPA meets are sanctioned by the IPL, but we record USPA meets as USPA.\nThe full list of valid Federation values is defined by modules/opltypes/src/federation.rs. Comments in that file help explain what each federation value means.\n\n\n\nMandatory. The start date of the meet in ISO 8601 format. ISO 8601 looks like YYYY-MM-DD: as an example, 1996-12-04 would be December 4th, 1996.\nMeets that last more than one day only have the start date recorded.\n\n\n\nMandatory. The country in which the meet was held.\nThe full list of valid Country values is defined by modules/opltypes/src/country.rs.\n\n\n\nOptional. The state, province, or region in which the meet was held.\nThe full list of valid State values is defined by modules/opltypes/src/state.rs.\n\n\n\nMandatory. The name of the meet.\nThe name is defined to never include the year or the federation. For example, the meet officially called 2019 USAPL Raw National Championships would have the MeetName Raw National Championshps."
  },
  {
    "objectID": "data/2019/2019-10-08/full_readme.html#csv-data-format",
    "href": "data/2019/2019-10-08/full_readme.html#csv-data-format",
    "title": "OpenPowerlifting Data Distribution README",
    "section": "",
    "text": "The OpenPowerlifting database is distributed as a CSV file for your convenience.\nThe CSV format used is simple: double-quotes and in-field commas are disallowed.\n\n\nMandatory. The name of the lifter in UTF-8 encoding.\nLifters who share the same name are distinguished by use of a # symbol followed by a unique number. For example, two lifters both named John Doe would have Name values John Doe #1 and John Doe #2 respectively.\n\n\n\nMandatory. The sex category in which the lifter competed, M or F.\nThe Sex column is defined by modules/opltypes/src/sex.rs.\n\n\n\nMandatory. The type of competition that the lifter entered.\nValues are as follows:\n\nSBD: Squat-Bench-Deadlift, also commonly called “Full Power”.\nBD: Bench-Deadlift, also commonly called “Ironman” or “Push-Pull”.\nSD: Squat-Deadlift, very uncommon.\nSB: Squat-Bench, very uncommon.\nS: Squat-only.\nB: Bench-only.\nD: Deadlift-only.\n\nThe Event column is defined by modules/opltypes/src/event.rs.\n\n\n\nMandatory. The equipment category under which the lifts were performed.\nNote that this does not mean that the lifter was actually wearing that equipment! For example, GPC-affiliated federations do not have a category that disallows knee wraps. Therefore, all lifters, even if they only wore knee sleeves, nevertheless competed in the Wraps equipment category, because they were allowed to wear wraps.\nValues are as follows:\n\nRaw: Bare knees or knee sleeves.\nWraps: Knee wraps were allowed.\nSingle-ply: Equipped, single-ply suits.\nMulti-ply: Equipped, multi-ply suits (includes Double-ply).\nStraps: Allowed straps on the deadlift (used mostly for exhibitions, not real meets).\n\nThe Equipment column is defined by modules/opltypes/src/equipment.rs.\n\n\n\nOptional. The age of the lifter on the start date of the meet, if known.\nAges can be one of two types: exact or approximate. Exact ages are given as integer numbers, for example 23. Approximate ages are given as an integer plus 0.5, for example 23.5.\nApproximate ages mean that the lifter could be either of two possible ages. For an approximate age of n + 0.5, the possible ages are n or n+1. For example, a lifter with the given age 23.5 could be either 23 or 24 – we don’t have enough information to know.\nApproximate ages occur because some federations only provide us with birth year information. So another way to think about approximate ages is that 23.5 implies that the lifter turns 24 that year.\nThe Age column is defined by modules/opltypes/src/age.rs.\n\n\n\nOptional. The age class in which the filter falls, for example 40-45.\nAgeClass is mostly useful because sometimes a federation will report that a lifter competed in the 50-54 divison without providing any further age information. This way, we can still tag them as 50-54, even if the Age column is empty.\nThe full range available to AgeClass is defined by modules/opltypes/src/ageclass.rs.\n\n\n\nOptional. Free-form UTF-8 text describing the division of competition, like Open or Juniors 20-23 or Professional.\nSome federations are configured in our database, which means that we have agreed on a limited set of division options for that federation, and we have rewritten their results to only use that set, and tests enforce that. Even still, divisions are not standardized between configured federations: it really is free-form text, just to provide context.\nInformation about age should not be extracted from the Division, but from the AgeClass column.\n\n\n\nOptional. The recorded bodyweight of the lifter at the time of competition, to two decimal places.\n\n\n\nOptional. The weight class in which the lifter competed, to two decimal places.\nWeight classes can be specified as a maximum or as a minimum. Maximums are specified by just the number, for example 90 means “up to (and including) 90kg.” minimums are specified by a + to the right of the number, for example 90+ means “above (and excluding) 90kg.”\nWeightClassKg is defined by modules/opltypes/src/weightclasskg.rs.\n\n\n\nOptional. First attempts for each of squat, bench, and deadlift, respectively. Maximum of two decimal places.\nNegative values indicate failed attempts.\nNot all federations report attempt information. Some federations only report Best attempts.\n\n\n\nOptional. Second attempts for each of squat, bench, and deadlift, respectively. Maximum of two decimal places.\nNegative values indicate failed attempts.\nNot all federations report attempt information. Some federations only report Best attempts.\n\n\n\nOptional. Third attempts for each of squat, bench, and deadlift, respectively. Maximum of two decimal places.\nNegative values indicate failed attempts.\nNot all federations report attempt information. Some federations only report Best attempts.\n\n\n\nOptional. Fourth attempts for each of squat, bench, and deadlift, respectively. Maximum of two decimal places.\nNegative values indicate failed attempts.\nFourth attempts are special, in that they do not count toward the Best3TotalKg. They are used for recording single-lift records.\n\n\n\nOptional. Maximum of the first three successful attempts for the lift.\nRarely may be negative: that is used by some federations to report the lowest weight the lifter attempted and failed.\n\n\n\nOptional. Sum of Best3SquatKg, Best3BenchKg, and Best3DeadliftKg, if all three lifts were a success. If one of the lifts was failed, or the lifter was disqualified for some other reason, the TotalKg is empty.\nRarely, mostly for older meets, a federation will report the total but not any lift information.\n\n\n\nMandatory. The recorded place of the lifter in the given division at the end of the meet.\nValues are as follows:\n\nPositive number: the place the lifter came in.\nG: Guest lifter. The lifter succeeded, but wasn’t eligible for awards.\nDQ: Disqualified. Note that DQ could be for procedural reasons, not just failed attempts.\nDD: Doping Disqualification. The lifter failed a drug test.\nNS: No-Show. The lifter did not show up on the meet day.\n\nThe Place column is defined by modules/opltypes/src/place.rs.\n\n\n\nOptional. A positive number if Wilks points could be calculated, empty if the lifter was disqualified.\nWilks is the most common formula used for determining Best Lifter in a powerlifting meet.\nThe calculation of Wilks points is defined by modules/coefficients/src/wilks.rs.\n\n\n\nOptional. A positive number if McCulloch points could be calculated, empty if the lifter was disqualified.\nMcCulloch is the name used by the USPA/IPL for Wilks points multiplied by an age-adjustment factor. McCulloch is technically just the coefficients for Masters lifters – coefficients for Junior and Sub-Junior lifters are called “Foster Coefficients.” Our implementation of McCulloch contains both.\nThe calculation of McCulloch points is defined by modules/coefficients/src/mcculloch.rs.\n\n\n\nOptional. A positive number if Glossbrenner points could be calculated, empty if the lifter was disqualified.\nGlossbrenner was created by Herb Glossbrenner as an update of the Wilks formula. It is most commonly used by GPC-affiliated federations.\nThe calculation of Glossbrenner points is defined by modules/coefficients/src/glossbrenner.rs.\n\n\n\nOptional. A positive number if IPF Points could be calculated, empty if the lifter was disqualified or IPF Points were undefined for the Event type.\nThe IPF formula is a normal distribution with a mean of 500 and a standard deviation of 100. The IPF adopted it beginning in 2019 to replace the Wilks formula.\nThe calculation of IPF points is defined by modules/coefficients/src/ipf.rs.\n\n\n\nOptional. Yes if the lifter entered a drug-tested category, empty otherwise.\nNote that this records whether the results count as drug-tested, which does not imply that the lifter actually took a drug test. Federations do not report which lifters, if any, were subject to drug testing.\n\n\n\nOptional. The home country of the lifter, if known.\nThe full list of valid Country values is defined by modules/opltypes/src/country.rs.\n\n\n\nMandatory. The federation that hosted the meet.\nNote that this may be different than the international federation that provided sanction to the meet. For example, USPA meets are sanctioned by the IPL, but we record USPA meets as USPA.\nThe full list of valid Federation values is defined by modules/opltypes/src/federation.rs. Comments in that file help explain what each federation value means.\n\n\n\nMandatory. The start date of the meet in ISO 8601 format. ISO 8601 looks like YYYY-MM-DD: as an example, 1996-12-04 would be December 4th, 1996.\nMeets that last more than one day only have the start date recorded.\n\n\n\nMandatory. The country in which the meet was held.\nThe full list of valid Country values is defined by modules/opltypes/src/country.rs.\n\n\n\nOptional. The state, province, or region in which the meet was held.\nThe full list of valid State values is defined by modules/opltypes/src/state.rs.\n\n\n\nMandatory. The name of the meet.\nThe name is defined to never include the year or the federation. For example, the meet officially called 2019 USAPL Raw National Championships would have the MeetName Raw National Championshps."
  },
  {
    "objectID": "data/2019/2019-10-15/readme.html",
    "href": "data/2019/2019-10-15/readme.html",
    "title": "Big mtcars",
    "section": "",
    "text": "This week’s data is from the EPA. The full data dictionary can be found at fueleconomy.gov.\nIt’s essentially a much much larger and updated dataset covering mtcars, the dataset we all know a bit too well!\nH/t to Ellis Hughes who had a recent blogpost covering this dataset."
  },
  {
    "objectID": "data/2019/2019-10-15/readme.html#big_epa_cars.csv",
    "href": "data/2019/2019-10-15/readme.html#big_epa_cars.csv",
    "title": "Big mtcars",
    "section": "big_epa_cars.csv",
    "text": "big_epa_cars.csv\nI left in ALL the data so people could look at various different things besides the classical mtcars!\nFull data dictionary can be found at fueleconomy.gov"
  },
  {
    "objectID": "data/2019/2019-10-29/readme.html",
    "href": "data/2019/2019-10-29/readme.html",
    "title": "NYC Squirrel Census",
    "section": "",
    "text": "This week’s data is from the NYC Squirrel Census - raw data at NY Data portal.\nH/t to Sara Stoudt for sharing this data, and Mine Cetinkaya-Rundel for her squirrel data package using the same data.\nCityLab’s Linda Poon wrote an article using this data."
  },
  {
    "objectID": "data/2019/2019-10-29/readme.html#nyc_squirrels.csv",
    "href": "data/2019/2019-10-29/readme.html#nyc_squirrels.csv",
    "title": "NYC Squirrel Census",
    "section": "nyc_squirrels.csv",
    "text": "nyc_squirrels.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nlong\ndouble\nLongitude\n\n\nlat\ndouble\nLatitude\n\n\nunique_squirrel_id\ncharacter\nIdentification tag for each squirrel sightings. The tag is comprised of “Hectare ID” + “Shift” + “Date” + “Hectare Squirrel Number.”\n\n\nhectare\ncharacter\nID tag, which is derived from the hectare grid used to divide and count the park area. One axis that runs predominantly north-to-south is numerical (1-42), and the axis that runs predominantly east-to-west is roman characters (A-I).\n\n\nshift\ncharacter\nValue is either “AM” or “PM,” to communicate whether or not the sighting session occurred in the morning or late afternoon.\n\n\ndate\ndouble\nConcatenation of the sighting session day and month.\n\n\nhectare_squirrel_number\ndouble\nNumber within the chronological sequence of squirrel sightings for a discrete sighting session.\n\n\nage\ncharacter\nValue is either “Adult” or “Juvenile.”\n\n\nprimary_fur_color\ncharacter\nValue is either “Gray,” “Cinnamon” or “Black.”\n\n\nhighlight_fur_color\ncharacter\nDiscrete value or string values comprised of “Gray,” “Cinnamon” or “Black.”\n\n\ncombination_of_primary_and_highlight_color\ncharacter\nA combination of the previous two columns; this column gives the total permutations of primary and highlight colors observed.\n\n\ncolor_notes\ncharacter\nSighters occasionally added commentary on the squirrel fur conditions. These notes are provided here.\n\n\nlocation\ncharacter\nValue is either “Ground Plane” or “Above Ground.” Sighters were instructed to indicate the location of where the squirrel was when first sighted.\n\n\nabove_ground_sighter_measurement\ncharacter\nFor squirrel sightings on the ground plane, fields were populated with a value of “FALSE.”\n\n\nspecific_location\ncharacter\nSighters occasionally added commentary on the squirrel location. These notes are provided here.\n\n\nrunning\nlogical\nSquirrel was seen running.\n\n\nchasing\nlogical\nSquirrel was seen chasing.\n\n\nclimbing\nlogical\nSquirrel was seen climbing.\n\n\neating\nlogical\nSquirrel was seen eating.\n\n\nforaging\nlogical\nSquirrel was seen foraging.\n\n\nother_activities\ncharacter\nOther activities\n\n\nkuks\nlogical\nSquirrel was heard kukking, a chirpy vocal communication used for a variety of reasons.\n\n\nquaas\nlogical\nSquirrel was heard quaaing, an elongated vocal communication which can indicate the presence of a ground predator such as a dog.\n\n\nmoans\nlogical\nSquirrel was heard moaning, a high-pitched vocal communication which can indicate the presence of an air predator such as a hawk.\n\n\ntail_flags\nlogical\nSquirrel was seen flagging its tail. Flagging is a whipping motion used to exaggerate squirrel’s size and confuse rivals or predators. Looks as if the squirrel is scribbling with tail into the air.\n\n\ntail_twitches\nlogical\nSquirrel was seen flagging its tail. Flagging is a whipping motion used to exaggerate squirrel’s size and confuse rivals or predators. Looks as if the squirrel is scribbling with tail into the air.\n\n\napproaches\nlogical\nSquirrel was seen approaching human, seeking food.\n\n\nindifferent\nlogical\nSquirrel was indifferent to human presence.\n\n\nruns_from\nlogical\n.Squirrel was seen running from humans, seeing them as a threat.\n\n\nother_interactions\ncharacter\nSighter notes on other types of interactions between squirrels and humans.\n\n\nlat_long\ncharacter\nCombined lat long\n\n\nzip_codes\ndouble\nzip codes\n\n\ncommunity_districts\ndouble\nCommunity districts\n\n\nborough_boundaries\ndouble\nBorough boundaries\n\n\ncity_council_districts\ndouble\nCity council districts\n\n\npolice_precincts\ndouble\nPolice precincts"
  },
  {
    "objectID": "data/2019/2019-11-12/readme.html",
    "href": "data/2019/2019-11-12/readme.html",
    "title": "Code in CRAN Packages",
    "section": "",
    "text": "This week’s data is from the CRAN courtesy of Phillip Massicotte.\nHe analyzed the lines of code and the different languages in all of the R packages on CRAN."
  },
  {
    "objectID": "data/2019/2019-11-12/readme.html#loc_cran_packages.csv",
    "href": "data/2019/2019-11-12/readme.html#loc_cran_packages.csv",
    "title": "Code in CRAN Packages",
    "section": "loc_cran_packages.csv",
    "text": "loc_cran_packages.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nfile\ndouble\nNumber of files\n\n\nlanguage\ncharacter\nProgramming Language\n\n\nblank\ndouble\nBlank Lines\n\n\ncomment\ndouble\nCommented Lines\n\n\ncode\ndouble\nLines of Code\n\n\npkg_name\ncharacter\nPackage Name\n\n\nversion\ncharacter\nPackage Version"
  },
  {
    "objectID": "data/2019/2019-11-26/readme.html",
    "href": "data/2019/2019-11-26/readme.html",
    "title": "Student Loan Payments",
    "section": "",
    "text": "This week’s data is from the Department of Education courtesy of Alex Albright.\nData idea comes from Dignity and Debt who is running a contest around data viz for understanding and spreading awareness around Student Loan debt.\nThere are already some gorgeous plots in the style of DuBois.\nI have uploaded the raw data and the clean data - definitely a nice dive into some data cleansing if you want to have a go at the raw Excel files."
  },
  {
    "objectID": "data/2019/2019-11-26/readme.html#loans.csv",
    "href": "data/2019/2019-11-26/readme.html#loans.csv",
    "title": "Student Loan Payments",
    "section": "loans.csv",
    "text": "loans.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nagency_name\ncharacter\nName of loan agency\n\n\nyear\ninteger\ntwo digits year\n\n\nquarter\ninteger\nQuarter (3 month period)\n\n\nstarting\ndouble\nTotal value in dollars at start of quarter\n\n\nadded\ndouble\nTotal value added during quarter\n\n\ntotal\ndouble\nTotal dollars repaid\n\n\nconsolidation\ndouble\nConsolidation reflects the dollar value of loans consolidated\n\n\nrehabilitation\ndouble\nRehabilitation reflects the dollar value of loans rehabilitated\n\n\nvoluntary_payments\ndouble\nVoluntary payments reflects the total amount of payments received from borrowers\n\n\nwage_garnishments\ndouble\nWage Garnishments reflect the total amount of wage garnishment payments"
  },
  {
    "objectID": "data/2019/2019-12-10/readme.html",
    "href": "data/2019/2019-12-10/readme.html",
    "title": "You can make it in R",
    "section": "",
    "text": "You can make it in R\nThis week’s data is a meta collection of data sources and code from Rafael Irizarry - his recent blog post You can replicate almost any plot with R covered a few news articles and how to replicate them in R. I included 4 datasets, but please note that there are many more datasets in his DS Labs package.\nI’d recommend checking out his blog post for plot examples and metadata.\n\n\nGet the Data\nmurders &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-10/international_murders.csv\")\n\ngun_murders &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-10/gun_murders.csv\")\n\ndiseases &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-10/diseases.csv\")\n\nnyc_regents &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-10/nyc_regents.csv\")\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# Either ISO-8601 date or year/week works!\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load(\"2019-12-10\")\ntuesdata &lt;- tidytuesdayR::tt_load(2019, week = 50)\n\ndiseases &lt;- tuesdata$diseases\n\n\nDictionary\n\ndiseases.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndisease\ninteger\nDisease name\n\n\nstate\ncharacter\nState\n\n\nyear\ndouble\nYear\n\n\nweeks_reporting\ninteger\nN of weeks reporting\n\n\ncount\ndouble\nCount of disease observed\n\n\npopulation\ndouble\nTotal population\n\n\n\n\n\nnyc_regents.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nscore\ndouble\nGrading score (0 - 100)\n\n\nintegrated_algebra\ndouble\nTotal observations\n\n\nglobal_history\ndouble\nTotal observations\n\n\nliving_environment\ndouble\nTotal observations\n\n\nenglish\ndouble\nTotal observations\n\n\nus_history\ndouble\nTotal observations\n\n\n\n\n\ninternational_murders.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\ncountry\n\n\ncount\ndouble\nTotal observations\n\n\nlabel\ncharacter\nTotal observations\n\n\ncode\ncharacter\n2 letter country code\n\n\n\n\n\ngun_murders.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\ncountry\n\n\ncount\ndouble\ngun related homicides per 100,000 people\n\n\n\n\n\n\nCleaning\npak::pak(\"rafalab/dslabs\")\n\nlibrary(dslabs)\nlibrary(tidyverse)\nlibrary(here)\n\nmurders &lt;- tibble(country = toupper(c(\"US\", \"Italy\", \"Canada\", \"UK\", \"Japan\", \"Germany\", \"France\", \"Russia\")),\n              count = c(3.2, 0.71, 0.5, 0.1, 0, 0.2, 0.1, 0),\n              label = c(as.character(c(3.2, 0.71, 0.5, 0.1, 0, 0.2, 0.1)), \"No Data\"),\n              code = c(\"us\", \"it\", \"ca\", \"gb\", \"jp\", \"de\", \"fr\", \"ru\"))\n\ngun_murders &lt;- tibble(country = toupper(c(\"United States\", \"Canada\", \"Portugal\", \"Ireland\", \"Italy\", \"Belgium\", \"Finland\", \"France\", \"Netherlands\", \"Denmark\", \"Sweden\", \"Slovakia\", \"Austria\", \"New Zealand\", \"Australia\", \"Spain\", \"Czech Republic\", \"Hungry\", \"Germany\", \"United Kingdom\", \"Norway\", \"Japan\", \"Republic of Korea\")),\n              count = c(3.61, 0.5, 0.48, 0.35, 0.35, 0.33, 0.26, 0.20, 0.20, 0.20, 0.19, 0.19, 0.18, 0.16,\n                        0.16, 0.15, 0.12, 0.10, 0.06, 0.04, 0.04, 0.01, 0.01))\n\ndiseases &lt;- dslabs::us_contagious_diseases\n\nnyc &lt;- dslabs::nyc_regents_scores\n\n\nmurders %&gt;% \n  write_csv(here(\"2019\", \"2019-12-10\",\"international_murders.csv\"))\n\ngun_murders %&gt;% \n  write_csv(here(\"2019\", \"2019-12-10\",\"gun_murders.csv\"))\n\n\ndiseases %&gt;% \n  write_csv(here(\"2019\", \"2019-12-10\",\"diseases.csv\"))\n\nnyc %&gt;% \n  write_csv(here(\"2019\", \"2019-12-10\",\"nyc_regents.csv\"))"
  },
  {
    "objectID": "data/2019/2019-12-24/readme.html",
    "href": "data/2019/2019-12-24/readme.html",
    "title": "Christmas Music Billboards",
    "section": "",
    "text": "Christmas Music Billboards\nThis week’s data is about Christmas songs on the hot-100 list! Clean data comes from Kaggle and originally from data.world.\nThe lyrics come courtest of Josiah Parry’s genius R package. It has several useful functions, mainly built around grabbing lyrics for specific artists, songs, or albums.\n\n\nGet the Data\nchristmas_songs &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-24/christmas_songs.csv\")\n\nchristmas_lyrics &lt;- readr::read_tsv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-12-24/christmas_lyrics.tsv\")\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# Either ISO-8601 date or year/week works!\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load(\"2019-12-24\")\ntuesdata &lt;- tidytuesdayR::tt_load(2019, week = 52)\n\nchristmas_songs &lt;- tuesdata$christmas_songs\n\n\nDictionary\n\nchristmas_songs.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nurl\ncharacter\nURL of billboard chart\n\n\nweekid\ndate\nDate, Month Day Year\n\n\nweek_position\ndouble\nWeek position (higher = better)\n\n\nsong\ncharacter\nSong Title\n\n\nperformer\ncharacter\nArtist\n\n\nsongid\ncharacter\nSong ID\n\n\ninstance\ndouble\nInstances on billboard\n\n\nprevious_week_position\ndouble\nPrevious week’s position on billboard\n\n\npeak_position\ndouble\nHighest position\n\n\nweeks_on_chart\ndouble\nNumber of weeks on billboard charts\n\n\nyear\ndouble\nyear\n\n\nmonth\ndouble\nmonth\n\n\nday\ndouble\nday\n\n\n\n\n\nchristmas_lyrics.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntitle\ncharacter\nSong title (same as song)\n\n\nartist\ncharacter\nArtist (same as performer)\n\n\nsongid\ncharacter\nsongid - join via this column\n\n\nweekid\ndate\ndate\n\n\ntrack_title\ncharacter\nTrack title\n\n\ntrack_n\ninteger\nTrack number\n\n\nline\ninteger\nLine number\n\n\nlyric\ncharacter\nLyric text\n\n\n\nlibrary(tidyverse)\nlibrary(genius)\n\ndf &lt;- read_csv(here(\"2019\", \"2019-12-24\", \"christmas_songs.csv\"))\n\nlyric_df &lt;- df %&gt;% \n  select(\"title\" = song,\n         \"artist\" = performer,\n         songid,\n         weekid) %&gt;% \n  add_genius(artist = artist, title = title) %&gt;% \n  select(-lyrics)\n\nlyric_clean %&gt;% \n  write_tsv(here(\"2019\", \"2019-12-24\",\"christmas_lyrics.tsv\"))"
  },
  {
    "objectID": "data/2020/2020-01-07/readme.html",
    "href": "data/2020/2020-01-07/readme.html",
    "title": "Australia Fires",
    "section": "",
    "text": "This week’s data is all about Australia, including it’s climate over time and recent fires. A good article currently is from the New York Times. The BBC also has an article with maps and news, but I am not 100% convinced their use of scale for fire points is 100% appropriate. It is using the NASA FIRMS data.\nA group of #rstats contributors have put together a list of resources for community help for those affected, please take a look at it here.\n\n\nThis is an ongoing situation, and the goal of sharing data here is to spread awareness of the Australian fires. PLEASE be cautious when plotting maps of ongoing fires - there are many considerations when using the NASA “Active Fires Dataset” via FIRMS - potential pitfalls are outlined here and at the source from NASA. It is nuanced in interpretation and plotting (1 km estimations). There is a very long user guide if you want to look further. It is apparently suggested to use nighttime data for most accuracy, but I have never used this data before.\nA live update of the fires from NASA can be seen here\nA safer dataset to use is from the New South Wales Rural Fire Service - this JSON file can be rapidly turned into a map, courtesy of Dean Marchiori.\nurl &lt;- \"http://www.rfs.nsw.gov.au/feeds/majorIncidents.json\" contains the newest updated data, I downloaded it for today (2020-01-06).\nI also found some The Guardian fire data, but it has not been updated. Some shapefiles/json/geojson though.\n\n\nNicholas Tierney has a good overview of plotting Australian climate data on GitHub. THe original heatmap is here.\nThe overall climate of Australia can be found on Wikipedia. The list of cities by population is here.\nFor climate data, temperature and rainfall was gathered from the Australian Bureau of Meterology (BoM). A number of weather stations were chosen, based on their proximity to major Australian cities such as Sydney, Perth, Brisbane, Canberra, and Adelaide. The South East region of Australia appears to be the most affected.\nRainfall data was sourced from:\nSubiaco, Sydney, Melbourne, Brisbane, Canberra, Adelaide\nTemp min/max data was sourced from:\n* BoM Climate Data Online\n\n\n\n# Get the Data\n\nrainfall &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-07/rainfall.csv')\ntemperature &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-07/temperature.csv')\n\n# IF YOU USE THIS DATA PLEASE BE CAUTIOUS WITH INTERPRETATION\nnasa_fire &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-07/MODIS_C6_Australia_and_New_Zealand_7d.csv')\n\n# For JSON File of fires\nurl &lt;- \"http://www.rfs.nsw.gov.au/feeds/majorIncidents.json\"\n\naus_fires &lt;- sf::st_read(url)\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-01-07') \ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 2)\n\n\nrainfall &lt;- tuesdata$rainfall\n\n\n\nPlease note that all weather station locations and metadata are in weather_station_ids.txt."
  },
  {
    "objectID": "data/2020/2020-01-07/readme.html#fire-data",
    "href": "data/2020/2020-01-07/readme.html#fire-data",
    "title": "Australia Fires",
    "section": "",
    "text": "This is an ongoing situation, and the goal of sharing data here is to spread awareness of the Australian fires. PLEASE be cautious when plotting maps of ongoing fires - there are many considerations when using the NASA “Active Fires Dataset” via FIRMS - potential pitfalls are outlined here and at the source from NASA. It is nuanced in interpretation and plotting (1 km estimations). There is a very long user guide if you want to look further. It is apparently suggested to use nighttime data for most accuracy, but I have never used this data before.\nA live update of the fires from NASA can be seen here\nA safer dataset to use is from the New South Wales Rural Fire Service - this JSON file can be rapidly turned into a map, courtesy of Dean Marchiori.\nurl &lt;- \"http://www.rfs.nsw.gov.au/feeds/majorIncidents.json\" contains the newest updated data, I downloaded it for today (2020-01-06).\nI also found some The Guardian fire data, but it has not been updated. Some shapefiles/json/geojson though.\n\n\nNicholas Tierney has a good overview of plotting Australian climate data on GitHub. THe original heatmap is here.\nThe overall climate of Australia can be found on Wikipedia. The list of cities by population is here.\nFor climate data, temperature and rainfall was gathered from the Australian Bureau of Meterology (BoM). A number of weather stations were chosen, based on their proximity to major Australian cities such as Sydney, Perth, Brisbane, Canberra, and Adelaide. The South East region of Australia appears to be the most affected.\nRainfall data was sourced from:\nSubiaco, Sydney, Melbourne, Brisbane, Canberra, Adelaide\nTemp min/max data was sourced from:\n* BoM Climate Data Online\n\n\n\n# Get the Data\n\nrainfall &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-07/rainfall.csv')\ntemperature &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-07/temperature.csv')\n\n# IF YOU USE THIS DATA PLEASE BE CAUTIOUS WITH INTERPRETATION\nnasa_fire &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-07/MODIS_C6_Australia_and_New_Zealand_7d.csv')\n\n# For JSON File of fires\nurl &lt;- \"http://www.rfs.nsw.gov.au/feeds/majorIncidents.json\"\n\naus_fires &lt;- sf::st_read(url)\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-01-07') \ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 2)\n\n\nrainfall &lt;- tuesdata$rainfall\n\n\n\nPlease note that all weather station locations and metadata are in weather_station_ids.txt."
  },
  {
    "objectID": "data/2020/2020-01-21/readme.html",
    "href": "data/2020/2020-01-21/readme.html",
    "title": "Spotify Songs",
    "section": "",
    "text": "@neonbrand via Unsplash - person holding space gray iPhone 6\n\n\n\nSpotify Songs\nThe data this week comes from Spotify via the spotifyr package. Charlie Thompson, Josiah Parry, Donal Phipps, and Tom Wolff authored this package to make it easier to get either your own data or general metadata arounds songs from Spotify’s API. Make sure to check out the spotifyr package website to see how you can collect your own data!\nKaylin Pavlik had a recent blogpost using the audio features to explore and classify songs. She used the spotifyr package to collect about 5000 songs from 6 main categories (EDM, Latin, Pop, R&B, Rap, & Rock).\nh/t to Jon Harmon & Neal Grantham.\n\nGet the data here\n# Get the Data\n\nspotify_songs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-01-21/spotify_songs.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO UPDATE tidytuesdayR from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-01-21') \ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 4)\n\n\nspotify_songs &lt;- tuesdata$spotify_songs\n\n\nData Dictionary\n\n\n\nspotify_songs.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\ntrack_album_id\ncharacter\nAlbum unique ID\n\n\ntrack_album_name\ncharacter\nSong album name\n\n\ntrack_album_release_date\ncharacter\nDate when album released\n\n\nplaylist_name\ncharacter\nName of playlist\n\n\nplaylist_id\ncharacter\nPlaylist ID\n\n\nplaylist_genre\ncharacter\nPlaylist genre\n\n\nplaylist_subgenre\ncharacter\nPlaylist subgenre\n\n\ndanceability\ndouble\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nkey\ndouble\nThe estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\n\n\nloudness\ndouble\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n\n\nmode\ndouble\nMode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n\n\nspeechiness\ndouble\nSpeechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n\n\nacousticness\ndouble\nA confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n\n\ninstrumentalness\ndouble\nPredicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n\n\nliveness\ndouble\nDetects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\ntempo\ndouble\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n\n\nduration_ms\ndouble\nDuration of song in milliseconds"
  },
  {
    "objectID": "data/2020/2020-02-04/readme.html",
    "href": "data/2020/2020-02-04/readme.html",
    "title": "NFL Stadium Attendance",
    "section": "",
    "text": "Empty NFL Stadium\n\n\n\nNFL Stadium Attendance\nThe data this week comes from Pro Football Reference team standings. Additional data on attendance also comes from Pro Football Reference here.\n\nGet the data here\n# Get the Data\n\nattendance &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-02-04/attendance.csv')\nstandings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-02-04/standings.csv')\ngames &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-02-04/games.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO UPDATE tidytuesdayR from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-02-04') \ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 6)\n\n\nattendance &lt;- tuesdata$attendance\n\n\nData Dictionary\nThese can be joined relatively nicely with dplyr::left_join(by = c(\"year\", \"team_name\", \"team\"))\n\n\n\nattendance.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nteam\ncharacter\nTeam City\n\n\nteam_name\ncharacter\nTeam name\n\n\nyear\ninteger\nSeason year\n\n\ntotal\ndouble\ntotal attendance across 17 weeks (1 week = no game)\n\n\nhome\ndouble\nHome attendance\n\n\naway\ndouble\nAway attendance\n\n\nweek\ncharacter\nWeek number (1-17)\n\n\nweekly_attendance\ndouble\nWeekly attendance number\n\n\n\n\n\nstandings.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nteam\ncharacter\nTeam city\n\n\nteam_name\ncharacter\nTeam name\n\n\nyear\ninteger\nseason year\n\n\nwins\ndouble\nWins (0 to 16)\n\n\nloss\ndouble\nLosses (0 to 16)\n\n\npoints_for\ndouble\npoints for (offensive performance)\n\n\npoints_against\ndouble\npoints for (defensive performance)\n\n\npoints_differential\ndouble\nPoint differential (points_for - points_against)\n\n\nmargin_of_victory\ndouble\n(Points Scored - Points Allowed)/ Games Played\n\n\nstrength_of_schedule\ndouble\nAverage quality of opponent as measured by SRS (Simple Rating System)\n\n\nsimple_rating\ndouble\nTeam quality relative to average (0.0) as measured by SRS (Simple Rating System)  SRS = MoV + SoS = OSRS + DSRS\n\n\noffensive_ranking\ndouble\nTeam offense quality relative to average (0.0) as measured by SRS (Simple Rating System)\n\n\ndefensive_ranking\ndouble\nTeam defense quality relative to average (0.0) as measured by SRS (Simple Rating System)\n\n\nplayoffs\ncharacter\nMade playoffs or not\n\n\nsb_winner\ncharacter\nWon superbowl or not\n\n\n\n\n\ngames.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nseason year, note that playoff games will still be in the previous season\n\n\nweek\ncharacter\nweek number (1-17, plus playoffs)\n\n\nhome_team\ncharacter\nHome team\n\n\naway_team\ncharacter\nAway team\n\n\nwinner\ncharacter\nWinning team\n\n\ntie\ncharacter\nIf a tie, the “losing” team as well\n\n\nday\ncharacter\nDay of week\n\n\ndate\ncharacter\nDate minus year\n\n\ntime\ncharacter\nTime of game start\n\n\npts_win\ndouble\nPoints by winning team\n\n\npts_loss\ndouble\nPoints by losing team\n\n\nyds_win\ndouble\nYards by winning team\n\n\nturnovers_win\ndouble\nTurnovers by winning team\n\n\nyds_loss\ndouble\nYards by losing team\n\n\nturnovers_loss\ndouble\nTurnovers by losing team\n\n\nhome_team_name\ncharacter\nHome team name\n\n\nhome_team_city\ncharacter\nHome team city\n\n\naway_team_name\ncharacter\nAway team name\n\n\naway_team_city\ncharacter\nAway team city"
  },
  {
    "objectID": "data/2020/2020-02-18/readme.html",
    "href": "data/2020/2020-02-18/readme.html",
    "title": "Food Consumption and CO2 Emissions",
    "section": "",
    "text": "Food Consumption and CO2 Emissions\nThe data this week comes from nu3 and was contributed by Kasia Kulma.\nKasia has put together a great guide on webscraping along with data cleaning and organization! Make sure to check out her blog post, and the raw code is duplicated as part of the cleaning script.\n\nGet the data here\n# Get the Data\n\nfood_consumption &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-02-18/food_consumption.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-02-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 8)\n\n\nfood_consumption &lt;- tuesdata$food_consumption\n\n\nData Dictionary\n\n\n\nfood_consumption.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry Name\n\n\nfood_category\ncharacter\nFood Category\n\n\nconsumption\ndouble\nConsumption (kg/person/year)\n\n\nco2_emmission\ndouble\nCo2 Emission (Kg CO2/person/year)\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(rvest)\n\n# Credit to Kasia and minorly edited to create output file and test plot\n# Blog post at https://r-tastic.co.uk/post/from-messy-to-tidy/\n\nurl &lt;- \"https://www.nu3.de/blogs/nutrition/food-carbon-footprint-index-2018\"\n\n# scrape the website\nurl_html &lt;- read_html(url)\n\n# extract the HTML table\nwhole_table &lt;- url_html %&gt;% \n  html_nodes('table') %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  .[[1]]\n\ntable_content &lt;- whole_table %&gt;%\n  select(-X1) %&gt;% # remove redundant column\n  filter(!dplyr::row_number() %in% 1:3) # remove redundant rows\n\nraw_headers &lt;- url_html %&gt;%\n  html_nodes(\".thead-icon\") %&gt;%\n  html_attr('title')\n\ntidy_bottom_header &lt;- raw_headers[28:length(raw_headers)]\ntidy_bottom_header[1:10]\n\nraw_middle_header &lt;- raw_headers[17:27]\nraw_middle_header\n\ntidy_headers &lt;- c(\n  rep(raw_middle_header[1:7], each = 2),\n  \"animal_total\",\n  rep(raw_middle_header[8:length(raw_middle_header)], each = 2),\n  \"non_animal_total\",\n  \"country_total\")\n\ntidy_headers\n\ncombined_colnames &lt;- paste(tidy_headers, tidy_bottom_header, sep = ';')\ncolnames(table_content) &lt;- c(\"Country\", combined_colnames)\nglimpse(table_content[, 1:10])\n\nlong_table &lt;- table_content %&gt;%\n  # make column names observations of Category variable\n  tidyr::pivot_longer(cols = -Country, names_to = \"Category\", values_to = \"Values\") %&gt;%\n  # separate food-related information from the metric\n  tidyr::separate(col = Category, into = c(\"Food Category\", \"Metric\"), sep = ';')\n\nglimpse(long_table)\n\ntidy_table &lt;- long_table %&gt;%\n  tidyr::pivot_wider(names_from = Metric, values_from = Values) %&gt;%\n  janitor::clean_names('snake')\n\nglimpse(tidy_table)\n\nfinal_table &lt;- tidy_table %&gt;%\n  rename(consumption = 3,\n         co2_emmission = 4) %&gt;%\n  filter(!stringr::str_detect(food_category, \"total\"))\n\nclean_table &lt;- final_table %&gt;% \n  mutate_at(vars(consumption, co2_emmission), parse_number)\n\nclean_table %&gt;% \n  write_csv(here::here(\"2020/2020-02-18\", \"food_consumption.csv\"))\n\nclean_table %&gt;% \n  ggplot(aes(x = fct_reorder(food_category, consumption), y = consumption, color = country)) +\n  geom_jitter() +\n  theme(legend.position = \"none\") +\n  coord_flip()"
  },
  {
    "objectID": "data/2020/2020-03-03/readme.html",
    "href": "data/2020/2020-03-03/readme.html",
    "title": "Hockey Goals",
    "section": "",
    "text": "Hockey net on ice\n\n\n\nHockey Goals\nThe data this week comes from HockeyReference.com. We have overall career goals (top_250.csv), season level goals (season_goals.csv), game level goals (game_goals.csv).\nIf you’d like to go beyond season or game-level data, check out MoneyPuck.com for shot-level data or additional game/season level data from 2007-current.\nThis week’s data visualization and article come from the Washington Post. They examined Alex Ovechkin’s career as he broke the 700 career goals mark. “Alexendar the Great” is already top 8 in goals all-time, and is only 34 years old. If he can keep his pace for a few more years, he has a shot at becoming the overall record holder for most goals in a career, potentially breaking Wayne Gretzky’s long-standing career goals record of 894.\n\nGet the data here\n# Get the Data\n\ngame_goals &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-03/game_goals.csv')\n\ntop_250 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-03/top_250.csv')\n\nseason_goals &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-03/season_goals.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-03-03')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 10)\n\n\ngame_goals &lt;- tuesdata$game_goals\n\n\nData Dictionary\n\n\n\ntop_250.csv\nPlease note this is the top 250 goal scorers as found here.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nraw_rank\ndouble\nRank of goals (blank if duplicate)\n\n\nplayer\ncharacter\nPlayer Name\n\n\nyears\ncharacter\nYears active (start - end)\n\n\ntotal_goals\ndouble\nTotal goals scored in the NHL\n\n\nurl_number\ndouble\nNumber for URL\n\n\nraw_link\ncharacter\nRaw player ID\n\n\nlink\ncharacter\nLink to player details on hockeyreference.com\n\n\nactive\ncharacter\nStatus: If still playing = Active, if retired = retired\n\n\nyr_start\ndouble\nFirst year in the NHL\n\n\n\n\n\ngame_goals.csv\nGoals for each player and each game (only for players who started at or after 1979-80 season). This is due to limited game-level data prior to 1980.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nplayer\ncharacter\nPlayer name\n\n\nseason\ndouble\nSeason year\n\n\nrank\ndouble\nRank equivalent to game_num for most\n\n\ndate\ndouble\nDate of game (ISO format)\n\n\ngame_num\ndouble\nGame number within each season\n\n\nage\ncharacter\nAge in year-days\n\n\nteam\ncharacter\nNHL team\n\n\nat\ncharacter\nAt: blank if at home, @ if at the opponent arena\n\n\nopp\ncharacter\nOpponent\n\n\nlocation\ncharacter\nLocation = location of game (home or away)\n\n\noutcome\ncharacter\nOutcome = Won, Loss, Tie\n\n\ngoals\ndouble\nGoals Scored by player\n\n\nassists\ndouble\nAssists - helped with goal for other player\n\n\npoints\ndouble\nPoints - Sum of goals + assists\n\n\nplus_minus\ndouble\nPlus Minus - Team points minus opponents points scored while on ice\n\n\npenalty_min\ndouble\nPenalty minutes - minutes spent in penalty box\n\n\ngoals_even\ndouble\nGoals scored while even-strength\n\n\ngoals_powerplay\ndouble\nGoals scored on powerplay\n\n\ngoals_short\ndouble\nGoals scored while short-handed\n\n\ngoals_gamewinner\ndouble\nGoals that were gamewinner\n\n\nassists_even\ndouble\nAssists while even strength\n\n\nassists_powerplay\ndouble\nAssists on powerplay\n\n\nassists_short\ndouble\nAssists on shorthanded\n\n\nshots\ndouble\nShots\n\n\nshot_percent\ndouble\nShot percent (goals/shots)\n\n\n\n\n\nseason_goals.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrank\ndouble\nOverall goals ranking (1 - 250)\n\n\nposition\ncharacter\nPosition = player position (C = center, RW = Right Wing, LW = left Wing)\n\n\nhand\ncharacter\nDominant hand (left or right)\n\n\nplayer\ncharacter\nPlayer name\n\n\nyears\ncharacter\nSeason years (year-yr)\n\n\ntotal_goals\ndouble\nTotal goals scored in career\n\n\nstatus\ncharacter\nStatus = retired or active\n\n\nyr_start\ndouble\nyear started in NHL\n\n\nseason\ncharacter\nSpecific season for the player\n\n\nage\ndouble\nAge during season\n\n\nteam\ncharacter\nTeam during season\n\n\nleague\ncharacter\nLeague during season\n\n\nseason_games\ndouble\nGames played in the season\n\n\ngoals\ndouble\nGoals scored in the season\n\n\nassists\ndouble\nAssists in the season\n\n\npoints\ndouble\nPoints in the season\n\n\nplus_minus\ndouble\nPlus Minus in the season - Team points minus opponents points scored while on ice\n\n\npenalty_min\ndouble\nPenalty Minutes in the season\n\n\ngoals_even\ndouble\nGoals scored while even strength in a season\n\n\ngoals_power_play\ndouble\nGoals scored on powerplay in a season\n\n\ngoals_short_handed\ndouble\nGoals short handed in a season\n\n\ngoals_game_winner\ndouble\nGoals that were game winner in a season\n\n\nheadshot\ncharacter\nPlayer headshot (URL to image of their head)\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2020/2020-03-17/readme.html",
    "href": "data/2020/2020-03-17/readme.html",
    "title": "The Office - Words and Numbers",
    "section": "",
    "text": "Schrute R package - image of a beet\n\n\n\nThe Office - Words and Numbers\nThe data this week comes from the schrute R package for The Office transcripts and data.world for IMDB ratings of each episode.\nIf you’d like to use the schrute R package for ALL the lines/dialogue from the show - please install it from CRAN via install.packages(\"schrute\"). A quick example from the vignette can be found here.\nIf you want to do text analysis - make sure to check out the tidytext package - a vignette can be found here and the Tidy Text Mining with R book can be found freely online here.\nLastly - the pudding analyzed The Office dialogue across a few charts - their article is here.\n\nGet the data here\n# Get the Data\n\noffice_ratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-17/office_ratings.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-03-17')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 12)\n\n\noffice_ratings &lt;- tuesdata$office_ratings\n\n\nData Dictionary\n\n\n\noffice_ratings.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ndouble\nSeason number\n\n\nepisode\ndouble\nEpisode number\n\n\ntitle\ncharacter\nTitle of episode\n\n\nimdb_rating\ndouble\nIMDB Rating (10 is best)\n\n\ntotal_votes\ndouble\nTotal votes by users\n\n\nair_date\ndate\nOriginal air date\n\n\n\n\n\nschrute data\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nindex\ninteger\nIndex\n\n\nseason\ncharacter\nSeason Number\n\n\nepisode\ncharacter\nSeason episode\n\n\nepisode_name\ncharacter\nEpisode title\n\n\ndirector\ncharacter\nEpisode Director\n\n\nwriter\ncharacter\nEpisode Writer\n\n\ncharacter\ncharacter\nEpisode Character\n\n\ntext\ncharacter\nDialogue as text\n\n\ntext_w_direction\ncharacter\nDialogue as text with direction\n\n\n\n\nCleaning Script\nNo cleaning this week!"
  },
  {
    "objectID": "data/2020/2020-03-31/readme.html",
    "href": "data/2020/2020-03-31/readme.html",
    "title": "Beer Production",
    "section": "",
    "text": "Large brewing kettles\n\n\n\nBeer Production\nThe data this week comes from the Alcohol and Tobacco Tax and Trade Bureau (TTB). H/t to Bart Watson for sharing the source of the data.\nThere’s a literal treasure trove of data here: - State-level beer production by year (2008-2019) - Number of brewers by production size by year (2008-2019) - Monthly beer stats aggregated across the US (2008-2019)\nSome considerations: - A barrel of beer for this data is 31 gallons - Most data is in barrels removed/taxed or produced - Removals = “Total barrels removed subject to tax by the breweries comprising the named strata of data”, essentially how much was produced and removed for consumption. - A LOT of data came from PDFs - I included all the code I used to grab data and tidy it up, take a peek and try out your own mechanism for getting the tables out.\nMassive shoutout to pdftools by ROpenSci and stringr for doing a lot of heavy lifting with the datacleaning and prep here.\n\nGet the data here\n# Get the Data\n\nbrewing_materials &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-31/brewing_materials.csv')\nbeer_taxed &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-31/beer_taxed.csv')\nbrewer_size &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-31/brewer_size.csv')\nbeer_states &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-03-31/beer_states.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-03-31')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 14)\n\n\nbrewing_materials &lt;- tuesdata$brewing_materials\n\n\nData Dictionary\n\n\n\nbrewing_materials.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndata_type\ncharacter\nPounds of Material - this is a sub-table from beer_taxed\n\n\nmaterial_type\ncharacter\nGrain product, Totals, Non-Grain Product (basically hops vs grains)\n\n\nyear\ndouble\nYear\n\n\nmonth\ninteger\nMonth\n\n\ntype\ncharacter\nActual line-item from material type\n\n\nmonth_current\ndouble\nCurrent number of barrels for this year/month\n\n\nmonth_prior_year\ndouble\nPrior year number of barrels for same month\n\n\nytd_current\ndouble\nCumulative year to date of current year\n\n\nytd_prior_year\ndouble\nCumulative year to date for prior year\n\n\n\n\n\nbeer_states.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstate\ncharacter\nState abbreviated\n\n\nyear\ninteger\nYear\n\n\nbarrels\ndouble\nBarrels produced within each type\n\n\ntype\ncharacter\nType of production/use (On premise, Bottles/Cans, Kegs/Barrels)\n\n\n\n\n\nbeer_taxed.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndata_type\ncharacter\nBarrels Produced\n\n\ntax_status\ncharacter\nThe Tax Status, factor with Totals, Taxable, Sub Total Taxable, Tax Free, Sub Total Tax-Free\n\n\nyear\ndouble\nYear\n\n\nmonth\ninteger\nMonth\n\n\ntype\ncharacter\nType of production, either Total Production (Production) or specific sub-category and sub-totals\n\n\nmonth_current\ndouble\nCurrent number of barrels for this year/month\n\n\nmonth_prior_year\ndouble\nPrior year number of barrels for same month\n\n\nytd_current\ndouble\nCumulative year to date of current year\n\n\nytd_prior_year\ndouble\nCumulative year to date for prior year\n\n\n\n\n\nbrewer_size.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nYear\n\n\nbrewer_size\ncharacter\nRange of production for brewer size, number of barrels produced\n\n\nn_of_brewers\ndouble\nNumber of brewers at that brewer size\n\n\ntotal_barrels\ndouble\nTotal barrels of beer produced at that brewer size\n\n\ntaxable_removals\ndouble\nTaxable barrels for removals - removals for consumption under taxation\n\n\ntotal_shipped\ndouble\nTotal barrels shipped - produced beer that is not taxed\n\n\n\n\nCleaning Script\nPlease see download script via download_beer.R\nPlease see cleaning scripts via scrape_beers.R"
  },
  {
    "objectID": "data/2020/2020-04-14/readme.html",
    "href": "data/2020/2020-04-14/readme.html",
    "title": "Rap Artists",
    "section": "",
    "text": "Rap Artists\nThe data this week comes from BBC Music by way of Simon Jockers at Datawrapper.\nThe raw data can be found on his Github. Album covers were retrived from Spotify, and you can access them via the Spotify API.\n\nEarlier this year, BBC Music asked more than 100 critics, artists, and other music industry folks from 15 countries for their five favorite hip-hop tracks. Then they broke down the results of the poll into one definitive list. But BBC Music didn’t just publish a best-of list, they also published the complete poll results and a description of the simple algorithm they ranked the songs with. - Simon Jockers\n\n\nWe awarded 10 points for first ranked track, eight points for second ranked track, and so on down to two points for fifth place. The song with the most points won. We split ties by the total number of votes: songs with more votes ranked higher. Any ties remaining after this were split by first place votes, followed by second place votes and so on: songs with more critics placing them at higher up the lists up ranked higher. – BBC Music\n\n\nGet the data here\n# Get the Data\n\npolls &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-14/polls.csv')\nrankings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-14/rankings.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-04-14')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 16)\n\n\npolls &lt;- tuesdata$polls\n\n\nData Dictionary\n\n\n\npolls.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrank\ndouble\nRank given by voter (1-5)\n\n\ntitle\ncharacter\nTitle of song\n\n\nartist\ncharacter\nArtist\n\n\ngender\ncharacter\nGender of artist\n\n\nyear\ndouble\nYear song released\n\n\ncritic_name\ncharacter\nName of critic\n\n\ncritic_rols\ncharacter\nCritic’s role\n\n\ncritic_country\ncharacter\nCritic’s primary country\n\n\ncritic_country2\ncharacter\nCritic’s secondary country\n\n\n\n\n\nrankings.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nID\ndouble\nID of song\n\n\ntitle\ncharacter\nTitle of song\n\n\nartist\ncharacter\nArtist’s name\n\n\nyear\ndouble\nYear song released\n\n\ngender\ncharacter\nGender of artist/group\n\n\npoints\ndouble\nTotal points awarded\n\n\nn\ndouble\nTotal votes (regardless of position)\n\n\nn1\ndouble\nNumber of votes as #1\n\n\nn2\ndouble\nNumber of votes as #2\n\n\nn3\ndouble\nNumber of votes as #3\n\n\nn4\ndouble\nNumber of votes as #4\n\n\nn5\ndouble\nNumber of votes as #5\n\n\n\n\nCleaning Script\nSimon Jocker’s GitHub"
  },
  {
    "objectID": "data/2020/2020-04-28/readme.html",
    "href": "data/2020/2020-04-28/readme.html",
    "title": "Broadway Weekly Grosses",
    "section": "",
    "text": "Closed red curtain\n\n\n\nBroadway Weekly Grosses\nHuge thanks to Alex Cookson who provided ALL of this week’s data, cleaning script, and readme!\nCheck out his recent blog post on the same data.\nThis data comes from Playbill. Weekly box office grosses comprise data on revenue and attendance figures for theatres that are part of The Broadway League, an industry association for, you guessed it, Broadway theatre.\nCPI data is from the U.S. Bureau of Labor Statistics. There are many, many measures of CPI, so the one used here is “All items less food and energy in U.S. city average, all urban consumers, seasonally adjusted” (table CUSR0000SA0L1E).\nCheck out all of the raw data and other details on Alex’s GitHub\n\nGet the data here\n# Get the Data\n\ngrosses &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-28/grosses.csv', guess_max = 40000)\nsynopses &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-28/synopses.csv')\ncpi &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-28/cpi.csv')\npre_1985_starts &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-04-28/pre-1985-starts.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE the tidytuesdayR version after Jan 2020.\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-04-28')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 18)\n\n\ngrosses &lt;- tuesdata$grosses\n\n\n\nData Dictionary\n\ngrosses.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nweek_ending\ndate\nDate of the end of the weekly measurement period. Always a Sunday.\n\n\nweek_number\ndouble\nWeek number in the Broadway season. The season starts after the Tony Awards, held in early June. Some seasons have 53 weeks.\n\n\nweekly_gross_overall\ndouble\nWeekly box office gross for all shows\n\n\nshow\ncharacter\nName of show. Some shows have the same name, but multiple runs.\n\n\ntheatre\ncharacter\nName of theatre\n\n\nweekly_gross\ndouble\nWeekly box office gross for individual show\n\n\npotential_gross\ndouble\nWeekly box office gross if all seats are sold at full price. Shows can exceed their potential gross by selling premium tickets and/or standing room tickets.\n\n\navg_ticket_price\ndouble\nAverage price of tickets sold\n\n\ntop_ticket_price\ndouble\nHighest price of tickets sold\n\n\nseats_sold\ndouble\nTotal seats sold for all performances and previews\n\n\nseats_in_theatre\ndouble\nTheatre seat capacity\n\n\npct_capacity\ndouble\nPercent of theatre capacity sold. Shows can exceed 100% capacity by selling standing room tickets.\n\n\nperformances\ndouble\nNumber of performances in the week\n\n\npreviews\ndouble\nNumber of preview performances in the week. Previews occur before a show’s official open.\n\n\n\n\n\nsynopses.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nshow\ncharacter\nName of show\n\n\nsynopsis\ncharacter\nPlot synopsis of show. Contains some missing values, especially for shows with multiple runs (due to how the data was collected).\n\n\n\n\n\ncpi.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear_month\ndate\nMonth of CPI value\n\n\ncpi\ndouble\nConsumer Price Index value for the given month\n\n\n\n\n\npre-1985-starts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nweek_ending\ndate\nDate of the end of the weekly measurement period\n\n\nshow\ncharacter\nName of show\n\n\nrun_start_week\ndate\nStarting week for shows that premiered before 1985-06-08 (the start of the dataset)\n\n\n\n\n\nCleaning Script\n# Load packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rvest)\n\n\n# Weekly grosses\n### Create function to scrape grosses table\nget_playbill_data = function(url) {\n  message(url)\n  \n  website &lt;- read_html(url)\n  \n  show_stats &lt;- list(\n    week_number = html_nodes(website, \".week-count .accent\") %&gt;% html_text(trim = TRUE),\n    weekly_gross_overall = html_nodes(website, \".week-total .accent\") %&gt;% html_text(trim = TRUE),\n    show = html_nodes(website, \".col-0 .data-value\") %&gt;% html_text(trim = TRUE),\n    theatre = html_nodes(website, \".col-0 .subtext\") %&gt;% html_text(trim = TRUE),\n    weekly_gross = html_nodes(website, \".col-1 .data-value\") %&gt;% html_text(trim = TRUE),\n    potential_gross = html_nodes(website, \"td.col-1 .subtext\") %&gt;% html_text(trim = TRUE),\n    avg_ticket_price = html_nodes(website, \".col-3 .data-value\") %&gt;% html_text(trim = TRUE),\n    top_ticket_price = html_nodes(website, \"td.col-3 .subtext\") %&gt;% html_text(trim = TRUE),\n    seats_sold = html_nodes(website, \".col-4 .data-value\") %&gt;% html_text(trim = TRUE),\n    seats_in_theatre = html_nodes(website, \"td.col-4 .subtext\") %&gt;% html_text(trim = TRUE),\n    pct_capacity = html_nodes(website, \".col-6 .data-value\") %&gt;% html_text(trim = TRUE),\n    performances = html_nodes(website, \".col-5 .data-value\") %&gt;% html_text(trim = TRUE),\n    previews = html_nodes(website, \"td.col-5 .subtext\") %&gt;% html_text(trim = TRUE)\n  )\n  \n  tibble(show_stats = show_stats) %&gt;%\n    mutate(variable_name = names(show_stats)) %&gt;%\n    pivot_wider(names_from = variable_name, values_from = show_stats) %&gt;%\n    unnest(cols = everything())\n}\n\n\n### Create tibble with list of URLs and scrape data\n### TAKES A LONG TIME (~10 HOURS)\nbroadway_grosses_raw &lt;-\n  tibble(week_ending = seq(ymd(\"1985-06-09\"), ymd(\"2020-03-01\"), by = \"1 week\")) %&gt;%\n  mutate(grosses_url = paste0(\"https://www.playbill.com/grosses?week=\", week_ending)) %&gt;%\n  mutate(week_data = map(grosses_url, possibly(get_playbill_data, NULL, quiet = FALSE)))\n\n### Clean grosses data\nbroadway_grosses &lt;- broadway_grosses_raw %&gt;%\n  unnest(week_data, keep_empty = TRUE) %&gt;%\n  mutate_at(vars(week_number:weekly_gross_overall, weekly_gross:previews),\n            parse_number) %&gt;%\n  mutate(\n    pct_capacity = pct_capacity / 100,\n    show = stringi::stri_trans_general(show, \"Latin-ASCII\")\n  ) %&gt;%\n  mutate_at(vars(potential_gross, top_ticket_price), ~ ifelse(. == 0, NA, .)) %&gt;%\n  select(-grosses_url)\n\n### Write to CSV\nbroadway_grosses %&gt;%\n  write_csv(\"./broadway-grosses/grosses.csv\")\n\n\n\n\n# Show synopses\n### Create function to scrape show synopses\nget_synopsis &lt;- function(url) {\n  message(url)\n  \n  read_html(url) %&gt;%\n    html_nodes(\".spotlight-search-result .bsp-list-promo-desc\") %&gt;%\n    html_text(trim = TRUE)\n}\n\nsynopses_raw &lt;- broadway_grosses %&gt;%\n  distinct(show) %&gt;%\n  mutate(\n    synopsis_url = paste0(\n      \"https://www.playbill.com/searchpage/search?q=\",\n      urltools::url_encode(show),\n      \"&qasset=\"\n    ),\n    synopsis = map(\n      synopsis_url,\n      possibly(get_synopsis, NA_character_, quiet = FALSE)\n    )\n  )\n\n# Clean synopsis data\nsynopses &lt;- synopses_raw %&gt;%\n  select(-synopsis_url) %&gt;%\n  unnest(cols = c(synopsis), keep_empty = TRUE)\n\n### Write to CSV\nsynopses %&gt;%\n  write_csv(\"./broadway-grosses/synopses.csv\")"
  },
  {
    "objectID": "data/2020/2020-05-12/readme.html",
    "href": "data/2020/2020-05-12/readme.html",
    "title": "volcano.csv",
    "section": "",
    "text": "# Volcano Eruptions\nThe data this week comes from The Smithsonian Institution.\nAxios put together a lovely plot of volcano eruptions since Krakatoa (after 1883) by elevation and type.\nFor more information about volcanoes check out the below Wikipedia article or specifically about VEI (Volcano Explosivity Index) see the Wikipedia article here. Lastly, Google Earth has an interactive site on “10,000 Years of Volcanoes”!\nPer Wikipedia:\n\nA volcano is a rupture in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface.\nEarth’s volcanoes occur because its crust is broken into 17 major, rigid tectonic plates that float on a hotter, softer layer in its mantle. Therefore, on Earth, volcanoes are generally found where tectonic plates are diverging or converging, and most are found underwater.\n\n\nErupting volcanoes can pose many hazards, not only in the immediate vicinity of the eruption. One such hazard is that volcanic ash can be a threat to aircraft, in particular those with jet engines where ash particles can be melted by the high operating temperature; the melted particles then adhere to the turbine blades and alter their shape, disrupting the operation of the turbine. Large eruptions can affect temperature as ash and droplets of sulfuric acid obscure the sun and cool the Earth’s lower atmosphere (or troposphere); however, they also absorb heat radiated from the Earth, thereby warming the upper atmosphere (or stratosphere). Historically, volcanic winters have caused catastrophic famines.\n\n\nVEI\nVolcano Explosivity Index:\n\n\n\nVolcano eruptions also can affect the global climate, a Nature Article has open-access data for a specific time-period of eruptions along with temperature anomolies and tree growth. More details can be found from NASA and the UCAR. A summary of the pay-walled Nature article can be found via the Smithsonian\n\nThe researchers detected 238 eruptions from the past 2,500 years, they report today in Nature. About half were in the mid- to high-latitudes in the northern hemisphere, while 81 were in the tropics. (Because of the rotation of the Earth, material from tropical volcanoes ends up in both Greenland and Antarctica, while material from northern volcanoes tends to stay in the north.) The exact sources of most of the eruptions are as yet unknown, but the team was able to match their effects on climate to the tree ring records.\nThe analysis not only reinforces evidence that volcanoes can have long-lasting global effects, but it also fleshes out historical accounts, including what happened in the sixth-century Roman Empire. The first eruption, in late 535 or early 536, injected large amounts of sulfate and ash into the atmosphere. According to historical accounts, the atmosphere had dimmed by March 536, and it stayed that way for another 18 months.\nTree rings, and people of the time, recorded cold temperatures in North America, Asia and Europe, where summer temperatures dropped by 2.9 to 4.5 degrees Fahrenheit below the average of the previous 30 years. Then, in 539 or 540, another volcano erupted. It spewed 10 percent more aerosols into the atmosphere than the huge eruption of Tambora in Indonesia in 1815, which caused the infamous “year without a summer”. More misery ensued, including the famines and pandemics. The same eruptions may have even contributed to a decline in the Maya empire, the authors say.\n\nThere are additional datasets from the Nature article available as Excel files, but they are a bit more complicated - feel free to explore at your own discretion! If you use any of the Nature data, please cite w/ DOI: https://doi.org/10.1038/nature14565.\n\n\nGet the data here\n# Get the Data\n\nvolcano &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-12/volcano.csv')\neruptions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-12/eruptions.csv')\nevents &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-12/events.csv')\ntree_rings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-12/tree_rings.csv')\nsulfur &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-12/sulfur.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n# PLEASE NOTE TO USE 2020 DATA YOU NEED TO USE tidytuesdayR version ? from GitHub\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-05-12')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 20)\n\n\nvolcano &lt;- tuesdata$volcano\n\n\nData Dictionary\n\n\nvolcano.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nvolcano_number\ndouble\nVolcano unique ID\n\n\nvolcano_name\ncharacter\nVolcano name\n\n\nprimary_volcano_type\ncharacter\nVolcano type (see wikipedia above for full details)\n\n\nlast_eruption_year\ncharacter\nLast year erupted\n\n\ncountry\ncharacter\nCountry\n\n\nregion\ncharacter\nRegion\n\n\nsubregion\ncharacter\nSub region\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\nelevation\ndouble\nElevation\n\n\ntectonic_settings\ncharacter\nPlate tectonic settings (subduction, intraplate, rift zone) + crust\n\n\nevidence_category\ncharacter\nType of evidence\n\n\nmajor_rock_1\ncharacter\nMajor rock type\n\n\nmajor_rock_2\ncharacter\nMajor rock type\n\n\nmajor_rock_3\ncharacter\nMajor rock type\n\n\nmajor_rock_4\ncharacter\nMajor rock type\n\n\nmajor_rock_5\ncharacter\nMajor rock type\n\n\nminor_rock_1\ncharacter\nMinor rock type\n\n\nminor_rock_2\ncharacter\nMajor rock type\n\n\nminor_rock_3\ncharacter\nMinor rock type\n\n\nminor_rock_4\ncharacter\nMinor rock type\n\n\nminor_rock_5\ncharacter\nMinor rock type\n\n\npopulation_within_5_km\ndouble\nTotal population within 5 km of volcano\n\n\npopulation_within_10_km\ndouble\nTotal population within 10 km of volcano\n\n\npopulation_within_30_km\ndouble\nTotal population within 30 km of volcano\n\n\npopulation_within_100_km\ndouble\nTotal population within 100 km of volcano\n\n\n\n\n\neruptions.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nvolcano_number\ndouble\nVolcano unique ID\n\n\nvolcano_name\ncharacter\nVolcano name\n\n\neruption_number\ndouble\nEruption number\n\n\neruption_category\ncharacter\nType of eruption\n\n\narea_of_activity\ncharacter\nArea of activity\n\n\nvei\ndouble\nVolcano Explosivity Index (0-8) see wikipedia above\n\n\nstart_year\ndouble\nStart year\n\n\nstart_month\ndouble\nStart month\n\n\nstart_day\ndouble\nStart day\n\n\nevidence_method_dating\ncharacter\nEvidence for dating volcano eruption\n\n\nend_year\ndouble\nEnd year\n\n\nend_month\ndouble\nEnd Month\n\n\nend_day\ndouble\nEnd day\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\n\n\n\nevents.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nvolcano_number\ndouble\nVolcano Unique ID\n\n\nvolcano_name\ncharacter\nVolcano name\n\n\neruption_number\ndouble\nEruption number\n\n\neruption_start_year\ndouble\nEruption start year\n\n\nevent_number\ndouble\nEvent number\n\n\nevent_type\ncharacter\nEvent type\n\n\nevent_remarks\ncharacter\nEvent remarks\n\n\nevent_date_year\ndouble\nEvent year\n\n\nevent_date_month\ndouble\nEvent month\n\n\nevent_date_day\ndouble\nEvent day\n\n\n\n\n\ntree_rings.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nYear of observation CE\n\n\nn_tree\ndouble\nTree ring z-scores relative to year = 1000-1099 (a z-score is a measure of variability from the mean - either positive or negative)\n\n\neurope_temp_index\ndouble\nPages 2K Temperature for Europe in Celsius relative to 1961 to 1990\n\n\n\n\n\nsulfur.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear w/ decimal CE\n\n\nneem\ndouble\nSulfur detected in ng/g from NEEM - ice cores from Greenland, data collected from melting ice cores, data range was 500 to 705 CE\n\n\nwdc\ndouble\nSulfur detected in ng/g from WDC - ice cores from Antartica, data collected from melting ice cores, data range was 500 to 705 CE\n\n\n\n\nCleaning Script\nlibrary(readxl)\nlibrary(tidyverse)\n\neruption_list &lt;- read_csv(\"2020/2020-05-12/eruption_list.csv\", skip = 1) %&gt;% \n  janitor::clean_names() %&gt;% \n  select(-contains(\"modifier\"), -contains(\"uncertainty\"))\n\nevent_list &lt;- read_csv(\"2020/2020-05-12/event_list.csv\", skip = 1) %&gt;% \n  janitor::clean_names() %&gt;% \n  select(-contains(\"modifier\"), -contains(\"uncertainty\"))\n\nvolcano_list &lt;- read_csv(\"2020/2020-05-12/volcano_list.csv\", skip = 1) %&gt;% \n  janitor::clean_names()\n\neruption_list %&gt;% \n  write_csv(\"2020/2020-05-12/eruptions.csv\")\n\nevent_list %&gt;% \n  write_csv(\"2020/2020-05-12/events.csv\")\n\nvolcano_list %&gt;% \n  write_csv(\"2020/2020-05-12/volcano.csv\")"
  },
  {
    "objectID": "data/2020/2020-05-26/readme.html",
    "href": "data/2020/2020-05-26/readme.html",
    "title": "Cocktails",
    "section": "",
    "text": "Img credit: Kobby Mendez\n\nCocktails\nThe data this week comes from Kaggle and Kaggle courtesy of Georgios Karamanis.\nThe Mr Boston dataset was acquired from the Mr. Boston Bartender’s Guide, while the cocktails.csv dataset was web-scraped as part of a hackathon.\nThese datasets are relatively clean, and have lots of interesting data to count, summarize, or possibly classify with a model!\nWhile I have tamed both datasets, the Mr. Boston dataset is IMO cleaner, the cocktails.csv dataset was web-scraped and has some classic “funkiness” that you get from web-scraping (empty rows, now lines \\n, etc). I’ve left both datasets for your exploration, where the cocktails.csv is probably better used for data cleaning/validation and the Mr. Boston appears to be cleaner and close to analysis-ready. However, there are additional columns in the cocktails.csv dataset, so maybe try joining or further cleaning! The cocktails.csv dataset also has more non-alcoholic drinks if you’d prefer to skip the alcohol.\n\nArticles\n\nMargarita Clustering – this doesn’t use our dataset, but provides a potential idea for how to approach this data\n\nInformation is Beautiful Graphic\n\nRecommender for Cocktail recipes\n\n\n\nWarning/Notes\nI’ve intentionally left the measure column as a string with a number + volume/unit so that you can try out potential strategies to cleaning it up.\nSome potential tools:\n- tidyr::separate(): Link\n- stringr::str_split(): Link\nThe cocktails.csv dataset was web-scraped and has some classic “funkiness” that you get from web-scraping (empty rows, new lines \\n with blank, etc)\n\n\nGet the data here\n# Get the Data\n\ncocktails &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-26/cocktails.csv')\nboston_cocktails &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-05-26/boston_cocktails.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-05-26')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 22)\n\n\ncocktails &lt;- tuesdata$cocktails\n\n\nData Dictionary\n\n\n\ncocktails.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrow_id\ndouble\nrow identifier\n\n\ndrink\ncharacter\ndrink name\n\n\ndate_modified\ndouble\ndate modified (web scraped)\n\n\nid_drink\ndouble\ndrink unique id\n\n\nalcoholic\ncharacter\nalcoholic, non alcoholic, optional\n\n\ncategory\ncharacter\nCategory, eg cocktail, shot, etc\n\n\ndrink_thumb\ncharacter\nthumbnail of the drink\n\n\nglass\ncharacter\nRecommended glass type\n\n\niba\ncharacter\nInternational Bartenders association category\n\n\nvideo\nlogical\nVideo to how to make\n\n\ningredient_number\ninteger\nIngredient number\n\n\ningredient\ncharacter\nIngredient\n\n\nmeasure\ncharacter\nMeasurement/volume of ingredient\n\n\n\n\n\nboston_cocktails.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of cocktail\n\n\ncategory\ncharacter\nCategory of cocktail\n\n\nrow_id\ninteger\nDrink identifier\n\n\ningredient_number\ninteger\nIngredient number\n\n\ningredient\ncharacter\nIngredient\n\n\nmeasure\ncharacter\nMeasurement/volume of ingredient\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\n# source for boston drinks\n\"https://www.kaggle.com/jenlooper/mr-boston-cocktail-dataset\"\n\n# Source for drinks\n\"https://www.kaggle.com/ai-first/cocktail-ingredients\"\n\n# Read in the data --------------------------------------------------------\n\ndrinks &lt;- read_csv(\"2020/2020-05-26/all_drinks.csv\") %&gt;% \n  janitor::clean_names() %&gt;% \n  rename(row_id = x1)\n\nboston_drks &lt;- read_csv(\"2020/2020-05-26/mr-boston-flattened.csv\")\n\n\n# pivot_longer drinks -----------------------------------------------------\n\ndrk_ing &lt;- drinks %&gt;% \n  select(row_id:str_iba, contains(\"ingredient\"), str_video) %&gt;% \n  \n  # pivot to take wide data to long\n  pivot_longer(cols = contains(\"ingredient\"), \n               names_to = \"ingredient_number\", \n               values_to = \"ingredient\") %&gt;% \n  # remove text and extract only the digits\n  mutate(ingredient_number = str_extract(ingredient_number, \"[:digit:]+\") %&gt;% \n           as.integer()) %&gt;% \n  # remove \"str_\" from any of the col names\n  set_names(nm = str_remove(names(.), \"str_\")) \n\ndrk_measure &lt;- drinks %&gt;% \n  # select only the join ids and cols w/ \"measure\"\n  select(row_id, str_drink, id_drink, contains(\"measure\")) %&gt;% \n  # pivot to take wide data to long\n  pivot_longer(cols = contains(\"measure\"), \n               names_to = \"measure_number\", \n               values_to = \"measure\") %&gt;% \n  # extract just digits\n  mutate(measure_number = str_extract(measure_number, \"[:digit:]+\") %&gt;% \n           as.integer()) %&gt;% \n  # remove str_ from any col names\n  set_names(nm = str_remove(names(.), \"str_\"))\n\n# join the two long dfs back together\nall_drks &lt;- left_join(drk_ing, drk_measure, \n                      by = c(\"row_id\", \"drink\", \"id_drink\", \n                             \"ingredient_number\" = \"measure_number\")) %&gt;% \n  filter(!is.na(measure) & !is.na(ingredient))\n\n# confirm if missing data\n# confirm if missing data\nanti_join(drk_ing, drk_measure, \n          by = c(\"row_id\", \"drink\", \"id_drink\", \n                 \"ingredient_number\" = \"measure_number\"))\n\nwrite_csv(all_drks, \"2020/2020-05-26/cocktails.csv\")\n\n# pivot_longer boston drinks ----------------------------------------------\n\nbs_drk_ing &lt;- boston_drks %&gt;% \n  mutate(row_id = row_number()) %&gt;% \n  select(name, category, row_id, contains(\"ingredient\")) %&gt;% \n  pivot_longer(cols = contains(\"ingredient\"), \n               names_to = \"ingredient_number\", \n               values_to = \"ingredient\") %&gt;% \n  mutate(ingredient_number = str_extract(ingredient_number, \"[:digit:]+\") %&gt;% \n           as.integer())\n\n\nbs_drk_ms &lt;- boston_drks %&gt;% \n  mutate(row_id = row_number()) %&gt;% \n  select(name, category, row_id, contains(\"measurement\")) %&gt;% \n  pivot_longer(cols = contains(\"measurement\"), \n               names_to = \"measure_number\", \n               values_to = \"measure\") %&gt;% \n  mutate(measure_number = str_extract(measure_number, \"[:digit:]+\") %&gt;% \n           as.integer())\n\nall_bs_drks &lt;- left_join(bs_drk_ing, bs_drk_ms, \n                         by = c(\"name\", \"category\", \"row_id\", \n                                \"ingredient_number\" = \"measure_number\")) %&gt;% \n  filter(!is.na(ingredient) & !is.na(measure))\n\n# confirm if missing data\nanti_join(bs_drk_ing, bs_drk_ms, \n          by = c(\"name\", \"category\", \"row_id\", \n                 \"ingredient_number\" = \"measure_number\"))\n\nwrite_csv(all_bs_drks, \"2020/2020-05-26/boston_cocktails.csv\")"
  },
  {
    "objectID": "data/2020/2020-06-09/readme.html",
    "href": "data/2020/2020-06-09/readme.html",
    "title": "African American Achievements",
    "section": "",
    "text": "David Blackwell\n\n\n\nAfrican American Achievements\nThe data this week comes from Wikipedia & Wikipedia. This will be a celebration of Black Lives, their achievements, and many of their battles against racism across their lives. This is in emphasis that Black Lives Matter and we’re focusing on a celebration of these lives. Each of those Wikipedia articles above have additional details and sub-links that are highly worth reading through.\nFor additional datasets related to describing racial problems that still exist in the US, please see a few of our previous #TidyTuesday datasets:\n- Note, if you decide to use these datasets please read through the source material to better understand the nuance behind the data.\n- School Diversity\n- Vera Institute Incarceration Trends\n- The Stanford Open Policing Project\nThe article for this week is the obituary for David Blackwell - Fought racism; became world famous statistician.\nThere is currently a Petition to rename the Fisher Lectureship after David Blackwell.\n\nThe R.A. Fisher Lectureship, established in 1963, is awarded annually to a statistician in recognition of outstanding contributions to aspects of statistics and probability that closely relate to the scientific collection and interpretation of data. Fisher was a prominent proponent of Eugenics and additionally: In 1950, Fisher opposed UNESCO’s The Race Question, believing that evidence and everyday experience showed that human groups differ profoundly “in their innate capacity for intellectual and emotional development” and concluded that the “practical international problem is that of learning to share the resources of this planet amicably with persons of materially different nature”, and that “this problem is being obscured by entirely well-intentioned efforts to minimize the real differences that exist”. The revised statement titled “The Race Concept: Results of an Inquiry” (1951) was accompanied by Fisher’s dissenting commentary.\nBy honoring Fisher we dishonor the entire field of Statistics.\n\nPlease consider contributing to this petition.\nWe’d also like to take the chance to highlight a few potential projects to support or get involved with:\n\n\nData for Black Lives\n\nData for Black Lives is a movement of activists, organizers, and mathematicians committed to the mission of using data science to create concrete and measurable change in the lives of Black people. Since the advent of computing, big data and algorithms have penetrated virtually every aspect of our social and economic lives. These new data systems have tremendous potential to empower communities of color. Tools like statistical modeling, data visualization, and crowd-sourcing, in the right hands, are powerful instruments for fighting bias, building progressive movements, and promoting civic engagement.\nBut history tells a different story, one in which data is too often wielded as an instrument of oppression, reinforcing inequality and perpetuating injustice. Redlining was a data-driven enterprise that resulted in the systematic exclusion of Black communities from key financial services. More recent trends like predictive policing, risk-based sentencing, and predatory lending are troubling variations on the same theme. Today, discrimination is a high-tech enterprise.\n\n\n\nBlack Girls Code\n\nBlack Girls CODE is devoted to showing the world that black girls can code, and do so much more. By reaching out to the community through workshops and after school programs, Black Girls CODE introduces computer coding lessons to young girls from underrepresented communities in programming languages such as Scratch or Ruby on Rails. Black Girls CODE has set out to prove to the world that girls of every color have the skills to become the programmers of tomorrow. By promoting classes and programs we hope to grow the number of women of color working in technology and give underprivileged girls a chance to become the masters of their technological worlds. Black Girls CODE’s ultimate goal is to provide African-American youth with the skills to occupy some of the 1.4 million computing job openings expected to be available in the U.S. by 2020, and to train 1 million girls by 2040.\n\n\n\nBlack in AI\n\nBlack in AI (BAI) is a multi-institutional, transcontinental initiative designed to create a place for sharing ideas, fostering collaborations, and discussing initiatives to increase the presence of Black individuals in the field of AI. To this end, we hold an annual technical workshop series, run mentoring programs, and maintain various fora for fostering partnerships and collaborations.\nIf you are in the field of AI and self-identify as Black, please fill out this Google Form to request to join. Note, due to the volume of requests, there may be delays in processing your application.\nWe also welcome allies to join our group using the Google form. Allies will be added to our email lists, where we send out group updates and requests for assistance.\n\nThe firsts.csv dataset has 479 records of African-Americans breaking the color barrier across a wide range of topics. I’ve adapted the raw text from Wikipedia to highlight: - The year of the event\n- The role/action/topic\n- The person or people involved\n- Addition of gender\n- A category of topics\nThe science.csv dataset also celebrates the achievements of African-Americans, specifically related to Patents and Scientific achievements. One of the amazing scientists present in this dataset is David Blackwell - an African-American Mathematician and Statistician with significant contributions to game theory, probability theory, information theory, and Bayesian statistics. There is currently a proposal to rename the Fisher Lectureship award after David Blackwell as mentioned above..\n\nGet the data here\n# Get the Data\n\nfirsts &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-09/firsts.csv')\nscience &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-09/science.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-06-09')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 24)\n\n\nfirsts &lt;- tuesdata$firsts\n\n\nData Dictionary\n\n\n\nfirsts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nYear of the achievement\n\n\naccomplishment\ncharacter\naccomplishment - the actual achievement or attainment\n\n\nperson\ncharacter\nThe person or persons who accomplished the specific accomplishment\n\n\ngender\ncharacter\nGender - indicates either female AND African-American, or a more general African-American first\n\n\ncategory\ncharacter\nA few meta-categories of different accomplishments\n\n\n\n\n\nscience.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of scientist/inventor\n\n\nbirth\ninteger\nBirth year\n\n\ndeath\ninteger\nDeath year (NA if still alive)\n\n\noccupation_s\ncharacter\nOccupation (1 or more occupation, separated by a ;)\n\n\ninventions_accomplishments\ncharacter\nInventions or accomplishment\n\n\nreferences\ncharacter\nReferences to articles\n\n\nlinks\ncharacter\nLinks to their Wikipedia page (contains images and more information)\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(rvest)\n\n# URL 1\nurl_science &lt;- \"https://en.wikipedia.org/wiki/List_of_African-American_inventors_and_scientists\"\n\nraw_html_sci &lt;- read_html(url_science)\n\nget_source &lt;- function(x){\n  raw_html_sci %&gt;% \n    html_nodes(\"tbody\") %&gt;% \n    .[[2]] %&gt;% \n    html_nodes(glue::glue(\"tr:nth-child({x})\")) %&gt;% \n    html_nodes(\"td:nth-child(1) &gt; a\") %&gt;% \n    html_attr(\"href\")\n}\n\nraw_sci_tab &lt;- raw_html_sci %&gt;% \n  html_table() %&gt;% \n  .[[2]] %&gt;% \n  janitor::clean_names() %&gt;% \n  as_tibble() %&gt;% \n  mutate(links = map(row_number(), get_source))\n\nclean_sci &lt;- raw_sci_tab %&gt;% \n  mutate(references = str_replace_all(references, \"\\\\]\", \",\"),\n         references = str_remove_all(references, \"\\\\[\")) %&gt;% \n  unnest(links) %&gt;% \n  mutate(links = paste0(\"https://en.wikipedia.org\", links)) %&gt;% \n  separate(years, into = c(\"birth\", \"death\"), sep = \"–\") %&gt;% \n  mutate(across(c(birth, death), as.integer)) %&gt;% \n  mutate(occupation_s = str_replace_all(occupation_s, \",\", \";\"))\n\nclean_sci %&gt;% \n  filter(str_detect(tolower(occupation_s), \"statistician\"))\n\n\nsci_citations &lt;- raw_html_sci %&gt;% \n  html_node(\"#mw-content-text &gt; div &gt; div.reflist &gt; div\") %&gt;% \n  html_nodes(\"li\") %&gt;% \n  html_text() %&gt;% \n  str_remove(\"\\\\^ \") %&gt;% \n  enframe() %&gt;% \n  rename(citation_num = name, citation = value) %&gt;% \n  mutate(citation = str_replace_all(citation, \"\\\"\", \"'\"),\n         citation = str_remove_all(citation, \"\\\\n\"))\n\nsci_citations\n\nclean_sci %&gt;%\n  add_row(tibble(\n    name = \"Amos, Harold\", birth = 1918, death = 2003, occupation_s = \"Microbiologist\",\n    inventions_accomplishments = \"First African-American department chair at Harvard Medical School\",\n    references = \"6,\", links = \"https://en.wikipedia.org/wiki/Harold_Amos\"), .before = 1\n  ) %&gt;% \n  write_csv(path = \"2020/2020-06-09/science.csv\")\n\nscience &lt;- read_csv(\"2020/2020-06-09/science.csv\")\n\n\n# Firsts ------------------------------------------------------------------\n\n\n\nfirst_url &lt;- \"https://en.wikipedia.org/wiki/List_of_African-American_firsts\"\n\nraw_first &lt;- read_html(first_url)\n\nget_year &lt;- function(id_num) {\n  raw_first %&gt;% \n    html_nodes(glue::glue(\"#mw-content-text &gt; div &gt; h4:nth-child({id_num}) &gt; span\")) %&gt;% \n    html_attr(\"id\") %&gt;% \n    .[!is.na(.)]\n}\n\nget_first &lt;- function(id_num){\n  raw_first %&gt;% \n    html_nodes(glue::glue(\"#mw-content-text &gt; div &gt; ul:nth-child({id_num})\")) %&gt;% \n    html_text() %&gt;% \n    str_split(\"\\n\")\n}\n\ntidyr::crossing(id_num = 9:389, count = 1:5)\n\nraw_first_df &lt;- tibble(id_num = 9:390) %&gt;% \n  mutate(year = map(id_num, get_year),\n         text = map(id_num, get_first))\n\nclean_first &lt;- raw_first_df %&gt;% \n  mutate(year = as.integer(year)) %&gt;% \n  fill(year) %&gt;% \n  unnest(text) %&gt;% \n  unnest(text) %&gt;% \n  separate(text, into = c(\"role\", \"person\"), sep = \": \") %&gt;% \n  mutate(person = str_remove_all(person, \"\\\\\\\\\"),\n         person = str_trim(person),\n         role = str_replace(role, \"African American\", \"African-American\")) %&gt;% \n  select(year, role, person)\n\nclean_first %&gt;% \n  group_by(year) %&gt;% \n  summarize(n =n())\n\nfirst_role &lt;- function(category){\n  str_detect(tolower(role), category)\n}\n\nedu &lt;- paste0(c(\n  \"practice\", \"graduate\", \"learning\", \"college\", \"university\", \"medicine\",\n  \"earn\", \"ph.d.\", \"professor\", \"teacher\", \"school\", \"nobel\", \"invent\", \"patent\",\n  \"medicine\", \"degree\", \"doctor\", \"medical\", \"nurse\", \"physician\", \"m.d.\", \"b.a.\", \"b.s.\", \"m.b.a\",\n  \"principal\", \"space\", \"astronaut\"\n), collapse = \"|\")\n\nreligion &lt;- c(\"bishop\", \"rabbi\", \"minister\", \"church\", \"priest\", \"pastor\", \"missionary\",\n              \"denomination\", \"jesus\", \"jesuits\", \"diocese\", \"buddhis\") %&gt;%\n  paste0(collapse = \"|\")\n\npolitics &lt;- c(\n  \"diplomat\", \"elected\", \"nominee\", \"supreme court\", \"legislature\", \"mayor\", \"governor\",\n  \"vice President\", \"president\", \"representatives\", \"political\", \"department\", \"peace prize\",\n  \"ambassador\", \"government\", \"white house\", \"postal\", \"federal\", \"union\", \"trade\",\n  \"delegate\", \"alder\", \"solicitor\", \"senator\", \"intelligience\", \"combat\", \"commissioner\",\n  \"state\", \"first lady\", \"cabinet\", \"advisor\", \"guard\", \"coast\", \"secretary\", \"senate\",\n  \"house\", \"agency\", \"staff\", \"national committee\"\n) %&gt;%\n  paste0(collapse = \"|\")\n\nsports &lt;- c(\n  \"baseball\", \"football\", \"basketball\", \"hockey\", \"golf\", \"tennis\",\n  \"championship\", \"boxing\", \"games\", \"medal\", \"game\", \"sport\", \"olympic\", \"nascar\",\n  \"coach\", \"trophy\", \"nba\", \"nhl\", \"nfl\", \"mlb\", \"stanley cup\", \"jockey\", \"pga\",\n  \"race\", \"driver\", \"ufc\", \"champion\"\n) %&gt;%\n  paste0(collapse = \"|\")\n\nmilitary &lt;- c(\n  \"serve\", \"military\", \"enlist\", \"officer\", \"army\", \"marine\", \"naval\",\n  \"officer\", \"captain\", \"command\", \"admiral\", \"prison\", \"navy\", \"general\",\n  \"force\"\n) %&gt;%\n  paste0(collapse = \"|\")\n\nlaw &lt;- c(\"american bar\", \"lawyer\", \"police\", \"judge\", \"attorney\", \"law\", \n         \"agent\", \"fbi\") %&gt;%\n  paste0(collapse = \"|\")\n\narts &lt;- c(\n  \"opera\", \"sing\", \"perform\", \"music\", \"billboard\", \"oscar\", \"television\",\n  \"movie\", \"network\", \"tony award\", \"paint\", \"author\", \"book\", \"academy award\", \"curator\",\n  \"director\", \"publish\", \"novel\", \"grammy\", \"emmy\", \"smithsonian\",\n  \"conduct\", \"picture\", \"pulitzer\", \"channel\", \"villain\", \"cartoon\", \"tv\", \"golden globe\",\n  \"comic\", \"magazine\", \"superhero\", \"pulitzer\", \"dancer\", \"opry\", \"rock and roll\", \"radio\",\n  \"record\") %&gt;%\n  paste0(collapse = \"|\")\n\nsocial &lt;- c(\"community\", \"freemasons\", \"vote\", \"voting\", \"rights\", \"signature\", \n            \"royal\", \"ceo\", \"community\", \"movement\", \"invited\", \"greek\", \"million\",\n            \"billion\", \"attendant\", \"chess\", \"pilot\", \"playboy\", \"own\", \"daughter\",\n            \"coin\", \"dollar\", \"stamp\", \"niagara\",\n            \"stock\", \"north pole\", \"reporter\", \"sail around the world\", \"press\", \"miss \",\n            \"everest\")  %&gt;%\n  paste0(collapse = \"|\")\n\nfirst_df &lt;- clean_first %&gt;% \n  mutate(gender = if_else(str_detect(role, \"woman|Woman|her|she|female\"), \n                          \"Female African American Firsts\", \"African-American Firsts\"),\n         role = str_remove_all(role, \"\\\"\"),\n         person = str_remove_all(person, \"\\\"\"),\n         category = case_when(\n           str_detect(tolower(role), military) ~ \"Military\",\n           str_detect(tolower(role), law) ~ \"Law\",\n           str_detect(tolower(role), arts) ~ \"Arts & Entertainment\",\n           str_detect(tolower(role), social) ~ \"Social & Jobs\",\n           str_detect(tolower(role), religion) ~ \"Religion\",\n           str_detect(tolower(role), edu) ~ \"Education & Science\",\n           str_detect(tolower(role), politics) ~ \"Politics\",\n           str_detect(tolower(role), sports) ~ \"Sports\",\n           TRUE ~ NA_character_\n         )) %&gt;% \n  rename(accomplishment = role)\n\nfirst_df %&gt;% write_csv(path = \"2020/2020-06-09/firsts.csv\")\n\nfirsts &lt;- read_csv(\"2020/2020-06-09/firsts.csv\")\n\nplot_ex &lt;- first_df %&gt;% \n  mutate(n = 1) %&gt;% \n  group_by(category) %&gt;% \n  mutate(roll_n = cumsum(n)) %&gt;% \n  ggplot(aes(x = year, y = roll_n, color = category)) +\n  geom_step(size = 1) +\n  theme(legend.position = \"top\") +\n  tomtom::theme_tomtom() +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6)) +\n  scale_x_continuous(breaks = seq(1750, 2020, 25)) +\n  geom_hline(yintercept = 0, size = 1, color = \"black\") +\n  labs(x = \"\", y = \"\",\n       title = \"Cumulative African-Americans firsts over time\",\n       subtitle = \"479 'Firsts' of African-Americans breaking the color barrier across a wide range of topics\",\n       caption = \"Data: wikipedia.org/wiki/List_of_African-American_firsts\")\n\nggsave(\"2020/2020-06-09/pic2.png\", plot_ex, height = 8, width = 14, units = \"in\", dpi = \"retina\")"
  },
  {
    "objectID": "data/2020/2020-06-23/readme.html",
    "href": "data/2020/2020-06-23/readme.html",
    "title": "Caribou Location Tracking",
    "section": "",
    "text": "Caribou Photo\n\n\n\nCaribou Location Tracking\nThis data tracks woodland caribou in northern British Columbia, a Canadian province. It consists of almost 250,000 location tags of 260 caribou, from 1988 to 2016.\nh/t to Alex Cookson for preparing this week’s data!\nThe tracking was part of a study prepared in 2014 by the B.C. Ministry of Environment & Climate Change to inform the management and recovery of the species, which is classified as “Vulnerable” on the International Union for the Conservation of Nature’s (IUCN) Red List.\nThe caribou range in North America/Canada is visualized courtesy of Wikipedia.\n\n\n\nRangifer tarandus Map NA\n\n\nData was accessed through Movebank, “a free online platform that helps researchers manage, share, analyze and archive animal movement data.”\nOriginal article citation\n\nBC Ministry of Environment (2014) Science update for the South Peace Northern Caribou (Rangifer tarandus caribou pop. 15) in British Columbia. Victoria, BC. 43 p. url:https://www2.gov.bc.ca/assets/gov/environment/plants-animals-and-ecosystems/wildlife-wildlife-habitat/caribou/science_update_final_from_web_jan_2014.pdf\n\nData package citation\n\nSeip DR, Price E (2019) Data from: Science update for the South Peace Northern Caribou (Rangifer tarandus caribou pop. 15) in British Columbia. Movebank Data Repository. https://doi.org/10.5441/001/1.p5bn656k\n\n\nGet the data here\n# Get the Data\n\nindividuals &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-23/individuals.csv')\nlocations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-06-23/locations.csv')\n\n# Or read in with tidytuesdayR package (https://github.com/dslc-io/tidytuesdayR)\n\n# Either ISO-8601 date or year/week works!\n\n# Install via pak::pak(\"dslc-io/tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-06-23')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 26)\n\n\nindividuals &lt;- tuesdata$individuals\n\n\n\nData Dictionary\n“Deployment” refers to when the animal was fitted with a location-tracking tag.\n\nindividuals.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nanimal_id\ncharacter\nIndividual identifier for animal\n\n\nsex\ncharacter\nSex of animal\n\n\nlife_stage\ncharacter\nAge class (in years) at beginning of deployment\n\n\npregnant\nlogical\nWhether animal was pregnant at beginning of deployment\n\n\nwith_calf\nlogical\nWhether animal had a calf at time of deployment\n\n\ndeath_cause\ncharacter\nCause of death\n\n\nstudy_site\ncharacter\nDeployment site or colony, or a location-related group such as the herd or pack name\n\n\ndeploy_on_longitude\ndouble\nLongitude where animal was released at beginning of deployment\n\n\ndeploy_on_latitude\ndouble\nLatitude where animal was released at beginning of deployment\n\n\ndeploy_on_comments\ncharacter\nAdditional information about tag deployment\n\n\ndeploy_off_longitude\ndouble\nLongitude where deployment ended\n\n\ndeploy_off_latitude\ndouble\nLatitude where deployment ended\n\n\ndeploy_off_type\ncharacter\nClassification of tag deployment end (see table below for full description)\n\n\ndeploy_off_comments\ncharacter\nAdditional information about tag deployment end\n\n\n\ndeploy_off_type classifications\n\n\n\n\n\n\n\ndeploy_off_type\ndescription\n\n\n\n\ncaptured\nTag remained on the animal but the animal was captured or confined\n\n\ndead\nDeployment ended with the death of the animal that was carrying the tag\n\n\nequipment failure\nTag stopped working\n\n\nfall off\nAttachment of the tag to the animal failed, and it fell of accidentally\n\n\nother\nCatch-all category for other deployment end types\n\n\nreleased\nTag remained on the animal but the animal was released from captivity or confinement\n\n\nremoval\nTag was purposefully removed from the animal\n\n\nunknown\nDeployment ended by an unknown cause\n\n\n\n\n\nlocations.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nevent_id\ndouble\nIdentifier for an individual measurement\n\n\nanimal_id\ncharacter\nIndividual identifier for animal\n\n\nstudy_site\ncharacter\nDeployment site or colony, or a location-related group such as the herd or pack name\n\n\nseason\ncharacter\nSeason (Summer/Winter) at time of measurement\n\n\ntimestamp\ndatetime\nDate and time of measurement\n\n\nlongitude\ndouble\nLongitude of measurement\n\n\nlatitude\ndouble\nLatitude of measurement\n\n\n\n\n\nCleaning Script\n# Load libraries\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Import data\nindividuals_raw &lt;- read_csv(\"./caribou-location-tracking/raw/Mountain caribou in British Columbia-reference-data.csv\")\nlocations_raw &lt;- read_csv(\"./caribou-location-tracking/raw/Mountain caribou in British Columbia-gps.csv\")\n\n# Clean individuals\nindividuals &lt;- individuals_raw %&gt;%\n  clean_names() %&gt;%\n  transmute(animal_id,\n            sex = animal_sex,\n            # Getting rid of whitespace to address inconsistent spacing\n            # NOTE: life stage is as of the beginning of deployment\n            life_stage = str_remove_all(animal_life_stage, \" \"),\n            reproductive_condition = animal_reproductive_condition,\n            # Cause of death \"cod\" is embedded in a comment field\n            death_cause = str_remove(animal_death_comments, \".*cod \"),\n            study_site,\n            deploy_on_longitude,\n            deploy_on_latitude,\n            # Renaming to maintain consistency \"deploy_on_FIELD\" and \"deploy_off_FIELD\"\n            deploy_on_comments = deployment_comments,\n            deploy_off_longitude,\n            deploy_off_latitude,\n            deploy_off_type = deployment_end_type,\n            deploy_off_comments = deployment_end_comments) %&gt;%\n  # reproductive_condition actually has two dimensions\n  separate(reproductive_condition, into = c(\"pregnant\", \"with_calf\"), sep = \";\", fill = \"left\") %&gt;%\n  mutate(pregnant = str_remove(pregnant, \"pregnant: ?\"),\n         with_calf = str_remove(with_calf, \"with calf: ?\")) %&gt;%\n  # TRUE and FALSE are indicated by Yes/No or Y/N\n  mutate_at(vars(pregnant:with_calf), ~ case_when(str_detect(., \"Y\") ~ TRUE,\n                                                   str_detect(., \"N\") ~ FALSE,\n                                                   TRUE ~ NA))\n\n# Clean locations\nlocations &lt;- locations_raw %&gt;%\n  clean_names() %&gt;%\n  transmute(event_id,\n            animal_id = individual_local_identifier,\n            study_site = comments,\n            season = study_specific_measurement,\n            timestamp,\n            longitude = location_long,\n            latitude = location_lat)\n\n# Write to CSV\nwrite_csv(individuals, \"./caribou-location-tracking/individuals.csv\")\nwrite_csv(locations, \"./caribou-location-tracking/locations.csv\")"
  },
  {
    "objectID": "data/2020/2020-07-07/readme.html",
    "href": "data/2020/2020-07-07/readme.html",
    "title": "Coffee ratings",
    "section": "",
    "text": "Coffee beans - Mae Mu @picoftasty\n\n\n\nCoffee ratings\nThe data this week comes from Coffee Quality Database courtesy of Buzzfeed Data Scientist James LeDoux. The original data can be found on James’ github. The data was re-posted to Kaggle.\n“These data were collected from the Coffee Quality Institute’s review pages in January 2018.”\nThrillist has an article on the top coffee-producing countries.\nYorgos Askalidis analyzed this data as well.\nThere is data for both Arabica and Robusta beans, across many countries and professionally rated on a 0-100 scale. All sorts of scoring/ratings for things like acidity, sweetness, fragrance, balance, etc - may be useful for either separating into visualizations/categories or for modeling/recommenders.\nWikipedia on Coffee Beans:\n\nThe two most economically important varieties of coffee plant are the Arabica and the Robusta; ~60% of the coffee produced worldwide is Arabica and ~40% is Robusta. Arabica beans consist of 0.8–1.4% caffeine and Robusta beans consist of 1.7–4% caffeine.\n\nWiki on Cupping\n\nCoffee cupping, or coffee tasting, is the practice of observing the tastes and aromas of brewed coffee. It is a professional practice but can be done informally by anyone or by professionals known as “Q Graders”. A standard coffee cupping procedure involves deeply sniffing the coffee, then loudly slurping the coffee so it spreads to the back of the tongue. The coffee taster attempts to measure aspects of the coffee’s taste, specifically the body (the texture or mouthfeel, such as oiliness), sweetness, acidity (a sharp and tangy feeling, like when biting into an orange), flavour (the characters in the cup), and aftertaste. Since coffee beans embody telltale flavours from the region where they were grown, cuppers may attempt to identify the coffee’s origin.\n\nImportantly - there is the concept of ethical or Fair Trade coffee - we’ll be covering more of the production numbers of Coffee in a future dataset.\n\nBased on the simple idea that the products bought and sold every day are connected to the livelihoods of others, fair trade is a way to make a conscious choice for a better world.\n\nFair Trade Coffee definition from Wikipedia:\n\nFair trade coffee is coffee that is certified as having been produced to fair trade standards by fair trade organizations, which create trading partnerships that are based on dialogue, transparency and respect, with the goal of achieving greater equity in international trade. These partnerships contribute to sustainable development by offering better trading conditions to coffee bean farmers. Fair trade organizations support producers and sustainable environmental farming practices and prohibit child labor or forced labor.\n\nIf you’re looking to buy some coffee - check out this list of 12 Black-Owned Coffee Brands.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n\ncoffee_ratings &lt;- tuesdata$coffee_ratings\n\n# Or read in the data manually\n\ncoffee_ratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-07-07/coffee_ratings.csv')\n\n\nData Dictionary\n\n\n\ncoffee_ratings.csv\nNote full description/examples at: Coffee Quality Institute\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntotal_cup_points\ndouble\nTotal rating/points (0 - 100 scale)\n\n\nspecies\ncharacter\nSpecies of coffee bean (arabica or robusta)\n\n\nowner\ncharacter\nOwner of the farm\n\n\ncountry_of_origin\ncharacter\nWhere the bean came from\n\n\nfarm_name\ncharacter\nName of the farm\n\n\nlot_number\ncharacter\nLot number of the beans tested\n\n\nmill\ncharacter\nMill where the beans were processed\n\n\nico_number\ncharacter\nInternational Coffee Organization number\n\n\ncompany\ncharacter\nCompany name\n\n\naltitude\ncharacter\nAltitude - this is a messy column - I’ve left it for some cleaning\n\n\nregion\ncharacter\nRegion where bean came from\n\n\nproducer\ncharacter\nProducer of the roasted bean\n\n\nnumber_of_bags\ndouble\nNumber of bags tested\n\n\nbag_weight\ncharacter\nBag weight tested\n\n\nin_country_partner\ncharacter\nPartner for the country\n\n\nharvest_year\ncharacter\nWhen the beans were harvested (year)\n\n\ngrading_date\ncharacter\nWhen the beans were graded\n\n\nowner_1\ncharacter\nWho owns the beans\n\n\nvariety\ncharacter\nVariety of the beans\n\n\nprocessing_method\ncharacter\nMethod for processing\n\n\naroma\ndouble\nAroma grade\n\n\nflavor\ndouble\nFlavor grade\n\n\naftertaste\ndouble\nAftertaste grade\n\n\nacidity\ndouble\nAcidity grade\n\n\nbody\ndouble\nBody grade\n\n\nbalance\ndouble\nBalance grade\n\n\nuniformity\ndouble\nUniformity grade\n\n\nclean_cup\ndouble\nClean cup grade\n\n\nsweetness\ndouble\nSweetness grade\n\n\ncupper_points\ndouble\nCupper Points\n\n\nmoisture\ndouble\nMoisture Grade\n\n\ncategory_one_defects\ndouble\nCategory one defects (count)\n\n\nquakers\ndouble\nquakers\n\n\ncolor\ncharacter\nColor of bean\n\n\ncategory_two_defects\ndouble\nCategory two defects (count)\n\n\nexpiration\ncharacter\nExpiration date of the beans\n\n\ncertification_body\ncharacter\nWho certified it\n\n\ncertification_address\ncharacter\nCertification body address\n\n\ncertification_contact\ncharacter\nCertification contact\n\n\nunit_of_measurement\ncharacter\nUnit of measurement\n\n\naltitude_low_meters\ndouble\nAltitude low meters\n\n\naltitude_high_meters\ndouble\nAltitude high meters\n\n\naltitude_mean_meters\ndouble\nAltitude mean meters\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nraw_arabica &lt;- read_csv(\"https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv\") %&gt;% \n  janitor::clean_names()\n\nraw_robusta &lt;- read_csv(\"https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/robusta_data_cleaned.csv\",\n                        col_types = cols(\n                          X1 = col_double(),\n                          Species = col_character(),\n                          Owner = col_character(),\n                          Country.of.Origin = col_character(),\n                          Farm.Name = col_character(),\n                          Lot.Number = col_character(),\n                          Mill = col_character(),\n                          ICO.Number = col_character(),\n                          Company = col_character(),\n                          Altitude = col_character(),\n                          Region = col_character(),\n                          Producer = col_character(),\n                          Number.of.Bags = col_double(),\n                          Bag.Weight = col_character(),\n                          In.Country.Partner = col_character(),\n                          Harvest.Year = col_character(),\n                          Grading.Date = col_character(),\n                          Owner.1 = col_character(),\n                          Variety = col_character(),\n                          Processing.Method = col_character(),\n                          Fragrance...Aroma = col_double(),\n                          Flavor = col_double(),\n                          Aftertaste = col_double(),\n                          Salt...Acid = col_double(),\n                          Balance = col_double(),\n                          Uniform.Cup = col_double(),\n                          Clean.Cup = col_double(),\n                          Bitter...Sweet = col_double(),\n                          Cupper.Points = col_double(),\n                          Total.Cup.Points = col_double(),\n                          Moisture = col_double(),\n                          Category.One.Defects = col_double(),\n                          Quakers = col_double(),\n                          Color = col_character(),\n                          Category.Two.Defects = col_double(),\n                          Expiration = col_character(),\n                          Certification.Body = col_character(),\n                          Certification.Address = col_character(),\n                          Certification.Contact = col_character(),\n                          unit_of_measurement = col_character(),\n                          altitude_low_meters = col_double(),\n                          altitude_high_meters = col_double(),\n                          altitude_mean_meters = col_double()\n                        )) %&gt;% \n  janitor::clean_names() %&gt;% \n  rename(acidity = salt_acid, sweetness = bitter_sweet,\n         aroma = fragrance_aroma, body = mouthfeel,uniformity = uniform_cup)\n\n\nall_ratings &lt;- bind_rows(raw_arabica, raw_robusta) %&gt;% \n  select(-x1) %&gt;% \n  select(total_cup_points, species, everything())\n\nall_ratings %&gt;% \n  skimr::skim()\n\nall_ratings %&gt;% \n  write_csv(\"2020/2020-07-07/coffee_ratings.csv\")"
  },
  {
    "objectID": "data/2020/2020-07-21/readme.html",
    "href": "data/2020/2020-07-21/readme.html",
    "title": "Australian Pets",
    "section": "",
    "text": "Pets\n\n\n\nAustralian Pets\nThe data this week comes from the RSPCA, Townsville Animal Complaints and Brisbane Open Data - Animal Complaints.\nh/t to Georgios Karamanis for suggesting part of this data.\n\nThe RSPCA is Australia’s oldest, largest and most trusted animal welfare organisation. With this privileged position comes great responsibility. This year we received1 124,146 animals into our animal shelters and adoption centres across the country.\nWith a great deal of effort from RSPCAs all over the country, adoption and reclaiming rates nationally have been increasing over time and significant improvements in the outcomes for cats and dogs (including kittens and puppies) have been achieved. This can be attributed to the introduction of new approaches and programs to increase the number of animals adopted and reunited with their owners.\n\nFor general data on the states/regions of Australian with population as of Dec 2019 - Wikipedia.\nThere’s a remarkable amount of possible data cleaning/aggregation here. The animal_outcomes dataset is pretty much ready to go, although you could pivot it longer to stack the regions. If you want to go really far down the rabbit hole, check out the cleaning script and see if you can improve upon it or approach things a different way. Note the data came from PDFs and I had to do a lot of manual spot-checks. Some fun dplyr::update_rows() which saved me a LOT of time.\nThe brisbane_complaints dataset has an interesting structure. No dates were reported within a dataset, so we had to add them from the file names within a purrr call. This cleaning step is pretty realistic! I left the data_range messy so you can further clean it up.\nNote that there is state-level data from the RSPCA, but only two cities for the animal complaints. Up to you if you want to extrapolate between the data, but I’m not confident it will be meaningful.\nThe RSPCA Report (lots of graphs to recreate) - RSPCA Report.\nJournal article - A Retrospective Analysis of Complaints to RSPCA Queensland, Australia, about Dog Welfare.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-07-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 30)\n\nanimal_outcomes &lt;- tuesdata$animal_outcomes\n\n# Or read in the data manually\n\nanimal_outcomes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-07-21/animal_outcomes.csv')\nanimal_complaints &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-07-21/animal_complaints.csv')\nbrisbane_complaints &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-07-21/brisbane_complaints.csv')\n\n\nData Dictionary\n\n\n\nanimal_outcomes.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nFull year\n\n\nanimal_type\ncharacter\nAnimal type (horse, wildlife, dog, cat, etc)\n\n\noutcome\ncharacter\nAnimal outcome - euthanized, released, rehomed, etc\n\n\nACT\ndouble\nACT - Australian Capital Territory\n\n\nNSW\ndouble\nNew South Wales\n\n\nNT\ndouble\nNorthern Territory\n\n\nQLD\ndouble\nQueensland\n\n\nSA\ndouble\nSouth Australia\n\n\nTAS\ndouble\nTasmania\n\n\nVIC\ndouble\nVictoria\n\n\nWA\ndouble\nWestern Australian\n\n\nTotal\ndouble\nAustralian Total\n\n\n\n\n\nanimal_complaints.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nAnimal Type\ncharacter\nAnimal Type\n\n\nComplaint Type\ncharacter\nComplaint type\n\n\nDate Received\ncharacter\nDate received (Month - year)\n\n\nSuburb\ncharacter\nSuburb/region\n\n\nElectoral Division\ncharacter\nElectoral Division\n\n\n\n\n\nbrisbane_complaints.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nnature\ncharacter\nNature of complaints (animal)\n\n\nanimal_type\ncharacter\nAnimal type\n\n\ncategory\ncharacter\nCategory of complaint\n\n\nsuburb\ncharacter\nSuburb where reported\n\n\ndate_range\ncharacter\nDate range (typically 1 quarter + year)\n\n\nresponsible_office\ncharacter\nResponsible office for the complaint\n\n\ncity\ncharacter\nCity (Brisbane)\n\n\n\n\nCleaning Script\n\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(rvest)\nlibrary(glue)\n\n# Brisbane data for complaints:\n\n# Source: https://www.data.brisbane.qld.gov.au/data/dataset/96bec69c-6170-4ef0-93f1-eda279149b97\n\nall_complaints &lt;- list.files(\"2020/2020-07-21/raw_csv\") %&gt;% \n  map_dfr(.f = function(x){\n    read_csv(file = paste0(\"2020/2020-07-21/raw_csv/\", x)) %&gt;% \n      mutate(date_range = x)\n  })\n\nbrisbane_complaints &lt;- all_complaints %&gt;% \n  mutate(date_range = str_remove(date_range, \"animal-compliance-|cars-bis-open-data-animal-related-complaints-\")) %&gt;% \n  set_names(nm= c(\"nature\", \"animal_type\", \"category\", \"suburb\", \"date_range\", \"responsible_office\")) %&gt;% \n  mutate(city = \"Brisbane\")\n\nbrisbane_complaints %&gt;% write_csv(\"2020/2020-07-21/brisbane_complaints.csv\")\n\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(rvest)\nlibrary(glue)\n\n# Get all urls for PDFs ---------------------------------------------------\n\nraw_pca &lt;- \"https://www.rspca.org.au/what-we-do/our-role-caring-animals/annual-statistics\" %&gt;%\n  read_html()\n\nall_urls &lt;- raw_pca %&gt;%\n  html_node(\"body &gt; div:nth-child(5) &gt; div.node.node-full-page &gt; div.paragraphs-items.paragraphs-items-field-shared-sections.paragraphs-items-field-shared-sections-full.paragraphs-items-full &gt; div &gt; div &gt; div &gt; div &gt; div &gt; div:nth-child(6)\") %&gt;%\n  html_nodes(\"p &gt; a\") %&gt;%\n  html_attr(\"href\") %&gt;%\n  enframe() %&gt;%\n  arrange(desc(name)) %&gt;%\n  mutate(name = c(1999:2015, 2017)) %&gt;%\n  add_row(name = 2018, value = \"https://www.rspca.org.au/sites/default/files/RSPCA%20Australia%20Annual%20Statistics%20final%202018-2019.pdf\") %&gt;%\n  set_names(nm = c(\"year\", \"url\"))\n\n# Download all the files\nsave_rspca &lt;- function(year, url){\n  download.file(url, destfile = glue::glue(\"rspca_{year}.pdf\"))\n}\n\n# Iterate across the files and save\nall_urls %&gt;% \n  pwalk(save_rspca)\n\n### Function to get data\nget_animals &lt;- function(year, url) {\n  url_rspca &lt;- url\n  \n  # Read PDF as character strings\n  raw_pdf &lt;- pdftools::pdf_text(url_rspca)\n  \n  # Get table number by page\n  page_num_df &lt;- raw_pdf %&gt;%\n    enframe() %&gt;%\n    mutate(value = str_trim(value) %&gt;%\n             word(1, 2))\n  \n  # Add logic for getting the right table number\n  page_num_pull &lt;- page_num_df %&gt;%\n    filter(str_detect(value, if_else(year &lt; 2007, \"Table 3\", \"Table 4\"))) %&gt;%\n    pull(name)\n  \n  # More logic because someone in 2011 decided to further wrap tables :facepalm:\n  page_num &lt;- if_else(year == 2011, list(5), list(page_num_pull))[[1]]\n  \n  # Tables can span up to 2 additional pages\n  # Combine these pages and split into rows by new lines\n  raw_text &lt;- raw_pdf[c(page_num:(page_num + 2))] %&gt;%\n    str_split(\"\\n\") %&gt;%\n    flatten_chr()\n  \n  # Find and limit to start/end of table\n  table_start &lt;- stringr::str_which(raw_text, \"Dogs\")\n  table_end &lt;- stringr::str_which(raw_text, \"Total animals|Total received|Total Animals\")\n  \n  table_end &lt;- table_end[[length(table_end)]]\n  \n  # Trim table of extra whitespace and limit to right rows\n  table_trimmed &lt;- raw_text[c(table_start:table_end)] %&gt;% str_trim()\n  \n  # squish table together and replace excess whitespace with a '|'\n  squished_table &lt;- str_replace_all(table_trimmed, \"\\\\s{2,}\", \"|\") %&gt;%\n    str_remove_all(\",\")\n  \n  raw_df &lt;- enframe(squished_table)\n  \n  # separate the columns\n  animal_df &lt;- raw_df %&gt;%\n    separate(\n      value,\n      into = c(\n        \"outcome\",\n        \"ACT\",\n        \"NSW\",\n        \"NT\",\n        \"QLD\",\n        \"SA\",\n        \"TAS\",\n        \"VIC\",\n        \"WA\",\n        \"Total\"\n      ),\n      sep = \"\\\\|\"\n    ) %&gt;%\n    # convert columns to double\n    mutate(across(c(ACT:Total), as.double)) %&gt;%\n    mutate(\n      year = year,\n      # Clean up the outcome column\n      outcome = str_to_title(outcome) %&gt;% str_remove(\"[:digit:]+\")\n    ) %&gt;%\n    mutate(\n      # create animal_type column\n      animal_type = if_else(\n        str_detect(outcome, paste(c(\"Dogs\", \"Cats\", \"Horses\", \"Livestock\", \"Other animals\", \"Other Animals\", \"Wildlife\"), collapse = \"|\")),\n        outcome,\n        NA_character_\n      ),\n      animal_type = str_to_title(animal_type)\n    ) %&gt;%\n    # fill down the animal type column\n    fill(animal_type)\n  \n  \n  animal_df\n}\n\n# Get all the data --------------------------------------------------------\n\n# Note you could do some of this in bulk with purrr, but since I had to\n# do so much extra cleaning outside of the core function I decided to \n# just do it manually and merge at the end\n\ndata_1999 &lt;- get_animals(1999, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics1999-2000.pdf\")\ndata_2000 &lt;- get_animals(2000, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202000%20-%202001.pdf\")\ndata_2001 &lt;- get_animals(2001, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202001%20-%202002.pdf\")\ndata_2002 &lt;- get_animals(2002, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202002-2003.pdf\")\ndata_2003 &lt;- get_animals(2003, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202003%20-%202004.pdf\") %&gt;%\n  filter(!is.na(Total))\ndata_2004 &lt;- get_animals(2004, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202004%20-%202005.pdf\") %&gt;%\n  filter(!is.na(Total))\ndata_2005 &lt;- get_animals(2005, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202005%20-%202006.pdf\") %&gt;%\n  rows_update(\n    tibble(name = 5, NT = NA, QLD = 456, SA = 97, TAS = 79, VIC = 472, WA = 133, Total = 1951)\n  ) %&gt;%\n  rows_update(\n    tibble(name = 10, SA = NA, TAS = 8, VIC = 100, WA = 0, Total = 1730)\n  ) %&gt;%\n  rows_update(\n    tibble(name = 7, NT = NA, QLD = 47, SA = 451, TAS = 127, VIC = 0, WA = 0, Total = 1015)\n  ) %&gt;%\n  rows_update(\n    tibble(\n      name = c(18, 19), NT = c(NA, NA), QLD = c(474, 21), SA = c(96, 0),\n      TAS = c(52, 300), VIC = c(218, 829), WA = c(52, 0), Total = c(1336, 1168)\n    )\n  ) %&gt;%\n  rows_update(\n    tibble(\n      name = c(60:66), NT = c(rep(NA, 7)),\n      QLD = c(54, 694, 80, 0, 645, 97, 1570),\n      SA = c(10, 108, 2, 0, 222, NA, 342),\n      TAS = c(5, 95, 1, 0, 11, 2, 114),\n      VIC = c(47, 329, 76, 0, 373, 20, 845),\n      WA = c(0, 6, 0, 0, 1, 0, 7),\n      Total = c(162, 1594, 224, 260, 2507, 481, 5228)\n    )\n  ) %&gt;%\n  filter(!is.na(Total))\n\ndata_2006 &lt;- get_animals(2006, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202006%20-%202007.pdf\") %&gt;%\n  rows_update(\n    tibble(name = 7, TAS = NA, VIC = 153, WA = 5, Total = 2087)\n  ) %&gt;%\n  rows_update(\n    tibble(name = 32, NT = NA, QLD = 30, SA = 5, TAS = 7, VIC = 42, WA = 0, Total = 95)\n  )\n\n# Note decided to use data_2006_upd instead of overwriting\n# One of the columns got dropped in parsing step, so I used it to\n# update rows in place\n\ndata_2006_upd &lt;- data_2006 %&gt;%\n  rows_update(\n    data_2006 %&gt;%\n      filter(name %in% c(44:50, 53:60)) %&gt;%\n      select(-ACT) %&gt;%\n      set_names(nm = c(\n        \"name\", \"outcome\", \"ACT\", \"NSW\", \"NT\", \"QLD\",\n        \"SA\", \"TAS\", \"VIC\", \"WA\", \"year\", \"animal_type\"\n      )) %&gt;%\n      mutate(Total = c(\n        1893, 142, 1502, 6035, 1318, 10890, 11051,\n        167, 1631, 169, 186, 2291, 227, 4671, 5228\n      ), .after = WA) %&gt;%\n      mutate(\n        outcome = c(\n          \"Released\", \"In Stock\", \"Transferred\", \"Euthanased\", \"Other\", \"Total\", \"Last Year's Total\",\n          \"Reclaimed\", \"Rehomed\", \"In Stock\", \"Transferred\", \"Euthanased\", \"Other\", \"Total\", \"Last Year's Total\"\n        ),\n        animal_type = c(rep(\"Wildlife\", 7), rep(\"Other Animals\", 8))\n      )\n  ) %&gt;%\n  filter(!is.na(Total))\n\ndata_2007 &lt;- get_animals(2007, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20national%20Statistics%202007-2008.pdf\") %&gt;%\n  filter(!is.na(Total))\n\ndata_2008 &lt;- get_animals(2008, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202008%20-%202009.pdf\") %&gt;%\n  rows_update(\n    tibble(\n      name = c(2, 3, 8, 16),\n      WA = c(621, 1045, 248, 958),\n      Total = c(22896, 19236, 22085, 19666)\n    )\n  ) %&gt;%\n  rows_update(\n    tibble(\n      name = 19,\n      NT = 212, QLD = 11045, SA = 2712, TAS = 1974, VIC = 9801, WA = 99, Total = 39495\n    )\n  ) %&gt;%\n  filter(!is.na(Total))\n\ndata_2009 &lt;- get_animals(2009, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202009%20-%202010.pdf\") %&gt;%\n  filter(!is.na(Total))\n\ndata_2010 &lt;- get_animals(2010, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202010%20-%202011.pdf\") %&gt;%\n  filter(!is.na(Total))\n\ndata_2011 &lt;- get_animals(2011, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202011-2012.pdf\") %&gt;%\n  filter(!is.na(Total))\n\ndata_2012 &lt;- get_animals(2012, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA%20Australia%20National%20Statistics%202012%20-%202013.pdf\") %&gt;%\n  filter(!is.na(Total))\n\ndata_2013 &lt;- get_animals(2013, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA_Australia-Annual_Statistics_2013-2014.pdf\") %&gt;%\n  filter(!is.na(Total))\n\ndata_2014 &lt;- get_animals(2014, \"https://www.rspca.org.au/sites/default/files/website/The-facts/Statistics/RSPCA_Australia-Annual_Statistics_2014-2015.pdf\") %&gt;%\n  filter(!is.na(Total))\n\ndata_2015 &lt;- get_animals(2015, \"https://www.rspca.org.au/sites/default/files/RSPCA%20Australia%20Annual%20Statistics%202015-2016%20.pdf\") %&gt;%\n  rows_update(\n    tibble(\n      name = 62, ACT = 581, NSW = 1361, NT = 34, QLD = 720, SA = 449, TAS = 191, VIC = 716, WA = 38, Total = 4090\n    )\n  ) %&gt;%\n  filter(!is.na(Total))\n\ndata_2016 &lt;- get_animals(2016, \"https://www.rspca.org.au/sites/default/files/RSPCA%20Australia%20Annual%20Statistics%20final%202016-2017.pdf\") %&gt;% \n  filter(!is.na(Total))\n\ndata_2017 &lt;- get_animals(2017, \"https://www.rspca.org.au/sites/default/files/RSPCA%20Australia%20Annual%20Statistics%202017-2018.pdf\") %&gt;%\n  filter(!is.na(Total))\n\ndata_2018 &lt;- get_animals(2018, \"https://www.rspca.org.au/sites/default/files/RSPCA%20Australia%20Annual%20Statistics%20final%202018-2019.pdf\") %&gt;%\n  filter(!is.na(Total))\n\n# Combine all the cleaned data by year!\n# note that 2016 was not reported. :sad:\n# I would recommend imputing if you want\n\ncomb_data &lt;- bind_rows(\n  list(\n    data_1999, data_2000, data_2001, data_2002, data_2003, data_2004, data_2005,\n    data_2006_upd, data_2007, data_2008, data_2009, data_2010, data_2011, \n    data_2012, data_2013, data_2014, data_2015, data_2016, data_2017, data_2018\n  )\n)\n\n# clean the final data\nclean_data &lt;- comb_data %&gt;%\n  filter(outcome %in% c(\n    \"Euthanased\", \"Other\", \"Rehomed\", \"Transferred\",\n    \"Reclaimed\", \"Currently In Care\", \"In Stock\",\n    \"Released\", \"In Our Care\"\n  )) %&gt;%\n  mutate(\n    outcome = case_when(\n      outcome == \"In Our Care\" ~ \"Currently In Care\",\n      outcome == \"Euthanased\" ~ \"Euthanized\",\n      TRUE ~ outcome\n    )\n  ) %&gt;%\n  select(year, animal_type, outcome, ACT:Total)\n\n# Sanity checking\nclean_data %&gt;% ggplot(aes(x = year, y = Total, color = outcome)) +\n  geom_line() +\n  facet_wrap(~animal_type, scales = \"free_y\")\n\n# Sanity checking\nclean_data %&gt;%\n  filter(outcome %in% c(\"Euthanized\", \"Rehomed\"), animal_type != \"Wildlife\") %&gt;%\n  ggplot(aes(x = year, y = Total, fill = outcome)) +\n  geom_col(position = \"fill\") +\n  facet_wrap(~animal_type)\n\n# Sanity checking\nclean_data %&gt;%\n  count(animal_type, sort = T)\n\n# Sanity checking Wildlife being an odd number\n# It's because first 3 years only had 3 outcomes\n\nclean_data %&gt;%\n  filter(animal_type == \"Wildlife\") %&gt;%\n  count(year)\n\nclean_data %&gt;% \n  write_csv(\"2020/2020-07-21/animal_outcomes.csv\")"
  },
  {
    "objectID": "data/2020/2020-08-04/readme.html",
    "href": "data/2020/2020-08-04/readme.html",
    "title": "European energy",
    "section": "",
    "text": "Wind farm\n\n\n\nEuropean energy\nThe data this week comes from Eurostat.\nH/t to Karim Douïeb who created a very nice graphic in Observable (D3), based off a Washington post article for US Energy. Their graphic can be found below.\n\nAdditional data can be found via the OECD.\nThere’s also a nice report for 2017 form the EU Power sector. Lots of graphics inside.\nThermal power according to Wikipedia:\n\nA thermal power station is a power station in which heat energy is converted to electric power. In most, a steam-driven turbine converts heat to mechanical power as an intermediate to electrical power. Water is heated, turns into steam and drives a steam turbine which drives an electrical generator.\n\nClean vs renewable vs fossil fuels by Peninsula Energy.\n\nClean energy is carbon-free energy that creates little to no greenhouse gas emissions. This is in contrast to fossil fuels, which produce a significant amount of greenhouse gas emissions, including carbon dioxide and methane. Renewable energy is energy that comes from resources that are naturally replenished such as sunlight, wind, water, and geothermal heat. Unlike fossil fuels, such as oil, natural gas and coal, which cannot be replaced, renewable energy regenerates naturally in a short period of time.\n\nOverall, this dataset was an exploration of pulling data from an Excel-based data product. This was a bit messy to start but applying techniques to extract the data out of repeated tables and into a tidy format is possible programatically with a bit of logic + readxl! Check out the cleaning script for the details.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-08-04')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 32)\n\nenergy_types &lt;- tuesdata$energy_types\n\n# Or read in the data manually\n\nenergy_types &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-04/energy_types.csv')\ncountry_totals &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-04/country_totals.csv')\n\n\nData Dictionary\nLimited to Level 1 or 2 production by type - either conventional thermal (fossil fuels), nuclear, hydro, wind, solar, geothermal, or other.\n\n\n\nenergy_types\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry ID\n\n\ncountry_name\ncharacter\nCountry name\n\n\ntype\ncharacter\nType of energy production\n\n\nlevel\ncharacter\nLevel - either total, level 1 or level 2. Where level 2 makes up level 1 that makes up the total.\n\n\n2016\ndouble\nEnergy in GWh (Gigawatt hours)\n\n\n2017\ndouble\nEnergy in GWh (Gigawatt hours)\n\n\n2018\ndouble\nEnergy in GWh (Gigawatt hours)\n\n\n\n\n\ncountry_totals\nLimited to total net production, along with imports, exports, energy lost, and energy supplied (net + import - export - energy absorbed by pumping).\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry ID\n\n\ncountry_name\ncharacter\nCountry name\n\n\ntype\ncharacter\nType of energy production\n\n\nlevel\ncharacter\nLevel - either total, level 1 or level 2. Where level 2 makes up level 1 that makes up the total.\n\n\n2016\ndouble\nEnergy in GWh (Gigawatt hours)\n\n\n2017\ndouble\nEnergy in GWh (Gigawatt hours)\n\n\n2018\ndouble\nEnergy in GWh (Gigawatt hours)\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(countrycode)\n\nraw_code &lt;- countrycode::codelist %&gt;% \n    select(country_name = country.name.en, country = eurostat)\n\nraw_excel &lt;- read_excel(\"2020/2020-08-04/Electricity_generation_statistics_2019.xlsx\", sheet = 3)\n  \nraw_excel %&gt;% \n    filter(!is.na(...4)) %&gt;% \n    mutate(country = str_remove_all(...4, \"[:digit:]\"), .before = ...1) %&gt;% \n    mutate(country = if_else(\n      str_length(country) &gt; 1, country, NA_character_), \n      country = str_extract(country, \"[:alpha:]+\")\n      ) %&gt;% \n    fill(country) %&gt;% \n  select(-c(...1, ...2, ...14:...18))\n\nrow_stat &lt;- read_excel(\"2020/2020-08-04/Electricity_generation_statistics_2019.xlsx\", \n                       sheet = 3,\n                       range = \"C48:C61\", col_names = FALSE)[[1]][c(1,3:14)] %&gt;% \n  str_remove(\"[:digit:]\") %&gt;% \n  str_remove(\"of which: \") %&gt;% \n  str_remove(\"\\\\.\") %&gt;% str_trim()\n\ncountry_range &lt;- tibble(row_start = seq(from = 46, to = 454, by = 34), \n       row_end = seq(from = 61, to = 469, by = 34)) %&gt;% \n  mutate(col1 = 4, col2 = col1 + 5, col3 = col2 + 5) %&gt;% \n  pivot_longer(cols = col1:col3, names_to = \"col_var\", values_to = \"col_start\") %&gt;% \n  mutate(col_end = col_start + 2) %&gt;% \n  select(-col_var) %&gt;% \n  slice(-n(), -(n()-1)) %&gt;% \n  mutate(row_stat = list(row_stat))\n\n\nget_country_stats &lt;- function(row_start, row_end, col_start, col_end, row_stat){\n  \n  # # pull the row_stat names\n  # row_stat &lt;- row_stat\n\n  # create the range programatically\n  col_range &lt;- glue::glue(\"{LETTERS[col_start]}{row_start}:{LETTERS[col_end]}{row_end}\")\n  \n  # read in the data section quietly\n  raw_data &lt;- suppressMessages(\n    read_excel(\"2020/2020-08-04/Electricity_generation_statistics_2019.xlsx\", \n                         sheet = 3,\n                         col_names = FALSE,\n                         range = col_range))\n  \n  \n  country_data &lt;-  raw_data %&gt;% \n    # set appropriate names\n    set_names(nm = c(2016:2018)) %&gt;% \n    # drop the year ranges\n    filter(!is.na(`2016`), `2016` != \"2016\") %&gt;% \n    # get the country into a column rather than a header\n    mutate(country = if_else(\n      is.na(`2017`), \n      `2016`, \n      NA_character_), \n      .before = `2016`) %&gt;% \n    # fill country down\n    fill(country) %&gt;% \n    # drop old country header\n    filter(!is.na(`2017`)) %&gt;% \n    # add row stat in\n    mutate(type = row_stat, \n           .after = country, \n           # add levels of the stats\n           level = c(\"Total\", \"Level 1\", \"Level 1\", \"Level 1\", \"Level 2\", \n                     \"Level 1\", \"Level 1\", \"Level 1\", \"Level 1\", \"Total\", \n                     \"Total\", \"Total\", \"Total\")) %&gt;% \n    # format as double\n    mutate(across(c(`2016`:`2018`), as.double))\n  \n  # return data\n  country_data\n}\n\nall_countries &lt;- country_range %&gt;% \n  pmap_dfr(get_country_stats) %&gt;% \n  left_join(raw_code, by = \"country\") %&gt;% \n  select(country, country_name, everything())\n\ncountry_totals &lt;- all_countries %&gt;% \n  filter(level == \"Total\")\n\ncountry_production &lt;- all_countries %&gt;% \n  filter(level != \"Total\")\n\n# sanity check\ncountry_totals %&gt;% \n  # filter(type == \"Total net production\") %&gt;% \n  pivot_longer(cols = `2016`:`2018`, names_to = \"year\", values_to = \"value\") %&gt;% \n  filter(type == \"Total net production\") %&gt;%\n  # count(type)%&gt;% \n  ggplot(aes(y = value, x = year, color = country, group = country)) +\n  geom_line()\n\nwrite_csv(country_totals, \"2020/2020-08-04/country_totals.csv\")\n\nwrite_csv(country_production, \"2020/2020-08-04/energy_types.csv\")"
  },
  {
    "objectID": "data/2020/2020-08-18/readme.html",
    "href": "data/2020/2020-08-18/readme.html",
    "title": "Plants in Danger",
    "section": "",
    "text": "Image credit to Florent Lavergne\n\n\n\nPlants in Danger\nThe data this week comes from the International Union for Conservation of Nature (IUCN) Red list of Threatened Species (Version 2020-1) and was scrapped and prepared by Florent Lavergne for his fantastic and unique infographic.\nHere is what Florent says about the rationale of this project:\n\nJust like animals, plants are going through an important biodiversity crisis. Many species from isolated areas are facing extinction due to human activities. Using distribution data from the International Union for Conservation of Nature (IUCN), I designed these network maps to inform on an important yet underrepresented topic.\n\nIn total, 500 plant species are considered extinct as of 2020. 19.6% of those were endemic to Madagascar, 12.8% to Hawaiian islands.\nNote that simply joining the threats and actions datasets together is not fully appropriate as the row alignment of threats and actions doesn’t correspond. You can do dplyr::left_join() without any problem, but again be warned that you shouldn’t make any decisions based off threat + action occuring or not occuring in the same observation.\n\nFurther reading:\n\nYou can find more details on threatened species, summary statistics, articles, and more on the different Red List categories on the IUCN main page.\nThis study published in Science in 2019 provides some general information about extinction risks of plants in general and some analyses and visualization about the African flora at risk.\nThe IUCN itself shared a blog post on the extinction risk of European endemic trees.\n\nCredit: Florent Lavergne and Cédric Scherer\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-08-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 34)\n\nplants &lt;- tuesdata$plants\n\n# Or read in the data manually\n\nplants &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-18/plants.csv')\nactions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-18/actions.csv')\nthreats &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-18/threats.csv')\n\n\nData Dictionary\n\n\n\nplants.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nbinomial_name\ncharacter\nSpecies name (Genus + species)\n\n\ncountry\ncharacter\nCountry of origin\n\n\ncontinent\ncharacter\nContinent of origin\n\n\ngroup\ncharacter\nTaxonomic group\n\n\nyear_last_seen\ncharacter\nPeriod species was last seen\n\n\nthreat_AA\ndouble\nThreat: Agriculture & Aquaculture\n\n\nthreat_BRU\ndouble\nThreat: Biological Resource Use\n\n\nthreat_RCD\ndouble\nThreat: Commercial Development\n\n\nthreat_ISGD\ndouble\nThreat: Invasive Species\n\n\nthreat_EPM\ndouble\nThreat: Energy Production & Mining\n\n\nthreat_CC\ndouble\nThreat: Climate Change\n\n\nthreat_HID\ndouble\nThreat: Human Intrusions\n\n\nthreat_P\ndouble\nThreat: Pollution\n\n\nthreat_TS\ndouble\nThreat: Transportation Corridor\n\n\nthreat_NSM\ndouble\nThreat: Natural System Modifications\n\n\nthreat_GE\ndouble\nThreat: Geological Events\n\n\nthreat_NA\ndouble\nThreat unknown\n\n\naction_LWP\ndouble\nCurrent action: Land & Water Protection\n\n\naction_SM\ndouble\nCurrent action: Species Management\n\n\naction_LP\ndouble\nCurrent action: Law & Policy\n\n\naction_RM\ndouble\nCurrent action: Research & Monitoring\n\n\naction_EA\ndouble\nCurrent action: Education & Awareness\n\n\naction_NA\ndouble\nCurrent action unknown\n\n\nred_list_category\ncharacter\nIUCN Red List category\n\n\n\n\n\nthreats.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nbinomial_name\ncharacter\nSpecies name (Genus + species)\n\n\ncountry\ncharacter\nCountry of origin\n\n\ncontinent\ncharacter\nContinent of origin\n\n\ngroup\ncharacter\nTaxonomic group\n\n\nyear_last_seen\ncharacter\nPeriod species was last seen\n\n\nred_list_category\ncharacter\nIUCN Red List category\n\n\nthreat_type\ncharacter\nType of threat\n\n\nthreatened\ndouble\nBinary 0 or 1 (not threatened by this), and 1 (threatened)\n\n\n\n\n\nactions.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nbinomial_name\ncharacter\nSpecies name (Genus + species)\n\n\ncountry\ncharacter\nCountry of origin\n\n\ncontinent\ncharacter\nContinent of origin\n\n\ngroup\ncharacter\nTaxonomic group\n\n\nyear_last_seen\ncharacter\nPeriod species was last seen\n\n\nred_list_category\ncharacter\nIUCN Red List category\n\n\naction_type\ncharacter\nType of action\n\n\naction_taken\ndouble\nBinary 0 (Action not taken) or 1 (Action Taken)\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(tidytext)\n\nplants_wide &lt;- read_csv(\"https://raw.githubusercontent.com/Z3tt/TidyTuesday/master/data/raw_plants/plants_extinct_wide.csv\")\n\nplants_wide %&gt;% \n  write_csv(here::here(\"2020\", \"2020-08-18\", \"plants.csv\"))\n\nthreats &lt;- plants_wide %&gt;% \n  select(-contains(\"action\")) %&gt;% \n  pivot_longer(cols = contains(\"threat\"), names_to = \"threat_type\", \n               values_to = \"threatened\", names_prefix = \"threat_\") %&gt;% \n  mutate(threat_type = case_when(\n    threat_type == \"AA\" ~ \"Agriculture & Aquaculture\",\n    threat_type == \"BRU\" ~ \"Biological Resource Use\",\n    threat_type == \"RCD\" ~ \"Commercial Development\",\n    threat_type == \"ISGD\" ~ \"Invasive Species\",\n    threat_type == \"EPM\" ~ \"Energy Production & Mining\",\n    threat_type == \"CC\" ~ \"Climate Change\",\n    threat_type == \"HID\" ~ \"Human Intrusions\",\n    threat_type == \"P\" ~ \"Pollution\",\n    threat_type == \"TS\" ~ \"Transportation Corridor\",\n    threat_type == \"NSM\" ~ \"Natural System Modifications\",\n    threat_type == \"GE\" ~ \"Geological Events\",\n    threat_type == \"NA\" ~ \"Unknown\",\n    TRUE ~ NA_character_\n  )) \n\nthreats %&gt;% \n  write_csv(here::here(\"2020\", \"2020-08-18\", \"threats.csv\"))\n\nthreat_filtered &lt;- threats %&gt;% \n  filter(threatened == 1) \n\nthreat_filtered %&gt;% \n  janitor::tabyl(threat_type, threatened)\n\nactions &lt;- plants_wide %&gt;% \n      select(-contains(\"threat\")) %&gt;% \n      pivot_longer(cols = contains(\"action\"), names_to = \"action_type\", \n                   values_to = \"action_taken\", names_prefix = \"action_\") %&gt;% \n      mutate(action_type = case_when(\n        action_type == \"LWP\" ~ \"Land & Water Protection\",\n        action_type == \"SM\" ~ \"Species Management\",\n        action_type == \"LP\" ~ \"Law & Policy\",\n        action_type == \"RM\" ~ \"Research & Monitoring\",\n        action_type == \"EA\" ~ \"Education & Awareness\",\n        action_type == \"NA\" ~ \"Unknown\",\n        TRUE ~ NA_character_\n      )) \n\nactions %&gt;% \n  write_csv(here::here(\"2020\", \"2020-08-18\", \"actions.csv\"))\n\naction_filtered &lt;- actions %&gt;% \n  filter(action_taken == 1) \n\naction_filtered %&gt;% \n  janitor::tabyl(action_type, action_taken)\n\nthreat_filtered %&gt;% \n  count(continent, group, threat_type) %&gt;% \n  ggplot(aes(y = tidytext::reorder_within(threat_type, n, continent), x = n, fill = group)) +\n  geom_col() +\n  tidytext::scale_y_reordered() +\n  facet_wrap(~continent, scales = \"free_y\", ncol = 1)"
  },
  {
    "objectID": "data/2020/2020-09-01/readme.html",
    "href": "data/2020/2020-09-01/readme.html",
    "title": "Global Crop Yields",
    "section": "",
    "text": "Harvest picture credit to: https://unsplash.com/@scottagoodwill\n\n\n\nGlobal Crop Yields\nThe data this week comes from Our World in Data. Note that there is a lot of data on that site, we’re looking at some subsets but feel free to use whatever data from there you find interesting! It’s all pretty clean and ready to go.\nAlso note that there are cases where the long data includes both continent (eg Africa, Americas), countries (USA, Afghanistan, etc) and regions (North Asia, East Asia, etc). You’ll need to be careful making assumptions when grouping and/or excluding specific groups.\n\nOur data on agricultural yields across crop types and by country are much more extensive from 1960 onwards. The UN Food and Agricultural Organization (FAO) publish yield estimates across a range of crop commodities by country over this period. The FAO report yield values as the national average for any given year; this is calculated by diving total crop output (in kilograms or tonnes) by the area of land used to grow a given crop (in hectares). There are likely to be certain regional and seasonal differences in yield within a given country, however, reported average yields still provide a useful indication of changes in productivity over time and geographical region.\n\nWe’ve also included data on the trade off between higher yields and land use, so there are some interesting changes to track beyond raw production.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-09-01')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 36)\n\nkey_crop_yields &lt;- tuesdata$key_crop_yields\n\n# Or read in the data manually\n\nkey_crop_yields &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-01/key_crop_yields.csv')\nfertilizer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-01/cereal_crop_yield_vs_fertilizer_application.csv')\ntractors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-01/cereal_yields_vs_tractor_inputs_in_agriculture.csv')\nland_use &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-01/land_use_vs_yield_change_in_cereal_production.csv')\narable_land &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-01/arable_land_pin.csv')\n\n\nData Dictionary\n\n\n\nkey_crop_yields.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry or Region Name\n\n\nCode\ncharacter\nCountry Code (note is NA for regions/continents)\n\n\nYear\ndouble\nYear\n\n\nWheat (tonnes per hectare)\ndouble\nWheat yield\n\n\nRice (tonnes per hectare)\ndouble\nRice Yield\n\n\nMaize (tonnes per hectare)\ndouble\nMaize yield\n\n\nSoybeans (tonnes per hectare)\ndouble\nSoybeans yield\n\n\nPotatoes (tonnes per hectare)\ndouble\nPotato yield\n\n\nBeans (tonnes per hectare)\ndouble\nBeans yield\n\n\nPeas (tonnes per hectare)\ndouble\nPeas yield\n\n\nCassava (tonnes per hectare)\ndouble\nCassava (yuca) yield\n\n\nBarley (tonnes per hectare)\ndouble\nBarley\n\n\nCocoa beans (tonnes per hectare)\ndouble\nCocoa\n\n\nBananas (tonnes per hectare)\ndouble\nBananas\n\n\n\n\n\narable_land\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry or Region Name\n\n\nCode\ncharacter\nCountry Code (note is NA for regions/continents)\n\n\nYear\ndouble\nYear\n\n\nArable land needed to produce a fixed quantity of crops ((1.0 = 1961))\ndouble\nArable land normalized to 1961\n\n\n\n\n\nfertilizer\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry or Region Name\n\n\nCode\ncharacter\nCountry Code (note is NA for regions/continents)\n\n\nYear\ndouble\nYear\n\n\nCereal yield (tonnes per hectare)\ndouble\nCereal yield in tonnes per hectare\n\n\nNitrogen fertilizer use (kilograms per hectare)\ndouble\nNitrogen fertilizer use kg per hectare\n\n\n\n\n\nland_use\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry or Region Name\n\n\nCode\ncharacter\nCountry Code (note is NA for regions/continents)\n\n\nYear\ndouble\nYear\n\n\ncereal yield index\ndouble\nCereal yield index\n\n\nchange to land area used for cereal production since 1961\ndouble\nChange to land area use for cereal production relative since 1961\n\n\ntotal population (gapminder)\ndouble\nTotal population from gapminder data\n\n\n\n\n\ntractor\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry or Region Name\n\n\nCode\ncharacter\nCountry Code (note is NA for regions/continents)\n\n\nYear\ndouble\nYear\n\n\nTractors per 100 sq km arable land\ndouble\nNumber of tractors per 100 sq km of arable land\n\n\nCereal yield (kilograms per hectare) (kg per hectare)\ndouble\nCereal yield in kg per hectare\n\n\nTotal population (Gapminder)\ndouble\nTotal population from gapminder\n\n\n\n\nCleaning Script\nNo real cleaning script today, but here’s an example of how to pivot the data wider to longer.\nlibrary(tidyverse)\n\nkey_crops &lt;- read_csv(\"2020/2020-09-01/key_crop_yields.csv\")\n\nlong_crops &lt;- key_crops %&gt;% \n  pivot_longer(cols = 4:last_col(),\n               names_to = \"crop\", \n               values_to = \"crop_production\") %&gt;% \n  mutate(crop = str_remove_all(crop, \" \\\\(tonnes per hectare\\\\)\")) %&gt;% \n  set_names(nm = names(.) %&gt;% tolower())\n\n\nlong_crops"
  },
  {
    "objectID": "data/2020/2020-09-15/readme.html",
    "href": "data/2020/2020-09-15/readme.html",
    "title": "US Spending on Kids",
    "section": "",
    "text": "US Spending on Kids\nThe data this week comes from Urban Institute courtesy of Joshua Rosenberg’s tidykids package.\nPer the Urban Institute:\n\nThis dataset provides a comprehensive accounting of public spending on children from 1997 through 2016. It draws on the US Census Bureau’s Annual Survey of State and Local Government Finances, as well as several federal and other noncensus sources, to capture state-by-state spending on education, income security, health, and other areas. The data were assembled by Julia Isaacs, Eleanor Lauderback, and Erica Greenberg of the Urban Institute, working in collaboration with Margot Jackson of Brown University for her study of public spending on children and class gaps in child development. This work has been supported (in part) by grant #83-18-23 from the Russell Sage Foundation and (in part) by the Eunice Kennedy Shriver National Institute of Child Health and Human Development of the National Institutes of Health under award #R03HD097421. The content is solely the responsibility of the authors and does not necessarily represent the official views of the foundation or National Institutes of Health.\n\nThey provide a raw Excel file, which Joshua cleaned for us into a tidy dataset.\nNote you can access this data via tidytuesdayR or via tidykids:tidykids\nAnother short article on Education Spending at Governing.com.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-09-15')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 38)\n\nkids &lt;- tuesdata$kids\n\n# Or read in the data manually\n\nkids &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-15/kids.csv')\n\n\nData Dictionary\n\n\n\nkids.csv\nNOTE full variable codebook at: tidykids pkgdown site.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstate\ncharacter\nUnited States state (and the District of Columbia)\n\n\nvariable\ncharacter\nVariable\n\n\nyear\ncharacter\nYear\n\n\nraw\ndouble\nThe value of the variable; a numeric value\n\n\ninf_adj\ndouble\nThe value of the variable, adjusted for inflation, a numeric value\n\n\ninf_adj_perchild\ndouble\nThe value of the variable adjusted for inflation, per child; a numeric value\n\n\n\n\nCleaning Script\nNo cleaning script!"
  },
  {
    "objectID": "data/2020/2020-09-29/readme.html",
    "href": "data/2020/2020-09-29/readme.html",
    "title": "Beyoncé and Taylor Swift Lyrics",
    "section": "",
    "text": "Beyoncé and Taylor Swift Lyrics\nThe data this week comes from Rosie Baillie and Dr. Sara Stoudt.\nBeyoncé’s top 100 - Billboard. Taylor Swift’s top 100 - Billboard.\nRosie put together a wonderful analysis of Taylor Swift lyrics! Can you do some similar work with Beyoncé’s work?\nText analysis guides in tidytext or Supervised Machine Learning for Text Analysis in R.\nThe beyonce palettes R pkg.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-09-29')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 40)\n\nbeyonce_lyrics &lt;- tuesdata$beyonce_lyrics\n\n# Or read in the data manually\n\nbeyonce_lyrics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-29/beyonce_lyrics.csv')\ntaylor_swift_lyrics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-29/taylor_swift_lyrics.csv')\nsales &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-29/sales.csv')\ncharts &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-09-29/charts.csv')\n\n\nData Dictionary\n\n\n\nbeyonce_lyrics.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nline\ncharacter\nLyric line\n\n\nsong_id\ndouble\nSong ID\n\n\nsong_name\ncharacter\nSong Name\n\n\nartist_id\ndouble\nArtist ID\n\n\nartist_name\ncharacter\nArtist Name\n\n\nsong_line\ndouble\nSong line number\n\n\n\n\n\ntaylor_swift_lyrics.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nArtist\ncharacter\nArtist\n\n\nAlbum\ncharacter\nAlbum name\n\n\nTitle\ncharacter\nTitle of song\n\n\nLyrics\ncharacter\nLyrics\n\n\n\n\n\nsales.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nartist\ncharacter\nArtist name\n\n\ntitle\ncharacter\nSong title\n\n\ncountry\ncharacter\nCountry for sales\n\n\nsales\ndouble\nSales in dollars\n\n\nreleased\ncharacter\nreleased date\n\n\nre_release\ncharacter\nRe-released date\n\n\nlabel\ncharacter\nLabel released under\n\n\nformats\ncharacter\nFormats released as\n\n\n\n\n\ncharts.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nartist\ncharacter\nArtist name\n\n\ntitle\ncharacter\nSong title\n\n\nreleased\ncharacter\nreleased date\n\n\nre_release\ncharacter\nRe-released date\n\n\nlabel\ncharacter\nLabel released under\n\n\nformats\ncharacter\nFormats released as\n\n\nchart\ncharacter\nCountry Chart\n\n\nchart_position\ncharacter\nHighest Chart position\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(rvest)\n\nts_url &lt;- \"https://en.wikipedia.org/wiki/Taylor_Swift_discography\"\n\nraw_ts_html &lt;- ts_url %&gt;% \n  read_html()\n\nts_raw &lt;- raw_ts_html %&gt;% \n  html_node(\"#mw-content-text &gt; div.mw-parser-output &gt; table:nth-child(10)\") %&gt;% \n  html_table(fill = TRUE) %&gt;% \n  data.frame() %&gt;% \n  janitor::clean_names() %&gt;% \n  tibble() %&gt;% \n  slice(-1, -nrow(.)) %&gt;% \n  mutate(album_details = str_split(album_details, \"\\n\"),\n         sales = str_split(sales, \"\\n\"),\n  ) %&gt;% \n  select(-certifications) %&gt;% \n  unnest_longer(album_details)  %&gt;% \n  separate(album_details, into = c(\"album_detail_type\", \"album_details\"), sep = \": \") %&gt;% \n  mutate(album_detail_type = if_else(album_detail_type == \"Re-edition\", \"Re-release\", album_detail_type)) %&gt;% \n  pivot_wider(names_from = album_detail_type, values_from = album_details) %&gt;% \n  select(-`na`) %&gt;% \n  janitor::clean_names() \n\nts_sales &lt;- ts_raw %&gt;% \n  unnest_longer(sales) %&gt;% \n  separate(sales, into = c(\"country\", \"sales\"), sep = \": \") %&gt;% \n  mutate(sales = str_trim(sales),\n         sales = parse_number(sales)) %&gt;% \n  select(title, country, sales, released:formats) %&gt;% \n  mutate(artist = \"Taylor Swift\", .before = title)\n\n\nts_chart &lt;- ts_raw %&gt;% \n  select(title, released:formats, contains(\"peak_chart\")) %&gt;% \n  pivot_longer(cols = contains(\"peak_chart\"), names_to = \"chart\", values_to = \"chart_position\") %&gt;% \n  mutate(\n    chart = str_remove(chart, \"peak_chart_positions\"),\n  chart = case_when(\n    chart == \"\" ~ \"US\",\n    chart == \"_1\" ~ \"AUS\",\n    chart == \"_2\" ~ \"CAN\",\n    chart == \"_3\" ~ \"FRA\",\n    chart == \"_4\" ~ \"GER\",\n    chart == \"_5\" ~ \"IRE\",\n    chart == \"_6\" ~ \"JPN\",\n    chart == \"_7\" ~ \"NZ\",\n    chart == \"_8\" ~ \"SWE\",\n    chart == \"_9\" ~ \"UK\",\n    TRUE ~ NA_character_\n  )\n  )  %&gt;% \n  mutate(artist = \"Taylor Swift\", .before = title)\n\n\n# Beyonce -----------------------------------------------------------------\n\n\nbey_url &lt;- \"https://en.wikipedia.org/wiki/Beyonc%C3%A9_discography\"\n\nraw_bey_html &lt;- bey_url %&gt;% \n  read_html()\n\nbey_raw &lt;- raw_bey_html %&gt;% \n  html_node(\"#mw-content-text &gt; div.mw-parser-output &gt; table:nth-child(14)\") %&gt;% \n  #mw-content-text &gt; div.mw-parser-output &gt; table:nth-child(14) &gt; tbody &gt; tr:nth-child(3) &gt; th &gt; i &gt; a\n  html_table(fill = TRUE) %&gt;% \n  data.frame() %&gt;% \n  janitor::clean_names() %&gt;% \n  tibble() %&gt;% \n  slice(-1, -nrow(.)) %&gt;% \n  mutate(album_details = str_split(album_details, \"\\n\"),\n         sales = str_split(sales, \"\\n\"),\n  ) %&gt;% \n  select(-certifications) %&gt;% \n  unnest_longer(album_details)  %&gt;% \n  separate(album_details, into = c(\"album_detail_type\", \"album_details\"), sep = \": \") %&gt;% \n  mutate(album_detail_type = if_else(album_detail_type == \"Re-edition\", \"Re-release\", album_detail_type)) %&gt;% \n  pivot_wider(names_from = album_detail_type, values_from = album_details) %&gt;% \n  janitor::clean_names() \n\nbey_sales &lt;- bey_raw %&gt;% \n  unnest_longer(sales) %&gt;% \n  separate(sales, into = c(\"country\", \"sales\"), sep = \": \") %&gt;% \n  mutate(sales = str_trim(sales),\n         sales = parse_number(sales)) %&gt;% \n  select(title, country, sales, released:label, formats = format)  %&gt;% \n  mutate(artist = \"Beyoncé\", .before = title)\n\nbey_chart &lt;- bey_raw %&gt;% \n  select(title, released:label, formats = format, contains(\"peak_chart\")) %&gt;% \n  pivot_longer(cols = contains(\"peak_chart\"), names_to = \"chart\", values_to = \"chart_position\") %&gt;% \n  mutate(\n    chart = str_remove(chart, \"peak_chart_positions\"),\n    chart = case_when(\n      chart == \"\" ~ \"US\",\n      chart == \"_1\" ~ \"AUS\",\n      chart == \"_2\" ~ \"CAN\",\n      chart == \"_3\" ~ \"FRA\",\n      chart == \"_4\" ~ \"GER\",\n      chart == \"_5\" ~ \"IRE\",\n      chart == \"_6\" ~ \"JPN\",\n      chart == \"_7\" ~ \"NZ\",\n      chart == \"_8\" ~ \"SWE\",\n      chart == \"_9\" ~ \"UK\",\n      TRUE ~ NA_character_\n    )\n  ) %&gt;% \n  mutate(artist = \"Beyoncé\", .before = title)\n\nall_sales &lt;- bind_rows(ts_sales, bey_sales)\nall_charts &lt;- bind_rows(ts_chart, bey_chart)\n\nwrite_csv(all_sales, \"2020/2020-09-29/sales.csv\")\nwrite_csv(all_charts, \"2020/2020-09-29/charts.csv\")"
  },
  {
    "objectID": "data/2020/2020-10-13/readme.html",
    "href": "data/2020/2020-10-13/readme.html",
    "title": "Datasaurus Dozen",
    "section": "",
    "text": "Datasaurus Dozen\nThe data this week comes from Alberto Cairo courtesy of Steph Locke + Lucy McGowan.\nH/t to: Jesus M. Castagnetto for sharing it with the TidyTuesday crew.\nThe original blogpost from Alberto. A quick look at the datasauRus R package from Steph and Lucy. An article on the datasets from AutoDesk.\nFrom Steph and Lucy’s datasauRus Vignette:\n\nThis package wraps the awesome Datasaurus Dozen dataset, which contains 13 sets of x-y data. Each sub-dataset has five statistics that are (almost) the same in each case. (These are the mean of x, mean of y, standard deviation of x, standard deviation of y, and Pearson correlation between x and y). However, scatter plots reveal that each sub-dataset looks very different. The dataset is intended to be used to teach students that it is important to plot their own datasets, rather than relying only on statistics.\nThe Datasaurus was created by Alberto Cairo in this great blog post.\nDatasaurus shows us why visualisation is important, not just summary statistics.\nHe’s been subsequently made even more famous in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice.\nIn the paper, Justin and George simulate a variety of datasets that the same summary statistics to the Datasaurus but have very different distributions.\nThis package looks to make these datasets available for use as an advanced Anscombe’s Quartet, available in R as anscombe.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-10-13')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 42)\n\ndatasaurus &lt;- tuesdata$datasaurus\n\n# Or read in the data manually\n\ndatasaurus &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-10-13/datasaurus.csv')\n\n\nData Dictionary\n\n\n\ndatasaurus.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndataset\ncharacter\nDataset name\n\n\nx\ndouble\nx-coordinate\n\n\ny\ndouble\ny-coordinate\n\n\n\n\nCleaning Script\nNo data cleaning this week, but a quick vignette from Steph and Lucy:\nlibrary(ggplot2)\n\nggplot(datasaurus_dozen, aes(x=x, y=y, colour=dataset))+\n  geom_point()+\n  theme_void()+\n  theme(legend.position = \"none\")+\n  facet_wrap(~dataset, ncol=3)"
  },
  {
    "objectID": "data/2020/2020-10-27/readme.html",
    "href": "data/2020/2020-10-27/readme.html",
    "title": "Canadian Wind Turbines",
    "section": "",
    "text": "Canadian Wind Turbines\nThe data this week comes from the Government of Canada.\nH/t to Will Noel, Tim Weis and Andrew Leach for collecting the data. Blake Shaffer and Alyssa Goldberg for visualizing and sharing!\nCanada’s National Observer Article on this dataset.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-10-27')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 44)\n\nwind_turbine &lt;- tuesdata$wind-turbine\n\n# Or read in the data manually\n\nwind_turbine &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-10-27/wind-turbine.csv')\n\n\nData Dictionary\n\n\n\nwind-turbine.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nobjectid\ndouble\nUnique ID\n\n\nprovince_territory\ncharacter\nProvince/territory\n\n\nproject_name\ncharacter\nProject name\n\n\ntotal_project_capacity_mw\ndouble\nElectrical capacity in megawatts\n\n\nturbine_identifier\ncharacter\nTurbine ID\n\n\nturbine_number_in_project\ncharacter\nTurbine number in project\n\n\nturbine_rated_capacity_k_w\ndouble\nTurbine capacity in kilowatts\n\n\nrotor_diameter_m\ndouble\nRotor diameter in meters\n\n\nhub_height_m\ndouble\nHub height in meters\n\n\nmanufacturer\ncharacter\nManufacturer\n\n\nmodel\ncharacter\nModel ID\n\n\ncommissioning_date\ncharacter\nCommission date\n\n\nlatitude\ndouble\nLatitude\n\n\nlongitude\ndouble\nLongitude\n\n\nnotes\ncharacter\nNotes about the data\n\n\n\n\nCleaning Script\nNo cleaning!"
  },
  {
    "objectID": "data/2020/2020-11-10/readme.html",
    "href": "data/2020/2020-11-10/readme.html",
    "title": "Historical Phone Usage",
    "section": "",
    "text": "Historical Phone Usage\nThe data this week comes from OurWorldInData.org.\n\nHannah Ritchie (2017) - “Technology Adoption”. Published online at OurWorldInData.org. Retrieved from: ‘https://ourworldindata.org/technology-adoption’ [Online Resource]\n\nPew research also has a nice article about the adoption of mobile phones by country.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-11-10')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 46)\n\nmobile &lt;- tuesdata$mobile\n\n# Or read in the data manually\n\nmobile &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-11-10/mobile.csv')\nlandline &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-11-10/landline.csv')\n\n\nData Dictionary\n\n\n\nmobile.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nentity\ncharacter\nCountry\n\n\ncode\ncharacter\nCountry code\n\n\nyear\ndouble\nYear\n\n\ntotal_pop\ndouble\nGapminder total population\n\n\ngdp_per_cap\ndouble\nGDP per capita, PPP (constant 2011 international $)\n\n\nmobile_subs\ndouble\nFixed mobile subscriptions (per 100 people)\n\n\ncontinent\ncharacter\nContinent\n\n\n\n\n\nlandline.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nentity\ncharacter\nCountry\n\n\ncode\ncharacter\nCountry code\n\n\nyear\ndouble\nYear\n\n\ntotal_pop\ndouble\nGapminder total population\n\n\ngdp_per_cap\ndouble\nGDP per capita, PPP (constant 2011 international $)\n\n\nlandline_subs\ndouble\nFixed telephone subscriptions (per 100 people)\n\n\ncontinent\ncharacter\nContinent\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(countrycode)\nlibrary(janitor)\n\nraw_mobile &lt;- read_csv(\"2020/2020-11-10/mobile-phone-subscriptions-vs-gdp-per-capita.csv\")\n\nraw_landline &lt;- read_csv(\"2020/2020-11-10/fixed-landline-telephone-subscriptions-vs-gdp-per-capita.csv\")\n\nmobile_df &lt;- raw_mobile %&gt;% \n  janitor::clean_names() %&gt;% \n  rename(\n    total_pop = 4,\n    \"gdp_per_cap\" = 6,\n    \"mobile_subs\" = 7\n  ) %&gt;% \n  filter(year &gt;= 1990) %&gt;% \n  select(-continent) %&gt;% \n  \n  mutate(continent = countrycode::countrycode(\n    entity,\n    origin = \"country.name\",\n    destination = \"continent\"\n  )) %&gt;% \n  filter(!is.na(continent))\n\nlandline_df &lt;- raw_landline %&gt;% \n  janitor::clean_names() %&gt;% \n  rename(\n    total_pop = 4,\n    \"gdp_per_cap\" = 6,\n    \"landline_subs\" = 7\n  ) %&gt;% \n  filter(year &gt;= 1990) %&gt;% \n  select(-continent) %&gt;% \n  mutate(continent = countrycode::countrycode(\n    entity,\n    origin = \"country.name\",\n    destination = \"continent\"\n  )) %&gt;% \n  filter(!is.na(continent))\n\nmobile_df %&gt;% \n  write_csv(\"2020/2020-11-10/mobile.csv\")\n\nlandline_df %&gt;% \n  write_csv(\"2020/2020-11-10/landline.csv\")"
  },
  {
    "objectID": "data/2020/2020-11-24/readme.html",
    "href": "data/2020/2020-11-24/readme.html",
    "title": "Washington Hiking",
    "section": "",
    "text": "Washington Hiking\nThe data this week comes from Washington Trails Association courtesy of the TidyX crew, Ellis Hughes and Patrick Ward!\nA video going through this data can be found on YouTube.\nTheir scraping code can be found on GitHub.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-11-24')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 48)\n\nhike_data &lt;- tuesdata$hike_data\n\n# Or read in the data manually\n\nhike_data &lt;- readr::read_rds(url('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-11-24/hike_data.rds'))\n\n\nData Dictionary\n\n\n\nhike_data.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of trail\n\n\nlocation\ncharacter\nLocation of Trail\n\n\nlength\ncharacter\nLength of trail (note that most have miles included)\n\n\ngain\ncharacter\nGain in elevation (Feet above sea level)\n\n\nhighpoint\ncharacter\nHighest point in feet above sea level\n\n\nrating\ncharacter\nUser submitted rating (out of 5)\n\n\nfeatures\ncharacter\nFeatures\n\n\ndescription\ncharacter\nDescription of trail\n\n\n\n\nCleaning Script\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nscrape_trails &lt;- function(start_int){\n  page_url &lt;- paste0(\n    \"https://www.wta.org/go-outside/hikes?b_start:int=\",\n    start_int\n  )\n  \n  page_html &lt;- read_html(page_url)\n  \n  page_html %&gt;% \n    \n    html_nodes(\".search-result-item\") %&gt;% \n    \n    map(\n      function(hike){\n        \n        hike_name &lt;- hike %&gt;% html_nodes(\".listitem-title\") %&gt;% html_nodes(\"span\") %&gt;%  html_text()\n        hike_location &lt;- hike %&gt;% html_node(\"h3\") %&gt;% html_text()\n        \n        hike_stats &lt;- hike %&gt;% html_node(\".hike-stats\")\n        \n        hike_length &lt;- hike_stats %&gt;% html_nodes(\".hike-length\") %&gt;%html_nodes(\"span\") %&gt;%  html_text()\n        hike_gain &lt;- hike_stats %&gt;% html_nodes(\".hike-gain\") %&gt;%html_nodes(\"span\") %&gt;%  html_text()\n        hike_highpoint &lt;- hike_stats %&gt;% html_nodes(\".hike-highpoint\") %&gt;%html_nodes(\"span\") %&gt;%  html_text()\n        hike_rating &lt;- hike_stats %&gt;% html_nodes(\".hike-rating\") %&gt;%html_nodes(\".current-rating\") %&gt;%  html_text()\n        \n        hike_desc &lt;- hike %&gt;% html_nodes(\".listing-summary\") %&gt;% html_text()\n        \n        hike_features &lt;- hike %&gt;% html_nodes(\".trip-features\") %&gt;% html_nodes(\"img\") %&gt;% html_attr(\"title\") %&gt;% list()\n        \n        tibble(\n          name = hike_name,\n          location = hike_location,\n          length = hike_length,\n          gain = hike_gain,\n          highpoint = hike_highpoint,\n          rating = hike_rating,\n          features = hike_features,\n          description = hike_desc\n        )\n      }) %&gt;% \n    bind_rows() %&gt;% \n    mutate(description = str_remove(description, \"\\n\") %&gt;% str_squish())\n}\n\nstart_int &lt;- c(1, seq(30, 3840, by = 30))\n\nhike_data &lt;- start_int %&gt;% \n  map_dfr(scrape_trails)\n\nsaveRDS(hike_data,file = \"2020/2020-11-24/hike_data.rds\")\n\nclean_hike_data &lt;- hike_data %&gt;% \n  mutate(\n    trip = case_when(\n      grepl(\"roundtrip\",length) ~ \"roundtrip\",\n      grepl(\"one-way\",length) ~ \"one-way\",\n      grepl(\"of trails\",length) ~ \"trails\"),\n    \n    length_total = as.numeric(gsub(\"(\\\\d+[.]\\\\d+).*\",\"\\\\1\", length)) * ((trip == \"one-way\") + 1),\n    \n    gain = as.numeric(gain),\n    highpoint = as.numeric(highpoint),\n    \n    location_general = gsub(\"(.*)\\\\s[-][-].*\",\"\\\\1\",location)\n  )\n\n\n\n\nhike_plot &lt;- ggplot(clean_hike_data) + \n  geom_rect(aes(\n    xmin = 0,\n    xmax = length_total,\n    ymin = 0,\n    ymax = gain,\n    label = name\n  ),\n  alpha = .4,\n  fill = \"#228B22\",\n  color = \"#765C48\"\n  ) + \n  facet_wrap(\n    ~ location_general,\n    scales = \"free_x\"\n  ) +\n  labs(\n    title = \"Washington State Hikes\",\n    x = \"Hike Length (miles)\",\n    y = \"Hike Elevation Gain (ft)\",\n    caption = \"Data from Washingon Trails Association (wta.org) | Viz by @ellis_hughes\"\n  )\n\n\nggplotly(hike_plot)"
  },
  {
    "objectID": "data/2020/2020-12-08/readme.html",
    "href": "data/2020/2020-12-08/readme.html",
    "title": "Women of 2020",
    "section": "",
    "text": "Women of 2020\nThe data this week comes from the BBC by way of Joshua Feldman.\n\nThe BBC has revealed its list of 100 inspiring and influential women from around the world for 2020.\nThis year 100 Women is highlighting those who are leading change and making a difference during these turbulent times.\nThe list includes Sanna Marin, who leads Finland’s all-female coalition government, Michelle Yeoh, star of the new Avatar and Marvel films and Sarah Gilbert, who heads the Oxford University research into a coronavirus vaccine, as well as Jane Fonda, a climate activist and actress.\nAnd in an extraordinary year - when countless women around the world have made sacrifices to help others - one name on the 100 Women list has been left blank as a tribute.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-12-08')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 50)\n\nwomen &lt;- tuesdata$women\n\n# Or read in the data manually\n\nwomen &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-12-08/women.csv')\n\n\nData Dictionary\n\n\n\nwomen.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of woman\n\n\nimg\ncharacter\nLink to headshot\n\n\ncategory\ncharacter\nCategory for award\n\n\ncountry\ncharacter\nCountry of residence\n\n\nrole\ncharacter\nRole/Career\n\n\ndescription\ncharacter\nDescription of the woman and their achievements\n\n\n\n\nCleaning Script\n# Load packages\n\nlibrary(rvest)\nlibrary(tidyverse)\n\n# Load web page\n\nbbc_women &lt;- html(\"https://www.bbc.co.uk/news/world-55042935\")\n\n# Save even and odd indices for data extraction later\n\nodd_index &lt;- seq(1,200,2)\neven_index &lt;- seq(2,200,2)\n\n# Extract name\n\nname &lt;- bbc_women %&gt;% \n  html_nodes(\"article h4\") %&gt;% \n  html_text()\n\n# Extract image\n\nimg &lt;- bbc_women %&gt;% \n  html_nodes(\".card__header\") %&gt;% \n  html_nodes(\"img\") %&gt;% \n  html_attr(\"src\")\n\nimg &lt;- img[odd_index]\n\n# Extract category\n\ncategory &lt;- bbc_women %&gt;% \n  html_nodes(\"article .card\") %&gt;% \n  str_extract(\"card category--[A-Z][a-z]+\") %&gt;% \n  str_remove_all(\"card category--\")\n\n# Extract country & role\n\ncountry_role &lt;- bbc_women %&gt;% \n  html_nodes(\".card__header__strapline__location\") %&gt;% \n  html_text()\n\ncountry &lt;- country_role[odd_index]\nrole &lt;- country_role[even_index]\n\n# Extract description\n\ndescription &lt;- bbc_women %&gt;% \n  html_nodes(\".first_paragraph\") %&gt;% \n  html_text()\n\n# Finalise data frame\n\ndf &lt;- data.frame(\n  name,\n  img,\n  category,\n  country,\n  role,\n  description\n)\n\n# Export\n\nwrite.csv(df, \"data.csv\", row.names=FALSE)"
  },
  {
    "objectID": "data/2020/2020-12-22/readme.html#source-data",
    "href": "data/2020/2020-12-22/readme.html#source-data",
    "title": "The Big Mac index",
    "section": "Source data",
    "text": "Source data\n\nOur source data are from several places. Big Mac prices are from McDonald’s directly and from reporting around the world; exchange rates are from Thomson Reuters; GDP and population data used to calculate the euro area averages are from Eurostat and GDP per person data are from the IMF World Economic Outlook reports.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-12-22')\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 52)\n\nbig-mac &lt;- tuesdata$big-mac\n\n# Or read in the data manually\n\nbig-mac &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-12-22/big-mac.csv')\n\n\nData Dictionary"
  },
  {
    "objectID": "data/2021/2020-12-29/readme.html",
    "href": "data/2021/2020-12-29/readme.html",
    "title": "Week 1",
    "section": "",
    "text": "Week 1\nThis was really just a bring your own dataset week.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2020-12-29\nBring your own data from 2020!"
  },
  {
    "objectID": "data/2021/2021-01-12/readme.html",
    "href": "data/2021/2021-01-12/readme.html",
    "title": "Art Collections",
    "section": "",
    "text": "Art Gallery\n\n\n\nArt Collections\nThe data this week comes from the Tate Art Museum.\n\nThe dataset in this repository was last updated in October 2014. Tate has no plans to resume updating this repository, but we are keeping it available for the time being in case this snapshot of the Tate collection is a useful tool for researchers and developers.\n\n\nHere we present the metadata for around 70,000 artworks that Tate owns or jointly owns with the National Galleries of Scotland as part of ARTIST ROOMS. Metadata for around 3,500 associated artists is also included.\n\n\nThe metadata here is released under the Creative Commons Public Domain CC0 licence. Images are not included and are not part of the dataset. Use of Tate images is covered on the Copyright and permissions page. You may also license images for commercial use.\n\n\nTate requests that you actively acknowledge and give attribution to Tate wherever possible. Attribution supports future efforts to release other data. It also reduces the amount of ‘orphaned data’, helping retain links to authoritative sources.\n\n\nHere are some examples of Tate data usage in the wild. Please submit a pull request with your creation added to this list.\n\n\nData visualisations by Florian Kräutli\nmachine imagined art by Shardcore\nThe Dimensions of Art by Jim Davenport\nArt as Data as Art and Part II by Os Keyes\nTate Acquisition Data by Zenlan\nTate Collection Geolocalized by Corentin Cournac, Mathieu Dauré, William Duclot and Pierre Présent\nAspect Ratio of Tate Artworks through Time by Joseph Lewis\n\nThere are JSON files with additional metadata in the original GitHub.\n\nSome recommendations\nThis is a dataset has lots of room for cleaning.\n\nThe place of birth/death can be converted to city/country\n\nThe medium has many distinct categories, and some of them can be collapsed by overlap\nYou could also practice converting the text dates to the actual years of interest (although the already exist in the data).\n\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-01-12')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 3)\n\nartwork &lt;- tuesdata$artwork\n\n# Or read in the data manually\n\nartwork &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-01-12/artwork.csv')\nartists &lt;- readr::read_csv(\"https://github.com/tategallery/collection/raw/master/artist_data.csv\")\n\n\nData Dictionary\n\n\n\nartwork.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nUnique ID\n\n\naccession_number\ncharacter\nAccession number\n\n\nartist\ncharacter\nArtist Name\n\n\nartistRole\ncharacter\nArtist or other attribution\n\n\nartistId\ndouble\nArtist ID\n\n\ntitle\ncharacter\nTitle of the piece of art\n\n\ndateText\ncharacter\nDate as raw text (pretty messy)\n\n\nmedium\ncharacter\nMedium of art, quite a lot of overlap\n\n\ncreditLine\ncharacter\nHow acquired\n\n\nyear\ndouble\nYear of creation\n\n\nacquisitionYear\ndouble\nYear acquired\n\n\ndimensions\ncharacter\nDimensions as character\n\n\nwidth\ndouble\nWidth of art\n\n\nheight\ndouble\nHeight of art\n\n\ndepth\ndouble\nDepth of art\n\n\nunits\ncharacter\nunits of measure\n\n\ninscription\ncharacter\ninscription if present\n\n\nthumbnailCopyright\nlogical\nThumbnail copyright\n\n\nthumbnailUrl\ncharacter\nThumbnail URL\n\n\nurl\ncharacter\nart URL\n\n\n\n\n\nartists.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nArtist ID\n\n\nname\ncharacter\nArtist Name\n\n\ngender\ncharacter\nArtist gender\n\n\ndates\ncharacter\nDate as a character\n\n\nyearOfBirth\ndouble\nYear of birth\n\n\nyearOfDeath\ndouble\nYear of death\n\n\nplaceOfBirth\ncharacter\nPlace of birth (typically city, country)\n\n\nplaceOfDeath\ncharacter\nPlace of death (typically city, country)\n\n\nurl\ncharacter\nArtist URL\n\n\n\n\nCleaning Script\nThere is no cleaning script for today, the data is already “tame”."
  },
  {
    "objectID": "data/2021/2021-01-26/readme.html",
    "href": "data/2021/2021-01-26/readme.html",
    "title": "Plastic Pollution",
    "section": "",
    "text": "Break free from plastic header\n\n\n\nPlastic Pollution\nThe data this week comes from Break Free from Plastic courtesy of Sarah Sauve.\nSarah put together a nice Blogpost on her approach to this data, which includes cleaning the data and a Shiny app!\nPer Sarah:\n\nI found out about Break Free From Plastic’s Brand Audits through my involvement with the local Social Justice Cooperative of Newfoundland and Labrador’s Zero Waste Action Team.\nOne of my colleagues and friends proposed an audit in St. John’s, partially to contribute to the global audit and as part of a bigger project to understand the sources of plastic in our city. We completed our audit in October 2020 and are the first submission to BFFP from Newfoundland! You can find our data presented in this Shiny dashboard.\n\n\nIt’s an interesting dataset, with lots of room to play around and so many options for visualization, plus plastic pollution is an important topic to talk about and raise awareness of! You can read BFFP’s Brand Audit Reports for 2018, 2019 and 2020 to get an idea of what they’ve done with the data.\n\nI downloaded the raw data from her Google Drive, and have a short cleaning script at the bottom of this readme. Note that the data has already been combined, but feel free to play around with the raw data itself.\n\nThe data is available through Google Drive; you can find the 2019 data here and the 2020 data here.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-01-26')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 5)\n\nplastics &lt;- tuesdata$plastics\n\n# Or read in the data manually\n\nplastics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-01-26/plastics.csv')\n\n\nData Dictionary\nNote that the plastic types are not in tidy format, and you’ll likely want to pivot_longer().\nThe plastic is categorized by recycling codes.\n\n\n\nplastics.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry of cleanup\n\n\nyear\ndouble\nYear (2019 or 2020)\n\n\nparent_company\ncharacter\nSource of plastic\n\n\nempty\ndouble\nCategory left empty count\n\n\nhdpe\ndouble\nHigh density polyethylene count (Plastic milk containers, plastic bags, bottle caps, trash cans, oil cans, plastic lumber, toolboxes, supplement containers)\n\n\nldpe\ndouble\nLow density polyethylene count (Plastic bags, Ziploc bags, buckets, squeeze bottles, plastic tubes, chopping boards)\n\n\no\ndouble\nCategory marked other count\n\n\npet\ndouble\nPolyester plastic count (Polyester fibers, soft drink bottles, food containers (also see plastic bottles)\n\n\npp\ndouble\nPolypropylene count (Flower pots, bumpers, car interior trim, industrial fibers, carry-out beverage cups, microwavable food containers, DVD keep cases)\n\n\nps\ndouble\nPolystyrene count (Toys, video cassettes, ashtrays, trunks, beverage/food coolers, beer cups, wine and champagne cups, carry-out food containers, Styrofoam)\n\n\npvc\ndouble\nPVC plastic count (Window frames, bottles for chemicals, flooring, plumbing pipes)\n\n\ngrand_total\ndouble\nGrand total count (all types of plastic)\n\n\nnum_events\ndouble\nNumber of counting events\n\n\nvolunteers\ndouble\nNumber of volunteers\n\n\n\n\nCleaning Script\nNOTE: This is not necessary to use this data, but is just an example of how I prepared the plastics.csv dataset, which is already available.\nlibrary(tidyverse)\nlibrary(fs)\n\nfiles_2020 &lt;- fs::dir_ls(\"2020 BFFP National Data Results\") %&gt;% \n  str_subset(\"csv\")\n\nfiles_2019 &lt;- fs::dir_ls(\"2019 Brand Audit Appendix _ Results by Country/Countries\") %&gt;% \n  str_subset(\"csv\")\n\ndata_2020 &lt;- files_2020 %&gt;% \n  map_dfr(read_csv, col_types = cols(\n    Country = col_character(),\n    Parent_company = col_character(),\n    Empty = col_double(),\n    HDPE = col_double(),\n    LDPE = col_double(),\n    O = col_double(),\n    PET = col_double(),\n    PP = col_double(),\n    PS = col_double(),\n    PVC = col_double(),\n    Grand_Total = col_character(),\n    num_events = col_double(),\n    volunteers = col_double()\n  )) %&gt;% \n  mutate(year = 2020, .after = Country) %&gt;% \n  mutate(Grand_Total = parse_number(Grand_Total)) %&gt;% \n  janitor::clean_names()\n\ndata_2019 &lt;- files_2019 %&gt;% \n  set_names(str_replace(., \".*[/]([^.]+)[.].*\", \"\\\\1\")) %&gt;% \n  map_dfr(read_csv, .id = \"country\", col_types = cols(\n    Country = col_character(),\n    Parent_company = col_character(),\n    Empty = col_double(),\n    HDPE = col_double(),\n    LDPE = col_double(),\n    O = col_double(),\n    PET = col_double(),\n    PP = col_double(),\n    PS = col_double(),\n    PVC = col_double(),\n    Grand_Total = col_double(),\n    num_events = col_double(),\n    volunteers = col_double()\n  )) %&gt;% \n  select(country, everything()) %&gt;% \n  mutate(year = 2019, .after = country) %&gt;% \n  janitor::clean_names()  %&gt;% \n  mutate(pp = if_else(is.na(pp_2), pp, pp_2 + pp),\n         ps = if_else(is.na(ps_2), ps, ps + ps_2)) %&gt;% \n  rename(parent_company = parent_co_final, num_events = number_of_events, volunteers= number_of_volunteers) %&gt;% \n  select(-ps_2, -pp_2)\n\ncombo_data &lt;- bind_rows(data_2019, data_2020) \n\ncombo_data %&gt;% \n  write_csv(\"2021/2021-01-26/plastics.csv\")"
  },
  {
    "objectID": "data/2021/2021-02-09/readme.html",
    "href": "data/2021/2021-02-09/readme.html",
    "title": "Wealth and income over time",
    "section": "",
    "text": "Wealth and income over time\nNote: The income_mean dataset appears to have some level of duplicate data for “Asian Alone” and “Asian Alone or in Combination”. We are not changing the dataset to avoid breaking old code, but you may want to start from the raw data if you are using this for anything important.\nThe data this week comes from the Urban Institute and the US Census.\nThe Urban Institute lays out nine charts about racial wealth inequality in America in this article. They include several summary-level datasets that I’ve included.\n\nNine Charts about Wealth Inequality in America Why hasn’t wealth inequality improved over the past 50 years? And why, in particular, has the racial wealth gap not closed? These nine charts illustrate how income inequality, earnings gaps, homeownership rates, retirement savings, student loan debt, and lopsided asset-building subsidies have contributed to these growing wealth disparities.\n\nThe US Census provides Historical Income Tables, of which we have joined several to compare wealth and income over time by race.\nThe Brookings Institute has another article detailing some additional reasons how historically wealth has been difficult to acquire or even institutionally kept away from many Black Americans.\n\nEfforts by Black Americans to build wealth can be traced back throughout American history. But these efforts have been impeded in a host of ways, beginning with 246 years of chattel slavery and followed by Congressional mismanagement of the Freedman’s Savings Bank (which left 61,144 depositors with losses of nearly $3 million in 1874), the violent massacre decimating Tulsa’s Greenwood District in 1921 (a population of 10,000 that thrived as the epicenter of African American business and culture, commonly referred to as “Black Wall Street”), and discriminatory policies throughout the 20th century including the Jim Crow Era’s “Black Codes” strictly limiting opportunity in many southern states, the GI bill, the New Deal’s Fair Labor Standards Act’s exemption of domestic agricultural and service occupations, and redlining. Wealth was taken from these communities before it had the opportunity to grow.\n\n\nReference Articles\n\nFreedman’s Savings Bank congressional mismanagement\n\nTulsa Race Massacre\n\nJim Crow Laws\n\nThe Pew Research Center also outlines demographic trends and economic well-being in their article and another article on details since the Great Recession (2016). The Federal Reserve has another article on Racial disparities in wealth.\nThere’s a ton of different datasets, all within the scope of wealth, income, or debt over time and by race. The data were fairly tame, but in Excel sheets that weren’t ready for analyses. I went ahead and cleaned the data, but feel free to play with the original raw data as well. Many of the datasets could be merged/joined but I would be careful about potential spurious correlations. A good starter dataset is the income_distribution.csv as it includes year, race, number of households, income median/mean, income bracket, and income distribution by bracket.\nMore appropriate for summary plots:\n- student_debt, retirement, home_owner, lifetime_earn, lifetime_wealth, race_wealth, income_time, and wealth_distribution are from the Urban Institute and are more appropriate for summary plots.\nMore appropriate for comparisons:\n- income_mean, income_distribution, income_limits, and income_aggregate are more appropriate as comparison datasets and have the full racial breakdown across the available years.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-02-09')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 7)\n\nlifetime_earn &lt;- tuesdata$lifetime_earn\n\n# Or read in the data manually\n\nlifetime_earn &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/lifetime_earn.csv')\nstudent_debt &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/student_debt.csv')\nretirement &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/retirement.csv')\nhome_owner &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/home_owner.csv')\nrace_wealth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/race_wealth.csv')\nincome_time &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/income_time.csv')\nincome_limits &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/income_limits.csv')\nincome_aggregate &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/income_aggregate.csv')\nincome_distribution &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/income_distribution.csv')\nincome_mean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-09/income_mean.csv')\n\n\nData Dictionary\n\n\n\nlifetime_earn.csv\nAverage lifetime earning by race/gender\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ngender\ncharacter\ngender column\n\n\nrace\ncharacter\nRacial group\n\n\nlifetime_earn\ndouble\nLifetime earnings\n\n\n\n\n\nstudent_debt.csv\nAverage family student loan debt for aged 25-55, by race and year normalized to 2016 dollars.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear of measure\n\n\nrace\ncharacter\nRacial group\n\n\nloan_debt\ndouble\nLoan debt\n\n\nloan_debt_pct\ndouble\nShare of families with student loan debt\n\n\n\n\n\nretirement.csv\nAverage family liquid retirement savings normalized to 2016 dollars.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\nrace\ncharacter\nRacial group\n\n\nretirement\ndouble\nRetirement dollars\n\n\n\n\n\nhome_owner.csv\nHome ownership percentage for families by\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\nrace\ncharacter\nRacial group\n\n\nhome_owner_pct\ndouble\nHome ownership by race/ethnicity\n\n\n\n\n\nrace_wealth.csv\nFamily wealth by race/year/measure normalized to 2016, with measures of central tendency with mean and median.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntype\ncharacter\nType of measure, either median or mean\n\n\nyear\ndouble\nYear\n\n\nrace\ncharacter\nRacial group\n\n\nwealth_family\ndouble\nFamily wealth by race/year/measure normalized to 2016.\n\n\n\n\n\nincome_time.csv\nFamily-level income by percentile and year.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nYear\n\n\npercentile\ncharacter\nIncome percentile (10th, 50th, 90th)\n\n\nincome_family\ndouble\nFamilial income\n\n\n\n\n\nincome_limits.csv\nFamilial income limits for each fifth by year and race.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\nrace\ncharacter\nRacial group\n\n\ndollar_type\ncharacter\nDollars in that year or normalized to 2019\n\n\nnumber\ndouble\nNumber of households by racial group\n\n\nincome_quintile\ncharacter\nIncome quintile as well as top 5%\n\n\nincome_dollars\ndouble\nIncome in US dollars, specific to dollar type\n\n\n\n\n\nincome_aggregate.csv\nShare of aggregate income received by each fifth and top 5% of each racial group/household.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\nrace\ncharacter\nRacial group\n\n\nnumber\ndouble\nNumber of households by racial group\n\n\nincome_quintile\ncharacter\nIncome quintile and/or top 5%\n\n\nincome_share\ndouble\nIncome share as a percentage\n\n\n\n\n\nincome_distribution.csv\nHouseholds by total money income, race, and hispanic origin of householder separated by year and income groups.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nYear\n\n\nrace\ncharacter\nRacial Group\n\n\nnumber\ndouble\nNumber of households\n\n\nincome_median\ninteger\nIncome median\n\n\nincome_med_moe\ninteger\nIncome median margin of error\n\n\nincome_mean\ninteger\nIncome mean\n\n\nincome_mean_moe\ninteger\nIncome mean margin of error\n\n\nincome_bracket\ncharacter\nIncome bracket (9 total brackets between &lt;$15,000 and &gt;$200,000\n\n\nincome_distribution\ndouble\nIncome distribution as the percentage of each year/racial group - should add up to 100 for a specific year and race.\n\n\n\n\n\nincome_mean.csv\nMean income received by each fifth and top 5% of each racial group.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\nrace\ncharacter\nRacial group\n\n\ndollar_type\ncharacter\nDollar type, ie dollar relative to that year or normalized to 2019\n\n\nincome_quintile\ncharacter\nIncome quintile and/or top 5%\n\n\nincome_dollars\ndouble\nIncome dollar average\n\n\n\n\nCleaning Script\nNote this is just how I cleaned the data, you can try to make some of this into functions, or you can work with the raw Excel files as I’ve uploaded them as well.\nlibrary(tidyverse)\nlibrary(readxl)\n\n### Urban Institute\n\n# Student Debt ------------------------------------------------------------\n\nstudent_debt_raw &lt;- read_excel(\"2021/2021-02-09/StudentLoans.xlsx\", skip = 1)\n\nstudent_debt_val &lt;- student_debt_raw %&gt;%\n  rename(year = 1) %&gt;%\n  slice(1:10) %&gt;%\n  arrange(desc(year)) %&gt;%\n  pivot_longer(cols = -year, names_to = \"race\", values_to = \"loan_debt\") %&gt;%\n  mutate(across(c(year, loan_debt), as.double))\n\nstudent_debt_pct &lt;- student_debt_raw %&gt;%\n  rename(year = 1) %&gt;%\n  slice(14:23) %&gt;%\n  arrange(desc(year)) %&gt;%\n  pivot_longer(cols = -year, names_to = \"race\", values_to = \"loan_debt_pct\") %&gt;%\n  mutate(across(c(year, loan_debt_pct), as.double))\n\nstudent_debt &lt;- left_join(student_debt_val, student_debt_pct, by = c(\"year\", \"race\"))\n\nstudent_debt \n\nstudent_debt %&gt;% \n  write_csv(\"2021/2021-02-09/student_debt.csv\")\n\n\n# Retirement --------------------------------------------------------------\n\n\nretirement_raw &lt;- read_excel(\"2021/2021-02-09/Retirement.xlsx\", skip = 1)\n\nretirement &lt;- retirement_raw %&gt;%\n  rename(year = 1) %&gt;%\n  slice(1:10) %&gt;%\n  mutate(across(everything(), as.double)) %&gt;%\n  pivot_longer(-year, names_to = \"race\", values_to = \"retirement\")\n\nretirement %&gt;% \n  write_csv(\"2021/2021-02-09/retirement.csv\")\n\n\n# Home Ownership ----------------------------------------------------------\n\nhome_raw &lt;- read_excel(\"2021/2021-02-09/Homeownership.xlsx\", skip = 1)\n\nhome_owner &lt;- home_raw %&gt;%\n  rename(year = 1) %&gt;%\n  filter(!is.na(Black)) %&gt;%\n  mutate(across(everything(), as.double)) %&gt;%\n  pivot_longer(-year, names_to = \"race\", values_to = \"home_owner_pct\")\n\nhome_owner %&gt;% \n  write_csv(\"2021/2021-02-09/home_owner.csv\")\n\n\n# Lifetime Earnings -------------------------------------------------------\n\nlifetime_raw &lt;- read_excel(\"2021/2021-02-09/LifetimeEarnings.xlsx\")\n\nlifetime_earn &lt;- lifetime_raw %&gt;%\n  set_names(nm = c(\"gender\", \"race\", \"lifetime_earn\")) %&gt;%\n  fill(gender) %&gt;%\n  filter(!is.na(race))\n\nlifetime_earn\n\nlifetime_earn %&gt;% \n  write_csv(\"2021/2021-02-09/lifetime_earn.csv\")\n\n\n# Wealth Lifetime ---------------------------------------------------------\n\nwealth_life_raw &lt;- read_excel(\"2021/2021-02-09/WealthbyRaceoverLifetime.xlsx\", skip = 1)\n\nwealth_life &lt;- wealth_life_raw %&gt;%\n  rename(race = 1) %&gt;%\n  filter(!is.na(`1983`)) %&gt;%\n  filter(!is.na(race)) %&gt;%\n  mutate(type = rep(c(\"Average\", \"Median\"), each = 2), .before = \"race\") %&gt;%\n  pivot_longer(cols = c(`1983`:`2016`), names_to = \"year\", values_to = \"wealth_lifetime\")\n\nwealth_life\n\nwealth_life %&gt;% \n  write_csv(\"2021/2021-02-09/lifetime_wealth.csv\")\n\n\n# Wealth by Race ----------------------------------------------------------\n\nwealth_race_raw &lt;- read_excel(\"2021/2021-02-09/WealthbyRace.xlsx\", skip = 1)\n\nwealth_race &lt;- wealth_race_raw %&gt;%\n  rename(year = 1) %&gt;%\n  filter(!is.na(White)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  mutate(across(everything(), as.double)) %&gt;%\n  mutate(type = rep(c(\"Average\", \"Median\"), each = 12), .before = \"year\") %&gt;%\n  pivot_longer(cols = c(`Non-White`:Hispanic), names_to = \"race\", values_to = \"wealth_family\")\n\nwealth_race\n\nwealth_race %&gt;% \n  write_csv(\"2021/2021-02-09/race_wealth.csv\")\n\n\n# Income Distribution -----------------------------------------------------\n\nincome_raw &lt;- read_excel(\"2021/2021-02-09/IncomeDistribution.xlsx\", skip = 1)\n\nincome_time &lt;- income_raw %&gt;%\n  filter(!is.na(`10th Percentile`)) %&gt;%\n  mutate(year = as.integer(Year)) %&gt;%\n  pivot_longer(\n    cols = 2:4,\n    names_to = \"percentile\",\n    values_to = \"income_family\",\n    names_pattern = \"([0-9]+[th]+)\"\n  ) %&gt;%\n  select(year, percentile, income_family)\n\nincome_time\n\nincome_time %&gt;% \n  write_csv(\"2021/2021-02-09/income_time.csv\")\n\n\n# Wealth Distribution -----------------------------------------------------\n\nwealth_raw &lt;- read_excel(\"2021/2021-02-09/WealthDistribution.xlsx\", skip = 1)\n\nwealth_distribution &lt;- wealth_raw %&gt;%\n  rename(percentile = ...1) %&gt;%\n  filter(!is.na(`1963`)) %&gt;%\n  pivot_longer(\n    cols = -percentile,\n    names_to = \"year\",\n    values_to = \"wealth_family\"\n  ) %&gt;%\n  mutate(across(c(year, percentile), as.integer))\n\nwealth_distribution\n\nwealth_distribution %&gt;% \n  write_csv(\"2021/2021-02-09/wealth_distribution.csv\")\n\nwealth_distribution %&gt;%\n  ggplot(aes(x = percentile, y = wealth_family, color = year, group = year)) +\n  geom_line() +\n  scale_color_viridis_c()\n\n### Census\n\n\n# H1a----------------------------------------------------------------------\n\nh01a_raw &lt;- read_excel(\"2021/2021-02-09/h01a.xlsx\", skip = 6)\n\nh1_asian &lt;- h01a_raw %&gt;%\n  rename(year = 1, number = 2, `Top 5%` = 7) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = rep(c(\"Asian Alone or in Combination\", \"Asian Alone\"), each = 36),\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 36),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = Lowest:last_col(),\n    names_to = \"income_quintile\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  )\n\nh1_asian\n\n\n# h1ar --------------------------------------------------------------------\n\nh01ar_raw &lt;- read_excel(\"2021/2021-02-09/h01ar.xlsx\", skip = 5)\n\nh1_all &lt;- h01ar_raw %&gt;%\n  rename(year = 1, number = 2, `Top 5%` = 7) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = \"All Races\",\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 53),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = Lowest:last_col(),\n    names_to = \"income_quintile\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  )\n\nh1_all\n\n\n# h1b ---------------------------------------------------------------------\n\nh01b_raw &lt;- read_excel(\"2021/2021-02-09/h01b.xlsx\", skip = 6)\n\nh1_black &lt;- h01b_raw %&gt;%\n  rename(year = 1, number = 2, `Top 5%` = 7) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;% \n  mutate(\n    race =\n      rep(\n        c(\n          rep(\"Black Alone or in Combination\", 18),\n          rep(\"Black Alone\", 53)\n        ),\n        times = 2\n      ),\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 71),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = Lowest:last_col(),\n    names_to = \"income_quintile\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  )\n\nh1_black\n\n\n# h1h ---------------------------------------------------------------------\n\nh01h_raw &lt;- read_excel(\"2021/2021-02-09/h01h.xlsx\", skip = 5)\n\nh1_hispanic &lt;- h01h_raw %&gt;%\n  rename(year = 1, number = 2, `Top 5%` = 7) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = \"Hispanic\",\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 48),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = Lowest:last_col(),\n    names_to = \"income_quintile\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  )\n\nh1_hispanic\n\n\n# h1w ---------------------------------------------------------------------\n\nh01w_raw &lt;- read_excel(\"2021/2021-02-09/h01w.xlsx\", skip = 6)\n\nh01w_raw\n\nh1_white &lt;- h01w_raw %&gt;%\n  rename(year = 1, number = 2, `Top 5%` = 7) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = \"White Alone\",\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 53),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = Lowest:last_col(),\n    names_to = \"income_quintile\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  )\n\nh1_white\n\n\n# h1wnh -------------------------------------------------------------------\n\nh01wnh_raw &lt;- read_excel(\"2021/2021-02-09/h01wnh.xlsx\", skip = 6)\n\nh01wnh_raw\n\nh1_white_nh &lt;- h01wnh_raw %&gt;%\n  rename(year = 1, number = 2, `Top 5%` = 7) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = \"White, Not Hispanic\",\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 48),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = Lowest:last_col(),\n    names_to = \"income_quintile\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  )\n\nh1_white_nh\n\n# h1 combo ----------------------------------------------------------------\n\n\n\nall_h1_comb &lt;- bind_rows(\n  h1_all,\n  h1_asian,\n  h1_black,\n  h1_hispanic,\n  h1_white,\n  h1_white_nh\n) %&gt;%\n  mutate(\n    number = as.integer(number),\n    number = number * 1000\n    )\n\n\nall_h1_comb %&gt;% \n  write_csv(\"2021/2021-02-09/income_limits.csv\")\n\n## H2\n\n\n# h2a ---------------------------------------------------------------------\n\n\nh2a_raw &lt;- read_excel(\"2021/2021-02-09/h02a.xlsx\", skip = 5)\n\nh2a_raw\n\nh2_asian &lt;- h2a_raw %&gt;%\n  rename(year = 1, number = 2) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = rep(c(\"Asian Alone or in Combination\", \"Asian Alone\"), each = 18),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_share\",\n    values_transform = list(income_share = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\"),\n    number = as.integer(number)\n  )\n\nh2_asian\n\n# h2ar --------------------------------------------------------------------\n\nh2ar_raw &lt;- read_excel(\"2021/2021-02-09/h02ar.xlsx\", skip = 4)\n\nh2ar_raw\n\nh2_all &lt;- h2ar_raw %&gt;%\n  rename(year = 1, number = 2) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = \"All Races\",\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_share\",\n    values_transform = list(income_share = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\"),\n    number = as.integer(number)\n  )\n\nh2_all\n\n# h2b ---------------------------------------------------------------------\n\nh2b_raw &lt;- read_excel(\"2021/2021-02-09/h02b.xlsx\", skip = 5)\n\nh2b_raw\n\nh2_black &lt;- h2b_raw %&gt;%\n  rename(year = 1, number = 2) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = c(\n      rep(\"Black Alone or in Combination\", 18),\n      rep(\"Black Alone\", 53)\n    ),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_share\",\n    values_transform = list(income_share = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\"),\n    number = as.integer(number)\n  )\n\nh2_black\n\n\n# h2h ---------------------------------------------------------------------\n\nh2h_raw &lt;- read_excel(\"2021/2021-02-09/h02h.xlsx\", skip = 4)\n\nh2h_raw\n\nh2_hispanic &lt;- h2h_raw %&gt;%\n  rename(year = 1, number = 2) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = \"Hispanic\",\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_share\",\n    values_transform = list(income_share = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\"),\n    number = as.integer(number)\n  )\n\nh2_hispanic\n\n# h2w ---------------------------------------------------------------------\n\nh2w_raw &lt;- read_excel(\"2021/2021-02-09/h02w.xlsx\", skip = 5)\n\nh2w_raw\n\nh2_white &lt;- h2w_raw %&gt;%\n  rename(year = 1, number = 2) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = \"White Alone\",\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_share\",\n    values_transform = list(income_share = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\"),\n    number = as.integer(number)\n  )\n\nh2_white\n\n# h2wnh -------------------------------------------------------------------\n\nh2wnh_raw &lt;- read_excel(\"2021/2021-02-09/h02wnh.xlsx\", skip = 5)\n\nh2wnh_raw\n\nh2_white_nh &lt;- h2wnh_raw %&gt;%\n  rename(year = 1, number = 2) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(number)) %&gt;%\n  mutate(\n    race = \"White, Not Hispanic\",\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_share\",\n    values_transform = list(income_share = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\"),\n    number = as.integer(number)\n  )\n\nh2_white_nh\n\n# h2 combo ----------------------------------------------------------------\n\nall_h2_comb &lt;- bind_rows(\n  h2_all,\n  h2_asian,\n  h2_black,\n  h2_hispanic,\n  h2_white,\n  h2_white_nh\n) %&gt;%\n  mutate(\n    number = as.integer(number),\n    number = number * 1000\n    )\n\nall_h2_comb\n\nall_h2_comb %&gt;% \n  write_csv(\"2021/2021-02-09/income_aggregate.csv\")\n\n## h3 ----\n\n# H3a----------------------------------------------------------------------\n\nh03a_raw &lt;- read_excel(\"2021/2021-02-09/h03a.xlsx\", skip = 5)\n\nh3_asian &lt;- h03a_raw %&gt;%\n  rename(year = 1) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(`Lowest\\nfifth`)) %&gt;%\n  mutate(\n    race = rep(c(\"Asian Alone or in Combination\", \"Asian Alone\"), each = 36),\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 36),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\")\n  )\n\nh3_asian\n\n\n# h3ar --------------------------------------------------------------------\n\nh03ar_raw &lt;- read_excel(\"2021/2021-02-09/h03ar.xlsx\", skip = 4)\n\nh03ar_raw\n\nh3_all &lt;- h03ar_raw %&gt;%\n  rename(year = 1) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(`Lowest\\nfifth`)) %&gt;%\n  mutate(\n    race = \"All Races\",\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 53),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\")\n  )\n\nh3_all\n\n\n\n# h3b ---------------------------------------------------------------------\n\nh03b_raw &lt;- read_excel(\"2021/2021-02-09/h03b.xlsx\", skip = 5)\n\nh3_black &lt;- h03b_raw %&gt;%\n  rename(year = 1) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(`Lowest\\nfifth`)) %&gt;%\n  mutate(\n    race =\n      rep(\n        c(\n          rep(\"Black Alone or in Combination\", 18),\n          rep(\"Black Alone\", 53)\n        ),\n        times = 2\n      ),\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 71),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\")\n  )\n\nh3_black\n\n########## START HERE ###########\n\n# h3h ---------------------------------------------------------------------\n\nh03h_raw &lt;- read_excel(\"2021/2021-02-09/h03h.xlsx\", skip = 4)\n\nh03h_raw\n\nh3_hispanic &lt;- h03h_raw %&gt;%\n  rename(year = 1) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(`Lowest\\nfifth`)) %&gt;%\n  mutate(\n    race = \"Hispanic\",\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 48),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\")\n  )\n\nh3_hispanic\n\n\n# h1w ---------------------------------------------------------------------\n\nh03w_raw &lt;- read_excel(\"2021/2021-02-09/h03w.xlsx\", skip = 5)\n\nh03w_raw\n\nh3_white &lt;- h03w_raw %&gt;%\n  rename(year = 1) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(`Lowest\\nfifth`)) %&gt;%\n  mutate(\n    race = \"White Alone\",\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 53),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\")\n  )\n\nh3_white\n\n\n# h3wnh -------------------------------------------------------------------\n\nh03wnh_raw &lt;- read_excel(\"2021/2021-02-09/h03wnh.xlsx\", skip = 5)\n\nh03wnh_raw\n\nh3_white_nh &lt;- h03wnh_raw %&gt;%\n  rename(year = 1) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(year = str_sub(year, 1, 4), year = as.integer(year)) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(`Lowest\\nfifth`)) %&gt;%\n  mutate(\n    race = \"White, Not Hispanic\",\n    dollar_type = rep(c(\"Current Dollars\", \"2019 Dollars\"), each = 48),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = 4:last_col(),\n    names_to = \"income_quintile\",\n    names_pattern = \"^(.*?)[\\n]\",\n    values_to = \"income_dollars\",\n    values_transform = list(income_dollars = as.double)\n  ) %&gt;%\n  mutate(\n    income_quintile = str_replace(income_quintile, \"5\", \"5%\")\n  )\n\nh3_white_nh\n\n# h1 combo ----------------------------------------------------------------\n\nall_h3_comb &lt;- bind_rows(\n  h3_all,\n  h3_asian,\n  h3_black,\n  h3_hispanic,\n  h3_white,\n  h3_white_nh\n)\n\nall_h3_comb \n\nall_h3_comb %&gt;% \n  count(year, sort = TRUE)\n\nall_h3_comb %&gt;% \n  write_csv(\"2021/2021-02-09/income_mean.csv\")\n\nall_h3_comb %&gt;%\n  filter(income_quintile != \"Top 5%\") %&gt;%\n  mutate(income_quintile = factor(income_quintile, levels = c(\"Lowest\", \"Second\", \"Middle\", \"Fourth\", \"Highest\"))) %&gt;%\n  filter(dollar_type == \"2019 Dollars\") %&gt;%\n  filter(race %in% c(\"White Alone\", \"Black Alone\", \"Hispanic\")) %&gt;%\n  ggplot(aes(x = year, y = income_dollars, color = race, fill = race, group = race)) +\n  geom_line() +\n  facet_wrap(~income_quintile, scales = \"free_y\")\n\n#### H17 ####\n\n\n# H17 ---------------------------------------------------------------------\n\nh17_raw &lt;- read_excel(\"2021/2021-02-09/h17.xlsx\", skip = 5)\n\nincome_distribution &lt;- h17_raw %&gt;%\n  rename(\n    year = 1,\n    number = 2,\n    income_median = Estimate...13,\n    income_mean = Estimate...15,\n    income_med_moe = `Margin of error (±)...14`,\n    income_mean_moe = `Margin of error (±)...16`\n  ) %&gt;%\n  filter(str_detect(year, \"2017 \\\\(|\\\\(38\", negate = TRUE)) %&gt;%\n  mutate(\n    year = str_sub(year, 1, 4),\n    year = as.integer(year),\n    number = as.integer(number)\n  ) %&gt;%\n  filter(!is.na(year)) %&gt;%\n  filter(!is.na(`Under $15,000`)) %&gt;%\n  select(-Total) %&gt;%\n  mutate(\n    race = c(\n      rep(\"All Races\", 53),\n      rep(\"White Alone\", 53),\n      rep(\"White Alone, Not Hispanic\", 48),\n      rep(\"Black Alone or in Combination\", 18),\n      rep(\"Black Alone\", 53),\n      rep(\"Asian Alone or in Combination\", 18),\n      rep(\"Asian Alone\", 33),\n      rep(\"Hispanic (Any Race)\", 48)\n    ),\n    .after = \"year\"\n  ) %&gt;%\n  pivot_longer(\n    cols = `Under $15,000`:`$200,000 and over`,\n    names_to = \"income_bracket\",\n    values_to = \"income_distribution\",\n    values_transform = list(income_distribution = as.double)\n  ) %&gt;%\n  mutate(\n    across(c(income_median:income_mean_moe), as.integer),\n    number = number * 1000\n    )\n\nincome_distribution \n\nincome_distribution %&gt;% \n  write_csv(\"2021/2021-02-09/income_distribution.csv\")\n\nincome_levels &lt;- c(\n  \"Under $15,000\",\n  \"$15,000 to $24,999\",\n  \"$25,000 to $34,999\",\n  \"$35,000 to $49,999\",\n  \"$50,000 to $74,999\",\n  \"$75,000 to $99,999\",\n  \"$100,000 to $149,999\",\n  \"$150,000 to $199,999\",\n  \"$200,000 and over\"\n)\n\n# test the plot\nincome_distribution %&gt;%\n  filter(race %in% c(\"Black Alone\", \"White Alone\", \"Hispanic (Any Race)\")) %&gt;%\n  filter(year &gt;= 1980) %&gt;%\n  mutate(income_bracket = factor(income_bracket, levels = income_levels)) %&gt;%\n  ggplot(aes(x = year, y = income_distribution, color = race, fill = race)) +\n  geom_col(position = \"fill\") +\n  facet_wrap(~income_bracket)"
  },
  {
    "objectID": "data/2021/2021-02-23/readme.html",
    "href": "data/2021/2021-02-23/readme.html",
    "title": "Employed Status",
    "section": "",
    "text": "Employed Status\nPlease note that the #DuboisChallenge is still on going! It might be interesting to revisit that data from last week OR apply the same visual techniques to this modern data.\nThe data this week comes from the BLS, specifically table cpsaat17 across several years.\n\nEmployed persons by industry, sex, race, and occupation\n\nI went ahead and downloaded the data by year for the past 6 years (2015 to 2020).\nI also scraped some earnings data from their self-service data tool, which provided an interesting challenge! If you want to play around with some “offline” web scraping it’s worth a shot.\n\nWeekly and hourly earnings data from the Current Population Survey\n\nThe BLS has a corresponding article with similar summary data.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-02-23')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 9)\n\nemployed &lt;- tuesdata$employed\n\n# Or read in the data manually\n\nemployed &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-23/employed.csv')\n\nearn &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-02-23/earn.csv')\n\n\nData Dictionary\n\n\n\nemployed.csv\n\nEmployed persons by industry, sex, race, and occupation\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nindustry\ncharacter\nIndustry group\n\n\nmajor_occupation\ncharacter\nMajor Occupation category\n\n\nminor_occupation\ncharacter\nMinor Occupation Categoru\n\n\nrace_gender\ncharacter\nRace or gender group (total, men/women, race)\n\n\nindustry_total\ndouble\nIndustry total\n\n\nemploy_n\ndouble\nEmployed number\n\n\nyear\ninteger\nYear\n\n\n\n\n\nearn.csv\n\nWeekly median earnings and number of persons employed by race/gender/age group over time\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsex\ncharacter\nGender\n\n\nrace\ncharacter\nRacial group\n\n\nethnic_origin\ncharacter\nEthnic origin (hispanic or non-hispanic)\n\n\nage\ncharacter\nAge group\n\n\nyear\ninteger\nYear\n\n\nquarter\ninteger\nQuarter\n\n\nn_persons\ndouble\nNumber of persons employed by group\n\n\nmedian_weekly_earn\ninteger\nMedian weekly earning in current dollars\n\n\n\n\nCleaning Script\nThis was an interesting problem, and I think I have a decent reproducible pipeline for this one!\nNote the data is already cleaned, but feel free to test the below out for yourself.\n#basic url\n# https://www.bls.gov/cps/aa2020/cpsaat17.xlsx\n\nlibrary(tidyverse)\nlibrary(glue)\n\n\nget_bls_report &lt;- function(year){\n  \n  report_url &lt;- glue::glue(\"https://www.bls.gov/cps/aa{year}/cpsaat17.xlsx\")\n  \n  download.file(report_url, destfile = glue(\"2021/2021-02-23/bls-{year}.xlsx\"))\n}\n\nex_2019 &lt;- readxl::read_excel(\"2021/2021-02-23/bls-2019.xlsx\")\n\n2015:2019 %&gt;% \n  walk(get_bls_report)\n\n# 2020 has no year in front of it\ndownload.file(\n  \"https://www.bls.gov/cps/cpsaat17.xlsx\", \n  destfile = \"2021/2021-02-23/bls-2020.xlsx\"\n  )\n\n# Raw BLS -----------------------------------------------------------------\n\nraw_2020 &lt;- readxl::read_excel(\"2021/2021-02-23/bls-2020.xlsx\", skip = 3) %&gt;% \n  slice(1:(n()-2))\n\nmajor_grp &lt;- raw_2020 %&gt;% \n  slice(1) %&gt;% \n  select(3:last_col()) %&gt;% \n  set_names(nm = glue::glue(\"...{1:ncol(.)}\")) %&gt;% \n  pivot_longer(cols = everything(), values_to = \"major_grp\") \n\nminor_grp &lt;- raw_2020 %&gt;% \n  slice(2) %&gt;% \n  select(3:last_col()) %&gt;% \n  set_names(nm = glue::glue(\"...{1:ncol(.)}\")) %&gt;% \n  pivot_longer(cols = everything(), values_to = \"minor_grp\") \n\ncombo_grp &lt;- left_join(major_grp, minor_grp, by = \"name\") %&gt;% \n  mutate(across(.fns = ~str_replace_all(.x, \"\\n\", \" \"))) %&gt;% \n  mutate(across(.fns = ~str_remove_all(.x, \"\\r\"))) %&gt;% \n  mutate(across(.fns = ~str_replace_all(.x, \"- \", \"\"))) %&gt;% \n  tidyr::fill(major_grp)\n\nname_fill &lt;- c(\"race_gender\", \"category\", \"total\", glue(\"...{1:11}\"))\n\nclean_2020 &lt;- raw_2020 %&gt;% \n  rename(category = 1) %&gt;% \n  mutate(\n    race_gender = if_else(\n      str_detect(category, \"Agriculture and related\"),\n      lag(category),\n      NA_character_\n      ),\n    .before = category\n    ) %&gt;% \n  fill(race_gender) %&gt;% \n  slice(5:n()) %&gt;% \n  set_names(nm = name_fill) %&gt;% \n  pivot_longer(cols = contains(\"...\"), names_to = \"name\", values_to = \"employ_n\") %&gt;% \n  left_join(combo_grp, by = \"name\") %&gt;% \n  mutate(year = 2020) %&gt;% \n  select(category, major_grp, minor_grp, race_gender, cat_total = total, employ_n)\n\n# Make a function!\n\nclean_bls &lt;- function(year){\n  \n  raw_df &lt;- readxl::read_excel(glue(\"2021/2021-02-23/bls-{year}.xlsx\"), skip = 3) %&gt;% \n    slice(1:(n()-2))\n  \n  major_grp &lt;- raw_df %&gt;% \n    slice(1) %&gt;% \n    select(3:last_col()) %&gt;% \n    set_names(nm = glue::glue(\"...{1:ncol(.)}\")) %&gt;% \n    pivot_longer(cols = everything(), values_to = \"major_grp\") \n  \n  minor_grp &lt;- raw_df %&gt;% \n    slice(2) %&gt;% \n    select(3:last_col()) %&gt;% \n    set_names(nm = glue::glue(\"...{1:ncol(.)}\")) %&gt;% \n    pivot_longer(cols = everything(), values_to = \"minor_grp\") \n  \n  combo_grp &lt;- left_join(major_grp, minor_grp, by = \"name\") %&gt;% \n    mutate(across(.fns = ~str_replace_all(.x, \"\\n\", \" \"))) %&gt;% \n    mutate(across(.fns = ~str_remove_all(.x, \"\\r\"))) %&gt;% \n    mutate(across(.fns = ~str_replace_all(.x, \"- \", \"\"))) %&gt;% \n    tidyr::fill(major_grp)\n  \n  name_fill &lt;- c(\"race_gender\", \"category\", \"total\", glue(\"...{1:11}\"))\n  \n  clean_df &lt;- raw_df %&gt;% \n    rename(category = 1) %&gt;% \n    mutate(\n      race_gender = if_else(\n        str_detect(category, \"Agriculture and related\"),\n        lag(category),\n        NA_character_\n      ),\n      .before = category\n    ) %&gt;% \n    fill(race_gender) %&gt;% \n    slice(5:n()) %&gt;% \n    set_names(nm = name_fill) %&gt;% \n    pivot_longer(cols = contains(\"...\"), names_to = \"name\", values_to = \"employ_n\") %&gt;% \n    left_join(combo_grp, by = \"name\") %&gt;% \n    select(industry = category, major_occupation = major_grp, minor_occupation = minor_grp, race_gender, industry_total = total, employ_n) %&gt;% \n    mutate(year = year)\n   \n  clean_df\n  \n}\n\n# combine the data\n\nall_bls &lt;- 2015:2020 %&gt;% \n  map_dfr(clean_bls) %&gt;% \n  arrange(desc(year)) %&gt;% \n  mutate(\n    industry_total = as.integer(industry_total) * 1000,\n    employ_n = as.integer(employ_n) * 1000\n    )\n\n# sanity check plot\n\nall_bls %&gt;% \n  filter(race_gender == \"Black or African American\") %&gt;% \n  filter(minor_occupation == \"Sales and related occupations\") %&gt;% \n  ggplot(aes(x = year, y = employ_n, group = industry)) +\n  geom_line()\n\nall_bls %&gt;% \n  write_csv(\"2021/2021-02-23/employed.csv\")\n  \n### Here I'm reading against the HTML file included\n\nlibrary(rvest)\n\nraw_html &lt;- read_html(\"2021/2021-02-23/bls-all.htm\")\n\nraw_html %&gt;% \n  html_nodes(\"table.catalog\") %&gt;% \n  html_table()\n\nall_catalog_raw &lt;- raw_html %&gt;% \n  html_nodes(\"table.catalog\") %&gt;% \n  html_table() \n\n\nall_catalog_clean &lt;- all_catalog_raw %&gt;% \n  map(clean_catalog)\n\nclean_catalog &lt;- function(table){\n  \n  table %&gt;% \n    pivot_wider(names_from = X1, values_from = X2) %&gt;% \n    janitor::clean_names()\n  \n}\n\nclean_catalog(all_catalog_raw[[2]])\n\n\nall_table_raw &lt;- raw_html %&gt;% \n  html_nodes(\"table.regular-data\")%&gt;% \n  html_table() \n\nall_table_clean &lt;- all_table_raw %&gt;% \n  map_dfr(clean_table, .id = \"id\") %&gt;% \n  mutate(id = as.integer(id))\n\nclean_table &lt;- function(table){\n  \n  table %&gt;% \n    filter(str_detect(Year, \"Corrected\", negate = TRUE)) %&gt;% \n    mutate(across(everything(), ~str_remove(.x, \"\\\\(C\\\\)\"))) %&gt;% \n    pivot_longer(Qtr1:Qtr4, names_to = \"quarter\", values_to = \"value\") %&gt;% \n    rename(year = Year)\n  \n}\n\ncombine_tables &lt;- bind_rows(all_table_clean) %&gt;% \n  mutate(id = 1:n())\n\ncombined_data &lt;- all_catalog_clean %&gt;% \n  bind_rows() %&gt;% \n  mutate(id = row_number()) %&gt;% \n  left_join(all_table_clean, by = \"id\") %&gt;% \n  mutate(\n    year = as.integer(year), \n    quarter = str_remove(quarter, \"Qtr\") %&gt;% as.integer(),\n    value = as.integer(value)\n  ) \n\ndata_earn &lt;- combined_data %&gt;% \n  filter(earnings != \"Person counts (number in thousands)\") %&gt;% \n  rename(median_weekly_earn = value) %&gt;% \n  select(industry:last_col(), -id)\n\ndata_earn \n\nfinal_bls_earn &lt;- combined_data %&gt;%\n  filter(earnings == \"Person counts (number in thousands)\") %&gt;%\n  rename(n_persons = value) %&gt;%\n  mutate(n_persons = n_persons * 1000) %&gt;%\n  select(industry:quarter, n_persons, -id) %&gt;%\n  left_join(\n    data_earn,\n    by = c(\n      \"industry\", \"occupation\", \"sex\", \"race\", \"ethnic_origin\", \"age\", \n      \"education\", \"class_of_worker\", \"labor_force_status\", \"year\", \"quarter\"\n      )\n  ) %&gt;% \n  select(sex, race, ethnic_origin, age, year:median_weekly_earn)\n\nfinal_bls_earn %&gt;% \n  write_csv(\"2021/2021-02-23/earn.csv\")\n\n# sanity check\nfinal_bls_earn %&gt;% \n  filter(quarter == 2, sex == \"Both Sexes\", race != \"All Races\") %&gt;% \n  ggplot(aes(x = year, y = median_weekly_earn, color = race)) +\n  geom_line() +\n  facet_wrap(~age)\n  distinct(quarter)"
  },
  {
    "objectID": "data/2021/2021-03-09/readme.html",
    "href": "data/2021/2021-03-09/readme.html",
    "title": "Bechdel Test",
    "section": "",
    "text": "Walmart shelf with DVD of Hunger Games movie\n\n\n\nBechdel Test\nThe data this week comes from FiveThirtyEight and the corresponding article from FiveThirtyEight.\n\nAudiences and creators know that on one level or another, there’s an inherent gender bias in the movie business — whether it’s the disproportionately low number of films with female leads, the process of pigeonholing actresses into predefined roles (action chick, romantic interest, middle-aged mother, etc.), or the lack of serious character development for women on screen compared to their male counterparts. What’s challenging is quantifying this dysfunction, putting numbers to a trend that is — at least anecdotally — a pretty clear reality.\nOne of the most enduring tools to measure Hollywood’s gender bias is a test originally promoted by cartoonist Alison Bechdel in a 1985 strip from her “Dykes To Watch Out For” series. Bechdel said that if a movie can satisfy three criteria — there are at least two named women in the picture, they have a conversation with each other at some point, and that conversation isn’t about a male character — then it passes “The Rule,” whereby female characters are allocated a bare minimum of depth. You can see a copy of that strip here.\n\n\nBechdel Test data sourced via Bechdeltest.com API.\n\nraw_bechdel.csv includes data from 1970 - 2020, for ONLY bechdel testing, while the movies.csv includes IMDB scores, budget/gross revenue, and ratings but only from 1970 - 2013.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-03-09')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 11)\n\nbechdel &lt;- tuesdata$bechdel\n\n# Or read in the data manually\n\nraw_bechdel &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-09/raw_bechdel.csv')\nmovies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-09/movies.csv')\n\n\nData Dictionary\n\n\n\nraw_bechdel.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ninteger\nYear of release\n\n\nid\ninteger\nID of film\n\n\nimdb_id\ncharacter\nIMDB ID\n\n\ntitle\ncharacter\nTitle of film\n\n\nrating\ninteger\nRating (0-3), 0 = unscored, 1. It has to have at least two [named] women in it, 2. Who talk to each other, 3. About something besides a man\n\n\n\n\n\nmovies.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\nimdb\ncharacter\nIMDB\n\n\ntitle\ncharacter\nTitle of movie\n\n\ntest\ncharacter\nBechdel Test outcome\n\n\nclean_test\ncharacter\nBechdel Test cleaned\n\n\nbinary\ncharacter\nBinary pass/fail of bechdel\n\n\nbudget\ndouble\nBudget as of release year\n\n\ndomgross\ncharacter\nDomestic gross in release year\n\n\nintgross\ncharacter\nInternational gross in release year\n\n\ncode\ncharacter\nCode\n\n\nbudget_2013\ndouble\nBudget normalized to 2013\n\n\ndomgross_2013\ncharacter\nDomestic gross normalized to 2013\n\n\nintgross_2013\ncharacter\nInternational gross normalized to 2013\n\n\nperiod_code\ndouble\nPeriod code\n\n\ndecade_code\ndouble\nDecade Code\n\n\nimdb_id\ncharacter\nIMDB ID\n\n\nplot\ncharacter\nPlot of movie\n\n\nrated\ncharacter\nRating of movie\n\n\nresponse\ncharacter\nResponse?\n\n\nlanguage\ncharacter\nLanguage of film\n\n\ncountry\ncharacter\nCountry produced in\n\n\nwriter\ncharacter\nWriter of film\n\n\nmetascore\ndouble\nMetascore rating (0-100)\n\n\nimdb_rating\ndouble\nIMDB Rating 0-10\n\n\ndirector\ncharacter\nDirector of movie\n\n\nreleased\ncharacter\nReleased date\n\n\nactors\ncharacter\nActors\n\n\ngenre\ncharacter\nGenre\n\n\nawards\ncharacter\nAwards\n\n\nruntime\ncharacter\nRuntime\n\n\ntype\ncharacter\nType of film\n\n\nposter\ncharacter\nPoster image\n\n\nimdb_votes\ncharacter\nIMDB Votes\n\n\nerror\ncharacter\nError?\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(jsonlite)\n\nraw_json &lt;- jsonlite::parse_json(url(\"http://bechdeltest.com/api/v1/getAllMovies\"))\n\nall_movies &lt;- raw_json %&gt;% \n  map_dfr(~as.data.frame(.x, stringsAsFactors = FALSE)) %&gt;% \n  rename(imdb_id = imdbid) %&gt;% \n  tibble()\n\nall_movies %&gt;% \n  filter(year &gt;= 1970) \n\n\n\ncleaned_bechdel &lt;- all_movies %&gt;% \n  mutate(title = case_when(\n    str_detect(title, \", The\") ~ str_remove(title, \", The\") %&gt;% paste(\"The\", .),\n    TRUE ~ str_replace(title, \"&#39;\", \"'\")\n  ))\n\ncleaned_bechdel %&gt;% \n  write_csv(\"2021/2021-03-09/raw_bechdel.csv\")\n\n# IMDB data ---------------------------------------------------------------\n\n\nimdb_json &lt;- jsonlite::parse_json(url(\"https://raw.githubusercontent.com/brianckeegan/Bechdel/master/imdb_data.json\"))\n\nall_imdb &lt;- imdb_json %&gt;%\n  map_dfr(~as.data.frame(.x, stringsAsFactors = FALSE))\n\ncleaned_imdb &lt;- all_imdb %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(metascore = parse_number(metascore),\n         imdb_rating = parse_number(imdb_rating),\n         year = as.integer(year)) %&gt;% \n  mutate(imdb_id = str_remove(imdb_id, \"tt\")) %&gt;% \n  tibble()\n\nall_imdb\n\n# 538 Data ----------------------------------------------------------------\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/bechdel/movies.csv\")\n\ncleaned_movies &lt;- movies %&gt;% \n  mutate(imdb_id = str_remove(imdb, \"tt\")) \n\ncombo_movies &lt;- cleaned_movies %&gt;% \n  left_join(cleaned_imdb) %&gt;% \n  janitor::clean_names() \n\ncombo_movies\n\ncombo_movies %&gt;% \n  write_csv(\"2021/2021-03-09/movies.csv\")"
  },
  {
    "objectID": "data/2021/2021-03-23/readme.html",
    "href": "data/2021/2021-03-23/readme.html",
    "title": "UN Votes",
    "section": "",
    "text": "Logo for the United Nations\n\n\n\nUN Votes\nThe data this week comes from Harvard’s Dataverse by way of Mine Çetinkaya-Rundel, David Robinson, and Nicholas Goguen-Compagnoni.\nOriginal Data citation:\n&gt; Citation: Erik Voeten “Data and Analyses of Voting in the UN General Assembly” Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013). Available at SSRN: http://ssrn.com/abstract=2111149\nThe {unvotes} R package is on CRAN! Github link\nThis is a wrapper around the datasets, although I’ve included them as CSVs in the TidyTuesday repo.\nGet package from CRAN:\ninstall.packages(\"unvotes\")\nMine Çetinkaya-Rundel wrote about the process of updating the package on the Citizen Statistician blog.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-03-23')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 13)\n\nunvotes &lt;- tuesdata$unvotes\n\n# Or read in the data manually\n\nunvotes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-23/unvotes.csv')\nroll_calls &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-23/roll_calls.csv')\nissues &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-03-23/issues.csv')\n\n\nData Dictionary\n\n\n\nunvotes.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrcid\ndouble\nThe roll call id; used to join with un_votes and un_roll_call_issues\n\n\ncountry\ncharacter\nCountry name, by official English short name\n\n\ncountry_code\ncharacter\n2-character ISO country code\n\n\nvote\ninteger\nVote result as a factor of yes/abstain/no\n\n\n\n\n\nroll_calls.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrcid\ninteger\n.\n\n\nsession\ndouble\nSession number. The UN holds one session per year; these started in 1946\n\n\nimportantvote\ninteger\nWhether the vote was classified as important by the U.S. State Department report “Voting Practices in the United Nations”. These classifications began with session 39\n\n\ndate\ndouble\nDate of the vote, as a Date vector\n\n\nunres\ncharacter\nResolution code\n\n\namend\ninteger\nWhether the vote was on an amendment; coded only until 1985\n\n\npara\ninteger\nWhether the vote was only on a paragraph and not a resolution; coded only until 1985\n\n\nshort\ncharacter\nShort description\n\n\ndescr\ncharacter\nLonger description\n\n\n\n\n\nissues.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrcid\ninteger\nThe roll call id; used to join with unvotes and un_roll_calls\n\n\nshort_name\ncharacter\nTwo-letter issue codes\n\n\nissue\ninteger\nDescriptive issue name\n\n\n\n\nCleaning Script\nThe data cleaning can be found in the data-raw folder of the package.\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(countrycode)\nlibrary(tidyr)\nlibrary(forcats)\n\nvlevels &lt;- c(\"yes\", \"abstain\", \"no\")\n\nload(\"data-raw/UNVotes2021.RData\")\n\nun_votes &lt;- completeVotes %&gt;%\n  filter(vote &lt;= 3) %&gt;%\n  mutate(\n    country = countrycode(ccode, \"cown\", \"country.name\"),\n    country_code = countrycode(ccode, \"cown\", \"iso2c\"),\n    # Match based on old version of data from unvotes package\n    country_code = case_when(\n      country == \"Czechoslovakia\" ~ \"CS\",\n      country == \"Yugoslavia\" ~ \"YU\",\n      country == \"German Democratic Republic\" ~ \"DD\",\n      country == \"Yemen People's Republic\" ~ \"YD\",\n      TRUE ~ country_code\n    ),\n    country = if_else(!is.na(Countryname) & Countryname == \"German Federal Republic\", \"Federal Republic of Germany\", country)\n  ) %&gt;%\n  select(rcid, country, country_code, vote) %&gt;%\n  mutate(vote = as.character(vote)) %&gt;%\n  mutate(vote = fct_recode(vote, yes = \"1\", abstain = \"2\", no = \"3\"))\n\ndescriptions &lt;- completeVotes %&gt;%\n  select(session, rcid, abstain, yes, no, importantvote, date, unres, amend, para, short, descr, me, nu, di, hr, co, ec) %&gt;%\n  distinct(rcid, .keep_all = TRUE)\n\nun_roll_calls &lt;- descriptions %&gt;%\n  select(rcid, session, importantvote:descr) %&gt;%\n  mutate(rcid = as.integer(rcid),\n         date = as.Date(date)) %&gt;%\n  arrange(rcid)\n\nun_roll_call_issues &lt;- descriptions %&gt;%\n  select(rcid, me:ec) %&gt;%\n  gather(short_name, value, me:ec) %&gt;%\n  mutate(rcid = as.integer(rcid),\n         value = as.numeric(value)) %&gt;%\n  filter(value == 1) %&gt;%\n  select(-value) %&gt;%\n  mutate(issue = fct_recode(short_name,\n                            \"Palestinian conflict\" = \"me\",\n                            \"Nuclear weapons and nuclear material\" = \"nu\",\n                            \"Colonialism\" = \"co\",\n                            \"Human rights\" = \"hr\",\n                            \"Economic development\" = \"ec\",\n                            \"Arms control and disarmament\" = \"di\"))\nSee Mine Çetinkaya-Rundel who wrote about the process of updating the package on the Citizen Statistician blog."
  },
  {
    "objectID": "data/2021/2021-04-06/readme.html",
    "href": "data/2021/2021-04-06/readme.html",
    "title": "Deforestation",
    "section": "",
    "text": "Image of a forest being cleared, representing industrial deforestation\n\n\n\nDeforestation\nThe data this week comes from Our World in Data.\nHannah Ritchie and Max Roser (2021) - “Forests and Deforestation”. Published online at OurWorldInData.org. Retrieved from: ‘https://ourworldindata.org/forests-and-deforestation’ [Online Resource]\nAdditional article from UCSD and about deforestation and its effects on climate change and disease.\nThere are a few datasets:\n\nDeforestation\n\nShare of forest area\n\nDrivers of deforestation\n\nDeforestation by commodity\n-Soybean production and use\n-Palm oil production\n\nQuotes below from the Our World in Data articles:\n\nThe net change in forest cover measures any gains in forest cover – either through natural forest expansion or afforestation through tree-planting – minus deforestation.\n\n\nHow much of the world’s land surface today is covered by forest?\nIn the visualization we see the breakdown of global land area.\n10% of the world is covered by glaciers, and a further 19% is barren land – deserts, dry salt flats, beaches, sand dunes, and exposed rocks. This leaves what we call ‘habitable land’.\nForests account for a little over one-third (38%) of habitable land area. This is around one-quarter (26%) of total (both habitable and uninhabitable) land area.\nThis marks a significant change from the past: global forest area has reduced significantly due to the expansion of agriculture. Today half of global habitable land is used for farming. The area used for livestock farming in particular is equal in area to the world’s forests.\n\n\nEvery year the world loses around 5 million hectares of forest. 95% of this occurs in the tropics. At least three-quarters of this is driven by agriculture – clearing forests to grow crops, raise livestock and produce products such as paper.1\nIf we want to tackle deforestation we need to understand two key questions: where we’re losing forests, and what activities are driving it. This allows us to target our efforts towards specific industries, products, or countries where they will have the greatest impact.\n\n\nMore than three-quarters (77%) of global soy is fed to livestock for meat and dairy production. Most of the rest is used for biofuels, industry or vegetable oils. Just 7% of soy is used directly for human food products such as tofu, soy milk, edamame beans, and tempeh. The idea that foods often promoted as substitutes for meat and dairy – such as tofu and soy milk – are driving deforestation is a common misconception.\nIn this article I address some key questions about palm oil production: how has it changed; where is it grown; and how this has affected deforestation and biodiversity. The story of palm oil is not as simple as it is often portrayed. Global demand for vegetable oils has increased rapidly over the last 50 years. Being the most productive oilcrop, palm has taken up a lot of this production. This has had a negative impact on the environment, particularly in Indonesia and Malaysia. But it’s not clear that the alternatives would have fared any better. In fact, because we can produce up to 20 times as much oil per hectare from palm versus the alternatives, it has probably spared a lot of environmental impacts from elsewhere.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-06')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 15)\n\nforest_change &lt;- tuesdata$forest_change\n\n# Or read in the data manually\n\nforest &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-06/forest.csv')\nforest_area &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-06/forest_area.csv')\nbrazil_loss &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-06/brazil_loss.csv')\nsoybean_use &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-06/soybean_use.csv')\nvegetable_oil &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-06/vegetable_oil.csv')\n\n\nData Dictionary\n\n\n\nforest.csv\nChange every 5 years for forest area in conversion.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nentity\ncharacter\nCountry\n\n\ncode\ncharacter\nCountry code\n\n\nyear\ndouble\nYear\n\n\nnet_forest_conversion\ndouble\nNet forest conversion in hectares\n\n\n\n\n\nforest_area.csv\nChange in global forest area as a percent of global forest area.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nentity\ncharacter\nCountry\n\n\ncode\ncharacter\nCountry Code\n\n\nyear\ninteger\nYear\n\n\nforest_area\ndouble\nPercent of global forest area\n\n\n\n\n\nbrazil_loss.csv\nLoss of Brazilian forest due to specific types.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nentity\ncharacter\nCountry\n\n\ncode\ncharacter\nCountry code\n\n\nyear\ndouble\nYear\n\n\ncommercial_crops\ndouble\nCommercial crops\n\n\nflooding_due_to_dams\ndouble\nFlooding\n\n\nnatural_disturbances\ndouble\nNatural disturbances\n\n\npasture\ndouble\nPasture for livestock\n\n\nselective_logging\ndouble\nLogging for lumber\n\n\nfire\ndouble\nFire loss\n\n\nmining\ndouble\nMining\n\n\nother_infrastructure\ndouble\nInfrastructure\n\n\nroads\ndouble\nRoads\n\n\ntree_plantations_including_palm\ndouble\nTree plantations\n\n\nsmall_scale_clearing\ndouble\nSmall scale clearing\n\n\n\n\n\nsoybean_use.csv\nSoybean production and use by year and country.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nentity\ncharacter\nCountry\n\n\ncode\ncharacter\nCountry Code\n\n\nyear\ndouble\nYear\n\n\nhuman_food\ndouble\nUse for human food (tempeh, tofu, etc)\n\n\nanimal_feed\ndouble\nUsed for animal food\n\n\nprocessed\ndouble\nProcessed into vegetable oil, biofuel, processed animal feed\n\n\n\n\n\nvegetable_oil.csv\nVegetable oil production by crop type and year.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nentity\ncharacter\nCountry\n\n\ncode\ncharacter\nCountry code\n\n\nyear\ndouble\nYear\n\n\ncrop_oil\ncharacter\nCrop that was used to produce vegetable oil\n\n\nproduction\ndouble\nOil production in tonnes\n\n\n\n\nCleaning Script\nJust renaming of columns this week."
  },
  {
    "objectID": "data/2021/2021-04-20/readme.html",
    "href": "data/2021/2021-04-20/readme.html",
    "title": "Netflix Shows",
    "section": "",
    "text": "Image of a person watching Netflix on their couch, with their feet propped up on the coffee table\n\n\n\nNetflix Shows\nThe data this week comes from Kaggle w/ credit to Shivam Bansal.\n\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\n# Or read in the data manually\n\nnetflix_titles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-04-20/netflix_titles.csv')\n\n\nData Dictionary\n\n\n\nnetflix_titles.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nshow_id\ncharacter\nUnique ID for every Movie / Tv Show\n\n\ntype\ncharacter\nIdentifier - A Movie or TV Show\n\n\ntitle\ncharacter\nTitle of the Movie / Tv Show\n\n\ndirector\ncharacter\nDirector of the Movie/Show\n\n\ncast\ncharacter\nActors involved in the movie / show\n\n\ncountry\ncharacter\nCountry where the movie / show was produced\n\n\ndate_added\ncharacter\nDate it was added on Netflix\n\n\nrelease_year\ndouble\nActual Release year of the movie / show\n\n\nrating\ncharacter\nTV Rating of the movie / show\n\n\nduration\ncharacter\nTotal Duration - in minutes or number of seasons\n\n\nlisted_in\ncharacter\nGenre\n\n\ndescription\ncharacter\nSummary description of the film/show\n\n\n\n\nCleaning Script\nNo cleaning script this week, enjoy!"
  },
  {
    "objectID": "data/2021/2021-05-04/readme.html",
    "href": "data/2021/2021-05-04/readme.html",
    "title": "Water Sources",
    "section": "",
    "text": "Logo for the Water Point Data Exchange\n\n\n\nWater Sources\nThe data this week comes from Water Point Data Exchange. Note that the data is limited to some core columns as the full dataset is ~300 Mb. Also data is largely filtered down to African sources.\nKaty Sill and Adam Kariv are speaking about this data source at csv,conf.\nPer WPDX\n\nThe amount of water point data being collected is growing rapidly as governments and development partners increasingly monitor water points over time. Without harmonization among these different data sources, the opportunity for learning will be limited, with the true potential of this information remaining untapped.\n\n\nBy establishing a platform for sharing water point data throughout the global water sector, WPDx adds value to the data already being collected. By bringing together diverse data sets, the water sector can establish an unprecedented understanding of water services.\n\n\nSharing this data has the potential to improve water access for millions of people as a result of better information available to governments, service providers, researchers, NGOs, and others.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-05-04')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 19)\n\nwater &lt;- tuesdata$water\n\n# Or read in the data manually\n\nwater &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-05-04/water.csv')\n\n\nData Dictionary\n\n\n\nwater.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrow_id\ndouble\nUnique ID\n\n\nlat_deg\ndouble\nLatitude\n\n\nlon_deg\ndouble\nLongitude\n\n\nreport_date\ncharacter\nDate water source was reported on\n\n\nstatus_id\ncharacter\nIdentify if any water is available on the day of the visit, recognizing that it may be a limited flow\n\n\nwater_source_clean\ncharacter\nDescribe the water source (e.g. shallow well, spring, borehole, river, pond, etc.)\n\n\nwater_tech\ncharacter\nDescribe the system being used to transport the water from the source to the point of collection (e.g. Handpump (include manufacturer, i.e. Afridev, IndiaMark II, Malda,etc.),Kiosk, Tapstand,etc.)\n\n\nfacility_type\ncharacter\nCategorized facility type based on JMP definitions\n\n\ncountry_name\ncharacter\nCountry name\n\n\ninstall_year\ndouble\nInstall year\n\n\ninstaller\ncharacter\nInstaller\n\n\npay\ncharacter\nProvide the payment amount and basis (e.g. monthly, per jerry can, when broken,etc.). If no amount is provided, the basis can be provided alone. An amount without a payment basis cannot be included\n\n\nstatus\ncharacter\nProvide a status of the physical/mechanical condition of thewater point\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nraw_df &lt;- read_csv(\"2021/2021-05-04/Water_Point_Data_Exchange__WPDx-Basic_.csv\")\n\nclean_df &lt;- raw_df %&gt;% \n  janitor::clean_names() %&gt;%\n  rename_with(~str_remove(.x, \"number_\")) %&gt;% \n  select(\n    row_id:status_id, water_source_clean:subjective_quality,rehab_year,\n    install_year,\n    -contains(\"adm\"), -contains(\"rehab\"), -contains(\"fecal\"),\n    -count, -new_georeferenced_column, -source, -management,-subjective_quality\n      ) %&gt;% \n  mutate(\n    status_id = case_when(\n      status_id == \"Yes\"~ \"y\",\n      status_id == \"No\" ~ \"n\",\n      status_id == \"Unknown\" ~ \"u\"\n    ),\n    report_date = str_sub(report_date, 1, 10)\n    ) %&gt;% \n  filter(\n    !country_name %in% c(\n      \"NA\", \"Slovenia\", \"El Salvador\", \"Iraq\", \"Malawi\", \"Bangladesh\",\n      \"Benin\", \"India\", \"Bolivia\", \"Honduras\", \"Guatemala\", \"Nicaragua\",\n      \"Afghanistan\", \"Nepal\", \"Haiti\", \"Mexico\", \"Indonesia\", \"Sri Lanka\",\n      \"Dominican Republublic\", \"Belize\", \"Myanmar\", \"Vanuatu\", \"Jordan\"\n      )\n    ) %&gt;% \n  select(-water_tech) %&gt;% \n  rename(water_source = water_source_clean, water_tech = water_tech_clean)\n\nclean_df %&gt;% \n  write_csv(\"2021/2021-05-04/water.csv\")"
  },
  {
    "objectID": "data/2021/2021-05-18/readme.html",
    "href": "data/2021/2021-05-18/readme.html",
    "title": "Ask a Manager Survey",
    "section": "",
    "text": "Logo for the Ask a Manager blog which is a white red-haired woman next to the words “Ask a Manager, and if you don’t I’ll tell you anyway”\n\n\nPlease note that the image above belongs to the Ask a Manager blog/Alison Green.\n\nAsk a Manager Survey\nThe data this week comes from the Ask a Manager Survey. H/t to Kaija Gahm for sharing it as an issue!\n\nThe salary survey a few weeks ago got a huge response — 24,000+ people shared their salaries and other info, which is a lot of raw data to sift through. Reader Elisabeth Engl kindly took the raw data and analyzed some of the trends in it and here’s what she found. (She asked me to note that she did this as a fun project to share some insights from the survey, rather than as a paid engagement.)\n\n\nThis data does not reflect the general population; it reflects Ask a Manager readers who self-selected to respond, which is a very different group (as you can see just from the demographic breakdown below, which is very white and very female).\n\nElisabeth Engl prepped some plots for the Ask a Manager blog using this data.\nThe survey itself is available here.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-05-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 21)\n\nsurvey &lt;- tuesdata$survey\n\n# Or read in the data manually\n\nsurvey &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-05-18/survey.csv')\n\n\nData Dictionary\n\n\n\nsurvey.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntimestamp\ncharacter\nTimestamp when survey submitted\n\n\nhow_old_are_you\ncharacter\nHow old are you (bracket range)\n\n\nindustry\ncharacter\nIndustry\n\n\njob_title\ncharacter\nJob title\n\n\nadditional_context_on_job_title\ncharacter\nAdditional context on job, free text\n\n\nannual_salary\ndouble\nAnnual salary in local currency\n\n\nother_monetary_comp\ncharacter\nAdditional other monetary comp\n\n\ncurrency\ncharacter\nLocal currency\n\n\ncurrency_other\ncharacter\nCurrency for other compensation\n\n\nadditional_context_on_income\ncharacter\nAdditional context on income (free text)\n\n\ncountry\ncharacter\nCountry currently working in\n\n\nstate\ncharacter\nState\n\n\ncity\ncharacter\nCity\n\n\noverall_years_of_professional_experience\ncharacter\nOverall years of professional experience (bracketed)\n\n\nyears_of_experience_in_field\ncharacter\nYears of experience in field (bracketed)\n\n\nhighest_level_of_education_completed\ncharacter\nHighest level of education completed\n\n\ngender\ncharacter\nGender\n\n\nrace\ncharacter\nRace\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-06-01/readme.html",
    "href": "data/2021/2021-06-01/readme.html",
    "title": "Survivor TV Show data!",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\n\nLogo for the survivoR R package, which is a black hexagon, a golden lit torch, and the word “survivoR” in white.\n\n\n\n\nSurvivor TV Show data!\nThe data this week comes from the survivorR R package by way of Daniel Oehm.\n\n596 episodes. 40 seasons. 1 package!\nsurvivoR is a collection of data sets detailing events across all 40 seasons of the US Survivor, including castaway information, vote history, immunity and reward challenge winners and jury votes.\n\nFull details about the package and additional datasets available in the package are available on GitHub. The package is on CRAN as survivoR and can be installed for ALL the datasets, themes, etc via install.packages(\"survivoR\").\nSome example code is available at Daniel’s Website.\nAdditional context/details about the Survivor TV show can be found on Wikipedia.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-06-01')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 23)\n\nsummary &lt;- tuesdata$summary\n\n# Or read in the data manually\n\nsummary &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-06-01/summary.csv')\n\n\nData Dictionary\n\n\n\nsummary.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason_name\ncharacter\nName of season\n\n\nseason\ninteger\nSeason number\n\n\nlocation\ncharacter\nSeason geo location\n\n\ncountry\ncharacter\nSeason country\n\n\ntribe_setup\ncharacter\nTribe Setup\n\n\nfull_name\ncharacter\nFull name of player\n\n\nwinner\ncharacter\nSeason winner\n\n\nrunner_ups\ncharacter\nRunner ups\n\n\nfinal_vote\ncharacter\nFinal vote for winner\n\n\ntimeslot\ncharacter\nTime slot on TV\n\n\npremiered\ndouble\nPremiered date\n\n\nended\ndouble\nEnded date\n\n\nfilming_started\ndouble\nFilming started date\n\n\nfilming_ended\ndouble\nFilming ended date\n\n\nviewers_premier\ndouble\nViewers (millions) at premier\n\n\nviewers_finale\ndouble\nViewers at finale\n\n\nviewers_reunion\ndouble\nViewers for reunion\n\n\nviewers_mean\ndouble\nViewers average\n\n\nrank\ndouble\nViewer ranking\n\n\n\n\n\nchallenges.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason_name\ncharacter\nName of season\n\n\nseason\ninteger\nSeason number\n\n\nepisode\ndouble\nEpisode number\n\n\ntitle\ncharacter\nEpisode title\n\n\nday\ndouble\nDay of season\n\n\nchallenge_type\ncharacter\nChallenge type\n\n\nwinners\ncharacter\nWinners names\n\n\nwinning_tribe\ncharacter\nWinning tribe\n\n\n\n\n\ncastaways.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason_name\ncharacter\nName of season\n\n\nseason\ninteger\nSeason number\n\n\nfull_name\ncharacter\nFull name of participant\n\n\ncastaway\ncharacter\nCastaway’s first name\n\n\nage\ndouble\nAge\n\n\ncity\ncharacter\nOrigin city\n\n\nstate\ncharacter\nOrigin state\n\n\npersonality_type\ncharacter\npersonality type\n\n\nday\ndouble\nDay of season\n\n\norder\ninteger\nOrder\n\n\nresult\ncharacter\nResult\n\n\njury_status\ncharacter\nJury status\n\n\noriginal_tribe\ncharacter\nOriginal tribe\n\n\nswapped_tribe\ncharacter\nSwapped tribe\n\n\nswapped_tribe2\ncharacter\nSwapped tribe 2\n\n\nmerged_tribe\ncharacter\nMerged tribe\n\n\ntotal_votes_received\ndouble\nTotal votes received\n\n\nimmunity_idols_won\ndouble\nImmunity idols won\n\n\n\n\n\nviewers.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason_name\ncharacter\nName of season\n\n\nseason\ninteger\nSeason number\n\n\nepisode_number_overall\ndouble\nEpisode number overall\n\n\nepisode\ndouble\nEpisode number\n\n\ntitle\ncharacter\nEpisode title\n\n\nepisode_date\ndouble\nDate\n\n\nviewers\ndouble\nViewers in millions\n\n\nrating_18_49\ndouble\nRating by viwers aged 18-49\n\n\nshare_18_49\ndouble\nShare of viewers aged 18-49\n\n\n\n\n\njury_votes.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason_name\ncharacter\nName of season\n\n\nseason\ninteger\nSeason number\n\n\ncastaway\ncharacter\nCastaway\n\n\nfinalist\ncharacter\nFinalist\n\n\nvote\ndouble\nVote\n\n\n\n\nCleaning Script\nNo cleaning script, just the CRAN package survivoR!"
  },
  {
    "objectID": "data/2021/2021-06-15/readme.html",
    "href": "data/2021/2021-06-15/readme.html",
    "title": "Du Bois and Juneteenth Revisited",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nCompany D, 8th Illinois Volunteer Regiment. Image collected by W.E.B .Du Bois and Thomas J Calloway, January 1, 1899. Photo: Universal History Archive/Universal Images Group via Getty Images\n\n\n\n\nDu Bois and Juneteenth Revisited\nThe data this week comes from Anthony Starks, Allen Hillery Sekou Tyler. The tweets aggregated show the #DuBoisChallenge tweets from 2021. The data used for the challenge is also accessible from TidyTuesday week 8 of 2021.\nSekou has also visualized the Twitter activity in a dashboard.\nPlease also revisit the 2020 data for data for and recognition of Juneteenth. While Juneteenth recognizes the political act of the freeing of the last American slaves in Texas, W.E.B Du Bois argued a more poignant detail: &gt; in a series of landmark works, is that some 500,000 Black Americans freed themselves from slavery by walking off their plantation, with many of them joining up with the Union Army. Then, with the help of some Union Army officers, they organized themselves into soldiers, officers, farmers, spies, cooks, and nurses who enabled the Union to win the war, while their absence on plantations crippled the South’s economy. This is the true Juneteenth: a freedom won by the freed.\n\nWEB Du Bois\nThe Intercept - Juneteenth\n\nJuneteenth details from the Intercept.\n\nThe holiday marks, generally, the end of slavery in the United States, and more specifically the order of Gen. Gordon Granger that declared the freedom of enslaved people in Texas, a state that was a stubborn holdout after Gen. Robert E. Lee had surrendered his army. The order was read on June 19, 1865, and lore has it that upon learning of emancipation, those it applied to stopped working instantly and launched into celebration.\nThat wasn’t the only work stoppage that Juneteenth celebrates. Some of the discourse surrounding Juneteenth is palatable to corporate America because it can reinforce some of the comforting ideas about the Civil War: that Abraham Lincoln and the generals of the Union Army, in their benevolence, freed the slaves. The reality, argued W.E.B Du Bois in a series of landmark works, is that some 500,000 Black Americans freed themselves from slavery by walking off their plantation, with many of them joining up with the Union Army. Then, with the help of some Union Army officers, they organized themselves into soldiers, officers, farmers, spies, cooks, and nurses who enabled the Union to win the war, while their absence on plantations crippled the South’s economy. This is the true Juneteenth: a freedom won by the freed.\n\n\nPBS on Juneteenth\n\n\nGet the data here\nPlease note that data can also be used from 2021 week 8 and 2020 week 24/25.\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-06-15')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 25)\n\ntweets &lt;- tuesdata$tweets\n\n# Or read in the data manually\n\ntweets &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-06-15/tweets.csv')\n\n\nData Dictionary\n\n\n\ntweets.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndatetime\ndouble\nDate and time of tweet\n\n\ncontent\ncharacter\nText for tweet\n\n\nretweet_count\ndouble\nRetweet count for tweet\n\n\nlike_count\ndouble\nLike count for tweet\n\n\nquote_count\ndouble\nQuote tweet count for tweet\n\n\ntext\ncharacter\nWhere tweet was posted from\n\n\nusername\ncharacter\nUsername of Tweeter\n\n\nlocation\ncharacter\nLocation tweeted from\n\n\nfollowers\ndouble\nFollowers of the tweeter\n\n\nurl\ncharacter\nCanonical url of tweet\n\n\nverified\nlogical\nIs user verified?\n\n\nlat\ndouble\nLatitude of user\n\n\nlong\ndouble\nLongitude of user\n\n\n\n\nCleaning Script\nNo cleaning script this week!"
  },
  {
    "objectID": "data/2021/2021-06-29/readme.html",
    "href": "data/2021/2021-06-29/readme.html",
    "title": "Animal Rescues",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nA fox walking past 10 Downing Street this week. The number of rescues involving the animals nearly doubled in 2020. Photograph: John Sibley/Reuters\n\n\n\n\nAnimal Rescues\nThe data this week comes from London.gov by way of Data is Plural and Georgios Karamanis.\n\nFox in bedroom, dog trapped in wall. The London Fire Brigade responds to hundreds of requests to rescue animals each year. Its monthly-updated spreadsheet of such events goes back to 2009; it lists the location and type of property, the kind of animal and rescue, hours spent, a (very) brief description, and more. [h/t Soph Warnes]\n\n\nThe London Fire Brigade attends a range of non-fire incidents (which we call ‘special services’). These ‘special services’ include assistance to animals that may be trapped or in distress.\nWe routinely get asked for information about the number of such incident attended by the London Fire Brigade and this data is published on the London Datastore to assist those who require it.\nThe data is provided from January 2009 and isupdated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.\nPlease note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.\n\nThe Guardian also published Animal rescues by London fire brigade rise 20% in pandemic year a few months back.\n\nLondon firefighters encountered a surge in callouts to rescue animals in 2020, figures show.\nThe London fire brigade (LFB) was involved in 755 such incidents – more than two a day. The number of rescues rose by 20% compared with 2019 when there were 602, with the biggest rise coming in the number of non-domestic animals rescued, according to the data.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-06-29')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 27)\n\nanimal_rescues &lt;- tuesdata$animal_rescues\n\n# Or read in the data manually\n\nanimal_rescues &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-06-29/animal_rescues.csv')\n\n\nData Dictionary\n\n\n\nanimal_rescues.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nincident_number\ndouble\nUnique incident ID\n\n\ndate_time_of_call\ncharacter\nDay and time of call (day/month/year hour:minute)\n\n\ncal_year\ndouble\nCalendar Year\n\n\nfin_year\ncharacter\nFiscal year\n\n\ntype_of_incident\ncharacter\nType of incident\n\n\npump_count\ncharacter\nPump count (number of trucks)\n\n\npump_hours_total\ncharacter\nPump hours total\n\n\nhourly_notional_cost\ndouble\nHourly cost\n\n\nincident_notional_cost\ncharacter\nTotal cost of incident\n\n\nfinal_description\ncharacter\nFinal description\n\n\nanimal_group_parent\ncharacter\nType of animal\n\n\noriginof_call\ncharacter\nWhere call originated\n\n\nproperty_type\ncharacter\nProperty type\n\n\nproperty_category\ncharacter\nProperty category\n\n\nspecial_service_type_category\ncharacter\nService type category\n\n\nspecial_service_type\ncharacter\nService type\n\n\nward_code\ncharacter\nWard Code\n\n\nward\ncharacter\nWard name\n\n\nborough_code\ncharacter\nBorough code\n\n\nborough\ncharacter\nBorough name\n\n\nstn_ground_name\ncharacter\nStation name\n\n\nuprn\ncharacter\nUnique property reference number\n\n\nstreet\ncharacter\nStreet name\n\n\nusrn\ncharacter\nunique street reference number\n\n\npostcode_district\ncharacter\nPostal code district\n\n\neasting_m\ncharacter\nEasting measure\n\n\nnorthing_m\ncharacter\nNorthing measure\n\n\neasting_rounded\ndouble\nEasting rounded\n\n\nnorthing_rounded\ndouble\nNorthing rounded\n\n\nlatitude\ncharacter\nLat\n\n\nlongitude\ncharacter\nLong\n\n\n\n\nCleaning Script\nNo cleaning this week, just janitor::clean_names()!"
  },
  {
    "objectID": "data/2021/2021-07-13/readme.html",
    "href": "data/2021/2021-07-13/readme.html",
    "title": "Scooby Doo Episodes",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nCast of Scooby Doo, aligned from left to right as Velma, Shaggy, Scooby, Fred, and Daphne set over a psychedlic rainbow background\n\n\n\n\nScooby Doo Episodes\nThe data this week comes from Kaggle thanks to manual data aggregation by plummye. Hat tip to Sara Stoudt for recommending this dataset!\n\nEvery Scooby-Doo episode and movie’s various variables.\nTook ~1 year to watch every Scooby-Doo iteration and track every variable. Many values are subjective by nature of watching but I tried my hardest to keep the data collection consistent.\nIf you plan to use this data for anything school/entertainment related you are free to (credit is always welcome).\n\nMore info about Scooby Doo can be found on ScoobyPedia.\n\nScoobypedia is an encyclopedia on the hit television series Scooby-Doo which has been airing for over 50 years!\nThe show follows the iconic mystery solving detectives, know as Mystery Inc., as they set out to solve crime and unmask criminals, bent on revenge or committing criminal acts for their own personal gain.\nTitular character, Scooby, is followed by his best pal Shaggy as both vie for Scooby Snacks on their adventures! Velma brings her extra intellect and initiative to them, setting out plans to catch criminals. Fred is the team’s leader while Daphne is bold and full of personality.\nWe are the go-to encyclopedia on all-things Scooby-Doo and are currently editing over 15,485 articles - we need your help! Create an account, Contribute to articles, and discuss the show on the number 1 Scooby-Doo\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-07-13')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 29)\n\nscoobydoo &lt;- tuesdata$scoobydoo\n\n# Or read in the data manually\n\nscoobydoo &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-07-13/scoobydoo.csv')\n\n\nData Dictionary\n\n\n\nscoobydoo.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nindex\ndouble\nIndex ordering based on Scoobypedia\n\n\nseries_name\ncharacter\nName of the series in which the episode takes place or in movies’ cases the Scoobypedia’s grouping classification\n\n\nnetwork\ncharacter\nNetwork the TV series takes place in, if it is a movie will use similar grouping as series.name variable\n\n\nseason\ncharacter\nSeason of TV Series, if not TV Series will default to the format\n\n\ntitle\ncharacter\nTitle of Episode/Movie\n\n\nimdb\ncharacter\nScore on IMDB (NULL if recently aired)\n\n\nengagement\ncharacter\nNumber of reviews on IMDB (NULL if very recently aired)\n\n\ndate_aired\ndouble\nDated aired in US\n\n\nrun_time\ndouble\nRun time in min\n\n\nformat\ncharacter\nType of media\n\n\nmonster_name\ncharacter\nName of monster\n\n\nmonster_gender\ncharacter\nBinary monster gender\n\n\nmonster_type\ncharacter\nMonster type\n\n\nmonster_subtype\ncharacter\nMonster subtype\n\n\nmonster_species\ncharacter\nmonster_species\n\n\nmonster_real\ncharacter\nWas monster real\n\n\nmonster_amount\ndouble\nMonster amount\n\n\ncaught_fred\ncharacter\nCaught by Fred\n\n\ncaught_daphnie\ncharacter\ncaught by Daphnie\n\n\ncaught_velma\ncharacter\ncaught by Velma\n\n\ncaught_shaggy\ncharacter\ncaught by Shaggy\n\n\ncaught_scooby\ncharacter\ncaught by Scooby\n\n\ncaptured_fred\ncharacter\ncaptured Fred\n\n\ncaptured_daphnie\ncharacter\ncaptured Daphnie\n\n\ncaptured_velma\ncharacter\ncaptured Velma\n\n\ncaptured_shaggy\ncharacter\ncaptured Shaggy\n\n\ncaptured_scooby\ncharacter\ncaptured Scooby\n\n\nunmask_fred\ncharacter\nunmask by fred\n\n\nunmask_daphnie\ncharacter\nunmask by Daphnie\n\n\nunmask_velma\ncharacter\nunmask by Velma\n\n\nunmask_shaggy\ncharacter\nunmask by Shaggy\n\n\nunmask_scooby\ncharacter\nunmask by Scooby\n\n\nsnack_fred\ncharacter\nsnack eaten by Fred\n\n\nsnack_daphnie\ncharacter\nsnack eaten by Daphnie\n\n\nsnack_velma\ncharacter\nsnack eaten by Velma\n\n\nsnack_shaggy\ncharacter\nsnack eaten by Shaggy\n\n\nsnack_scooby\ncharacter\nsnack eaten by Scooby\n\n\nunmask_other\nlogical\nunmask by other\n\n\ncaught_other\nlogical\ncaught by other\n\n\ncaught_not\nlogical\nNot caught\n\n\ntrap_work_first\ncharacter\nTrap work first\n\n\nsetting_terrain\ncharacter\nSetting type of terrain\n\n\nsetting_country_state\ncharacter\nsetting country state\n\n\nsuspects_amount\ndouble\nsuspects amount\n\n\nnon_suspect\ncharacter\nnon suspect\n\n\narrested\ncharacter\narrested\n\n\nculprit_name\ncharacter\nculprit name\n\n\nculprit_gender\ncharacter\nculprit binary gender\n\n\nculprit_amount\ndouble\nculprit amount\n\n\nmotive\ncharacter\nmotive\n\n\nif_it_wasnt_for\ncharacter\nPhrase at the end of show, ie “if it wasnt for …”\n\n\nand_that\ncharacter\nand that\n\n\ndoor_gag\nlogical\ndoor gag\n\n\nnumber_of_snacks\ncharacter\nnumber of snacks\n\n\nsplit_up\ncharacter\nsplit up\n\n\nanother_mystery\ncharacter\nanother mystery\n\n\nset_a_trap\ncharacter\nset a trap\n\n\njeepers\ncharacter\nTimes “jeepers” said\n\n\njinkies\ncharacter\nTimes “jinkies” said\n\n\nmy_glasses\ncharacter\nTimes “my glasses” said\n\n\njust_about_wrapped_up\ncharacter\nTimes “just about wrapped up” said\n\n\nzoinks\ncharacter\nTimes “zoinks”said\n\n\ngroovy\ncharacter\nTimes “groovy” said\n\n\nscooby_doo_where_are_you\ncharacter\nTimes “scooby doo where are you” said\n\n\nrooby_rooby_roo\ncharacter\nTimes “rooby_rooby_roo” said\n\n\nbatman\nlogical\nbatman in episode\n\n\nscooby_dum\nlogical\nscooby_dum in episode\n\n\nscrappy_doo\nlogical\nscrappy_doo in episode\n\n\nhex_girls\nlogical\nhex_girls in episode\n\n\nblue_falcon\nlogical\nblue_falcon in episode\n\n\nfred_va\ncharacter\nFred voice actor\n\n\ndaphnie_va\ncharacter\nDaphnie voice actor\n\n\nvelma_va\ncharacter\nvelma voice actor\n\n\nshaggy_va\ncharacter\nshaggy voice actor\n\n\nscooby_va\ncharacter\nscooby voice actor\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-07-27/readme.html",
    "href": "data/2021/2021-07-27/readme.html",
    "title": "The Olympics",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nThe rings are five interlocking rings, coloured blue, yellow, black, green and red on a white field, known as the “Olympic rings”. The symbol was originally created in 1913 by Coubertin. He appears to have intended the rings to represent the five continents: Europe, Africa, Asia, America, and Oceania.\n\n\n\n\nThe Olympics\nThe data this week comes from Kaggle.\n\nThis is a historical dataset on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2016. I scraped this data from www.sports-reference.com in May 2018. The R code I used to scrape and wrangle the data is on GitHub. I recommend checking my kernel before starting your own analysis.\nNote that the Winter and Summer Games were held in the same year up until 1992. After that, they staggered them such that Winter Games occur on a four year cycle starting with 1994, then Summer in 1996, then Winter in 1998, and so on. A common mistake people make when analyzing this data is to assume that the Summer and Winter Games have always been staggered.\n\n\nThe Olympic data on www.sports-reference.com is the result of an incredible amount of research by a group of Olympic history enthusiasts and self-proclaimed ‘statistorians’. Check out their blog for more information. All I did was consolidated their decades of work into a convenient format for data analysis.\n\nFiveThirtyEight has a similar forecast updating throughout the Olympic Games.\n\nAn updating medal count for every competing nation compared with the number of medals we thought each would have won so far — along with how many more they might take home.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-07-27')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 31)\n\nolympics &lt;- tuesdata$olympics\n\n# Or read in the data manually\n\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-07-27/olympics.csv')\n\n\nData Dictionary\n\n\n\nolympics.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nAthlete ID\n\n\nname\ncharacter\nAthlete Name\n\n\nsex\ncharacter\nAthlete Sex\n\n\nage\ndouble\nAthlete Age\n\n\nheight\ndouble\nAthlete Height in cm\n\n\nweight\ndouble\nAthlete weight in kg\n\n\nteam\ncharacter\nCountry/Team competing for\n\n\nnoc\ncharacter\nnoc region\n\n\ngames\ncharacter\nOlympic games name\n\n\nyear\ndouble\nYear of olympics\n\n\nseason\ncharacter\nSeason either winter or summer\n\n\ncity\ncharacter\nCity of Olympic host\n\n\nsport\ncharacter\nSport\n\n\nevent\ncharacter\nSpecific event\n\n\nmedal\ncharacter\nMedal (Gold, Silver, Bronze or NA)\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\nevents &lt;- read_csv(\"2021/2021-07-27/athlete_events.csv\")\n\nevents %&gt;%\n  janitor::clean_names() %&gt;% \n  write_csv(\"2021/2021-07-27/olympics.csv\")"
  },
  {
    "objectID": "data/2021/2021-08-10/readme.html",
    "href": "data/2021/2021-08-10/readme.html",
    "title": "BEA Infrastructure Investment",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nLogo for the Bureau of Economic Analysis - it’s the letters “b” “e” “a” in blue with an increasing line chart in blue/orange to the left\n\n\n\n\nBEA Infrastructure Investment\nThe data this week comes from Bureau of Economic Analysis. The raw .xlsx file is included, or can be downloaded directly from the BEA Working paper series.\nNote that there are additional datasets in the Excel file, but the 3 primary datasets are already cleaned and saved as individual .csv files.\nh/t to Donald Schneider and DataIsPlural\nThe Working Paper by Jennifer Bennett et al\n\nInfrastructure provides critical support for economic activity, and assessing its role requires reliable measures. This paper provides an overview of U.S. infrastructure data in the National Economic Accounts. After developing definitions of basic, social, and digital infrastructure, we assess trends in each of these categories and their components. Results are mixed depending on the category. Investment in some important types of basic infrastructure has barely or not kept up with depreciation and population growth in recent decades, while some other categories look better. We also show that the average age of most types of infrastructure in the U.S. has been rising, and the remaining service life has been falling. This paper also presents new prototype estimates of state-level investment in highways, highlighting the wide variation across states. In addition, we present new prototype data on maintenance expenditures for highways. In terms of future research, we believe that deprecation rates warrant additional attention given that current estimates are based on 40-year old research and are well below those used in Canada and other countries. We also believe that additional creative work on price indexes for infrastructure would be valuable. Finally, all of the data in this paper are downloadable, and we hope that the analysis in this paper and the availability of data will spur additional research.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-08-10')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 33)\n\ninvestment &lt;- tuesdata$investment\n\n# Or read in the data manually\n\ninvestment &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-08-10/investment.csv')\nchain_investment &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-08-10/chain_investment.csv')\nipd &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-08-10/ipd.csv')\n\n\nData Dictionary\n\n\n\ninvestment.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncategory\ncharacter\nCategory of investment\n\n\nmeta_cat\ncharacter\nGroup category of investment\n\n\ngroup_num\ndouble\nGroup number of investment\n\n\nyear\ninteger\nYear of investment\n\n\ngross_inv\ndouble\nGross investment in millions of USD\n\n\n\n\n\nchain_investment.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncategory\ncharacter\nCategory of investment\n\n\nmeta_cat\ncharacter\nGroup category of investment\n\n\ngroup_num\ndouble\nGroup number of investment\n\n\nyear\ninteger\nYear of investment\n\n\ngross_inv_chain\ndouble\nGross investment (chained 2021 dollars) in millions of USD\n\n\n\n\n\nipd.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncategory\ncharacter\nCategory of investment\n\n\nmeta_cat\ncharacter\nGroup category of investment\n\n\ngroup_num\ndouble\nGroup number of investment\n\n\nyear\ninteger\nYear of investment\n\n\ngross_inv_ipd\ndouble\nImplicit Price Deflators (IPDs) An implicit price deflator is the ratio of the current-dollar value of a series, such as gross domestic product (GDP), to its corresponding chained-dollar value, multiplied by 100.\n\n\n\n\nCleaning Script\nlibrary(readxl)\nlibrary(tidyverse)\n\nraw_df  &lt;- readxl::read_excel(\"2021/2021-08-10/infrastructure-data-may-2020.xlsx\", sheet = 2, skip = 2)\n\ngross_inv &lt;- raw_df %&gt;% \n  rename(group = ...1, category = ...2) %&gt;% \n  filter(!is.na(category)) %&gt;% \n  mutate(\n    meta_cat = if_else(!is.na(group), category, NA_character_), \n    group_num = group,\n    .after = \"category\"\n    ) %&gt;% \n\n  fill(meta_cat, group_num) %&gt;%\n  pivot_longer(names_to = \"year\", values_to = \"gross_inv\", cols = `1947`:`2017`,\n               names_transform = list(year = as.integer)) %&gt;% \n  filter(is.na(group)) %&gt;% \n  select(-group)\n\n\nraw_df2  &lt;- readxl::read_excel(\"2021/2021-08-10/infrastructure-data-may-2020.xlsx\", sheet = 3, skip = 2)\n\nchain_inv &lt;- raw_df2 %&gt;% \n  rename(group = ...1, category = ...2) %&gt;%\n  mutate(\n    meta_cat = if_else(!is.na(group), category, NA_character_), \n    group_num = group,\n    .after = \"category\"\n  ) %&gt;% \n  \n  fill(meta_cat, group_num) %&gt;%\n  pivot_longer(names_to = \"year\", values_to = \"gross_inv_chain\", cols = `1947`:`2017`,\n               names_transform = list(year = as.integer)) %&gt;% \n  filter(is.na(group)) %&gt;% \n  select(-group)\n\n\nraw_df3  &lt;- readxl::read_excel(\"2021/2021-08-10/infrastructure-data-may-2020.xlsx\", sheet = 4, skip = 2)\n\nipd &lt;- raw_df3 %&gt;% \n  rename(group = ...1, category = ...2) %&gt;% \n  filter(!is.na(category)) %&gt;% \n  mutate(\n    meta_cat = if_else(!is.na(group), category, NA_character_), \n    group_num = group,\n    group_num = if_else(category == \"GDP\", 0, group_num),\n    .after = \"category\"\n  ) %&gt;% \n  \n  fill(meta_cat, group_num) %&gt;%\n  pivot_longer(names_to = \"year\", values_to = \"gross_inv_ipd\", cols = `1947`:`2017`,\n               names_transform = list(year = as.integer)) %&gt;% \n  filter(is.na(group)) %&gt;% \n  select(-group) %&gt;% \n  mutate(meta_cat = if_else(category == \"GDP\", \"GDP\", meta_cat))\n\ngross_inv %&gt;% write_csv(\"2021/2021-08-10/investment.csv\")\nchain_inv %&gt;% write_csv(\"2021/2021-08-10/chain_investment.csv\")"
  },
  {
    "objectID": "data/2021/2021-08-24/readme.html",
    "href": "data/2021/2021-08-24/readme.html",
    "title": "Lemurs",
    "section": "",
    "text": "Lemurs\nThe data this week comes from Duke Lemur Center and was cleaned by Jesse Mostipak.\n\nAbout Lemurs and the Duke Lemur Center\n\n\nThe Duke Lemur Center houses over 200 lemurs across 14 species – the most diverse population of lemurs on Earth, outside their native Madagascar.\nLemurs are the most threatened group of mammals on the planet, and 95% of lemur species are at risk of extinction. Our mission is to learn everything we can about lemurs – because the more we learn, the better we can work to save them from extinction. They are endemic only to Madagascar, so it’s essentially a one-shot deal: once lemurs are gone from Madagascar, they are gone from the wild.\nBy studying the variables that most affect their health, reproduction, and social dynamics, the Duke Lemur Center learns how to most effectively focus their conservation efforts. And the more we learn about lemurs, the better we can educate the public around the world about just how amazing these animals are, why they need to be protected, and how each and every one of us can make a difference in their survival.\n\n\nTaxonomic Code\nYou can use the following table for the taxonomic code, or retrieve it by registering for the Duke Lemur Center Database here.\n\n\n\n\n\n\n\n\n\nTaxon\nLatin name\nCommon name\n\n\n\n\nCMED\nCheirogaleus medius\nFat-tailed dwarf lemur\n\n\nDMAD\nDaubentonia madagascariensis\nAye-aye\n\n\nEALB\nEulemur albifrons\nWhite-fronted brown lemur\n\n\nECOL\nEulemur collaris\nCollared brown lemur\n\n\nECOR\nEulemur coronatus\nCrowned lemur\n\n\nEFLA\nEulemur flavifrons\nBlue-eyed black lemur\n\n\nEFUL\nEulemur fulvus\nCommon brown lemur\n\n\nEMAC\nEulemur macaco\nBlack lemur\n\n\nEMON\nEulemur mongoz\nMongoose lemur\n\n\nERUB\nEulemur rubriventer\nRed-bellied lemur\n\n\nERUF\nEulemur rufus\nRed-fronted brown lemur\n\n\nESAN\nEulemur sanfordi\nSanford’s brown lemur\n\n\nEUL\nEulemur Eulemur\nhybrid\n\n\nGMOH\nGalago moholi\nMohol bushbaby\n\n\nHGG\nHapalemur griseus griseus\nEastern lesser bamboo lemur\n\n\nLCAT\nLemur catta\nRing-tailed lemur\n\n\nLTAR\nLoris tardigradus\nSlender loris\n\n\nMMUR\nMircocebus murinus\nGray mouse lemur\n\n\nMZAZ\nMirza coquereli\nNorthern giant mouse lemur\n\n\nNCOU\nNycticebus coucang\nSlow loris\n\n\nNPYG\nNycticebus pygmaeus\nPygmy slow loris\n\n\nOGG\nOtolemur garnettii garnettii\nNorthern greater galago\n\n\nPCOQ\nPropithecus coquereli\nCoquerel’s sifaka\n\n\nPPOT\nPerodicticus potto\nPotto\n\n\nVAR\nVarecia Varecia\nhybrid\n\n\nVRUB\nVarecia rubra\nRed ruffed lemur\n\n\nVVV\nVarecia variegata variegata\nBlack-and-white ruffed lemur\n\n\n\n\nOpen Source article\nFor more in-depth information on this dataset, please see: Life history profiles for 27 strepsirrhine primate taxa generated using captive data from the Duke Lemur Center\nAuthors & Citation\nZehr, SM, Roach RG, Haring D, Taylor J, Cameron FH, Yoder AD. Life history profiles for 27 strepsirrhine primate taxa generated using captive data from the Duke Lemur Center. Sci. Data 1:140019 doi: 10.1038/sdata.2014.19 (2014).\n\n\nHeader image Photo by Anggit Rizkianto on Unsplash\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-08-24')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 35)\n\nlemurs &lt;- tuesdata$lemurs\n\n# Or read in the data manually\n\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-08-24/lemur_data.csv')\n\n\nData Dictionary\n\n\n\nlemur_data.csv\n\n\n\n\n\n\n\n\n﻿variable\nclass\ndescription\n\n\n\n\ntaxon\ncharacter\nTaxonomic code: in most cases, comprised of the first letter of the genus and the first three letters of the species; if taxonomic designation is a subspecies, comprised of the first letter of genus, species, and subspecies, and hybrids are indicated by the first three letters of the genus.\n\n\ndlc_id\ncharacter\nSpecimen ID: Unique DLC number assigned at accession of animal\n\n\nhybrid\ncharacter\nHybrid status: N=not a hybrid. S=species hybrid. B=subspecies hybrid. If sire is one of multiple possible and animal could be a hybrid, it is designated a hybrid.\n\n\nsex\ncharacter\nM=male. F=Female. ND=Not determined\n\n\nname\ncharacter\nHouse name: Animal name assigned at DLC\n\n\ncurrent_resident\ncharacter\nWhether or not the animal currently lives in the DLC colony.\n\n\nstud_book\ncharacter\nRegional or global unique ID among captive individuals of that species; assigned by AZA studbook keeper. Not all individuals have studbook numbers in this data record.\n\n\ndob\ndouble\nDate of Birth (DOB) is an exact date unless there is an entry in the ‘Estimated_DOB’ field.\n\n\nbirth_month\ndouble\nBirth Month\n\n\nestimated_dob\ncharacter\nWhether or not the date of birth is an estimate. Y=estimated to the nearest year, M to the nearest month, and D to the nearest day. If there is a number after the letter code, that indicates to the Nth value of the code. U=unknown. If there is no entry in this field, DOB is not an estimate.\n\n\nbirth_type\ncharacter\nWhether the animal was captive-born (CB), wild-born (WB) or of unknown birth type (UNK)\n\n\nbirth_institution\ncharacter\nName or ISIS abbreviation of institution where animal was born. Duke Prim=DLC. For wild caught animals, birth institution = country of origin, if known\n\n\nlitter_size\ndouble\nNumber of infants in the litter the focal animal was born into (including focal animal). Only indicated where verifiable (born at DLC). A missing value indicates that the litter size is unknown\n\n\nexpected_gestation\ndouble\nValues based on DLC observations and reports from the literature, in days\n\n\nestimated_concep\ndouble\nDate of estimated conception: Calculated as (DOB-Expect ed_Gestation)\n\n\nconcep_month\ndouble\nMonth of estimated conception: Identified from Estimated_Concep\n\n\ndam_id\ncharacter\nSpecimen ID of female parent. DLC unique ID preferred if there is one. Local ID of another ISIS-reporting institution if known and no DLC number exists. ‘Wild’ indicates dam of wild-caught individual. ‘Unk’ or no data indicates dam is unknown\n\n\ndam_name\ncharacter\nHouse name of female parent (at DLC)\n\n\ndam_taxon\ncharacter\nTaxon of female parent: Based on taxonomic code (see description)\n\n\ndam_dob\ndouble\nDate of birth of female parent: If female parent is wild caught or of unknown origin, this date is an estimate\n\n\ndam_age_at_concep_y\ndouble\nEstimated age of female parent at conception of focal animal: in years ((Estimated_Concep-Dam_DOB)/365)\n\n\nsire_id\ncharacter\nSpecimen ID of male parent: DLC number preferred if there is one. Local ID of another ISIS-reporting institution if known and no DLC number exists. ‘Wild’ indicates sire of wild-caught individual. ‘MULT’ indicates multiple possible sires. A following number indicates number of possibilities (e.g. MULT2). ‘Unk’ or no data indicates unknown sire and may include cases of multiple possible sires.\n\n\nsire_name\ncharacter\nHouse name of male parent (at DLC)\n\n\nsire_taxon\ncharacter\nTaxon of male parent: Based on taxonomic code described above\n\n\nsire_dob\ndouble\nDate of birth of male parent: If male parent is wild caught or of unknown origin, this date is an estimate.\n\n\nsire_age_at_concep_y\ndouble\nEstimated age of male parent at conception of focal animal: in years ((Estimated_Concep-Dam_DOB)/365)\n\n\ndod\ndouble\nDate of death: Verified date an animal died. Missing indicates animal is either alive or status is unknown\n\n\nage_at_death_y\ndouble\nAge of animal at verifiable date of death, in years ((DODDOD)/ 365). Missing indicates animal is either alive or status is unknown.\n\n\nage_of_living_y\ndouble\nAge if alive: Verifiable living age of DLC-owned animals and/or current residents at DLC on loan, in years as of the date the datafile was updated ((date of last upda te-DOB)/365). Missing indicates animal is either dead or status is unknown.\n\n\nage_last_verified_y\ndouble\nLast verifiable age: Age of animal at most recent date a non-DLC owned, non-current resident animal was verifiably alive, in years. Dates were obtained from ISIS as entered by other institutions (dates of live weight or animal transfer) or via direct communication from other animal facilities. ((DateLastVerified-DOB)/365). Missing indicates animal is known to be dead or alive.\n\n\nage_max_live_or_dead_y\ndouble\nMaximum age: The animal’s age from any of the three age categories (each individual must have a value in one of the three) indicating the maximum age the animal could have achieved as of the date the datafile was updated.\n\n\nn_known_offspring\ndouble\nNumber of offspring: Number of offspring the individual is known to have produced. There may be additional offspring for this individual if they were born at another institution or if this individual is a possible, rather than known, parent.\n\n\ndob_estimated\ncharacter\nWhether or not the date of birth is an estimate. Y=estimated to the nearest year, M to the nearest month, and D to the nearest day. If there is a number after the letter code, that indicates to the Nth value of the code. U=unknown. If there is no data in this field, DOB is not an estimate.\n\n\nweight_g\ndouble\nWeight: Animal weight, in grams. Weights under 500g generally to nearest 0.1-1g; Weights &gt;500g generally to the nearest 1-20g.\n\n\nweight_date\ndouble\nWeight date: Date animal was weighed\n\n\nmonth_of_weight\ndouble\nWeight month: Month of the year the animal was weighed\n\n\nage_at_wt_d\ndouble\nAge in days: Age of the animal when the weight was taken, in days (Wei ght_Date-DOB)\n\n\nage_at_wt_wk\ndouble\nAge in weeks: Age of the animal when the weight was taken, in weeks ((Weight_Date-DOB)/7))\n\n\nage_at_wt_mo\ndouble\nAge in months: Age of the animal when the weight was taken, in months (((Weight_Date-DOB)/365)*12)\n\n\nage_at_wt_mo_no_dec\ndouble\nAge in months with no decimal: AgeAtWt_mo value rounded down to a whole number for use in computing average individual weights (FLOOR(AgeAtWt_mo))\n\n\nage_at_wt_y\ndouble\nAge in years: Age of the animal when the weight was taken, in years ((weight_date-DOB)/365\n\n\nchange_since_prev_wt_g\ndouble\nWeight difference: Difference, in grams, between this weight and the animal’s previous weight\n\n\ndays_since_prev_wt\ndouble\nDays difference: Difference, in days, between the date of this weight and the date of the animal’s previous weight\n\n\navg_daily_wt_change_g\ndouble\nAverage daily change: Average daily weight change, in grams, between this weight and the animal’s previous weight\n\n\ndays_before_death\ndouble\nDays before death: Number of days before the animal’s death the weight was taken (DOD- Weight_Date). Missing indicates animal is either alive or status is unknown.\n\n\nr_min_dam_age_at_concep_y\ndouble\nDam minimum age at conception, in years, for the species from the life history summary table. Used to calculate ‘Age_Category’ as described below.\n\n\nage_category\ncharacter\nAge category: IJ (infant or juvenile): (AgeAtWt_yr &lt; R_Min_Dam_Age_AtConcep_yr). Young-adult: (Min_Dam_AgeAtConcep_yr &lt;= AgeAtWt_yr &lt; 2xMin_Dam_AgeAtConcep_ yr). Adult: AgeAtWt_yr &gt;= 2xMin_Dam_A geAtConcep_yr\n\n\npreg_status\ncharacter\nPregnancy status: Whether or not animal is pregnant on date weight was taken. P=pregnant, NP=not pregnant (all males coded NP)\n\n\nexpected_gestation_d\ndouble\nExpected gestation length: Values based on DLC observations and reports from the literature, in days\n\n\nconcep_date_if_preg\ndouble\nConception date: Estimated date of infant conception if the weight was taken while a female was pregnant\n\n\ninfant_dob_if_preg\ndouble\nInfant Date of Birth: Date of infant birth if the weight was taken while a female was pregnant\n\n\ndays_before_inf_birth_if_preg\ndouble\nDays until birth: Days remaining in the pregnancy if the weight was taken while a female was pregnant\n\n\npct_preg_remain_if_preg\ndouble\n% of pregnancy remaining: Percentage of pregnancy remaining when weight was taken from a pregnant female calculated as (days_BF_inf_birth/Expected_Gestation)\n\n\ninfant_lit_sz_if_preg\ndouble\nInfant litter size: Number of infants in the litter a female produced if she was pregnant on date weight was taken\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-09-07/readme.html",
    "href": "data/2021/2021-09-07/readme.html",
    "title": "Formula 1 Races",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nThe start of the 2018 Austrian Grand Prix - the image is of two long lines of F1 cars racing around the corner of the Austrian Grand Prix track.\n\n\n\n\nFormula 1 Races\nThe data this week comes from the Ergast API, which has a CC-BY license. H/t to Sara Stoudt for sharing the link to the data by way of Data is Plural!\nFiveThirtyEight published a nice article on “Who’s The Best Formula One Driver Of All Time?”. While the ELO data is not present in this dataset, you could calculate your own rating or using the {elo} package to create ELO scores.\nPer Wikipedia, Formula 1:\n\nFormula One (also known as Formula 1 or F1) is the highest class of international auto racing for single-seater formula racing cars sanctioned by the Fédération Internationale de l’Automobile (FIA). The World Drivers’ Championship, which became the FIA Formula One World Championship in 1981, has been one of the premier forms of racing around the world since its inaugural season in 1950. The word formula in the name refers to the set of rules to which all participants’ cars must conform. A Formula One season consists of a series of races, known as Grands Prix, which take place worldwide on both purpose-built circuits and closed public roads.\nThe results of each race are evaluated using a points system to determine two annual World Championships: one for drivers, the other for constructors. Each driver must hold a valid Super Licence, the highest class of racing licence issued by the FIA. The races must run on tracks graded “1” (formerly “A”), the highest grade-rating issued by the FIA. Most events occur in rural locations on purpose-built tracks, but several events take place on city streets.\n\nEach team can be called a “constructor” and they have two drivers. For example, Lewis Hamilton is the primary (driver) for the Mercedes team (constructor).\n\nLicense\n\nComplete images of the Ergast database are published shortly after each race under the Attribution-NonCommercial-ShareAlike 3.0 Unported Licence.\n\n\n\nData\nThere is an option for raw CSVs (which is what is included in this repo), a SQL database, or querying the raw API. This is a great dataset to practice with using the httr package to query an API, SQL against the database or dbplyr against the database! You can also work with the raw CSVs and practice your dplyr::left_join() and friends. Read more about dplyr joins in the dplyr “joins” documentation.\nIf you wish to query the raw API, you can check the docs for example, to get a table of all drivers who have ever finished #1 in the championship: http://ergast.com/mrd/methods/status/. There is also an option to download the SQL database itself.\ndownload.file(\n  \"http://ergast.com/downloads/f1db_ansi.sql.gz\", \n  destfile = \"f1db-mysql.zip\"\n)\nWorking with this data will require you to do several left joins, for example to get the standings for each race/driver. Each of the tables listed in the data dictionary have their keys for joining. If you don’t want to dig too deep into the data, then I would recommend starting here. This is a good dataset of results by race, driver, season!\ndriver_results_df &lt;- driver_standings %&gt;% \n  left_join(races, by = \"raceId\") %&gt;% \n  rename(driver_url = url) %&gt;% \n  left_join(drivers, by = \"driverId\")\n  \nglimpse(driver_results_df)\n\n#&gt; Rows: 33,206\n#&gt; Columns: 22\n#&gt; $ driverStandingsId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, #&gt; 14, 1…\n#&gt; $ raceId            &lt;dbl&gt; 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, #&gt; 19, …\n#&gt; $ driverId          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, #&gt; 8, …\n#&gt; $ points            &lt;dbl&gt; 10, 8, 6, 5, 4, 3, 2, 1, 14, 11, 6, 6, 10, 3, #&gt; 2,…\n#&gt; $ position          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 6, 7, 4, 9, 10, #&gt; 2,…\n#&gt; $ positionText      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"1\", #&gt; \"3\"…\n#&gt; $ wins              &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, #&gt; 1, …\n#&gt; $ year              &lt;dbl&gt; 2008, 2008, 2008, 2008, 2008, 2008, 2008, #&gt; 2008, …\n#&gt; $ round             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, #&gt; 2, …\n#&gt; $ circuitId         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, #&gt; 2, …\n#&gt; $ name              &lt;chr&gt; \"Australian Grand Prix\", \"Australian Grand #&gt; Prix\"…\n#&gt; $ date              &lt;date&gt; 2008-03-16, 2008-03-16, 2008-03-16, #&gt; 2008-03-16,…\n#&gt; $ time              &lt;chr&gt; \"04:30:00\", \"04:30:00\", \"04:30:00\", #&gt; \"04:30:00\", …\n#&gt; $ driver_url        &lt;chr&gt; #&gt; \"http://en.wikipedia.org/wiki/2008_Australian_Gr…\n#&gt; $ driverRef         &lt;chr&gt; \"hamilton\", \"heidfeld\", \"rosberg\", \"alonso\", #&gt; \"ko…\n#&gt; $ number            &lt;chr&gt; \"44\", \"\\\\N\", \"6\", \"14\", \"\\\\N\", \"\\\\N\", \"\\\\N\", #&gt; \"7\"…\n#&gt; $ code              &lt;chr&gt; \"HAM\", \"HEI\", \"ROS\", \"ALO\", \"KOV\", \"NAK\", #&gt; \"BOU\",…\n#&gt; $ forename          &lt;chr&gt; \"Lewis\", \"Nick\", \"Nico\", \"Fernando\", #&gt; \"Heikki\", \"…\n#&gt; $ surname           &lt;chr&gt; \"Hamilton\", \"Heidfeld\", \"Rosberg\", \"Alonso\", #&gt; \"Ko…\n#&gt; $ dob               &lt;date&gt; 1985-01-07, 1977-05-10, 1985-06-27, #&gt; 1981-07-29,…\n#&gt; $ nationality       &lt;chr&gt; \"British\", \"German\", \"German\", \"Spanish\", #&gt; \"Finni…\n#&gt; $ url               &lt;chr&gt; #&gt; \"http://en.wikipedia.org/wiki/Lewis_Hamilton\", \"…\nTo query the raw API, you can use httr, just make sure to end the call/url in .json to return JSON data.\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(httr)\n\nstanding &lt;- 1\nraw_json &lt;- httr::GET(url = glue::glue(\n  \"http://ergast.com/api/f1/driverStandings/{standing}/drivers.json\")) %&gt;% \n  content(type = \"text\", encoding = \"UTF-8\") %&gt;% \n  jsonlite::parse_json(simplifyVector = FALSE) \n  \nraw_json %&gt;% \n  View()\n  \nwinner_table &lt;- raw_json$MRData$DriverTable$Drivers %&gt;%\n  tibble(data = .) %&gt;%\n  unnest_wider(data)\n  \nwinner_table %&gt;% glimpse()\n#&gt; Rows: 30\n#&gt; Columns: 8\n#&gt; $ driverId        &lt;chr&gt; \"alonso\", \"mario_andretti\", \"ascari\", \"j…\n#&gt; $ permanentNumber &lt;chr&gt; \"14\", NA, NA, NA, \"22\", NA, NA, NA, NA, …\n#&gt; $ code            &lt;chr&gt; \"ALO\", NA, NA, NA, \"BUT\", NA, NA, NA, NA…\n#&gt; $ url             &lt;chr&gt; \"http://en.wikipedia.org/wiki/Fernando_A…\n#&gt; $ givenName       &lt;chr&gt; \"Fernando\", \"Mario\", \"Alberto\", \"Jack\", …\n#&gt; $ familyName      &lt;chr&gt; \"Alonso\", \"Andretti\", \"Ascari\", \"Brabham…\n#&gt; $ dateOfBirth     &lt;chr&gt; \"1981-07-29\", \"1940-02-28\", \"1918-07-13\"…\n#&gt; $ nationality     &lt;chr&gt; \"Spanish\", \"American\", \"Italian\", \"Austr…\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-09-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 37)\n\nresults &lt;- tuesdata$results\n\n# Or read in the data manually\n\ncircuits &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/circuits.csv')\nconstructor_results &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/constructor_results.csv')\nconstructor_standings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/constructor_standings.csv')\nconstructors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/constructors.csv')\ndriver_standings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/driver_standings.csv')\ndrivers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/drivers.csv')\nlap_times &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/lap_times.csv')\npit_stops &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/pit_stops.csv')\nqualifying &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/qualifying.csv')\nraces &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/races.csv')\nresults &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/results.csv')\nseasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/seasons.csv')\nstatus &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-07/status.csv')\n\n\nData Dictionary\n\n\n\nList of Tables\n\n\n\n\ncircuits\n\n\nconstructorResults\n\n\nconstructorStandings\n\n\nconstructors\n\n\ndriverStandings\n\n\ndrivers\n\n\nlapTimes\n\n\npitStops\n\n\nqualifying\n\n\nraces\n\n\nresults\n\n\nseasons\n\n\nstatus\n\n\n\n\n\n\nGeneral Notes\n\n\n\n\nDates, times and durations are in ISO 8601 format\n\n\nDates and times are UTC\n\n\nStrings use UTF-8 encoding\n\n\nPrimary keys are for internal use only\n\n\nFields ending with “Ref” are unique identifiers for external use\n\n\nA grid position of ‘0’ is used for starting from the pitlane\n\n\nLabels used in the positionText fields:\n\n\n“D” - disqualified\n\n\n“E” - excluded\n\n\n“F” - failed to qualify\n\n\n“N” - not classified\n\n\n“R” - retired\n\n\n“W” - withdrew\n\n\n\n\n\ncircuits.csv\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\ncircuitId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\ncircuitRef\nvarchar(255)\nNO\n\n\n\nUnique circuit identifier\n\n\nname\nvarchar(255)\nNO\n\n\n\nCircuit name\n\n\nlocation\nvarchar(255)\nYES\n\nNULL\n\nLocation name\n\n\ncountry\nvarchar(255)\nYES\n\nNULL\n\nCountry name\n\n\nlat\nfloat\nYES\n\nNULL\n\nLatitude\n\n\nlng\nfloat\nYES\n\nNULL\n\nLongitude\n\n\nalt\nint(11)\nYES\n\nNULL\n\nAltitude (metres)\n\n\nurl\nvarchar(255)\nNO\nUNI\n\n\nCircuit Wikipedia page\n\n\n\n\n\nconstructor_results table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nconstructorResultsId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\nraceId\nint(11)\nNO\n\n0\n\nForeign key link to races table\n\n\nconstructorId\nint(11)\nNO\n\n0\n\nForeign key link to constructors table\n\n\npoints\nfloat\nYES\n\nNULL\n\nConstructor points for race\n\n\nstatus\nvarchar(255)\nYES\n\nNULL\n\n“D” for disqualified (or null)\n\n\n\n\n\nconstructor_standings table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nconstructorStandingsId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\nraceId\nint(11)\nNO\n\n0\n\nForeign key link to races table\n\n\nconstructorId\nint(11)\nNO\n\n0\n\nForeign key link to constructors table\n\n\npoints\nfloat\nNO\n\n0\n\nConstructor points for season\n\n\nposition\nint(11)\nYES\n\nNULL\n\nConstructor standings position (integer)\n\n\npositionText\nvarchar(255)\nYES\n\nNULL\n\nConstructor standings position (string)\n\n\nwins\nint(11)\nNO\n\n0\n\nSeason win count\n\n\n\n\n\nconstructors table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nconstructorId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\nconstructorRef\nvarchar(255)\nNO\n\n\n\nUnique constructor identifier\n\n\nname\nvarchar(255)\nNO\nUNI\n\n\nConstructor name\n\n\nnationality\nvarchar(255)\nYES\n\nNULL\n\nConstructor nationality\n\n\nurl\nvarchar(255)\nNO\n\n\n\nConstructor Wikipedia page\n\n\n\n\n\ndriver_standings table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\ndriverStandingsId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\nraceId\nint(11)\nNO\n\n0\n\nForeign key link to races table\n\n\ndriverId\nint(11)\nNO\n\n0\n\nForeign key link to drivers table\n\n\npoints\nfloat\nNO\n\n0\n\nDriver points for season\n\n\nposition\nint(11)\nYES\n\nNULL\n\nDriver standings position (integer)\n\n\npositionText\nvarchar(255)\nYES\n\nNULL\n\nDriver standings position (string)\n\n\nwins\nint(11)\nNO\n\n0\n\nSeason win count\n\n\n\n\n\ndrivers table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\ndriverId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\ndriverRef\nvarchar(255)\nNO\n\n\n\nUnique driver identifier\n\n\nnumber\nint(11)\nYES\n\nNULL\n\nPermanent driver number\n\n\ncode\nvarchar(3)\nYES\n\nNULL\n\nDriver code e.g. “ALO”\n\n\nforename\nvarchar(255)\nNO\n\n\n\nDriver forename\n\n\nsurname\nvarchar(255)\nNO\n\n\n\nDriver surname\n\n\ndob\ndate\nYES\n\nNULL\n\nDriver date of birth\n\n\nnationality\nvarchar(255)\nYES\n\nNULL\n\nDriver nationality\n\n\nurl\nvarchar(255)\nNO\nUNI\n\n\nDriver Wikipedia page\n\n\n\n\n\nlap_times table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nraceId\nint(11)\nNO\nPRI\nNULL\n\nForeign key link to races table\n\n\ndriverId\nint(11)\nNO\nPRI\nNULL\n\nForeign key link to drivers table\n\n\nlap\nint(11)\nNO\nPRI\nNULL\n\nLap number\n\n\nposition\nint(11)\nYES\n\nNULL\n\nDriver race position\n\n\ntime\nvarchar(255)\nYES\n\nNULL\n\nLap time e.g. “1:43.762”\n\n\nmilliseconds\nint(11)\nYES\n\nNULL\n\nLap time in milliseconds\n\n\n\n\n\npit_stops table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nraceId\nint(11)\nNO\nPRI\nNULL\n\nForeign key link to races table\n\n\ndriverId\nint(11)\nNO\nPRI\nNULL\n\nForeign key link to drivers table\n\n\nstop\nint(11)\nNO\nPRI\nNULL\n\nStop number\n\n\nlap\nint(11)\nNO\n\nNULL\n\nLap number\n\n\ntime\ntime\nNO\n\nNULL\n\nTime of stop e.g. “13:52:25”\n\n\nduration\nvarchar(255)\nYES\n\nNULL\n\nDuration of stop e.g. “21.783”\n\n\nmilliseconds\nint(11)\nYES\n\nNULL\n\nDuration of stop in milliseconds\n\n\n\n\n\nqualifying table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nqualifyId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\nraceId\nint(11)\nNO\n\n0\n\nForeign key link to races table\n\n\ndriverId\nint(11)\nNO\n\n0\n\nForeign key link to drivers table\n\n\nconstructorId\nint(11)\nNO\n\n0\n\nForeign key link to constructors table\n\n\nnumber\nint(11)\nNO\n\n0\n\nDriver number\n\n\nposition\nint(11)\nYES\n\nNULL\n\nQualifying position\n\n\nq1\nvarchar(255)\nYES\n\nNULL\n\nQ1 lap time e.g. “1:21.374”\n\n\nq2\nvarchar(255)\nYES\n\nNULL\n\nQ2 lap time\n\n\nq3\nvarchar(255)\nYES\n\nNULL\n\nQ3 lap time\n\n\n\n\n\nraces table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nraceId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\nyear\nint(11)\nNO\n\n0\n\nForeign key link to seasons table\n\n\nround\nint(11)\nNO\n\n0\n\nRound number\n\n\ncircuitId\nint(11)\nNO\n\n0\n\nForeign key link to circuits table\n\n\nname\nvarchar(255)\nNO\n\n\n\nRace name\n\n\ndate\ndate\nNO\n\n0000-00-00\n\nRace date e.g. “1950-05-13”\n\n\ntime\ntime\nYES\n\nNULL\n\nRace start time e.g.”13:00:00”\n\n\nurl\nvarchar(255)\nYES\nUNI\nNULL\n\nRace Wikipedia page\n\n\n\n\n\nresults table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nresultId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\nraceId\nint(11)\nNO\n\n0\n\nForeign key link to races table\n\n\ndriverId\nint(11)\nNO\n\n0\n\nForeign key link to drivers table\n\n\nconstructorId\nint(11)\nNO\n\n0\n\nForeign key link to constructors table\n\n\nnumber\nint(11)\nYES\n\nNULL\n\nDriver number\n\n\ngrid\nint(11)\nNO\n\n0\n\nStarting grid position\n\n\nposition\nint(11)\nYES\n\nNULL\n\nOfficial classification, if applicable\n\n\npositionText\nvarchar(255)\nNO\n\n\n\nDriver position string e.g. “1” or “R”\n\n\npositionOrder\nint(11)\nNO\n\n0\n\nDriver position for ordering purposes\n\n\npoints\nfloat\nNO\n\n0\n\nDriver points for race\n\n\nlaps\nint(11)\nNO\n\n0\n\nNumber of completed laps\n\n\ntime\nvarchar(255)\nYES\n\nNULL\n\nFinishing time or gap\n\n\nmilliseconds\nint(11)\nYES\n\nNULL\n\nFinishing time in milliseconds\n\n\nfastestLap\nint(11)\nYES\n\nNULL\n\nLap number of fastest lap\n\n\nrank\nint(11)\nYES\n\n0\n\nFastest lap rank, compared to other drivers\n\n\nfastestLapTime\nvarchar(255)\nYES\n\nNULL\n\nFastest lap time e.g. “1:27.453”\n\n\nfastestLapSpeed\nvarchar(255)\nYES\n\nNULL\n\nFastest lap speed (km/h) e.g. “213.874”\n\n\nstatusId\nint(11)\nNO\n\n0\n\nForeign key link to status table\n\n\n\n\n\nseasons table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nyear\nint(11)\nNO\nPRI\n0\n\nPrimary key e.g. 1950\n\n\nurl\nvarchar(255)\nNO\nUNI\n\n\nSeason Wikipedia page\n\n\n\n\n\nstatus table\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\nDescription\n\n\n\n\nstatusId\nint(11)\nNO\nPRI\nNULL\nauto_increment\nPrimary key\n\n\nstatus\nvarchar(255)\nNO\n\n\n\nFinishing status e.g. “Retired”\n\n\n\n\n\nCleaning Script\nNot a real cleaning script, just me exploring the data structures.\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(httr)\n\n# if you want SQL with tables\n# download.file(\n#   \"http://ergast.com/downloads/f1db_ansi.sql.gz\", \n#   destfile = \"2021/2021-09-07/f1db-mysql.zip\"\n#   )\n\ndownload.file(\n  \"http://ergast.com/downloads/f1db_csv.zip\", \n  destfile = \"2021/2021-09-07/f1db.zip\"\n)\n\nunzip(\"2021/2021-09-07/f1db.zip\", exdir = \"2021/2021-09-07/\")\n\nraw_data &lt;- map(\n  fs::dir_ls(\"2021/2021-09-07/\", glob = \"*.csv\"),\n  read_csv\n  ) %&gt;% \n  set_names(nm = str_remove(names(.), \"2021/2021-09-07/\"))\n\nraw_data %&gt;% \n  str(max.level = 1)\n\n# Example of JSON/HTTR\nraw_json &lt;- httr::GET(url = glue::glue(\n  \"http://ergast.com/api/f1/driverStandings/{standing}/drivers.json\")) %&gt;% \n  content(type = \"text\", encoding = \"UTF-8\")\n\nraw_json%&gt;% \n  View()\n\nraw_json$MRData$DriverTable$Drivers %&gt;%\n  tibble(data = .) %&gt;%\n  unnest_wider(data)\n\ndriver_standings %&gt;% \n  left_join(raw_data$races.csv, by = \"raceId\") %&gt;% \n  rename(driver_url = url) %&gt;% \n  left_join(raw_data$drivers.csv, by = \"driverId\")\n\nfile_names &lt;- fs::dir_ls(\"2021/2021-09-07/\", glob = \"*.csv\") %&gt;% \n  str_remove(\"2021/2021-09-07/\") %&gt;% \n  str_remove(\".csv\")\n  \nfile_names"
  },
  {
    "objectID": "data/2021/2021-09-21/readme.html",
    "href": "data/2021/2021-09-21/readme.html",
    "title": "Emmy Awards and Nominees",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n\nAn image of the Emmy award, it is a golden trophy with lightning-shaped angel wings. The figure is holding a hollow sphere with extended arms.\n\n\n\n\nEmmy Awards and Nominees\nThe data this week comes from emmys.com.\nSusie Lu has a nice visualization of similar data. Statista also has a nice visualization looking at Netflix specifically, and an older one segmented by Network.\nThe Emmy Award:\n\nAn Emmy Award, or simply Emmy, is a trophy presented at one of the numerous annual American events or competitions that each recognize achievements in a particular sector of the television industry. The Emmy is considered one of the four major entertainment awards in the United States, the others being the Grammy (for music), the Oscar (Academy Award) (for film), and the Tony (for theatre). The two events that receive the most media coverage are the Primetime Emmy Awards and the Daytime Emmy Awards, which recognize outstanding work in American primetime and daytime entertainment programming, respectively.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-09-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 39)\n\nnominees &lt;- tuesdata$nominees\n\n# Or read in the data manually\n\nnominees &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-09-21/nominees.csv')\n\n\nData Dictionary\n\n\n\nnominees.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncategory\ncharacter\nAward Category\n\n\nlogo\ncharacter\nLogo or headshot\n\n\nproduction\ncharacter\nProduction Team\n\n\ntype\ncharacter\nType - either nominee or winner\n\n\ntitle\ncharacter\nTitle of show\n\n\ndistributor\ncharacter\nDistributor of show\n\n\nproducer\ncharacter\nProducer of show\n\n\nyear\ninteger\nYear of award\n\n\npage\ninteger\nPage number\n\n\npage_id\ninteger\nPage ID (unique count/page)\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(rvest)\n\nclean_nominee &lt;- function(x){\n  \n  trimmed_nom &lt;- str_trim(x) %&gt;% \n    str_split(\"\\n\") %&gt;% \n    pluck(1) %&gt;%\n    str_trim()\n  \n  out_len &lt;- length(trimmed_nom)\n  \n  names_add &lt;- c(\n    c(\"type\", \"title\", \"distributor\", \"producer\"),rep(\"blank\", out_len - 4)\n  )\n  \n  trimmed_nom %&gt;% \n    set_names(nm = names_add)\n}\n\nclean_production &lt;- function(raw_html){\n  \n  data_out &lt;- map2(\n    1:10, rep(c(\"odd\", \"even\"), 5),\n    ~raw_html %&gt;% \n      html_nodes(glue::glue(\"div.views-row.views-row-{.x}.views-row-{.y}\")) %&gt;% \n      html_nodes(\".winner-list\") %&gt;% \n      map(~html_nodes(.x, \"li\")) %&gt;% \n      map(html_text) \n  ) %&gt;% \n    map(pluck, 1)\n  \n  data_out\n  \n}\n\n\nscrape_pages &lt;- function(page_num){\n  \n  cat(scales::percent_format()(page_num/1000), \"\\n\")\n  \n  url &lt;- glue::glue(\"https://www.emmys.com/awards/nominations/award-search?page={page_num}\")\n  \n  raw_html &lt;- url %&gt;% \n    read_html() %&gt;% \n    html_nodes(\"#block-system-main &gt; div &gt; div &gt; section:nth-child(3) &gt; div &gt; div\")\n  \n  category &lt;- raw_html %&gt;% \n    # html_node(\"#block-system-main &gt; div &gt; div &gt; section:nth-child(3) &gt; div &gt; div\") %&gt;% \n    html_nodes(\"h5\") %&gt;% \n    html_text()\n  \n  logos &lt;- raw_html %&gt;% \n    html_nodes(\"div.image.img.col-4.col-md-3.col-xl-2 &gt; a &gt; img\") %&gt;% \n    html_attr(\"src\")\n  \n  outcome &lt;- raw_html %&gt;% \n    html_nodes(\"ul.nominee, ul.winner\") %&gt;% \n    html_text() %&gt;% \n    map(clean_nominee)\n  \n  out_len &lt;- map(outcome, length)\n  \n  production &lt;- clean_production(raw_html)\n  \n  tibble(\n    category = category,\n    logo = logos,\n    production = production,\n    outcome = outcome\n  ) %&gt;%\n    mutate(year = str_sub(category, -4, -1) %&gt;% as.integer(),\n           page = page_num,\n           page_id = row_number())\n  \n}\n\nsafe_scrape &lt;- safely(scrape_pages)\n\nmap_all &lt;- 0:2362 %&gt;% map(safe_scrape)\n\nall_results &lt;- map_all %&gt;% \n  map_dfr(\"result\") \n\nclean_df &lt;- all_results %&gt;%\n  rowwise() %&gt;% \n  mutate(\n    fix = ifelse(str_detect(title, \", as\"), 1, 0),\n    title = ifelse(fix == 1, distributor, title),\n    distributor = ifelse(fix == 1 && !is.na(producer), producer, distributor),\n    producer = ifelse(fix == 1 && !is.na(blank), blank, producer),\n    producer = ifelse(!is.na(blank), blank, producer)\n  ) \n\nfinal_df &lt;- clean_df %&gt;% \n  select(-contains(\"blank\"), -fix)"
  },
  {
    "objectID": "data/2021/2021-10-05/readme.html",
    "href": "data/2021/2021-10-05/readme.html",
    "title": "Registered Nurses",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nRegistered Nurses\nThe data this week comes from Data.World.\nThe BLS also wrote about Registered Nurses by state.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-10-05')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 41)\n\nnurses &lt;- tuesdata$nurses\n\n# Or read in the data manually\n\nnurses &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-05/nurses.csv')\n\n\nData Dictionary\n\n\n\nnurses.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nState\ncharacter\nState\n\n\nYear\ndouble\nYear\n\n\nTotal Employed RN\ndouble\nTotal Employed Registered Nurses\n\n\nEmployed Standard Error (%)\ndouble\nEmployed standard error (%)\n\n\nHourly Wage Avg\ndouble\nHourly wage average\n\n\nHourly Wage Median\ndouble\nHourly wage median\n\n\nAnnual Salary Avg\ndouble\nAnnual salary average\n\n\nAnnual Salary Median\ndouble\nAnnual salary median\n\n\nWage/Salary standard error (%)\ndouble\nWage/salary standard error %\n\n\nHourly 10th Percentile\ndouble\nHourly 10th Percentile\n\n\nHourly 25th Percentile\ndouble\nHourly 25th Percentile\n\n\nHourly 75th Percentile\ndouble\nHourly 75th Percentile\n\n\nHourly 90th Percentile\ndouble\nHourly 90th Percentile\n\n\nAnnual 10th Percentile\ndouble\nAnnual 10th Percentile\n\n\nAnnual 25th Percentile\ndouble\nAnnual 25th Percentile\n\n\nAnnual 75th Percentile\ndouble\nAnnual 75th Percentile\n\n\nAnnual 90th Percentile\ndouble\nAnnual 90th Percentile\n\n\nLocation Quotient\ndouble\nLocation Quotient\n\n\nTotal Employed (National)_Aggregate\ndouble\nTotal Employed (National)_Aggregate\n\n\nTotal Employed (Healthcare, National)_Aggregate\ndouble\nTotal Employed (Healthcare, National)_Aggregate\n\n\nTotal Employed (Healthcare, State)_Aggregate\ndouble\nTotal Employed (Healthcare, State)_Aggregate\n\n\nYearly Total Employed (State)_Aggregate\ndouble\nYearly Total Employed (State)_Aggregate\n\n\n\n\nCleaning Script\nNo cleaning script but definitely explore:\ntidyr::pivot_longer()"
  },
  {
    "objectID": "data/2021/2021-10-19/readme.html",
    "href": "data/2021/2021-10-19/readme.html",
    "title": "Giant Pumpkins",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nGiant Pumpkins\nThe data this week comes from BigPumpkins.com.\n\nThe Great Pumpkin Commonwealth’s (GPC) mission cultivates the hobby of growing giant pumpkins throughout the world by establishing standards and regulations that ensure quality of fruit, fairness of competition, recognition of achievement, fellowship and education for all participating growers and weigh-off sites.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-10-19')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 43)\n\npumpkins &lt;- tuesdata$pumpkins\n\n# Or read in the data manually\n\npumpkins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-10-19/pumpkins.csv')\n\n\nData Dictionary\n\n\n\npumpkins.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ncharacter\nYear-type\n\n\nplace\ncharacter\nPlace/ranking\n\n\nweight_lbs\ncharacter\nWeight in pounds\n\n\ngrower_name\ncharacter\nName of grower\n\n\ncity\ncharacter\nCity\n\n\nstate_prov\ncharacter\nState/Province\n\n\ncountry\ncharacter\nCountry\n\n\ngpc_site\ncharacter\nGPC site (great pumpkin commonwealth)\n\n\nseed_mother\ncharacter\nSeed mother\n\n\npollinator_father\ncharacter\nFather\n\n\nott\ncharacter\nOver the top inches, can be used to estimate weight\n\n\nest_weight\ncharacter\nEstimated weight in lbs\n\n\npct_chart\ncharacter\nPercent on chart\n\n\nvariety\ncharacter\nVariety of pumpkin\n\n\n\nTypes: F = “Field Pumpkin”, P = “Giant Pumpkin”, S = “Giant Squash”, W = “Giant Watermelon”, L = “Long Gourd” (length in inches, not weight in pounds), T = Tomato\nGreat Pumpkin Commonwealth rule book\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-11-02/readme.html",
    "href": "data/2021/2021-11-02/readme.html",
    "title": "Making maps with R",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nMaking maps with R\nNovember is the #30DayMapChallenge month! More details can be found on the 30DayMapChallenge GH page.\n\n\n🎉Categories for #30DayMapChallenge 2021 🎉Join in to create maps around these themes. Use your creativity and share your results with others! Starts 01/11/2021. More information: https://t.co/OmTLB6u2cL pic.twitter.com/IzWAw21ns6\n\n— Topi Tjukanov (@tjukanov) October 1, 2021\n\n\nThe data this week comes from spData package and the spDataLarge package. The files are loaded inside those packages, so rather than loading the data as typical, please install those packages and use the data from there.\nSome example code from Geocompution with R book.\nRather than supplying datasets, this week we’ll be exploring the excellent Geocomputation with R book.\n\nGet the data here\n# Get the Data\n\nlibrary(spData)\n\n# note that spDataLarge needs to be installed via:\n# install.packages(\"spDataLarge\", \n# repos = \"https://nowosad.github.io/drat/\", type = \"source\")\nlibrary(spDataLarge) \n\n\nCleaning Script\nlibrary(sf)\nlibrary(raster)\nlibrary(dplyr)\nlibrary(spData)\n\n# note that this needs to be installed like\n# install.packages(\"spDataLarge\", \n# repos = \"https://nowosad.github.io/drat/\", type = \"source\")\n\nlibrary(spDataLarge) \n\n\nlibrary(tmap)    # for static and interactive maps\nlibrary(leaflet) # for interactive maps\nlibrary(ggplot2) # tidyverse data visualization package\n\n# Add fill layer to nz shape\ntm_shape(nz) +\n  tm_fill() \n# Add border layer to nz shape\ntm_shape(nz) +\n  tm_borders() \n# Add fill and border layers to nz shape\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders() \n\nmap_nz = tm_shape(nz) + tm_polygons()\nclass(map_nz)\n#&gt; [1] \"tmap\"\n#&gt; \nmap_nz1 = map_nz +\n  tm_shape(nz_elev) + tm_raster(alpha = 0.7)\n\nnz_water = st_union(nz) %&gt;% st_buffer(22200) %&gt;% \n  st_cast(to = \"LINESTRING\")\nmap_nz2 = map_nz1 +\n  tm_shape(nz_water) + tm_lines()\n\nmap_nz3 = map_nz2 +\n  tm_shape(nz_height) + tm_dots()\n\ntmap_arrange(map_nz1, map_nz2, map_nz3)"
  },
  {
    "objectID": "data/2021/2021-11-16/readme.html",
    "href": "data/2021/2021-11-16/readme.html",
    "title": "BlackInDataWeek",
    "section": "",
    "text": "The Black in Data week overview. Black in data runs from November 14th to the 21st. Sunday Nov 14th is #BlackInDataRollCall. Mon, Nov 15th is #BlackInDataJourney, Tues, Nov 16th is #BlackInDataSkills, Wed, Nov 17th is #BlackInDataViz, Thurs, Nov 18th is #BlackInDataJustice, Fri, Nov 19th is #BlackInDataMentorship, and Sat, Nov 20th is Black in Data Week closing ceremony\n\n\n\nBlackInDataWeek\n\nWe’re celebrating current and future Black Data Professionals, ALL WEEK LONG!!!\n\nFull details at: Black in Data Week\n\nFollow along via the #BlackInDataWeek hashtag.\n\n\nA week-long celebration #BlackInDataWeek to: (1) highlight the valuable work and experiences of Black people in the field of Data (2) provide community (3) educational and professional resources. To sign up for events, click here!\n\nTo learn more about #BlackInDataWeek, please see their website.\nThe #TidyTuesday community will be taking a break from our own data this week and asking you to engage with, promote, and support Black people in data.\nYou’re welcome to revisit older datasets from #TidyTuesday this week, but especially on Wednesday Nov 17th, please create space and engage with the #BlackInDataViz hashtag on Twitter rather than promoting your own work.\nInfo below from the Black in Data Week event:\nSchedule of Events, ALL TIMES EST:\n\nMONDAY\nData Careers over Age 40 10:00 AM - 11:00 AM, Kimberly Deas, Wenona Young\nTransition to Tech 4:00 PM - 4:30 PM, Brittany City\nStarting/Switching to a Data Career 5:00 PM - 5:30 PM, Jarrett Hurms\nData Careers Fireside Chat 6:00 PM- 7:00 PM, Chrystal Parker, Andrea Hobby, Odane Dunbar, Lauren Shores, Kimberly Deas\nTips for Going into Academia to Teach Data 8:00 PM - 8:30 PM, Jarrett Hurms\nData Careers in Public Health 9:00 PM - 9:30 PM, Andrea Hobby & Barbara Tisi\n\n\nTUESDAY\nIntroduction to DeepNote 1:00 PM - 2:00 PM, Sia Seko\nNow You’re Speaking My Language: Becoming Literate in Data 5:00 PM - 5:45 PM, Aliyah Wakil\nIntro to Chem Informatics 6:00 PM -6:30 PM, Kimberly Deas\nBeing Professional Online 7:00 PM - 7:45 PM, Pierre DuBois\nDecoding the infinite playlist: Practical data science in Python with the Spotify API 8:00 PM - 8:45 PM, Matthew Finney\n\n\nWEDNESDAY\nRe-Creating the Du Bois Data Stories 10:00 AM - 10:30 AM, Anthony Starks\nMake your data sing - An introduction to Data Visualization, 12 - 12:30 PM, Soti Coker\nCOVID-19 Health Disparities 2021 Update 7:00 PM - 7:30 PM, Andrea Hobby\nData is Poetry and Storytelling through Code, Numbers & Stats 7:30 - 8:00 PM, Kofi Oduro\n\n\nTHURSDAY\nPromise & Perils of Machine Learning & Fairness 9:00 AM - 9:30 AM, Lauren Rhue\nDiscovering Harmful Algorithms 12:00 PM - 1:30 PM, Ayodele Odubela\nCentering Data Equity 2:00 PM- 3:00 PM, Brandeis Marshall\nUsing Data to Build Towards Equity and Justice 3:00 PM - 3:45 PM, Allen Hillery\nMapping Police Violence 5:00 PM- 5:45 PM, Samuel Sinyangwe\n\n\nFRIDAY\nBlack in Data Mentorship & Networking Event 7:00 PM - 9:00 PM\n\n\nSATURDAY\nBlack in Data Closing Ceremony Keynote 11:30 AM - 1:00 PM, Wenona Young, MS"
  },
  {
    "objectID": "data/2021/2021-11-30/readme.html",
    "href": "data/2021/2021-11-30/readme.html",
    "title": "Cricket",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\nCricket from Wikipedia\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nCricket\nThe data this week comes from ESPN Cricinfo by way of Hassanasir.\n\nCricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 22-yard (20-metre) pitch with a wicket at each end, each comprising two bails balanced on three stumps. The game proceeds when a player on the fielding team, called the bowler, “bowls” (propels) the ball from one end of the pitch towards the wicket at the other end. The batting side’s players score runs by striking the bowled ball with a bat and running between the wickets, while the bowling side tries to prevent this by keeping the ball within the field and getting it to either wicket, and dismiss each batter (so they are “out”). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side either catching a hit ball before it touches the ground, or hitting a wicket with the ball before a batter can cross the crease line in front of the wicket to complete a run. When ten batters have been dismissed, the innings ends and the teams swap roles. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches.\n\n\nA One Day International (ODI) is a form of limited overs cricket, played between two teams with international status, in which each team faces a fixed number of overs, currently 50, with the game lasting up to 9 hours.[1][2] The Cricket World Cup, generally held every four years, is played in this format. One Day International matches are also called Limited Overs Internationals (LOI), although this generic term may also refer to Twenty20 International matches. They are major matches and considered the highest standard of List A, limited-overs competition.\n\n\nThe Cricket World Cup (officially known as ICC Men’s Cricket World Cup)[2] is the international championship of One Day International (ODI) cricket. The event is organised by the sport’s governing body, the International Cricket Council (ICC), every four years, with preliminary qualification rounds leading up to a finals tournament. The tournament is one of the world’s most viewed sporting events and is considered the “flagship event of the international cricket calendar” by the ICC\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-11-30')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 49)\n\nmatches &lt;- tuesdata$matches\n\n# Or read in the data manually\n\nmatches &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-11-30/matches.csv')\n\n\nData Dictionary\n\n\n\nmatches.csv\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nmatch_id\nCharacter\nMatch ID as per ESPN Cric Info\n\n\nteam1\nCharacter\nTeam playing first\n\n\nteam2\nCharacter\nTeam playing second\n\n\nscore_team1\nNumber\nTeam one score\n\n\nscore_team2\nNumber\nteam one score\n\n\nwickets_team2\nNumber\nwickets fallen for team one; if 10 it means all out.\n\n\nwickets_team\nNumber\nwickets fallen for team one; if 10 it means all out.\n\n\nteam1_away_or_home\nCharacter\nWhether team 1 was playing at home or away\n\n\nteam2_home_away\nCharacter\nWhether team 2 was playing at\n\n\nwinner\nCharacter\nwho won the match/match result\n\n\nmargin\nNumber\nwhat was the margin of the victory.\n\n\nmargin_type\nCharacter\nwhat was the type of margin of victory; if team playing first one, then it’s recorded as runs by which it one, if team playing second won, they it’s number of wickets they won by.\n\n\ntime_of_day\nCharacter\nDay or Day and Night. (Day matches start in the morning and finish by/before sunset. Day and night matches start in the afternoon continue late in the evening; flood lights are used after the sunset).\n\n\nseries\nCharacter\nName of the series.\n\n\nplayer_of_match\nCharacter\nWho was given player of the match.\n\n\nplayer_of_match_team\nCharacter\nWhich team did the player of match belong. Mostly player of the match belongs to the winning team; but rarely it’s given to a player from the loosing team, if his performance was outstanding).\n\n\nvenue\nCharacter\nWhat was ground name.\n\n\ntoss\nCharacter\nWho won the toss.\n\n\ntoss_decision\nCharacter\nWhat decision did the toss winning team made.\n\n\nball_remaining\nCharacter\nhow many balls were remaining.\n\n\nground\nCharacter\nWhat was the name of the ground (Cricinfo has both ground and venue details so I kept them here).\n\n\nground_city\nCharacter\nCity of the Ground.\n\n\nground_country\nCharacter\nCountry of the ground.\n\n\nmatch_date\nCharacter\nMatch date - some matches were played over two days so dates are included for both dates; for analysis either of the date can be taken).\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2021/2021-12-14/readme.html",
    "href": "data/2021/2021-12-14/readme.html",
    "title": "Spice Up Your Life! ",
    "section": "",
    "text": "Please add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media."
  },
  {
    "objectID": "data/2021/2021-12-14/readme.html#data-dictionaries",
    "href": "data/2021/2021-12-14/readme.html#data-dictionaries",
    "title": "Spice Up Your Life! ",
    "section": "Data dictionaries",
    "text": "Data dictionaries\nA data dictionary for each data set is provided here."
  },
  {
    "objectID": "data/2021/2021-12-14/readme.html#example-use",
    "href": "data/2021/2021-12-14/readme.html#example-use",
    "title": "Spice Up Your Life! ",
    "section": "Example use",
    "text": "Example use\nThe R code below uses the studio_album_tracks data set to produce summary statistics for selected audio features.\n# Load libraries\nlibrary(dplyr)\n\n# Read data into R\nstudio_album_tracks &lt;- readr::read_csv(\"https://github.com/jacquietran/spice_girls_data/raw/main/data/studio_album_tracks.csv\")\n\n# For each album, calculate mean values for danceability, energy, and valence\nstudio_album_tracks %&gt;%\n  group_by(album_name) %&gt;%\n  summarise(\n    danceability_mean = mean(danceability),\n    energy_mean = mean(energy),\n    valence_mean = mean(valence)) %&gt;%\n  ungroup() %&gt;%\n  # Set factor levels of album_name\n  mutate(\n    album_name = factor(\n      album_name, levels = c(\"Spice\", \"Spiceworld\", \"Forever\"))) %&gt;%\n  arrange(album_name)"
  },
  {
    "objectID": "data/2021/2021-12-14/readme.html#useful-packages",
    "href": "data/2021/2021-12-14/readme.html#useful-packages",
    "title": "Spice Up Your Life! ",
    "section": "Useful packages",
    "text": "Useful packages\n\nspotifyr: https://www.rcharlie.com/spotifyr/index.html\ngeniusr: https://ewenme.github.io/geniusr/\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-12-14')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 51)\n\nstudio_album_tracks &lt;- tuesdata$studio_album_tracks\n\n# Or read in the data manually\n\nstudio_album_tracks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2021/2021-12-14/studio_album_tracks.csv')\n\n\nData Dictionary"
  },
  {
    "objectID": "data/2021/readme.html",
    "href": "data/2021/readme.html",
    "title": "2021 Data",
    "section": "",
    "text": "2021 Data\nArchive of datasets and articles from the 2021 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2020-12-29\nBring your own data from 2020!\nNA\nNA\n\n\n2\n2021-01-05\nTransit Cost Project\nTransitCosts.com\nTransit Costs Case Study\n\n\n3\n2021-01-12\nArt Collections\nTate Collection\nAspect Ratio of Artworks through Time\n\n\n4\n2021-01-19\nKenya Census\nrKenyaCensus\nIntroducing rKenyaCensus\n\n\n5\n2021-01-26\nPlastic Pollution\nBreak Free from Plastic\nSarah Sauve\n\n\n6\n2021-02-02\nHBCU Enrollment\nData.World & Data.World\nHBCU Donations Article\n\n\n7\n2021-02-09\nWealth and Income\nUrban Institute & US Census\nUrban Institute\n\n\n8\n2021-02-16\nW.E.B. Du Bois Challenge\nDu Bois Data Challenge\nAnthony Starks - Recreating Du Bois’s data portraits\n\n\n9\n2021-02-23\nEmployment and Earnings\nBLS\nBLS Article\n\n\n10\n2021-03-02\nSuperBowl Ads\nFiveThirtyEight\nFiveThirtyEight\n\n\n11\n2021-03-09\nBechdel Test\nFiveThirtyEight\nFiveThirtyEight\n\n\n12\n2021-03-16\nVideo Games + Sliced\nSteam\nSteamCharts\n\n\n13\n2021-03-23\nUN Votes\nHarvard Dataverse\nCitizen Statistician\n\n\n14\n2021-03-30\nMakeup Shades\nThe Pudding data\nThe Pudding\n\n\n15\n2021-04-06\nGlobal deforestation\nOur World in Data\nOur World in Data\n\n\n16\n2021-04-13\nUS Post Offices\nCameron Blevins and Richard W. Helbock\nUS Post Offices\n\n\n17\n2021-04-20\nNetflix Titles\nKaggle\nFlixable\n\n\n18\n2021-04-27\nCEO Departures\nGentry et al.\ninvestors.com\n\n\n19\n2021-05-04\nWater Access Points\nWPDX\nWPDX\n\n\n20\n2021-05-11\nUS Broadband\nMicrosoft GitHub\nThe Verge\n\n\n21\n2021-05-18\nAsk a Manager Salary Survey\nAsk a Manager\nAsk a Manager\n\n\n22\n2021-05-25\nMario Kart World Records\nMario Kart World Records\nMario Kart Record-breaking\n\n\n23\n2021-06-01\nSurvivor TV Show\nsurvivoR Package\nDaniel’s Oehm’s Website\n\n\n24\n2021-06-08\nGreat Lakes Fish\nGreat Lakes Database\nDetroit Free Press\n\n\n25\n2021-06-15\nWEB Du Bois and Juneteenth\n#DuBoisChallenge tweets\nThe Intercept\n\n\n26\n2021-06-22\nPublic Park Access\nTPL\nCityLab\n\n\n27\n2021-06-29\nAnimal Rescues\nLondon.gov\nThe Guardian\n\n\n28\n2021-07-06\nInternational Independence Days\nWikipedia\nWorldAtlas.com\n\n\n29\n2021-07-13\nScooby Doo\nKaggle\nScoobyPedia\n\n\n30\n2021-07-20\nUS Droughts\nDrought Monitor\nNYTimes & CNN\n\n\n31\n2021-07-27\nOlympic Medals\nKaggle\nFinancial Times & FiveThirtyEight\n\n\n32\n2021-08-03\nParalympic Medals\nIPC\nIPC\n\n\n33\n2021-08-10\nBEA Infrastructure Investment\nBEA\nBEA\n\n\n34\n2021-08-17\nStar Trek Voice Commands\nSpeechInteraction.org\nSpeechInteraction.org\n\n\n35\n2021-08-24\nLemurs\nKaggle\nZehr et al, 2014 - Nature\n\n\n36\n2021-08-31\nBird Baths\nCleary et al, 2016\nThe Conversation & Cleary et al, 2016\n\n\n37\n2021-09-07\nFormula 1 Races\nergast.com/mrd/db\nFiveThirtyEight\n\n\n38\n2021-09-14\nBillboard Top 100\nData.World\nThePudding\n\n\n39\n2021-09-21\nEmmy Awards\nEmmys\nSusie Lu\n\n\n40\n2021-09-28\nNBER Papers\nNBER\nnberwp R package\n\n\n41\n2021-10-05\nRegistered Nurses\nData.World\nBLS\n\n\n42\n2021-10-12\nGlobal Seafood\nOurWorldinData.org\nOurWorldinData.org\n\n\n43\n2021-10-19\nBig Pumpkins\nBigPumpkins.com\nGreat Pumpkin Commonwealth\n\n\n44\n2021-10-26\nUltra Trail Running\nBjnNowak-Github Repo\nRunRepeat.com\n\n\n45\n2021-11-02\nMaking maps with R\nGeocomputation with R\nGeocomputation with R\n\n\n46\n2021-11-09\nLearning with afrilearndata\nafrilearndata\nafrilearndata\n\n\n47\n2021-11-16\n#BlackInDataWeek 2021\n#BlackInDataWeek\n#BlackInDataWeek\n\n\n48\n2021-11-23\nDr. Who\ndatardis Pkg\nR and Omics, datardis package\n\n\n49\n2021-11-30\nWorld Cup Cricket\nESPN Cricinfo\nWikipedia\n\n\n50\n2021-12-07\nSpiders\nWorld Spiders Database\nMajer et al, 2015\n\n\n51\n2021-12-14\nSpice Girls\nSpice Girls by Jacquie Tran\nSpice Girls by Jacquie Tran\n\n\n52\n2021-12-21\nStarbucks drinks\nStarbucks drinks\nBehance - Starbucks infographics",
    "crumbs": [
      "Datasets",
      "2021"
    ]
  },
  {
    "objectID": "data/2022/2022-01-11/readme.html",
    "href": "data/2022/2022-01-11/readme.html",
    "title": "Bee Colonies",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nBee Colonies\nThe data this week comes from the USDA, hat tip to Georgios Karamanis.\n\nThis report provides information on honey bee colonies in terms of number of colonies, maximum, lost, percent lost, added, renovated, and percent renovated, as well as colonies lost with Colony Collapse Disorder symptoms with both over and less than five colonies. The report also identifies colony health stressors with five or more colonies. The data for operations with honey bee colonies are collected from a stratified sample of operations that responded as having honey bees on the Bee and Honey Inquiry and from the NASS list frame. For operations with five or more colonies, data was collected on a quarterly basis; operations with less than five colonies were collected with an annual survey.\n\nMore details on bee colony losses at: - Bee Informed\n- Auburn University\n- The Guardian\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-01-11')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 2)\n\ncolony &lt;- tuesdata$colony\n\n# Or read in the data manually\n\ncolony &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-01-11/colony.csv')\nstressor &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-01-11/stressor.csv')\n\n\nData Dictionary\n\n\n\ncolony.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ncharacter\nyear\n\n\nmonths\ncharacter\nmonth\n\n\nstate\ncharacter\nState Name (note there is United States and Other States)\n\n\ncolony_n\ninteger\nNumber of colonies\n\n\ncolony_max\ninteger\nMaximum colonies\n\n\ncolony_lost\ninteger\nColonies lost\n\n\ncolony_lost_pct\ninteger\nPercent of total colonies lost\n\n\ncolony_added\ninteger\nColonies added\n\n\ncolony_reno\ninteger\nColonies renovated\n\n\ncolony_reno_pct\ninteger\nPercent of colonies renovated\n\n\n\n\n\nstressor.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ncharacter\nYear\n\n\nmonths\ncharacter\nMonth range\n\n\nstate\ncharacter\nState Name (note there is United States and Other States)\n\n\nstressor\ncharacter\nStress type\n\n\nstress_pct\ndouble\nPercent of colonies affected by stressors anytime during the quarter, colony can be affected by multiple stressors during same quarter.\n\n\n\n\nCleaning Script\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(fs)\n\n# download site\nurl &lt;- \"https://usda.library.cornell.edu/concern/publications/rn301137d?locale=en\"\n\n# raw html\nraw_html &lt;- read_html(url)\n\n# all the download urls for zip files\nall_urls &lt;- raw_html %&gt;% \n  html_elements(\"#content-wrapper &gt; div.row.p-content &gt; div.col-sm-7 &gt; table\") %&gt;% \n  html_elements(\"a\") %&gt;% \n  html_attr(\"href\") %&gt;% \n  str_subset(\".zip\") \n\n# download to correct directory\ndownload.file(all_urls, destfile = paste0(\"2022/2022-01-11/\", basename(all_urls)))\n\n# unzip into their respective folders\nwalk(basename(all_urls), ~ unzip(\n  paste0(\"2022/2022-01-11/\", .x),\n  exdir = paste0(\"2022/2022-01-11/bee-\", str_sub(.x, -6, -5))\n))\n\n\n#  Clean Colony data ------------------------------------------------------\n\n\nclean_bee_colonies &lt;- function(file){\n  \n  col_labs &lt;- c(\"state\", \"colony_n\", \"colony_max\", \"colony_lost\", \n    \"colony_lost_pct\", \"colony_added\", \"colony_reno\", \"colony_reno_pct\")\n  \n  raw_df &lt;- read_csv(file,  skip = 2, col_names = FALSE)\n  \n  date_rng &lt;- read_csv(file, skip = 0, col_names = FALSE) %&gt;% \n    slice(2) %&gt;% \n    pull(X3) %&gt;% \n    str_replace(\" [5-6]/$\", \"\") %&gt;% \n    word(-2, -1)\n  \n  clean_df &lt;- suppressWarnings(raw_df %&gt;% \n    filter(!is.na(X4))  %&gt;%\n    filter(X2 == \"d\") %&gt;% \n    select(X3:X10) %&gt;% \n    set_names(nm = col_labs) %&gt;% \n    mutate(date_range = date_rng, .before = state) %&gt;% \n    separate(date_range, into = c(\"months\", \"year\"), sep = \" \") %&gt;% \n    select(year, months, state, everything()) %&gt;% \n    mutate(across(contains(\"colony\"), as.integer)))\n  \n  clean_df\n}\n\n\n# Clean stress data -------------------------------------------------------\n\n\nclean_bee_stress &lt;- function(file){\n  \n  col_labs &lt;- c(\"state\", \"colony_n\", \"colony_max\", \"colony_lost\", \n    \"colony_lost_pct\", \"colony_added\", \"colony_reno\", \"colony_reno_pct\")\n\n  \n  raw_df &lt;- read_csv(file,  skip = 4, col_names = FALSE)\n  \n  date_rng &lt;- read_csv(file, skip = 0, col_names = FALSE) %&gt;% \n    slice(2) %&gt;% \n    pull(X3) %&gt;% \n    str_replace(\" [5-6]/$\", \"\") %&gt;% \n    word(-2, -1)\n  \n  stress_nm &lt;- c(\"state\", \"Varroa mites\", \"Other pests/parasites\", \"Disesases\",\n    \"Pesticides\", \"Other\", \"Unknown\")\n  \n  clean_df &lt;- suppressWarnings(raw_df %&gt;% \n      filter(!is.na(X4))  %&gt;%\n      filter(X2 == \"d\") %&gt;% \n      select(X3:X9) %&gt;% \n      set_names(nm = stress_nm) %&gt;% \n      pivot_longer(cols = -state, names_to = \"stressor\", values_to = \"stress_pct\") %&gt;% \n      mutate(date_range = date_rng, .before = state) %&gt;% \n      separate(date_range, into = c(\"months\", \"year\"), sep = \" \") %&gt;% \n      select(year, months, state, everything()) %&gt;% \n      mutate(stress_pct = as.double(stress_pct)))\n  \n  clean_df\n}\n\n\n# Get the file index ------------------------------------------------------\n\n\nall_html_index &lt;- dir_ls(\"data/2022/2022-01-11\", recurse = TRUE, glob = \"*htm\") %&gt;% \n  str_subset(\"bee-\") %&gt;% \n  str_subset(\"index\")\n\nget_index &lt;- function(index_file){\n  ind_yr &lt;- str_sub(index_file, -17, -16)\n  \n  raw_text &lt;- index_file %&gt;% \n    read_html() %&gt;%\n    html_text() %&gt;% \n    str_split(\"\\r\\n[0-9]+\") %&gt;% \n    unlist() %&gt;% \n    map(~str_split(.x, \"\\r\\n\")) \n  \n  raw_text %&gt;% \n    tibble(data = .) %&gt;% \n    unchop(data) %&gt;% \n    unnest_wider(data) %&gt;% \n    select(file = ...2, desc = ...3) %&gt;% \n    mutate(type = case_when(\n      str_detect(desc, \"Colonies, Maximum, Lost\") ~ \"Colony\",\n      str_detect(desc, \"Colony Health Stressors\") ~ \"Stressor\",\n      TRUE ~ NA_character_\n    )) %&gt;% \n    filter(!is.na(type)) %&gt;% \n    mutate(year = ind_yr, .before = file)\n  \n}\n\nall_index_files &lt;- map_dfr(all_html_index, get_index)\n\n# Split data by type\nsplit_files &lt;- all_index_files %&gt;% \n  mutate(dir_file = glue::glue(\"data/2022/2022-01-11/bee-{year}/{file}\")) %&gt;% \n  select(type, dir_file, year, file, desc) %&gt;% \n  group_split(type)\n\n# aggregate the clean data by type\n\nall_colonies &lt;-  map_dfr(split_files[[1]]$dir_file, clean_bee_colonies) %&gt;% \n  distinct(year, months, state, .keep_all = TRUE) %&gt;% \n  mutate(state = str_remove(state, \" 4/| 5/\")) %&gt;% \n  filter(state %in% c(state.name, \"Other States\", \"United States\"))\n\nall_stress &lt;-  map_dfr(split_files[[2]]$dir_file, clean_bee_stress) %&gt;% \n  distinct(year, months, state, stressor, .keep_all = TRUE) %&gt;% \n  mutate(state = str_remove(state, \" 4/| 5/\")) %&gt;% \n  filter(state %in% c(state.name, \"Other States\", \"United States\"))\n\n# check data\nall_colonies %&gt;% \n  count(year, months) %&gt;% \n  filter(n &gt; 47)\n\nall_stress %&gt;% \n  count(year, months) %&gt;% \n  filter(n &gt; 282)\n\nall_colonies %&gt;% \n  write_csv(\"data/2022/2022-01-11/colony.csv\")\n\nall_stress %&gt;% \n  write_csv(\"data/2022/2022-01-11/stressor.csv\")"
  },
  {
    "objectID": "data/2022/2022-01-25/readme.html",
    "href": "data/2022/2022-01-25/readme.html",
    "title": "Board Games",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nBoard Games\nThe data this week comes from Kaggle by way of Board Games Geek, with a hattip to David and Georgios.\nNote that the two datasets can be joined on the id column.\nThis data contains an overview of many games and their user ratings. There is even more data available on kaggle but it is too large for TidyTuesday (user text reviews data &gt; 1 Gb, around 15-19 million reviews!)\nNice overview of the data:\nIn Python and in R and another R.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-01-25')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 4)\n\nratings &lt;- tuesdata$ratings\n\n# Or read in the data manually\n\nratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-01-25/ratings.csv')\ndetails &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-01-25/details.csv')\n\n\nData Dictionary\n\n\n\nratings.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nnum\ndouble\nGame number\n\n\nid\ndouble\nGame ID\n\n\nname\ncharacter\nGame name\n\n\nyear\ndouble\nGame year\n\n\nrank\ndouble\nGame rank\n\n\naverage\ndouble\nAverage rating\n\n\nbayes_average\ndouble\nBayes average rating\n\n\nusers_rated\ndouble\nUsers rated\n\n\nurl\ncharacter\nGame url\n\n\nthumbnail\ncharacter\nGame thumbnail\n\n\n\n\n\ndetails.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nnum\ndouble\nGame number\n\n\nid\ndouble\nGame ID\n\n\nprimary\ncharacter\nPrimary name\n\n\ndescription\ncharacter\nDescription of game\n\n\nyearpublished\ndouble\nYear published\n\n\nminplayers\ndouble\nMin n of players\n\n\nmaxplayers\ndouble\nMax n of players\n\n\nplayingtime\ndouble\nPlaying time in minutes\n\n\nminplaytime\ndouble\nMin play time\n\n\nmaxplaytime\ndouble\nMax plat tome\n\n\nminage\ndouble\nminimum age\n\n\nboardgamecategory\ncharacter\nCategory\n\n\nboardgamemechanic\ncharacter\nMechanic\n\n\nboardgamefamily\ncharacter\nBoard game family\n\n\nboardgameexpansion\ncharacter\nExpansion\n\n\nboardgameimplementation\ncharacter\nImplementation\n\n\nboardgamedesigner\ncharacter\nDesigner\n\n\nboardgameartist\ncharacter\nArtist\n\n\nboardgamepublisher\ncharacter\nPublisher\n\n\nowned\ndouble\nNum owned\n\n\ntrading\ndouble\nNum trading\n\n\nwanting\ndouble\nNum wanting\n\n\nwishing\ndouble\nNum wishing\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-02-08/readme.html",
    "href": "data/2022/2022-02-08/readme.html",
    "title": "Tuskegee Airmen",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nTuskegee Airmen\nThe data this week comes from the Tuskegee Airmen Challenge as part of the Veterans Advocacy Tableau User Group with data sourced from the CAF (Commemorative Air Force) CAF. This dataset is released in honor of Black History Month and honors the sacrifices and challenges that these African American pilots faced in WWII and beyond.\nCredit to Ethan Lang and Timothy Blaisdell of the VA-TUG for preparing the challenge and Allen Hillery for collaborating on connecting us all and getting the challenge kicked off. Credit to Anthony Starks, Sekou Tyler, and Allen Hillery for kicking off the WEB DuBois challenge in past years and this year’s challenge for 2022. If you would like to participate in the dataset this week please use the #TuskegeeAirmenChallenge hashtag in addition to #TidyTuesday\nThe WEB DuBois challenge is live for 10 weeks, and you can use the #DuBoisChallenge2022 hashtag to participate in this challenge. Full details available in the DuBois data portraits repo. Anthony Starks has written an excellent article covering the details of last year’s challenge and this year’s new challenge in the Nightingale by DVS.\nLots of great detail on the Tuskegee Airmen Wikipedia page, and see the article by the Dr. Daniel Haulman of the Airforce.\n\nThe Tuskegee Airmen /tʌsˈkiːɡiː/[1] were a group of primarily African American military pilots (fighter and bomber) and airmen who fought in World War II. They formed the 332d Expeditionary Operations Group and the 477th Bombardment Group of the United States Army Air Forces.\n\n\nThe Tuskegee Airmen were the first African-American military aviators in the United States Armed Forces. During World War II, black Americans in many U.S. states were still subject to the Jim Crow laws and the American military was racially segregated, as was much of the federal government. The Tuskegee Airmen were subjected to discrimination, both within and outside of the army.\n\n\nBefore the Tuskegee Airmen, no African-American had been a U.S. military pilot. In 1917, African-American men had tried to become aerial observers but were rejected.[6] African-American Eugene Bullard served in the French air service during World War I because he was not allowed to serve in an American unit. Instead, Bullard returned to infantry duty with the French.[7]\nThe racially motivated rejections of World War I African-American recruits sparked more than two decades of advocacy by African-Americans who wished to enlist and train as military aviators.\nBecause of the restrictive nature of selection policies, the situation did not seem promising for African-Americans, since in 1940 the U.S. Census Bureau reported there were only 124 African-American pilots in the nation.[10] The exclusionary policies failed dramatically when the Air Corps received an abundance of applications from men who qualified, even under the restrictive requirements. Many of the applicants had already participated in the Civilian Pilot Training Program, unveiled in late December 1938 (CPTP). Tuskegee University had participated since 1939.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-02-08')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 6)\n\nairmen &lt;- tuesdata$airmen\n\n# Or read in the data manually\n\nairmen &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-02-08/airmen.csv')\n\n\nData Dictionary\n\n\n\nairmen.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nFull name\n\n\nlast_name\ncharacter\nlast name\n\n\nfirst_name\ncharacter\nFirst name\n\n\ngraduation_date\ndatetime\nGraduation date\n\n\nrank_at_graduation\ncharacter\nMilitary rank\n\n\nclass\ncharacter\nClass ID\n\n\ngraduated_from\ncharacter\nGraduated from\n\n\npilot_type\ncharacter\nPilot type\n\n\nmilitary_hometown_of_record\ncharacter\nHometown of record\n\n\nstate\ncharacter\nState of record\n\n\naerial_victory_credits\ncharacter\nAerial victory credit\n\n\nnumber_of_aerial_victory_credits\ndouble\nNumber of aerial victory credits\n\n\nreported_lost\ncharacter\nReported lost\n\n\nreported_lost_date\ndouble\nReported lost date\n\n\nreported_lost_location\ncharacter\nReported lost location\n\n\nweb_profile\ncharacter\nWeb profile\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-02-22/readme.html",
    "href": "data/2022/2022-02-22/readme.html",
    "title": "Freedom in the World",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nFreedom in the World\nThe data this week comes from Freedom House and the United Nations by way of Arthur Cheib.\nI want to note that a “Freedom Index” from any source have potential bias or miscalculations. While the index appears to cover many social issues including freedom of religion, expression, etc this data (like any data) can be approached with skepticism. Data such as this are far from perfect and may misrepresent nuanced political situations or oversimplify difficult to record/measure political nuance.\nThank you to a few users who suggested alternative resources or critiques.\nJeppe Viero\nDavid Ranzolin\n\nFreedom in the World, Freedom House’s flagship publication, is the standard-setting comparative assessment of global political rights and civil liberties. Published annually since 1972, the survey ratings and narrative reports on 195 countries and 15 related and disputed territories are used by policymakers, the media, international corporations, civic activists, and human rights defenders.\n\nFreedom House has written about their index - notably an article in 2018 on “Democracy in Crisis” and “10 years of Decline in Global Freedom””.\npic1.png. A line chart of the trajectory of USA’s freedom in the world aggregate score on the y-axis and year on the x-axis. The title of the chart is ‘Trajectory of the United States, The past year brought further faster erosion of America’s own democratic standards, damaging its credibility as a champion of good governance and human rights.’ The x-axis in years goes from 2008 to 2017. The y-axis or the Freedom in the World index for the USA started at 94 in 2008 until 2010 and then dropped gradually from 94 to 92 in 2014. The Freedom index was 90 in 2015, 89 in 2016 and 86 in 2017.\npic2.png. The chart is titled ’ A decade of decline, Countries with net declines in aggregate score have outnumbered those with gains for the past 10 years.’ The area chart displays year on the x-axis from 2006 to 2015 and the number of countries improving or declining in their Freedom aggregate score from 30 to 80 on the y-axis. Declined countries were 59, 59, 60, 67, 49, 54, 61, 54, 58, 72 from 2006 to 2015. Improved countries were 56, 43, 38, 34, 34, 37, 42, 40, 32, 43 from 2006 to 2015. The chart represents a greater number of countries with declining freedom in every year versus improving freedom.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-02-22')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 8)\n\nfreedom &lt;- tuesdata$freedom\n\n# Or read in the data manually\n\nfreedom &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-02-22/freedom.csv')\n\n\nData Dictionary\n\n\n\nfreedom.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry Name\n\n\nyear\ndouble\nYear\n\n\nCL\ndouble\nCivil Liberties\n\n\nPR\ndouble\nPolitical rights\n\n\nStatus\ncharacter\nStatus (Free F, Not Free NF, Partially Free PF)\n\n\nRegion_Code\ndouble\nUN Region code\n\n\nRegion_Name\ncharacter\nUN Region Name\n\n\nis_ldc\ndouble\nIs a least developed country (binary 0/1)\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-03-08/readme.html",
    "href": "data/2022/2022-03-08/readme.html",
    "title": "EU Student mobility",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nEU Student mobility\nThe data this week comes from Data.Europa hattip to Data is Plural.\nWimdu wrote a short blog post on the most popular ERASMUS destinations.\nThe ERASMUS program: EU programme for education, training, youth and sport\n\nErasmus students are those that take advantage of the Erasmus exchange program, a well supported and organised scheme that has been in operation since the late 1980’s. It allows for students to study at universities in the EU member states for set periods of time. Erasmus students study a wide variety of subjects but most use the program for advancing their language skills with a view to working in the international sphere, and it is advised that anyone interested seeks information on the Erasmus scheme online.\nThe European Credit Transfer System means that academic credits you earn in your course while abroad will count towards your qualification.\n\n\nSimilar mobility periods are aggregated where possible (same sending/receiving organisation, same status regrading fewer oppts, gender, age, …) in order to reduce file size. Mobility periods started in 2020 and 2021 will be published once they are finalised.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-03-08')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 10)\n\nerasmus &lt;- tuesdata$erasmus\n\n# Or read in the data manually\n\nerasmus &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-03-08/erasmus.csv')\n\n\nData Dictionary\n\n\n\nerasmus.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nproject_reference\ncharacter\nProject reference is an aggregation of several information (YYYY-X-AAAA-KKKKK-NNNNN) where YYYY represent year, X represents the round within the call year, AAAA represents the National Agency managing the project, KKKKK is the key action code and NNNNNN is an auto generated number\n\n\nacademic_year\ncharacter\nOnly relevant for higher education (KA103, KA107) - Year-Month (YYYY-MM)\n\n\nmobility_start_month\ncharacter\nYear-Month (YYYY-MM)\n\n\nmobility_end_month\ncharacter\nYear-Month (YYYY-MM)\n\n\nmobility_duration\ndouble\nExact duration of the mobility in calendar days (date2-date1)\n\n\nactivity_mob\ncharacter\n.\n\n\nfield_of_education\ncharacter\nParticipant field of education\n\n\nparticipant_nationality\ncharacter\nCode (DE, FR, BE, …..)\n\n\neducation_level\ncharacter\nIncluded where relevant\n\n\nparticipant_gender\ncharacter\nMale/Female/Undefined\n\n\nparticipant_profile\ncharacter\nStaff or learner, training can be retrieved from activity field\n\n\nspecial_needs\ncharacter\nYes/no\n\n\nfewer_opportunities\ncharacter\nYes/no\n\n\ngroup_leader\ncharacter\nYes/no\n\n\nparticipant_age\ndouble\nAge at start of mobility in years\n\n\nsending_country_code\ncharacter\nCode (DE, FR, BE, …..)\n\n\nsending_city\ncharacter\nCity of sending organisation\n\n\nsending_organization\ncharacter\nName of organisation\n\n\nsending_organisation_erasmus_code\ncharacter\nOrganisation Erasmus code\n\n\nreceiving_country_code\ncharacter\nCode (DE, FR, BE, …..)\n\n\nreceiving_city\ncharacter\nCity of receiving organisationn\n\n\nreceiving_organization\ncharacter\nName of organisation\n\n\nreceiving_organisation_erasmus_code\ncharacter\nOrganisation Erasmus code\n\n\nparticipants\ndouble\nTotal number of participants\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-03-22/readme.html",
    "href": "data/2022/2022-03-22/readme.html",
    "title": "Baby Names",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nBaby Names\nThe data this week comes from babynames R package from Hadley Wickham. Note that other datasets exist, like the nzbabynames package from Emily Kothe.\nNote there are datasets for USA and New Zealand, with lifetables and names for each. You can also install the packages for the full datasets themselves.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-03-22')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 12)\n\nbabynames &lt;- tuesdata$babynames\n\n# Or read in the data manually\n\nbabynames &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-03-22/babynames.csv')\n\n\nData Dictionary\n\n\n\nbabynames.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear of birth\n\n\nsex\ncharacter\nBinary sex of the baby\n\n\nname\ncharacter\nName of the baby\n\n\nn\ninteger\nRaw count\n\n\nprop\ndouble\nProportion of total births for that year\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-04-05/readme.html",
    "href": "data/2022/2022-04-05/readme.html",
    "title": "Publications List",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nPublications List\nThe data this week comes from Project Oasis by way of Data is Plural.\nRead the full report\n\nYou can browse a comprehensive list of digitally focused, local news organizations below. Filter your view with the options to the right to narrow the scope of organizations that appear. Clicking on an organization will take you to its profile page, which includes more details about the publisher. All information in the database was self-reported by the organizations or is publicly available.\n\n\nProject Oasis is a collaboration between UNC Hussman School of Journalism and Media, LION Publishers, Douglas K. Smith and the Google News Initiative to map the progress and choices of locally focused digital news publishers, and share the most relevant insights with you. See below for FAQs about the project and the criteria used in the research.\n\nMore articles by NiemanLab\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-04-05')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 14)\n\nnews_orgs &lt;- tuesdata$news_orgs\n\n# Or read in the data manually\n\nnews_orgs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-04-05/news_orgs.csv')\n\n\nData Dictionary\n\n\n\nnews_orgs.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npublication_name\ncharacter\nName\n\n\nparent_publication\ncharacter\nParent publication\n\n\nurl\ncharacter\nWebsite\n\n\nowner\ncharacter\nOwner name\n\n\nis_owner_founder\ncharacter\nIs the owner the founder?\n\n\ncity\ncharacter\nCity\n\n\nstate\ncharacter\nState\n\n\ncountry\ncharacter\nCountry\n\n\nprimary_language\ncharacter\nLanguage\n\n\nprimary_language_other\nlogical\nOther lang\n\n\ntax_status_founded\ncharacter\nTax status when founded\n\n\ntax_status_current\ncharacter\nTax status current\n\n\nyear_founded\ndouble\nYear founded\n\n\ntotal_employees\ncharacter\nTotal N of employees\n\n\nbudget_percent_editorial\ncharacter\nBudget % editorial\n\n\nbudget_percent_revenue_generation\ncharacter\nBudget % revenue generation\n\n\nbudget_percent_product_technology\ncharacter\nBudget percent product tech\n\n\nbudget_percent_administration\ncharacter\nBudget percent admin\n\n\nproducts\ncharacter\nProducts\n\n\nproducts_other\ncharacter\nProducts other\n\n\ndistribution\ncharacter\nDistribution method\n\n\ndistribution_method_other\ncharacter\nDistribution method other\n\n\ngeographic_area\ncharacter\nGeographic area\n\n\ncore_editorial_strategy_characteristics\ncharacter\nCore editorial strategy\n\n\ncore_editorial_strategy_characteristics_other\ncharacter\nCore editorial strategy other\n\n\ncoverage_topics\ncharacter\nCoverage topics\n\n\ncoverage_topics_other\nlogical\nCoverage topics other\n\n\nunderrepresented_communities\ncharacter\nUnderrepresented community types\n\n\nunderrepresented_communities_not_listed\ncharacter\nOther community\n\n\nrevenue_streams\ncharacter\nRevenue stream\n\n\nrevenue_stream_other\nlogical\nRevenue stream other\n\n\nrevenue_stream_additional_info\nlogical\nRevenue stream additional\n\n\nrevenue_stream_largest\ncharacter\nLargest revenue stream\n\n\nrevenue_streams_largest_other\ncharacter\nLargest revenue stream other\n\n\npaywall_or_gateway\ncharacter\nPaywall or gateway?\n\n\npaywall_or_gateway_other\nlogical\nPaywall or gateway other\n\n\nadvertising_products\ncharacter\nAdvertising products\n\n\nadvertising_product_other\nlogical\nADvertising products other\n\n\nreal_world_impacts\ncharacter\nReal world impacts\n\n\nsummary\ncharacter\nSummary\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-04-19/readme.html",
    "href": "data/2022/2022-04-19/readme.html",
    "title": "Crossword Puzzles and Clues",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nCrossword Puzzles and Clues\nData this week comes from Cryptic Crossword Clues. A short TDS article on Crosswords\n\ncryptics.georgeho.org is a dataset of cryptic crossword1 clues, indicators and charades, collected from various blogs and digital archives.\n\n\nThis dataset is a significant work of crossword archivism and scholarship, as acquiring historical crosswords and structuring their contents require focused effort and tedious cleaning that few are willing to do for such trivial data - for example, according to this selection guide2, the Library of Congress explicitly does not collect crossword puzzles.\n\n\nThis project indexes various blogs and digital archives for cryptic crosswords. Several fields - such as clues, answers, clue numbers, annotations or commentary, puzzle title and publication date - are parsed and extracted into a tabular dataset. The result is over half a million clues from cryptic crosswords over the past twelve years.\n\n\nTwo other datasets are subsequently derived from the clues - wordplay indicators and charades (a.k.a. substitutions). All told, the derived datasets contain over twelve thousand wordplay indicators and over sixty thousand charades.\n\n\nGet the data here\nhttps://www.sciencebase.gov/catalog/item/60ba5a00d34e86b9388d86bc\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-04-19')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 16)\n\nbig_dave &lt;- tuesdata$big_dave\n\n# Or read in the data manually\n\nbig_dave &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-04-19/big_dave.csv')\ntimes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-04-19/times.csv')\n\n\nData Dictionary\n\n\n\nbig_dave.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrowid\ndouble\nrow ID\n\n\nclue\ncharacter\nClue\n\n\nanswer\ncharacter\nAnswer\n\n\ndefinition\ncharacter\nDefinition\n\n\nclue_number\ncharacter\nClue Number\n\n\npuzzle_date\ndouble\nPuzzle date\n\n\npuzzle_name\ncharacter\nPuzzle name\n\n\nsource_url\ncharacter\nSource URL\n\n\nsource\ncharacter\nSource\n\n\n\n\nCleaning Script\nraw_df &lt;- read_csv(\"https://cryptics.georgeho.org/data/clues.csv?_stream=on&source=bigdave44&_size=max\")\n\nraw_df %&gt;% \n  write_csv(\"2022/2022-04-19/big_dave.csv\")\n\ntimes &lt;- read_csv(\"https://cryptics.georgeho.org/data/clues.csv?_stream=on&source=times_xwd_times&_size=max\")\n\ntimes %&gt;% \n  write_csv(\"2022/2022-04-19/times.csv\")"
  },
  {
    "objectID": "data/2022/2022-05-03/readme.html",
    "href": "data/2022/2022-05-03/readme.html",
    "title": "US Solar/Wind",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nUS Solar/Wind\nThe data this week comes from the Berkeley Lab. See the technical brief on the emp.lbl.gov site.\nhatttip to Data is Plural\n\nBerkeley Lab’s “Utility-Scale Solar, 2021 Edition” presents analysis of empirical plant-level data from the U.S. fleet of ground-mounted photovoltaic (PV), PV+battery, and concentrating solar-thermal power (CSP) plants with capacities exceeding 5 MWAC. While focused on key developments in 2020, this report explores trends in deployment, technology, capital and operating costs, capacity factors, the levelized cost of solar energy (LCOE), power purchase agreement (PPA) prices, and wholesale market value.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-05-03')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 18)\n\ncapacity &lt;- tuesdata$capacity\n\n# Or read in the data manually\n\ncapacity &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-03/capacity.csv')\nwind &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-03/wind.csv')\nsolar &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-03/solar.csv')\naverage_cost &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-03/average_cost.csv')\n\n\nData Dictionary\n\n\n\ncapacity.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntype\ncharacter\nType of power (solar, nuclear, wind, etc)\n\n\nyear\ndouble\nYear\n\n\nstandalone_prior\ndouble\nStandalone prior gigawatts\n\n\nhybrid_prior\ndouble\nHybrid prior gigagwatts\n\n\nstandalone_new\ndouble\nStandalone new gigawatts\n\n\nhybrid_new\ndouble\nHybrid new gigawatts\n\n\ntotal_gw\ndouble\nTotal gigawatts\n\n\n\n\n\naverage_cost.csv\nAverage cost for each type of power in dollars/MWh\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\ngas_mwh\ndouble\nAverage Gas sourced dollars/MWh\n\n\nsolar_mwh\ndouble\naverage Solar sourced dollars/MWh\n\n\nwind_mwh\ndouble\nAverage Wind sourced dollars MWh\n\n\n\n\n\nwind.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndouble\nISO date\n\n\nwind_mwh\ndouble\nWind projected price in $/MWh\n\n\nwind_capacity\ndouble\nWind projected capacity in Gigawatts\n\n\n\n\n\nsolar.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndouble\nISO date\n\n\nsolar_mwh\ndouble\nsolar projected price in $/MWh\n\n\nsolar_capacity\ndouble\nSolar projected capacity in Gigawatts\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(readxl)\n\nutil_df &lt;- read_excel(\n  \"2022/2022-05-03/2021_utility-scale_solar_data_update_0.xlsm\", sheet = \"PV & Wind PPAs vs. Gas\",\n  skip = 26)\n\nyr_avg &lt;- util_df |&gt; \n  select(1:4) |&gt; \n  set_names(nm = c(\"year\", \"gas_mwh\", \"solar_mwh\", \"wind_mwh\")) |&gt; \n  filter(!is.na(year))\n\nyr_avg |&gt; \n  write_csv(\"2022/2022-05-03/average_cost.csv\")\n\nwind_df &lt;- util_df |&gt; \n  select(6:8) |&gt; \n  set_names(nm = c(\"date\", \"wind_mwh\", \"wind_capacity\")) |&gt; \n  filter(!is.na(date)) |&gt; \n  mutate(date = as.Date(date))\n\nwind_df |&gt; \n  write_csv(\"2022/2022-05-03/wind.csv\")\n\nsolar_df &lt;- util_df |&gt; \n  select(10:12) |&gt; \n  set_names(nm = c(\"date\", \"solar_mwh\", \"solar_capacity\")) |&gt; \n  filter(!is.na(date)) |&gt; \n  mutate(date = as.Date(date))\n\nsolar_df |&gt; \n  write_csv(\"2022/2022-05-03/solar.csv\")\n\ngen_df &lt;- read_excel(\n  \"2022/2022-05-03/2021_utility-scale_solar_data_update_0.xlsm\", sheet = \"All Capacity in Queues\",\n  skip = 25)\n\ngen_capacity &lt;- gen_df |&gt; \n  select(1, 3:8) |&gt; \n  set_names(nm = c(\"type\", \"year\", \"standalone_prior\", \"hybrid_prior\", \"standalone_new\", \"hybrid_new\", \"total_gw\")) |&gt; \n  filter(!is.na(year)) |&gt; \n  fill(type)\n\ngen_capacity |&gt; \n  write_csv(\"2022/2022-05-03/capacity.csv\")"
  },
  {
    "objectID": "data/2022/2022-05-17/readme.html",
    "href": "data/2022/2022-05-17/readme.html",
    "title": "Eurovision Contest   :microphone:",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nEurovision\nThe data this week comes from Eurovision. Hattip to Tanya Shapiro and Bob Rudis for sharing some methods to cleaning/scraping this data.\nThe country to country voting data comes from Data.World\nCredit to Tanya Shapiro for the below readme content.\n\n\nThe following project uses data from the Eurovision Song Contest site. Data was scraped using R and packages such as rvest and jsonlite. The resulting dataset represents information for all contestants for each year and by round (e.g. semi-final, final). It is important to note that changes in scoring system have occured through different points in time, e.g. semi-final rounds were not introduced until 2005.\n\n\nAbout\n\n\nExcerpt taken from Wikipedia:\n\n\nThe Eurovision Song Contest (French: Concours Eurovision de la chanson), sometimes abbreviated to ESC and often known simply as Eurovision, is an international songwriting competition organised annually by the European Broadcasting Union (EBU), featuring participants representing primarily European countries. Each participating country submits an original song to be performed on live television and radio, transmitted to national broadcasters via the EBU’s Eurovision and Euroradio networks, with competing countries then casting votes for the other countries’ songs to determine a winner.\n\n\nBased on the Sanremo Music Festival held in Italy since 1951, Eurovision has been held annually since 1956 (apart from 2020), making it the longest-running annual international televised music competition and one of the world’s longest-running television programmes. Active members of the EBU, as well as invited associate members, are eligible to compete, and as of 2022, 52 countries have participated at least once. Each participating broadcaster sends one original song of three minutes duration or less to be performed live by a singer or group of up to six people aged 16 or older. Each country awards two sets of 1–8, 10 and 12 points to their favourite songs, based on the views of an assembled group of music professionals and the country’s viewing public, with the song receiving the most points declared the winner. Other performances feature alongside the competition, including a specially-commissioned opening and interval act and guest performances by musicians and other personalities, with past acts including Cirque du Soleil, Madonna and the first performance of Riverdance. Originally consisting of a single evening event, the contest has expanded as new countries joined (including countries outside of Europe, such as Australia), leading to the introduction of relegation procedures in the 1990s, and eventually the creation of semi-finals in the 2000s. As of 2022, Germany has competed more times than any other country, having participated in all but one edition, while Ireland holds the record for the most victories, with seven wins in total.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-05-17')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 20)\n\neurovision &lt;- tuesdata$eurovision\n\n# Or read in the data manually\n\neurovision &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-17/eurovision.csv')\n\n\nData Dictionary\n\n\n\neurovision.csv\n\n\n\n\n\n\n\n\nvariable\nclass\nDescription\n\n\n\n\nevent\ncharacter\nEvent Name, e.g. Helsinki 2007\n\n\nhost_city\ncharacter\nHost city name, e.g. Helsinki\n\n\nyear\ninteger\nEvent year, e.g. 2007\n\n\nhost_country\ncharacter\nHost city country, e.g. Finland\n\n\nevent_url\ncharacter\nLink to event\n\n\nsection\ncharacter\nFinal, semi-final, first semi final, second-semi-final\n\n\nartist\ncharacter\nName of performer/participant\n\n\nsong\ncharacter\nSong title name\n\n\nartist_url\ncharacter\nLink to participant bio\n\n\nimage_url\ncharacter\nLink to participant image\n\n\nartist_country\ncharacter\nParticipant country, e.g. Austria\n\n\ncountry_emoji\ncharacter\nEmoji code for Country\n\n\nrunning_order\ninteger\nRunning order for the teams\n\n\ntotal_points\ninteger\nPoints\n\n\nrank\ninteger\nNumeric rank, e.g. 2\n\n\nrank_ordinal\ncharacter\nOrdinal rank, e.g. 2nd\n\n\nqualified\nlogical\nIs the team qualified\n\n\nwinner\nlogical\nWas this team the grand champion\n\n\n\n\neurovision-votes.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear\n\n\nsemi_final\ncharacter\nSemi-final or final\n\n\nedition\ncharacter\nWhich edition\n\n\njury_or_televoting\ncharacter\nJury or televoting\n\n\nfrom_country\ncharacter\nCountry that voted\n\n\nto_country\ncharacter\nCountry receiving votes\n\n\npoints\ndouble\nPoints\n\n\nduplicate\ncharacter\nDuplicate\n\n\n\n\n\nCleaning Script\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(countrycode)\nlibrary(stringi)\nlibrary(V8)\n\n# credit to Tanya Shapiro that was adapted for the URLs for each year\n# Web Scraping Events by Year ----\nurl_events &lt;- \"https://eurovision.tv/events\"\n\nevents &lt;- url_events %&gt;%\n  read_html() %&gt;%\n  html_elements(\"div.relative\") %&gt;%\n  html_elements(\"h4\") %&gt;%\n  html_text() %&gt;%\n  str_squish()\n\nevent_links &lt;- url_events %&gt;%\n  read_html() %&gt;%\n  html_elements(\"div.w-full\") %&gt;%\n  html_elements(\"a.absolute\") %&gt;%\n  html_attr(\"href\")\n\ncountries &lt;- url_events %&gt;%\n  read_html() %&gt;%\n  html_elements(\"div.relative\") %&gt;%\n  html_elements(\"span.px-4\") %&gt;%\n  html_text() %&gt;%\n  str_squish()\n\ncountries &lt;- countries[-3]\n\nby_year &lt;- tibble(\n  event = events,\n  host_city = events,\n  host_country = countries,\n  url = event_links\n) %&gt;%\n  separate(host_city, c(\"host_city\", \"year\"), sep = \" (?=[0-9])\") %&gt;%\n  mutate(year = as.integer(year))\n\nby_year\n\n\n\n# Get Eurovision ----------------------------------------------------------\n# credit to bob rudis for initial script that was adapted\nget_euro &lt;- function(url, section) {\n  cli::cli_alert_info(\"{url} at {section}\\n\")\n\n  ctx &lt;- v8()\n\n  pg &lt;- read_html(paste0(url, \"/\", section))\n\n  html_nodes(pg, xpath = \".//table[contains(@x-data, 'JSON')]\") |&gt;\n    html_attr(\"x-data\") |&gt;\n    stri_replace_first_regex(\"^table\\\\(\", \"var tbl = \") |&gt;\n    stri_replace_last_regex(\"\\\\)$\", \"\") |&gt;\n    ctx$eval()\n\n  ctx$get(\"tbl\") |&gt;\n    tibble::as_tibble()\n}\n\nget_euro(\"https://eurovision.tv/event/rotterdam-2021\", \"second-semi-final\")\n# https://eurovision.tv/event/rotterdam-2021/participants\n# https://eurovision.tv/event/rotterdam-2021/first-semi-final\n# https://eurovision.tv/event/rotterdam-2021/second-semi-final\n# https://eurovision.tv/event/rotterdam-2021/grand-final\n\n\nall_scores &lt;- by_year |&gt;\n  mutate(section = case_when(\n    year &gt;= 2008 ~ list(c(\"first-semi-final\", \"second-semi-final\", \"grand-final\")),\n    between(year, 2004, 2007) ~ list(c(\"semi-final\", \"grand-final\")),\n    TRUE ~ list(\"final\")\n  )) |&gt;\n  unnest_longer(section) |&gt;\n  mutate(data = map2(url, section, ~ get_euro(.x, .y)))\n\nall_scores_old &lt;- all_scores\n\nall_scores &lt;- all_scores |&gt;\n  bind_rows(score_below_2000)\n\nall_part &lt;- all_scores |&gt;\n  rename(event_url = url) |&gt;\n  unchop(data) |&gt;\n  unnest_wider(data) |&gt;\n  unpack(participant) |&gt;\n  unpack(country)\n\nclean_part &lt;- all_part |&gt;\n  rename(\n    artist = name,\n    song = performance, artist_url = url, \n    artist_country = nickname, country_emoji = emoji\n  )\n\n\nclean_part |&gt;\n  count(year) |&gt;\n  arrange(desc(year)) |&gt;\n  ggplot(aes(x = year, y = n, group = 1)) +\n  geom_line()\n\nclean_part |&gt;\n  write_csv(\"2022/2022-05-17/eurovision.csv\")"
  },
  {
    "objectID": "data/2022/2022-05-31/readme.html",
    "href": "data/2022/2022-05-31/readme.html",
    "title": "2022 Axios-Harris Poll",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\n2022 Axios-Harris Poll\nThe data this week comes from Axios and Harris Poll. Also see The Harris Poll overview for more details.\nNo poll is perfect, but methodology is included below, and see the links for Axios and Harris Poll or The Harris Poll overview for more details.\n\nThis survey is the result of a partnership between Axios and Harris Poll to gauge the reputation of the most visible brands in America, based on 20 years of Harris Poll research. From Trader Joe’s to Disney, here’s how this year’s class stacks up.\nMethodology: The Axios Harris Poll 100 is based on a survey of 33,096 Americans in a nationally representative sample conducted March 11-April 3, 2022. The two-step process starts fresh each year by surveying the public’s top-of-mind awareness of companies that either excel or falter in society.\nThese 100 “most visible companies” are then ranked by a second group of Americans across the seven key dimensions of reputation to arrive at the ranking. If a company is not on the list, it did not reach a critical level of visibility to be measured.\n\n\nPhase 1\nIn February 2022, The Harris Poll fielded three rounds of “nominations” among a nationally representative sample of Americans to determine which companies were top-of-mind for the public. Respondents were asked which two companies they think have the best reputation and which two have the worst reputation. Both sets of nominations were then combined into a single list, with subsidiaries and brands tallied added to their parent organizations. The 100 companies with the most nominations were then selected to be on the ‘Most Visible’ list.\n\n\nPhase 2\nThe “ratings” phase of the survey was conducted among 33,096 online interviews from March 11th to April 2nd, 2022 among a nationally representative sample of U.S. adults. Respondents were randomly assigned two companies to rate for which they answered they were very or somewhat familiar with. Each company received approximately 325 ratings. An RQ score is calculated by: [ (Sum of ratings of each of the 9 attributes)/(the total number of attributes answered x 7) ] x 100. Score ranges: 80 & above: Excellent | 75-79: Very Good | 70-74: Good | 65-69: Fair | 55-64: Poor | 50-54: Very Poor | Below 50: Critical\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-05-31')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 22)\n\npoll &lt;- tuesdata$poll\n\n# Or read in the data manually\n\npoll &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-31/poll.csv')\nreputation &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-05-31/reputation.csv')\n\n\nData Dictionary\n\n\n\npoll.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncompany\ncharacter\nCompany Name\n\n\nindustry\ncharacter\nIndustry group\n\n\n2022_rank\ninteger\n2022 Rank (1 is better than 100)\n\n\n2022_rq\ndouble\n2022 RQ score. An RQ score is calculated by: [ (Sum of ratings of each of the 9 attributes)/(the total number of attributes answered x 7) ] x 100. Score ranges: 80 & above: Excellent; 75-79: Very Good ; 70-74: Good ; 65-69: Fair ; 55-64: Poor ; 50-54: Very Poor ; Below 50: Critical\n\n\nchange\ninteger\nChange in rank from 2021\n\n\nyear\ninteger\nYear for that rank/RQ\n\n\nrank\ninteger\nRank corresponding to the year\n\n\nrq\ndouble\nRQ score corresponding to the year\n\n\n\n\n\nreputation.csv\n\nAll ranks in this for current year only\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncompany\ncharacter\nCompany Name\n\n\nindustry\ncharacter\nIndustry group\n\n\nname\ncharacter\nName of reputation category (P&S = Product and Service)\n\n\nscore\ndouble\nScore for reputation category\n\n\nrank\ninteger\nRank for reputation category\n\n\n\n\nCleaning Script\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(gt)\nlibrary(gtExtras)\n\ntab_url &lt;- \"https://graphics.axios.com/2022-05-16-harris-poll/index.html?initialWidth=469&childId=av-2022-05-16-harris-poll-69AC2&parentTitle=The%202022%20Axios%20Harris%20Poll%20100%20reputation%20rankings&parentUrl=https%3A%2F%2Fwww.axios.com%2F2022%2F05%2F24%2F2022-axios-harris-poll-100-rankings\"\ntab_js &lt;- \"https://graphics.axios.com/2022-05-16-harris-poll/js/app.a8dd96951f9ea55e4346.min.js?a8dd96951f9ea55e4346\"\n\nraw_txt &lt;- readLines(tab_js)\n\nraw_json &lt;- raw_txt |&gt; \n  paste0(collapse = \"\") |&gt; \n  gsub(\n    x = _, \n    pattern = \".*o\\\\(M,n\\\\)\\\\}\\\\},function\\\\(n\\\\)\\\\{n\\\\.exports=JSON\\\\.parse\\\\('\",\n    replacement = \"\"\n  ) |&gt; \n  gsub(\n    x = _,\n    pattern = \"'\\\\)\\\\},function\\\\(n,t,r\\\\)\\\\{n\\\\.exports.*\",\n    replacement = \"\"\n  ) |&gt; \n  str_remove_all(\"\\\\\\\\\")\n\n\njson_out &lt;- jsonlite::fromJSON(raw_json, simplifyVector = FALSE)\n\nraw_df &lt;-json_out |&gt; \n  tibble(data = _) |&gt; \n  unnest_wider(data) \n\nglimpse(raw_df)\n\nraw_df |&gt; \n  write_rds(\"axios-harris-poll.rds\")\n\njs_df &lt;- raw_df |&gt; \n  unnest_longer(history) |&gt; \n  unnest_wider(history) |&gt; \n  select(-dimensions)\n\njs_df\n\njs_df |&gt; write_csv(\"2022/2022-05-31/axios.csv\")\n\njs_df |&gt; \n  filter(!is.na(rank) | year == 2021)\n\naxios_vars &lt;- raw_df |&gt; \n  select(-history) |&gt; \n  unnest_wider(dimensions) |&gt; \n  pivot_longer(names_to = \"name\", values_to = \"vals\", cols = TRUST:CULTURE) |&gt; \n  unnest_wider(vals)\n\naxios_vars |&gt; \n  select(company, industry, name, score, rank) |&gt;   write_csv(\"2022/2022-05-31/reputation.csv\")\n\nraw_df |&gt; \n  select(-history, -dimensions) |&gt; \n  rename(change_icon = change) |&gt; \n  head(20) |&gt; \n  gt::gt() |&gt; \n  gtExtras::gt_fa_rank_change(change_icon, font_color = \"match\") |&gt; \n  gt::cols_label(change_icon = \"\")"
  },
  {
    "objectID": "data/2022/2022-06-14/readme.html",
    "href": "data/2022/2022-06-14/readme.html",
    "title": "Drought Conditions in the US",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nDrought Conditions in the US\nThe data this week comes from the National Integrated Drought Information System.\nThis web page provides more information about the drought conditions data.\n\nThe Standardized Precipitation Index (SPI) is an index to characterize meteorological drought on a range of timescales, ranging from 1 to 72 months, for the lower 48 U.S. states. The SPI is the number of standard deviations that observed cumulative precipitation deviates from the climatological average. NOAA’s National Centers for Environmental Information produce the 9-month SPI values below on a monthly basis, going back to 1895.*\n\nCredit: Spencer Schien\nAdditional data from the Drought Monitor with API access from: https://droughtmonitor.unl.edu/DmData/DataDownload/WebServiceInfo.aspx#comp\n\nThe Drought Severity and Coverage Index is an experimental method for converting drought levels from the U.S. Drought Monitor map to a single value for an area. DSCI values are part of the U.S. Drought Monitor data tables. Possible values of the DSCI are from 0 to 500. Zero means that none of the area is abnormally dry or in drought, and 500 means that all of the area is in D4, exceptional drought.\n\nThis dataset was covered by the NY Times and CNN.\nThe dataset for today ranges from 2001 to 2021, but again more data is available at the Drought Monitor.\nDrought classification can be found on the US Drought Monitor site.\nPlease reference the data as seen below:\n\nThe U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the University of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and Atmospheric Administration. Map courtesy of NDMC.\n\nSome maps and other interesting summaries can be found on the Drought Monitor site and their Map Collection.\nSome limitations of the data expanded on the Drought Monitor site.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-06-14')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 24)\n\ndrought &lt;- tuesdata$drought\n\n# Or read in the data manually\n\ndrought &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-14/drought.csv')\ndrought_fips &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-14/drought-fips.csv')\n\n\nData Dictionary\n\n\n\ndrought.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\n0\ndouble\n\n\n\nDATE\ncharacter\nDate\n\n\nD0\ndouble\nAbnormally dry\n\n\nD1\ndouble\nModerate drought\n\n\nD2\ndouble\nSevere drought\n\n\nD3\ndouble\nExtreme drought\n\n\nD4\ndouble\nExceptional drought\n\n\n-9\ndouble\n\n\n\nW0\ndouble\nAbnormally wet\n\n\nW1\ndouble\nModerate wet\n\n\nW2\ndouble\nSevere wet\n\n\nW3\ndouble\nExtreme wet\n\n\nW4\ndouble\nExceptional wet\n\n\nstate\ncharacter\nState\n\n\n\n\n\ndrought-fips.csv\nFIPS can be processed via tidycensus or tigris R packages. Note that the FIPS code needs to be split for processing.\nSee: https://walker-data.com/tidycensus/reference/fips_codes.html\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nState\ncharacter\nState name\n\n\nFIPS\ncharacter\nFIPS id (first two digits = state, last 3 digits = county)\n\n\nDSCI\ndouble\nDrought Score (0 to 500) Zero means that none of the area is abnormally dry or in drought, and 500 means that all of the area is in D4, exceptional drought.\n\n\ndate\ndouble\ndate in ISO\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-06-28/readme.html",
    "href": "data/2022/2022-06-28/readme.html",
    "title": "UK Paygap",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization:\nChart type\nIt’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph\nType of data\nWhat data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year\nReason for including the chart\nThink about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales\nLink to data or source\nDon’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nUK Paygap\nThe data this week comes from gender-pay-gap.service.gov.uk. The online tool reports by gender and occupation. The online quiz lets you test your knowledge/guesses.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-06-28')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 26)\n\npaygap &lt;- tuesdata$paygap\n\n# Or read in the data manually\n\npaygap &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-06-28/paygap.csv')\n\n\nData Dictionary\n\n\n\npaygap.csv\n\n\n\n\n\n\n\n\nField\nDescription\nSource\n\n\n\n\nEmployerName\nThe name of the employer at the time of reporting\nVia CoHo API or manually entered by user when adding an employer to their account\n\n\nEmployerID\nUnique ID assigned to each employer that is consistent across every reporting year\nGenerated by the system\n\n\nAddress\nThe current registered address of the employer\nVia CoHo API or manually entered by user when adding an employer to their account\n\n\nPostCode\nThe postal code of the current registered address of the employer\nVia CoHo API or manually entered by user when adding an employer to their account\n\n\nCompanyNumber\nThe Company Number of the employer as listed on Companies House (null for public sector)\nVia CoHo API\n\n\nSicCodes\nList of comma-separated SIC codes used to describe the employer’s purpose and sectors of work at the time of reporting\nVia CoHo API or manually entered by user when adding an employer to their account\n\n\nDiffMeanHourlyPercent\nMean % difference between male and female hourly pay (negative = women’s mean hourly pay is higher)\nEntered by a user when reporting GPG data\n\n\nDiffMedianHourlyPercent\nMedian % difference between male and female hourly pay (negative = women’s median hourly pay is higher)\nEntered by a user when reporting GPG data\n\n\nDiffMeanBonusPercent\nMean % difference between male and female bonus pay (negative = women’s mean bonus pay is higher)\nEntered by a user when reporting GPG data\n\n\nDiffMedianBonusPercent\nMedian % difference between male and female bonus pay (negative = women’s median bonus pay is higher)\nEntered by a user when reporting GPG data\n\n\nMaleBonusPercent\nPercentage of male employees paid a bonus\nEntered by a user when reporting GPG data\n\n\nFemaleBonusPercent\nPercentage of female employees paid a bonus\nEntered by a user when reporting GPG data\n\n\nMaleLowerQuartile\nPercentage of males in the lower hourly pay quarter\nEntered by a user when reporting GPG data\n\n\nFemaleLowerQuartile\nPercentage of females in the lower hourly pay quarter\nEntered by a user when reporting GPG data\n\n\nMaleLowerMiddleQuartile\nPercentage of males in the lower middle hourly pay quarter\nEntered by a user when reporting GPG data\n\n\nFemaleLowerMiddleQuartile\nPercentage of females in the lower middle hourly pay quarter\nEntered by a user when reporting GPG data\n\n\nMaleUpperMiddleQuartile\nPercentage of males in the upper middle hourly pay quarter\nEntered by a user when reporting GPG data\n\n\nFemaleUpperMiddleQuartile\nPercentage of females in the upper middle hourly pay quarter\nEntered by a user when reporting GPG data\n\n\nMaleTopQuartile\nPercentage of males in the top hourly pay quarter\nEntered by a user when reporting GPG data\n\n\nFemaleTopQuartile\nPercentage of females in the top hourly pay quarter\nEntered by a user when reporting GPG data\n\n\nCompanyLinkToGPGInfo\nVoluntary link to additional GPG data published by the reporting employer\nEntered by a user when reporting GPG data\n\n\nResponsiblePerson\nThe name of the responsible person who confirms that the published information is accurate - Employers covered by the private sector regulations only\nEntered by a user when reporting GPG data\n\n\nEmployerSize\nNumber of employees employed by an employer\nEntered by a user when reporting GPG data\n\n\nCurrentName\nThe current name of the employer\nVia CoHo API or manually entered by user when adding an employer to their account\n\n\nSubmittedAfterTheDeadline\nTRUE/FALSE value showing whether the employee submitted their GPG data after the relevant reporting deadline. If a report is updated after the initial submission, it is marked as late only if the figures are changed\nGenerated by the system\n\n\nDueDate\nThe date that the GPG data should have been submitted by. Format: dd/MM/yyyy HH:mm:ss\nGenerated by the system\n\n\nDateSubmitted\nDate that GPG data was submitted (if this was updated after the initial submission, this date also changes). Format: dd/MM/yyyy HH:mm:ss\nGenerated by the system\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(fs)\n\npay_files &lt;- fs::dir_ls(path = \"2022/2022-06-28/\", glob = \"*csv\")\nraw_df &lt;- vroom::vroom(pay_files)\n\n\nclean_df &lt;- raw_df |&gt; \n  janitor::clean_names() |&gt; \n  mutate(\n    across(c(due_date, date_submitted),lubridate::as_datetime),\n    employer_name = str_remove_all(employer_name, \"\\\"\"),\n    employer_name = str_replace_all(employer_name, \", |,\", \", \")\n    ) \n\nclean_df |&gt; \n  glimpse()\n\nclean_df |&gt; \n  write_csv(\"2022/2022-06-28/paygap.csv\")"
  },
  {
    "objectID": "data/2022/2022-07-12/readme.html",
    "href": "data/2022/2022-07-12/readme.html",
    "title": "European Flights",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nEuropean Flights\nThe data this week comes from Eurocontrol. A brief article covers this data at ec.europa.eu. Hattip to Data is Plural.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-07-12')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 28)\n\nflights &lt;- tuesdata$flights\n\n# Or read in the data manually\n\nflights &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-07-12/flights.csv')\n\n\nData Dictionary\n\n\n\nflights.csv\n\n\n\n\n\n\n\n\n\n\nColumn name\nData Source\nLabel\nDescription\nExample\n\n\n\n\nYEAR\nNetwork Manager\nYEAR\nReference year\n2014\n\n\nMONTH_NUM\nNetwork Manager\nMONTH\nMonth (numeric)\n1\n\n\nMONTH_MON\nNetwork Manager\nMONTH_MON\nMonth (3-letter code)\nJAN\n\n\nFLT_DATE\nNetwork Manager\nDATE_FLT\nDate of flight\n01-Jan-2014\n\n\nAPT_ICAO\nNetwork Manager\nAPT_ICAO\nICAO 4-letter airport designator\nEDDM\n\n\nAPT_NAME\nPRU\nAPT_NAME\nAirport name\nMunich\n\n\nSTATE_NAME\nPRU\nSTATE_NAME\nName of the country in which the airport is located\nGermany\n\n\nFLT_DEP_1\nNetwork Manager\nDepartures - (NM)\nNumber of IFR departures\n278\n\n\nFLT_ARR_1\nNetwork Manager\nIFR arrivals - (NM)\nNumber of IFR arrivals\n241\n\n\nFLT_TOT_1\nNetwork Manager\nIFR flights (arr + dep) - (NM)\nNumber total IFR movements\n519\n\n\nFLT_DEP_IFR_2\nAirport Operator\nIFR departures - (APT)\nNumber of IFR departures\n278\n\n\nFLT_ARR_IFR_2\nAirport Operator\nIFR arrivals - (APT)\nNumber of IFR arrivals\n241\n\n\nFLT_TOT_IFR_2\nAirport Operator\nIFR flights (arr + dep) - (APT)\nNumber total IFR movements\n519\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-08-02/readme.html",
    "href": "data/2022/2022-08-02/readme.html",
    "title": "Oregon Spotted Frog",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nOregon Spotted Frog\nData this week come from USGS. Read more about them at USGS.gov\nOregon spotted frog (Rana pretiosa) telemetry and habitat use at Crane Prairie Reservoir in Oregon, USA.\nRadio-telemetry has been used to study late-season movement and habitat use by Oregon spotted frogs (Rana pretiosa) at Crane Prairie Reservoir in Oregon. This dataset includes individual frog location data and habitat use during each tracking event that occurred roughly weekly between September and late November of 2018.\n“…study adds to the limited data on late-season activity and habitat use in R. pretiosa. Information on seasonal habitat use, movement between seasonal habitat types, and habitats that are particularly valuable for maintaining populations is important to conservation and management. We tracked frogs within the main reservoir and in surrounding pond and river habitats, allowing for comparisons of movement relative to different habitat types and predatory threats.”\nCitation Pearl, C.A., Rowe, J.C., McCreary, B., and Adams, M.J., 2022, Oregon spotted frog (Rana pretiosa) telemetry and habitat use at Crane Prairie Reservoir in Oregon, USA: U.S. Geological Survey data release, https://doi.org/10.5066/P9DACPCV.\nMore resources: - https://en.wikipedia.org/wiki/Oregon_spotted_frog - https://wdfw.wa.gov/species-habitats/species/rana-pretiosa - https://amphibiaweb.org/species/5131\nKeywords: #Aquatic Biology, #Ecology, #Wildlife Biology\nRaw data available at: https://www.sciencebase.gov/catalog/item/60ba5a00d34e86b9388d86bc\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-08-02')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 31)\n\nfrogs &lt;- tuesdata$frogs\n\n# Or read in the data manually\n\nfrogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-08-02/frogs.csv')\n\n\nData Dictionary\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nSite\ncharacter\nlocation\n\n\nSubsite\ncharacter\nlocation second level (Cow Camp Pond,Cow Camp River,N Res,NE Res,SE Pond,W Res)\n\n\nHabType\ncharacter\nlocation third level (Pond,Reservoir,River)\n\n\nSurveyDate\ncharacter\ndate\n\n\nOrdinal\ncharacter\nOrdinal day from January 1, 2018 on which telemetry data were collected\n\n\nFrequency\ncharacter\nUnique transmitter frequency associated with each individual frog\n\n\nUTME_83\ncharacter\nUTM coordinates (1,000-meter grid squares into tenths or hundredths)\n\n\nUTMN_83\ncharacter\nUTM coordinates (1,000-meter grid squares into tenths or hundredths)\n\n\nInterval\ncharacter\ninteger (0 to 12)\n\n\nFemale\ncharacter\ngender - binary (0,1)\n\n\nWater\ncharacter\nwater (Deep water,No water,Shallow water,Unknown water)\n\n\nType\ncharacter\nwater type (Marsh/Pond,Non-aquatic,Reservoir,Stream/Canal)\n\n\nStructure\ncharacter\nstructure (Herbaceous veg,Leaf litter,Open,Woody debris,Woody veg)\n\n\nSubstrate\ncharacter\nsubstrate (Flocc,Mineral soil,Organic soil,Unknown substrate)\n\n\nBeaver\ncharacter\nbeaver (Burrow,Channel/runway,Lodge,No beaver)\n\n\nDetection\ncharacter\ndetection type (Captured,No visual,Visual)\n\n\n\n\n\nCleaning script\nlibrary(tidyverse)\nfrog &lt;- read_csv(\"2022/2022-08-02/Oregon_spotted_frog_telemetry_at_Crane_Prairie_OR.csv\",\n  col_names = TRUE,\n  trim_ws = FALSE,\n  skip = 2\n)"
  },
  {
    "objectID": "data/2022/2022-08-16/readme.html",
    "href": "data/2022/2022-08-16/readme.html",
    "title": "Open Psychometrics",
    "section": "",
    "text": "Please add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media."
  },
  {
    "objectID": "data/2022/2022-08-16/readme.html#about",
    "href": "data/2022/2022-08-16/readme.html#about",
    "title": "Open Psychometrics",
    "section": "About",
    "text": "About\nProject collecting and analyzing data from the Open-Source Psychometrics Project. The datasets include information about characters from different universes and their respective personality traits.\nAbout the Open-Source Psychometrics Project (excerpt from website):\n\nThis website provides a collection of interactive personality tests with detailed results that can be taken for personal entertainment or to learn more about personality assessment. These tests range from very serious and widely used scientific instruments popular psychology to self produced quizzes. A special focus is given to the strengths, weaknesses and validity of the various systems."
  },
  {
    "objectID": "data/2022/2022-08-16/readme.html#data",
    "href": "data/2022/2022-08-16/readme.html#data",
    "title": "Open Psychometrics",
    "section": "Data",
    "text": "Data\nI randomly selected 100 different univereses (e.g. Game of Thrones, Bob’s Burgers, Westworld, etc) and collected information about their respective characters. Dataset includes information on 890 characters total. Information for the entire project can also be downloaded directly from opensychometrics.org. Note, the full zip files are codified - i.e. characteters and questions are expressed as varchar IDs and require lookups.\nThere are a total of 400 different personality questions (that’s a lot of traits!). One recommendation from the project suggests this data can be used for cool projects like dimension reduction - i.e. which traits are similar and convey the same info?\nInformation about Scoring from their site:\n\nThe idea of this test is to match takers to a fictional character based on similarity of personality.\n\n\nA fictional character does not have a real personality, but people might perceive it to have one. It is unknown if this perception of personality actually has the same structure as human individual differences.\n\n\nThis test assumes that a character’s assumed personality is reflected in the average ratings of individuals. To collect this data a survey was developed. In it, the volunteer respondent rates 30 characters on 1 trait each, randomly drawn from a bank of 30 traits. With enough data, all the individual surveys can be combined into a comprehensive database of assumed personality."
  },
  {
    "objectID": "data/2022/2022-08-16/readme.html#related-data-visuals",
    "href": "data/2022/2022-08-16/readme.html#related-data-visuals",
    "title": "Open Psychometrics",
    "section": "Related Data Visuals",
    "text": "Related Data Visuals\nThere are tons of ways to explore this data. Recently, Tanya used it to compare characters from Westworld.\n\n\n\nplot\n\n\nAlso see their article: https://openpsychometrics.org/tests/characters/documentation/\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-08-16')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 33)\n\ncharacters &lt;- tuesdata$characters\n\n# Or read in the data manually\n\ncharacters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-08-16/characters.csv')"
  },
  {
    "objectID": "data/2022/2022-08-16/readme.html#dictionary",
    "href": "data/2022/2022-08-16/readme.html#dictionary",
    "title": "Open Psychometrics",
    "section": "Dictionary",
    "text": "Dictionary\n\nCharacters\nHigh level information about characters. Includes a notability score and links to related pages.\n\n\n\nvariable\ntype\ndescription\n\n\n\n\nid\nvarchar\nCharacter ID\n\n\nname\nvarchar\nCharacter Name\n\n\nuni_id\nvarchar\nUniverse ID, e.g. GOT\n\n\nuni_name\nvarchar\nUniverse Name, e.g. Game of Thrones\n\n\nnotability\nnum\nNotability Score\n\n\nlink\nvarchar\nLink to Character Page\n\n\nimage_link\nvarchar\nLink to Character Image\n\n\n\n\n\nPsychology Stats\nPersonality/Psychometric Stats per character.\n\n\n\nvariable\ntype\ndescription\n\n\n\n\nchar_id\nvarchar\nCharacter ID\n\n\nchar_name\nvarchar\nCharacter Name\n\n\nuni_id\nvarchar\nUniverse ID, e.g. GOT\n\n\nuni_name\nvarchar\nUniverse Name, e.g. Game of Thrones\n\n\nquestion\nvarchar\nPersonality Question - e.g. messy/neat\n\n\npersonality\nvarchar\nCharacter Personality, e.g. neat\n\n\navg_rating\nnum\nScore out of 100\n\n\nrank\nint\nRank\n\n\nrating_sd\nnum\nRating Standard Deviation\n\n\nnumber_ratings\nint\nNumber of Ratings (Responses)\n\n\n\n\n\nMyers-Briggs\nUsers who took the personal personality assessment tests were subsequently asked to self-identify their Myers-Briggs types. Dataset contains results.\n\n\n\nvariable\ntype\ndescription\n\n\n\n\nchar_id\nvarchar\nCharacter ID\n\n\nchar_name\nvarchar\nCharacter Name\n\n\nuni_id\nvarchar\nUniverse ID, e.g. GOT\n\n\nuni_name\nvarchar\nUniverse Name, e.g. Game of Thrones\n\n\nmyers_briggs\nvarchar\nMyers Briggs Type, e.g. ENFP\n\n\navg_match_perc\nnum\nPercentage match\n\n\nnumber_users\nint\nnumber of user respondents\n\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-08-30/readme.html",
    "href": "data/2022/2022-08-30/readme.html",
    "title": "Pell Awards",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nPell Awards\nThe data this week comes from US Department of EducationU.S. Department of Education.\nThe data this week is already packaged in an R package hosted called Pell grant in CRAN. The original data source is US Department of Education.\nThis package vignette talks in more detail about the data and how it was sourced. To check out how the data was sourced and cleaned check download data directly page in the vignette.\nCredit: Arafath Hossain\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-08-30')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 35)\n\npell &lt;- tuesdata$pell\n\n# Or read in the data manually\n\npell &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-08-30/pell.csv')\n\n\nData Dictionary\n\n\n\npell.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nSTATE\ninteger\nState shortcode\n\n\nAWARD\ndouble\nAward amount in USD\n\n\nRECIPIENT\ndouble\nTotal number of recipients by year, name\n\n\nNAME\ninteger\nName of college/university\n\n\nSESSION\ninteger\nSession group\n\n\nYEAR\ninteger\nYear\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-09-13/readme.html",
    "href": "data/2022/2022-09-13/readme.html",
    "title": "Bigfoot",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nBigfoot\nThe data this week comes from Bigfoot Field Researchers Organization (BFRO) by way of Data.World.\nA bigfoot article by Timothy Renner.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-09-13')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 37)\n\nbigfoot &lt;- tuesdata$bigfoot\n\n# Or read in the data manually\n\nbigfoot &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-09-13/bigfoot.csv')\n\n\nData Dictionary\n\n\n\nbigfoot.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nobserved\ncharacter\nobserved\n\n\nlocation_details\ncharacter\nlocation_details\n\n\ncounty\ncharacter\ncounty\n\n\nstate\ncharacter\nstate\n\n\nseason\ncharacter\nseason\n\n\ntitle\ncharacter\ntitle\n\n\nlatitude\ndouble\nlatitude\n\n\nlongitude\ndouble\nlongitude\n\n\ndate\ndouble\ndate\n\n\nnumber\ndouble\nnumber\n\n\nclassification\ncharacter\nclassification\n\n\ngeohash\ncharacter\ngeohash\n\n\ntemperature_high\ndouble\ntemperature_high\n\n\ntemperature_mid\ndouble\ntemperature_mid\n\n\ntemperature_low\ndouble\ntemperature_low\n\n\ndew_point\ndouble\ndew_point\n\n\nhumidity\ndouble\nhumidity\n\n\ncloud_cover\ndouble\ncloud_cover\n\n\nmoon_phase\ndouble\nmoon_phase\n\n\nprecip_intensity\ndouble\nprecip_intensity\n\n\nprecip_probability\ndouble\nprecip_probability\n\n\nprecip_type\ncharacter\nprecip_type\n\n\npressure\ndouble\npressure\n\n\nsummary\ncharacter\nsummary\n\n\nuv_index\ndouble\nuv_index\n\n\nvisibility\ndouble\nvisibility\n\n\nwind_bearing\ndouble\nwind_bearing\n\n\nwind_speed\ndouble\nwind_speed\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-09-27/readme.html",
    "href": "data/2022/2022-09-27/readme.html",
    "title": "Artists in the USA",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nArtists in the USA\nThe data this week comes from arts.gov by way of Data is Plural\nArtists in the Workforce: National and State Estimates for 2015-2019\n\nThis Arts Data Profile gives national and state-level estimates of artists in the workforce. The figures derive from American Community Survey (ACS) data covering 2015-2019. The ACS is conducted by the U.S. Census Bureau. State-level estimates are available for the total number of artists and for each individual type of artist (workers in any of 13 specific artist occupations).\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-09-27')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 39)\n\nartists &lt;- tuesdata$artists\n\n# Or read in the data manually\n\nartists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-09-27/artists.csv')\n\n\nData Dictionary\n\n\n\nartists.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstate\ncharacter\nstate/territory\n\n\nrace\ncharacter\nrace\n\n\ntype\ncharacter\ntype of artists\n\n\nall_workers_n\ndouble\nall worker count\n\n\nartists_n\ndouble\nartist count\n\n\nartists_share\ndouble\nartist share\n\n\nlocation_quotient\ndouble\nLocation quotients (LQ) measure an artist occupation’s concentration in the labor force, relative to the U.S. labor force share. For example, an LQ of 1.2 indicates that the state’s labor force in an occupation is 20 percent greater than the occupation’s national labor force share. An LQ of 0.8 indicates that the state’s labor force in an occupation is 20 percent below the occupation’s national labor force share.\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(fs)\n\nall_xl &lt;- fs::dir_ls(\"2022/2022-09-27/ADP-31-artists-in-the-workforce-StateTables/\")\n\nall_xl |&gt; \n  str_subset(\"AllArtists\", negate = TRUE)\n\ntest_df &lt;- all_xl[2] |&gt; read_xlsx()\ntest_df |&gt; glimpse()\n\nall_xl\n\nnames(test_df)[1] |&gt; \n  str_extract(art_pattern) |&gt; \n  str_to_title()\n\nread_and_clean &lt;- function(file){\n  \n  raw_full &lt;- read_excel(file)\n  \n  art_pattern &lt;- \"(?&lt;=Number of ).+(?= in the U.S. labor force, for all the states and Puerto Rico: 2015-2019)\" \n  \n  art_type &lt;- names(raw_full)[1] |&gt; \n    str_extract(art_pattern) |&gt; \n    str_to_title()\n  \n  races &lt;- c(\"Hispanic\", \"White\", \"African-American\", \"Asian\", \"Other\")\n  \n  race_data &lt;- function(sheet, race){\n    read_excel(file, sheet = sheet, skip = 3) |&gt; \n      mutate(race = race, .after = 1) |&gt; \n      slice(c(-1,-2)) |&gt; \n      filter(!is.na(State)) |&gt; \n      mutate(across(3:6, as.numeric)) |&gt; \n      select(1:6) |&gt; \n      set_names(\n        nm = c(\n          \"state\", \"race\", \"all_workers_n\", \"artists_n\", \"artists_share\", \"location_quotient\")\n        ) |&gt; \n      mutate(type = art_type, .before = 3)\n  }\n  \n  map2_dfr(2:6, races, race_data)\n  \n}\n\ntest_all &lt;- all_xl[2] |&gt; \n  read_and_clean()\n\ntest_all |&gt; glimpse()\n\nall_df &lt;- map_dfr(all_xl[2:length(all_xl)], read_and_clean)\n\nall_df |&gt; \n  glimpse()\n\nall_df |&gt; \n  write_csv(\"2022/2022-09-27/artists.csv\")"
  },
  {
    "objectID": "data/2022/2022-10-11/readme.html",
    "href": "data/2022/2022-10-11/readme.html",
    "title": "Ravelry Yarn",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nRavelry Yarn\nThe data this week comes from ravelry.com by way of Alice Walsh.\nAlso see the {ravelRy} R package by Kaylin Pavlik.\n\nRavelry describes itself as a social networking and organizational tool for knitters, crocheters, designers, spinners, weavers and dyers.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-10-11')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 41)\n\nyarn &lt;- tuesdata$yarn\n\n# Or read in the data manually\n\nyarn &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-10-11/yarn.csv')\n\n\nData Dictionary\n\n\n\nyarn.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndiscontinued\nlogical\ndiscontinued true/false\n\n\ngauge_divisor\ndouble\ngauge divisor - The number of inches that equal min_gauge to max_gauge stitches\n\n\ngrams\ndouble\nUnit weight in grams\n\n\nid\ndouble\nid\n\n\nmachine_washable\nlogical\nmachine washable true/false\n\n\nmax_gauge\ndouble\nmax gauge - The max number of stitches that equal gauge_divisor\n\n\nmin_gauge\ndouble\nmin gauge - The min number of stitches that equal gauge_divisor\n\n\nname\ncharacter\nname\n\n\npermalink\ncharacter\npermalink\n\n\nrating_average\ndouble\nrating average - The average rating out of 5\n\n\nrating_count\ndouble\nrating count\n\n\nrating_total\ndouble\nrating total\n\n\ntexture\ncharacter\ntexture - Texture free text\n\n\nthread_size\ncharacter\nthread size\n\n\nwpi\ndouble\nwraps per inch\n\n\nyardage\ndouble\nyardage\n\n\nyarn_company_name\ncharacter\nYarn company name\n\n\nyarn_weight_crochet_gauge\nlogical\nYarn weight crochet gauge - Crochet gauge for the yarn weight category\n\n\nyarn_weight_id\ndouble\nYarn weight ID - Identifier for the yarn weight category\n\n\nyarn_weight_knit_gauge\ncharacter\nYarn weight knit gauge - Knit guage for the yarn weight category\n\n\nyarn_weight_name\ncharacter\nYarn weight name - Name for the yarn weight category\n\n\nyarn_weight_ply\ndouble\nYarn weight ply - Ply for the yarn weight category\n\n\nyarn_weight_wpi\ncharacter\nYarn weight wraps per inch - Wraps per inch for the yarn weight category\n\n\ntexture_clean\ncharacter\nTexture clean - Texture with some light text cleaning\n\n\n\n\nCleaning Script\nClean script source: https://github.com/awalsh17/ravelry_yarns\n# Call to ravelry API\n\nlibrary(dplyr)\nlibrary(httr)\nlibrary(jsonlite)\n\n# get the information on 100,000 yarns -------\n# iterate over all pages (1000 results per) 100,000 total, api user/pass was saved to env\n\npages &lt;- c(1:100)\n\nresp &lt;- purrr::map(pages, ~httr::GET(\n  url = paste0(\"https://api.ravelry.com/yarns/search.json?sort=best&page_size=1000&page=\", .x),\n  authenticate(Sys.getenv(\"RAVELRY_API_USER\"), Sys.getenv(\"RAVELRY_API_PASS\"))))\n\n# first item is the yarn data\n\nyarn_all &lt;- purrr::map_dfr(resp, ~fromJSON(content(.x, as = \"text\"))[[1]])\n\n# second is the paginator - 100 pages\n\n# write out raw\n\nsaveRDS(yarn_all, \"data/yarn_raw.Rds\")\n\n# unnest the data frame, remove first_photo and personal_attributes\n\nyarn_all &lt;- tidyr::unnest(yarn_all, yarn_weight, names_sep = \"_\") %&gt;%\n  select(-first_photo, -personal_attributes)\n\n# yarn_weight_min_gauge and yarn_weight_max_gauge are always missing\n\nyarn_all &lt;- select(yarn_all, -yarn_weight_max_gauge, -yarn_weight_min_gauge)\n\n# clean the texture variable\n\nyarn_all$texture_clean &lt;- stringr::str_trim(yarn_all$texture)\nyarn_all$texture_clean &lt;- stringr::str_to_lower(yarn_all$texture_clean)\n\n# write out the final csv\n\nwrite.csv(yarn_all, \"data/yarn.csv\", row.names = FALSE)\n\n\n\n\n# Query for more yarn information -----\n\n# need to split request up into chunks of 100\n\nchunks &lt;- dplyr::ntile(1:100000, 1000)\nyarn_ids &lt;- purrr::map(\n  1:1000,\n  ~paste(unique(yarn_all$id)[chunks == .x], collapse = \"+\"))\n\nresp &lt;- purrr::map(\n  yarn_ids,\n  ~httr::GET(url = paste0(\"https://api.ravelry.com/yarns.json?ids=\", .x),\n             authenticate(Sys.getenv(\"RAVELRY_API_USER\"), Sys.getenv(\"RAVELRY_API_PASS\"))))\n\nyarn_json &lt;- purrr::map(\n  resp,\n  ~parse_json(content(.x, as = \"text\"), simplifyVector = TRUE)$yarns)\nyarn_json &lt;- unlist(yarn_json, recursive = FALSE)\n\n# yarn_fibers with have more than one row per yarn\n\nyarn_fibers &lt;- purrr::map_dfr(yarn_json, ~.x[[\"yarn_fibers\"]], .id = \"yarn_id\") %&gt;%\n  tidyr::unnest(fiber_type, names_sep = \"_\") %&gt;%\n  select(-fiber_category, -id) %&gt;%\n  rename(id = yarn_id) # make primary key id for yarn\n\n# yarn_attributes have more than one row per yarn (care, color, dye)\n\nyarn_attributes &lt;- purrr::map_dfr(yarn_json, ~.x[[\"yarn_attributes\"]], .id = \"yarn_id\") %&gt;%\n  tidyr::unnest(yarn_attribute_group, names_sep = \"_\") %&gt;%\n  select(-id, -yarn_attribute_group_id) %&gt;%\n  rename(id = yarn_id) # make primary key id for yarn\n\n# this has all the possible values for the yarn_attribute_group\n\nyarn_attribute_groups &lt;- httr::GET(\n  url = \"https://api.ravelry.com/yarn_attributes/groups.json\",\n  authenticate(Sys.getenv(\"RAVELRY_API_USER\"), Sys.getenv(\"RAVELRY_API_PASS\")))\n\nyarn_attribute_groups &lt;- fromJSON(content(yarn_attribute_groups, as = \"text\"))[[1]] %&gt;%\n  tidyr::unnest(yarn_attributes, names_sep = \"_\") %&gt;%\n  select(-children) # for construction attributes, just remove for here\n\n# write out yarn_fibers and yarn_attributes\n\nwrite.csv(yarn_fibers, \"data/yarn_fibers.csv\", row.names = FALSE)\nwrite.csv(yarn_attribute_groups, \"data/yarn_attribute_groups.csv\", row.names = FALSE)\nwrite.csv(yarn_attributes, \"data/yarn_attributes.csv\", row.names = FALSE)"
  },
  {
    "objectID": "data/2022/2022-10-25/readme.html",
    "href": "data/2022/2022-10-25/readme.html",
    "title": "Great British Bakeoff",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nGreat British Bakeoff\nThe data this week comes from the bakeoff package from Alison Hill, Chester Ismay, and Richard Iannone.\nUse the R package for all the data and raw datasets, and make use of the built-in palettes/scales!\ninstall.packages(\"bakeoff\")\nlibrary(tidyverse)\nlibrary(bakeoff)\n\nplot_off1 &lt;- bakeoff::ratings %&gt;% \n  mutate(ep_id = row_number()) %&gt;%\n  select(ep_id, viewers_7day, series, episode)\n\n# create coordinates for labels\nseries_labels &lt;- plot_off1 %&gt;% \n  group_by(series) %&gt;% \n  summarize(y_position = median(viewers_7day) + 1,\n            x_position = mean(ep_id))\n# make the plot\nggplot(plot_off1, aes(x = ep_id, y = viewers_7day, fill = factor(series))) +\n  geom_col(alpha = .9) +\n  ggtitle(\"Series 8 was a Big Setback in Viewers\",\n          subtitle= \"7-Day Viewers across All Series/Episodes\") +\n  geom_text(data = series_labels, aes(label = series,\n                                      x = x_position, \n                                      y = y_position)) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank()) + \n  scale_fill_bakeoff(guide = \"none\")\nFor some introductory plots, also see Data Visualization in the Tidyverse - The Great Tidy Plot Off by Alison Hill.\n\nThe Great British Bake Off (often abbreviated to Bake Off or GBBO) is a British television baking competition, produced by Love Productions, in which a group of amateur bakers compete against each other in a series of rounds, attempting to impress two judges with their baking skills. One contestant is eliminated in each round, and the winner is selected from the contestants who reach the final. The first episode was aired on 17 August 2010, with its first four series broadcast on BBC Two, until its growing popularity led the BBC to move it to BBC One for the next three series. After its seventh series, Love Productions signed a three-year deal with Channel 4 to produce the series for the broadcaster. - Wikipedia\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-10-25')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 43)\n\nbakers &lt;- tuesdata$bakers\n\n# Or read in the data manually\n\nbakers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-10-25/bakers.csv')\n\n\nData Dictionary\n\n\n\nchallenges\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseries\ninteger\nseries\n\n\nepisode\ninteger\nepisode\n\n\nbaker\ncharacter\nbaker\n\n\nresult\ncharacter\nresult\n\n\nsignature\ncharacter\nsignature\n\n\ntechnical\ninteger\ntechnical\n\n\nshowstopper\ncharacter\nshowstopper\n\n\n\n\n\nbakers\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseries\ndouble\nseries\n\n\nbaker\ncharacter\nbaker\n\n\nstar_baker\ninteger\nstar_baker\n\n\ntechnical_winner\ninteger\ntechnical_winner\n\n\ntechnical_top3\ninteger\ntechnical_top3\n\n\ntechnical_bottom\ninteger\ntechnical_bottom\n\n\ntechnical_highest\ndouble\ntechnical_highest\n\n\ntechnical_lowest\ndouble\ntechnical_lowest\n\n\ntechnical_median\ndouble\ntechnical_median\n\n\nseries_winner\ninteger\nseries_winner\n\n\nseries_runner_up\ninteger\nseries_runner_up\n\n\ntotal_episodes_appeared\ndouble\ntotal_episodes_appeared\n\n\nfirst_date_appeared\ndouble\nfirst_date_appeared\n\n\nlast_date_appeared\ndouble\nlast_date_appeared\n\n\nfirst_date_us\ndouble\nfirst_date_us\n\n\nlast_date_us\ndouble\nlast_date_us\n\n\npercent_episodes_appeared\ndouble\npercent_episodes_appeared\n\n\npercent_technical_top3\ndouble\npercent_technical_top3\n\n\nbaker_full\ncharacter\nbaker_full\n\n\nage\ndouble\nage\n\n\noccupation\ncharacter\noccupation\n\n\nhometown\ncharacter\nhometown\n\n\nbaker_last\ncharacter\nbaker_last\n\n\nbaker_first\ncharacter\nbaker_first\n\n\n\n\n\nratings\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseries\ndouble\nseries\n\n\nepisode\ndouble\nepisode\n\n\nuk_airdate\ndouble\nuk_airdate\n\n\nviewers_7day\ndouble\nviewers_7day\n\n\nviewers_28day\ndouble\nviewers_28day\n\n\nnetwork_rank\ndouble\nnetwork_rank\n\n\nchannels_rank\ndouble\nchannels_rank\n\n\nbbc_iplayer_requests\ndouble\nbbc_iplayer_requests\n\n\nepisode_count\ndouble\nepisode_count\n\n\nus_season\ndouble\nus_season\n\n\nus_airdate\ncharacter\nus_airdate\n\n\n\n\n\nepisodes\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseries\ndouble\nseries\n\n\nepisode\ndouble\nepisode\n\n\nbakers_appeared\ninteger\nbakers_appeared\n\n\nbakers_out\ninteger\nbakers_out\n\n\nbakers_remaining\ninteger\nbakers_remaining\n\n\nstar_bakers\ninteger\nstar_bakers\n\n\ntechnical_winners\ninteger\ntechnical_winners\n\n\nsb_name\ncharacter\nsb_name\n\n\nwinner_name\ncharacter\nwinner_name\n\n\neliminated\ncharacter\neliminated\n\n\n\n\nCleaning Script"
  },
  {
    "objectID": "data/2022/2022-11-08/readme.html",
    "href": "data/2022/2022-11-08/readme.html",
    "title": "Radio Stations",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nRadio Stations\nThe data this week comes from Wikipedia.\nThe dataset included was mined from all 50 states, tidying column names, binding and aggregating.\nErin’s blogpost on Visualizing the Geography of FM Radio. Data sourced from FCC.\nCredit: Frank Hull\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-11-08')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 45)\n\nstate_stations &lt;- tuesdata$state_stations\n\n# Or read in the data manually\n\nstate_stations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-08/state_stations.csv')\n\n\nData Dictionary\n\n\n\nstate_stations.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncall_sign\ncharacter\nCall Sign\n\n\nfrequency\ncharacter\nfrequency\n\n\ncity\ncharacter\ncity\n\n\nlicensee\ncharacter\nlicensee\n\n\nformat\ncharacter\nformat\n\n\nstate\ncharacter\nstate\n\n\n\n\n\nfm_service_contour_current.zip\nA large Zip file (~200 Mb on disk when unzipped) - see cleaning script for use.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\napplication_id\ncharacter\napplication_id\n\n\nservice\ncharacter\nservice\n\n\nlms_application_id\ncharacter\nlms_application_id\n\n\ndts_site_number\ncharacter\ndts_site_number\n\n\nsite_lat\ndouble\nSite of station latitude\n\n\nsite_long\ndouble\nSite of station longitude\n\n\nangle\ninteger\nangle (0 to 360)\n\n\ndeg_lat\ndouble\nLatitude for angle\n\n\ndeg_lng\ndouble\nLongitude for angle\n\n\n\n\n\nstation_info.csv\nCan be joined:\nstate_stations |&gt; dplyr::right_join(station_info, by = c(\"call_sign\"))\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncall_sign\ncharacter\nCall sign\n\n\nfacility_id\ndouble\nFacility id\n\n\nservice\ncharacter\nService\n\n\nlicensee\ncharacter\nLicensee\n\n\nstatus\ncharacter\nStatus\n\n\ndetails\ncharacter\nDetails\n\n\n\n\nCleaning Script\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(rvest)\n\n\nall_states &lt;- datasets::state.name\nall_states &lt;- gsub(\" \", \"_\", all_states)\n\n\nget_stations &lt;- function(state){\n\nroot &lt;- paste0(\"https://en.wikipedia.org/wiki/List_of_radio_stations_in_\", state) \n tables &lt;- read_html(root) %&gt;% html_nodes(\"table\")\n  stations &lt;- tables[1] %&gt;% # ideally all pages will have same format\n              html_table(header = TRUE) %&gt;% \n              do.call(rbind, .) %&gt;% \n              clean_names() %&gt;% \n              mutate(frequency = as.character(frequency)) %&gt;%                    # handling Ohio special case (frequency / band split)\n              rename_if(startsWith(names(.), \"city\"), ~ (\"city\")) %&gt;%            # handling naming issues, citation handler\n              rename_at(vars(matches(\"format\")), ~ \"format\") %&gt;%                 # handling naming issues, Oklahoma handler\n              rename_if(startsWith(names(.), \"licensee\"), ~ (\"licensee\")) %&gt;%    # handling naming issues, citation handler\n              rename_if(startsWith(names(.), \"owner\"), ~ (\"licensee\")) %&gt;%       # South Dakota handler\n              select(call_sign, frequency, city, licensee, format) %&gt;% \n              mutate(\n                state = state\n              )\n  \n  return(stations)\n}\n\nstate_stations &lt;- map_dfr(all_states, get_stations)\n\nFor the Contour cleaning:\nlibrary(tidyverse)\n\nraw_contour &lt;- read_delim(\n  \"2022/2022-11-08/FM_service_contour_current.txt\",\n  delim = \"|\"\n)\n\nconv_contour &lt;- raw_contour |&gt;\n  select(-last_col()) |&gt;\n  set_names(nm = c(\n    \"application_id\", \"service\", \"lms_application_id\", \"dts_site_number\", \"transmitter_site\",\n    glue::glue(\"deg_{0:360}\")\n  ))\n\nlng_contour &lt;- conv_contour |&gt;\n  separate(\n    transmitter_site, \n    into = c(\"site_lat\", \"site_long\"), \n    sep = \" ,\") |&gt;\n  pivot_longer(\n    names_to = \"angle\",\n    values_to = \"values\",\n    cols = deg_0:deg_360\n  ) |&gt;\n  mutate(\n    angle = str_remove(angle, \"deg_\"),\n    angle = as.integer(angle)\n  ) |&gt;\n  separate(\n    values,\n    into = c(\"deg_lat\", \"deg_lng\"),\n    sep = \" ,\"\n  )"
  },
  {
    "objectID": "data/2022/2022-11-22/readme.html",
    "href": "data/2022/2022-11-22/readme.html",
    "title": "Museums",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nMuseums\nThe data this week comes from Mapping Museums project.\n\nThe project’s research team has gathered, cleansed, and codified data relating to over 4000 UK museums - almost double the number of museums covered in any previous survey. It covers the period from 1960 to date. The challenges we faced and the processes we adopted to collect, integrate, and cleanse the data are described in our publications\nThe following files are available for download, which capture the status of the Mapping Museums database at the formal end of the project in September 2021:\nLicense: The above data is free to use under the terms of the Creative Commons (BY) license. This allows users to copy, distribute, remix, and build upon the research, so long the Mapping Museums team is credited with its original creation.\nPlease use the following citation if you do download the data: Data downloaded from the Mapping Museums website at www.mappingmuseums.org, Accessed on 2022-11-22.\nAlso, we would be very interested to know how you are using, or planning to use, this data. Please use the Get in Touch link to tell us about your work.\n\nGlossary of terms: https://museweb.dcs.bbk.ac.uk/glossary\nKey findings: https://museweb.dcs.bbk.ac.uk/findings\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-11-22')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 47)\n\nmuseums &lt;- tuesdata$museums\n\n# Or read in the data manually\n\nmuseums &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-11-22/museums.csv')\n\n\nData Dictionary\n\n\n\nmuseums.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nmuseum_id\ncharacter\nmuseum_id\n\n\nName_of_museum\ncharacter\nName of_museum\n\n\nAddress_line_1\ncharacter\nAddress_line_1\n\n\nAddress_line_2\ncharacter\nAddress_line_2\n\n\nVillage,_Town_or_City\ncharacter\nVillage,_Town_or_City\n\n\nPostcode\ncharacter\nPostcode\n\n\nLatitude\ndouble\nLatitude\n\n\nLongitude\ndouble\nLongitude\n\n\nAdmin_area\ncharacter\nAdmin_area\n\n\nAccreditation\ncharacter\nAccreditation\n\n\nGovernance\ncharacter\nGovernance\n\n\nSize\ncharacter\nSize\n\n\nSize_provenance\ncharacter\nSize_provenance\n\n\nSubject_Matter\ncharacter\nSubject_Matter\n\n\nYear_opened\ncharacter\nYear_opened\n\n\nYear_closed\ncharacter\nYear_closed\n\n\nDOMUS_Subject_Matter\ncharacter\nDOMUS_Subject_Matter\n\n\nDOMUS_identifier\ndouble\nDOMUS_identifier\n\n\nPrimary_provenance_of_data\ncharacter\nPrimary_provenance_of_data\n\n\nIdentifier_used_in_primary_data_source\ncharacter\nIdentifier_used_in_primary_data_source\n\n\nArea_Deprivation_index\ndouble\nArea_Deprivation_index\n\n\nArea_Deprivation_index_crime\ndouble\nArea_Deprivation_index_crime\n\n\nArea_Deprivation_index_education\ndouble\nArea_Deprivation_index_education\n\n\nArea_Deprivation_index_employment\ndouble\nArea_Deprivation_index_employment\n\n\nArea_Deprivation_index_health\ndouble\nArea_Deprivation_index_health\n\n\nArea_Deprivation_index_housing\ndouble\nArea_Deprivation_index_housing\n\n\nArea_Deprivation_index_income\ndouble\nArea_Deprivation_index_income\n\n\nArea_Deprivation_index_services\ndouble\nArea_Deprivation_index_services\n\n\nArea_Geodemographic_group\ncharacter\nArea_Geodemographic_group\n\n\nArea_Geodemographic_group_code\ncharacter\nArea_Geodemographic_group_code\n\n\nArea_Geodemographic_subgroup\ncharacter\nArea_Geodemographic_subgroup\n\n\nArea_Geodemographic_subgroup_code\ncharacter\nArea_Geodemographic_subgroup_code\n\n\nArea_Geodemographic_supergroup\ncharacter\nArea_Geodemographic_supergroup\n\n\nArea_Geodemographic_supergroup_code\ncharacter\nArea_Geodemographic_supergroup_code\n\n\nNotes\ncharacter\nNotes\n\n\n\n\nCleaning Script\nClean data - no script"
  },
  {
    "objectID": "data/2022/2022-12-06/readme.html",
    "href": "data/2022/2022-12-06/readme.html",
    "title": "Elevators",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nElevators\nThe data this week comes from the Elevators data package.\n\nThis data package contains a data set of the registered elevator devices in New York City provided by the Department of Buildings in response to a September 2015 FOIL request.\nThe package is available for use and the raw data files are available in the github repository: https://github.com/EmilHvitfeldt/elevators/tree/main/data-raw\nLicense: The above data is free to use under the terms of the MIT license.\nPlease use the following citation if you do download the data: Hvitfeldt E (2022). elevators: Data Package Containing Information About Elevators in NYC. https://github.com/EmilHvitfeldt/elevators, https://emilhvitfeldt.github.io/elevators/.\n\nSome initial examples: https://emilhvitfeldt.github.io/elevators/\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-12-06')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 49)\n\nelevators &lt;- tuesdata$elevators\n\n# Or read in the data manually\n\nelevators &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-12-06/elevators.csv')\n\n\nData Dictionary\n\n\n\nelevators.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nDV_DEVICE_NUMBER\ncharacter\nUnique identifier for the elevator\n\n\nDevice Status\ncharacter\nDevice status\n\n\nDV_DEVICE_STATUS_DESCRIPTION\ncharacter\nDevlice status description\n\n\nBIN\ndouble\nBuilding Identification Number\n\n\nTAX_BLOCK\ndouble\nID for tax block. Smaller than borough\n\n\nTAX_LOT\ndouble\nID for tax lot. Smaller than tax block\n\n\nHOUSE_NUMBER\ncharacter\nHouse number, very poorly parsed, use with caution\n\n\nSTREET_NAME\ncharacter\nStreet name, very poorly parsed, use with caution\n\n\nZIP_CODE\ndouble\nZip code, formatted to 5 digits. 0 and 99999 are marked as NA\n\n\nBorough\ncharacter\nBorough\n\n\nDevice Type\ncharacter\nType of device. Most common type is “Passenger Elevator”\n\n\nDV_LASTPER_INSP_DATE\ndouble\nDate, refers to the last periodic inspection by the Department of Buildings. These dates will no longer be accurate, as they were collected by November 2015\n\n\nDV_LASTPER_INSP_DISP\ncharacter\nDisplay, last periodic inspection\n\n\nDV_APPROVAL_DATE\ncharacter\nDate of approval for elevator\n\n\nDV_MANUFACTURER\ncharacter\nName of manufacturer, poorly cleaned. Most assigned NA\n\n\nDV_TRAVEL_DISTANCE\ncharacter\nDistance traveled, not cleaned. Mixed formats\n\n\nDV_SPEED_FPM\ncharacter\nSpeed in feet/minute\n\n\nDV_CAPACITY_LBS\ndouble\nCapacity in lbs\n\n\nDV_CAR_BUFFER_TYPE\ncharacter\nBuffer type. A buffer is a device designed to stop a descending car or counterweight beyond its normal limit and to soften the force with which the elevator runs into the pit during an emergency. Takes values “Oil”, “Spring”, and NA\n\n\nDV_GOVERNOR_TYPE\ncharacter\nGovernor type, An overspeed governor is an elevator device which acts as a stopping mechanism in case the elevator runs beyond its rated speed\n\n\nDV_MACHINE_TYPE\ncharacter\nMachine type, labels unknown.\n\n\nDV_SAFETY_TYPE\ncharacter\nSafety type, labels unknown.\n\n\nDV_MODE_OPERATION\ncharacter\nOperation mode, labels unknown\n\n\nDV_STATUS_DATE\ndouble\nStatus date\n\n\nDV_FLOOR_FROM\ncharacter\nLowest floor, not cleaned. Mixed formats\n\n\nDV_FLOOR_TO\ncharacter\nHighest floor, not cleaned. Mixed formats\n\n\n…27\nlogical\n…27\n\n\nLATITUDE\ndouble\nLatitude of elevator\n\n\nLONGITUDE\ndouble\nLongitude of elevator\n\n\n\n\nCleaning Script\nClean data - no script"
  },
  {
    "objectID": "data/2022/2022-12-20/readme.html",
    "href": "data/2022/2022-12-20/readme.html",
    "title": "Weather Forecast Accuracy",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nWeather Forecast Accuracy\nThe data this week comes from the USA National Weather Service, as emailed by the University of Illinois list server and processed by Sai Shreyas Bhavanasi, Harrison Lanier, Lauren Schmiedeler, and Clayton Strauch at the Saint Louis University Department of Mathematics and Statistics. Thank you to Darrin Speegle for bringing the data to our attention!\n\nThe goal of this data science capstone project has been to acquire national weather data to learn which areas of the U.S. struggle with weather prediction and the possible reasons why. Specifically, we focused on the error in high and low temperature forecasting.\n\nThe data includes 16 months of forecasts and observations from 167 cities, as well as a separate data.frame of information about those cities and some other American cities.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-12-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 51)\n\nweather_forecasts &lt;- tuesdata$weather_forecasts\ncities &lt;- tuesdata$cities\noutlook_meanings &lt;- tuesdata$outlook_meanings\n\n# Or read in the data manually\n\nweather_forecasts &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-12-20/weather_forecasts.csv')\ncities &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-12-20/cities.csv')\noutlook_meanings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2022/2022-12-20/outlook_meanings.csv')\n\n\nData Dictionary\n\n\n\nweather_forecasts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\ndate described by the forecast and observation\n\n\ncity\nfactor\ncity\n\n\nstate\nfactor\nstate or territory\n\n\nhigh_or_low\nfactor\nweather the forecast is for the high temperature of the low temperature\n\n\nforecast_hours_before\ninteger\nthe number of hours before the observation (one of 12, 24, 36, or 48)\n\n\nobserved_temp\ninteger\nthe actual observed temperature on that date (high or low)\n\n\nforecast_temp\ninteger\nthe predicted temperature on that date (high or low)\n\n\nobserved_precip\ndouble\nthe observed precipitation on that date, in inches; note that some observations lack an indication of precipitation, while others explicitly report 0\n\n\nforecast_outlook\nfactor\nan abbreviation for the general outlook, such as precipitation type\n\n\npossible_error\nfactor\neither (1) “none” if the row contains no potential errors or (2) the name of the variable that is the cause of the potential error\n\n\n\n\n\ncities.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncity\ncharacter\ncity\n\n\nstate\ncharacter\nstate or territory\n\n\nlon\ndouble\nlongitude\n\n\nlat\ndouble\nlatitude\n\n\nkoppen\ncharacter\nKöppen climate classification\n\n\nelevation\ndouble\nelevation in meters\n\n\ndistance_to_coast\ndouble\ndistance_to_coast in miles\n\n\nwind\ndouble\nmean wind speed\n\n\nelevation_change_four\ndouble\ngreatest elevation change in meters out of the four closest points to this city in a collection of elevations used by the team at Saint Louis University\n\n\nelevation_change_eight\ndouble\ngreatest elevation change in meters out of the eight closest points to this city in a collection of elevations used by the team at Saint Louis University\n\n\navg_annual_precip\ndouble\naverage annual precipitation in inches\n\n\n\n\n\noutlook_meanings.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nforecast_outlook\ncharacter\nan abbreviation for the general outlook, such as precipitation type\n\n\nmeaning\ncharacter\nthe meaning of that abbreviation\n\n\n\n\nCleaning Script\n# Grab the data, then add some additional information.\n\nlibrary(tidyverse)\nlibrary(here)\n\nweather_forecasts &lt;- read_csv(\"https://raw.githubusercontent.com/speegled/weather_forecasts/main/data/email_data_expanded.csv\")\nglimpse(weather_forecasts)\ncities &lt;- read_csv(\"https://raw.githubusercontent.com/speegled/weather_forecasts/main/data/cities.csv\")\n\n# Translate outlooks from https://www.nws.noaa.gov/directives/sym/pd01005004curr.pdf\n\nsome_translations &lt;- c(\n  BLGSNO = \"Blowing Snow\",\n  BLZZRD = \"Blizzard\",\n  DRZL = \"Drizzle\",\n  FLRRYS = \"Snow Flurries\",\n  FZDRZL = \"Freezing Drizzle\",\n  FZRAIN = \"Freezing Rain\",\n  MOCLDY = \"Mostly Cloudy\",\n  PTCLDY = \"Partly Cloudy\",\n  RNSNOW = \"Rain and Snow\",\n  SHWRS = \"Rain Showers\",\n  SNOSHW = \"Snow Showers\" ,\n  TSTRMS = \"Thunderstorms\",\n  VRYHOT = \"Very Hot\",\n  VRYCLD = \"Very Cold\" \n)\n\n# The ones that aren't in that list are the same as their code.\n  \noutlook_meanings &lt;- weather_forecasts |&gt; \n  distinct(forecast_outlook) |&gt; \n  dplyr::filter(!is.na(forecast_outlook)) |&gt; \n  dplyr::mutate(\n    meaning = dplyr::if_else(\n      forecast_outlook %in% names(some_translations),\n      some_translations[forecast_outlook],\n      stringr::str_to_title(forecast_outlook)\n    )\n  )\n\nweather_forecasts |&gt; write_csv(\n  here(\n    \"data\",\n    \"2022\",\n    \"2022-12-20\",\n    \"weather_forecasts.csv\"\n  )\n)\n\ncities |&gt; write_csv(\n  here(\n    \"data\",\n    \"2022\",\n    \"2022-12-20\",\n    \"cities.csv\"\n  )\n)\n\noutlook_meanings |&gt; write_csv(\n  here(\n    \"data\",\n    \"2022\",\n    \"2022-12-20\",\n    \"outlook_meanings.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2022/readme.html",
    "href": "data/2022/readme.html",
    "title": "2022 Data",
    "section": "",
    "text": "2022 Data\nArchive of datasets and articles from the 2022 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2022-01-04\nBring your own data from 2022!\nNA\nNA\n\n\n2\n2022-01-11\nBee Colony losses\nUSDA\nBee Informed\n\n\n3\n2022-01-18\nChocolate Bar ratings\nFlavors of Cacao\nWill Canniford on Kaggle\n\n\n4\n2022-01-25\nBoard games\nKaggle\nAlyssa Goldberg\n\n\n5\n2022-02-01\nDog breeds\nAmerican Kennel Club\nVox\n\n\n6\n2022-02-08\nTuskegee Airmen\nCommemorative Airforce (CAF) by way of the VA-TUG\nWikipedia & Air Force Historical Research Agency\n\n\n7\n2022-02-15\n#DuBoisChallenge2022\nAnthony Starks\nNightingale by DVS\n\n\n8\n2022-02-22\nWorld Freedom index\nUN and Freedom House\nFreedom House\n\n\n9\n2022-03-01\nAlternative Fuel Stations\nUS DOT\nEIA\n\n\n10\n2022-03-08\nErasmus student mobility\nData.Europa.eu\nWimdu.co\n\n\n11\n2022-03-15\nCRAN/BIOC Vignettes\nRobert Flight GitHub\nRobert Flight GitHub\n\n\n12\n2022-03-22\nBaby names\nUS babynames & nzbabynames\nEmily Kothe’s nzbabynames vignette\n\n\n13\n2022-03-29\nCollegiate Sports Budgets\nEquity in Athletics Data Analysis\nNPR\n\n\n14\n2022-04-05\nDigital Publications\nProject Oasis\nProject Oasis Report\n\n\n15\n2022-04-12\nIndoor Air Pollution\nOurWorldInData.org\nOurWorldInData.org\n\n\n16\n2022-04-19\nCrossword Puzzles and Clues\nCryptics.georgeho.org\nTowards Data Science\n\n\n17\n2022-04-26\nKaggle Hidden Gems\nKaggle\nKaggle - Notebooks of the Week\n\n\n18\n2022-05-03\nSolar/Wind utilities\nBerkeley Lab\nBerkeley Lab report\n\n\n19\n2022-05-10\nNYTimes best sellers\nPost45 Data\nFinding Trends in NY Times Best Sellers - Kailey Smith\n\n\n20\n2022-05-17\nEurovision\nEurovision\nTanya Shapiro\n\n\n21\n2022-05-24\nWomen’s Rugby\nWomen’s Rugby - ScrumQueens\nScrumQueens\n\n\n22\n2022-05-31\nCompany reputation poll\nAxios and Harris Poll\nThe Harris Poll\n\n\n23\n2022-06-07\nPride Corporate Accountability Project\nData For Progress\nData For Progress\n\n\n24\n2022-06-14\nUS Drought\nDrought.gov\nDrought.gov report\n\n\n25\n2022-06-21\nJuneteenth\nWEB DuBois style by Anthony Starks\nIsabella Benabaye’s blog on Juneteenth\n\n\n26\n2022-06-28\nUK Gender pay gap\ngender-pay-gap.service.gov.uk\nons.gov.uk\n\n\n27\n2022-07-05\nSan Francisco Rentals\nKate Pennington\nMatrix-Berkeley\n\n\n28\n2022-07-12\nEuropean flights\nEurocontrol\nec.europa.eu\n\n\n29\n2022-07-19\nTechnology Adoption\ndata.nber.org\nwww.cgdev.org\n\n\n30\n2022-07-26\nBring your own data\nNone\nNone\n\n\n31\n2022-08-02\nOregon Spotted Frog\nusgs.gov spotted frog data\nusgs.gov spotted-frog-article\n\n\n32\n2022-08-09\nFerris Wheels\nferriswheels\nferriswheels\n\n\n33\n2022-08-16\nOpen Source Psychometrics\nOpen-Source Psychometrics Project\nCharacter Personality\n\n\n34\n2022-08-23\nCHIP dataset\nCHIP Dataset\narxiv paper\n\n\n35\n2022-08-30\nPell Grants\nUS Dept of Education\npell R package\n\n\n36\n2022-09-06\nLEGO database\nrebrickable\nrebrickable\n\n\n37\n2022-09-13\nBigfoot\nData.World\nFinding Bigfoot\n\n\n38\n2022-09-20\nHydro Wastewater plants\nMacedo et al, 2022\nHydroWASTE v1.0\n\n\n39\n2022-09-27\nArtists in the USA\narts.gov\nArtists in the Workforce\n\n\n40\n2022-10-04\nProduct Hunt products\ncomponents.one\nThe Gamer and the Nihilist by Andrew Thompson\n\n\n41\n2022-10-11\nRavelry data\nravelry.com\n{ravelRy} R package\n\n\n42\n2022-10-18\nStranger things dialogue\n8flix.com\nfreeCodeCamp & ‘stringr things’\n\n\n43\n2022-10-25\nGreat British Bakeoff\nbakeoff pkg\nData Visualization in the Tidyverse - The Great Tidy Plot Off\n\n\n44\n2022-11-01\nHorror Movies\nThe Movie Database\nTanya Shapiro’s Horror Movies\n\n\n45\n2022-11-08\nRadio Stations\nWikipedia\nVisualizing the Geography of FM Radio\n\n\n46\n2022-11-15\nWeb page metrics\nhttpArchive.org\nDataWrapper & Data is Plural\n\n\n47\n2022-11-22\nUK Museums\nMuseWeb by way of Data Is Plural\nMuseWeb Key Findings\n\n\n48\n2022-11-29\nFIFA World Cup\nKaggle FIFA World Cup\nDataset Notebooks\n\n\n49\n2022-12-06\nElevators\nElevators data\nElevators data package and examples\n\n\n50\n2022-12-13\nMonthly State Retail Sales\nUS Census Bureau Monthly State Retails Sales\nInteractive Visualization from US Census Bureau\n\n\n51\n2022-12-20\nWeather Forecast Accuracy\nWeather Forecast Capstone Project\nWeather Forecast Capstone Project\n\n\n52\n2022-12-27\nStar Trek Timelines\nrtrek package\nrtrek package",
    "crumbs": [
      "Datasets",
      "2022"
    ]
  },
  {
    "objectID": "data/2023/2023-01-10/readme.html",
    "href": "data/2023/2023-01-10/readme.html",
    "title": "Project FeederWatch",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nProject FeederWatch\nThe data this week comes from the Project FeederWatch.\n\nFeederWatch is a November-April survey of birds that visit backyards, nature centers, community areas, and other locales in North America. Citizen scientists could birds in areas with plantings, habitat, water, or food that attracts birds. The schedule is completely flexible. People count birds as long as they like on days of their choosing, then enter their counts online. This allows anyone to track what is happening to birds around your home and to contribute to a continental data-set of bird distribution and abundance.\n\n\nFeederWatch data show which bird species visit feeders at thousands of locations across the continent every winter. The data also indicate how many individuals of each species are seen. This information can be used to measure changes in the winter ranges and abundances of bird species over time.\n\nA subset of the 2021 data is included for this TidyTuesday, but data available through 1988 is available for download on FeederWatch Raw Dataset Downloads page\n\nProject FeederWatch is operated by the Cornell Lab of Ornithology and Birds Canada. Since 2016, Project FeederWatch has been sponsored by Wild Bird Unlimited.\n\n\nAcknowledging FeederWatch.\n\n\nThe Cornell Lab of Ornithology and Birds Canada are committed to making data gathered through our citizen science programs freely accessible to students, journalists, and the general public.”\n\n\n“This unique dataset is completely dependent on the efforts of our network of volunteer participants. We ask that all data analysts give credit to the thousands of participants who have made FeederWatch possible, as well as to Birds Canada and the Cornell Lab of Ornithology for developing and managing the program.”\n\nExamples of analyses are included with the raw data and there is a section to Explore the data.\nMore details on analyzing this dataset:\nOver 30 Years of Standardized Bird Counts at Supplementary Feeding Stations in North America: A Citizen Science Data Report for Project FeederWatch by David N. Bonter and Emma I. Greig\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-01-10')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 02)\n\nfeederwatch &lt;- tuesdata$feederwatch\n\n# Or read in the data manually\n\nfeederwatch &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-10/PFW_2021_public.csv')\nsite_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv')\n\n\nData Dictionary\n\nThe Project FeederWatch Data Dictionary explains all fields and codes used in the database and is essential for understanding the dataset.\n\n\n\n\nPFW_2021_public.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nloc_id\ncharacter\nUnique identifier for each survey site\n\n\nlatitude\ndouble\nLatitude in decimal degrees for each survey site\n\n\nlongitude\ndouble\nLongitude in decimal degrees for each survey site\n\n\nsubnational1_code\ncharacter\nCountry abbreviation and State or Province abbreviation of each survey site. Note that the files may contain some “XX” locations. These are sites that were incorrectly placed by the user (e.g., site plotted in the ocean.)\n\n\nentry_technique\ncharacter\nVariable indicating method of site localization\n\n\nsub_id\ncharacter\nUnique identifier for each checklist\n\n\nobs_id\ncharacter\nUnique identifier for each observation of a species\n\n\nMonth\ndouble\nMonth of 1st day of two-day observation period\n\n\nDay\ndouble\nDay of 1st day of two-day observation period\n\n\nYear\ndouble\nYear of 1st day of two-day observation period\n\n\nPROJ_PERIOD_ID\ncharacter\nCalendar year of end of FeederWatch season\n\n\nspecies_code\ncharacter\nBird species observed, stored as 6-letter species codes\n\n\nhow_many\ndouble\nMaximum number of individuals seen at one time during observation period\n\n\nvalid\ndouble\nValidity of each observation based on flagging system\n\n\nreviewed\ndouble\nReview state of each observation based on flagging system\n\n\nday1_am\ndouble\nVariable indicating if observer watched during morning of count Day 1\n\n\nday1_pm\ndouble\nVariable indicating if observer watched during afternoon of count Day 1\n\n\nday2_am\ndouble\nVariable indicating if observer watched during morning of count Day 2\n\n\nday2_pm\ndouble\nVariable indicating if observer watched during afternoon of count Day 2\n\n\neffort_hrs_atleast\ndouble\nParticipant estimate of survey time for each checklist\n\n\nsnow_dep_atleast\ndouble\nParticipant estimate of minimum snow depth during a checklist\n\n\nData_Entry_Method\ncharacter\nData entry method for each checklist (e.g., web, mobile app or paper form)\n\n\n\n\n\nPFW_count_site_data_public_2021.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nloc_id\ncharacter\nloc_id\n\n\nproj_period_id\ncharacter\nproj_period_id\n\n\nyard_type_pavement\ndouble\nyard_type_pavement\n\n\nyard_type_garden\ndouble\nyard_type_garden\n\n\nyard_type_landsca\ndouble\nyard_type_landsca\n\n\nyard_type_woods\ndouble\nyard_type_woods\n\n\nyard_type_desert\ndouble\nyard_type_desert\n\n\nhab_dcid_woods\ndouble\nhabitat type decidious woods\n\n\nhab_evgr_woods\ndouble\nhabitat type evergreen woods\n\n\nhab_mixed_woods\ndouble\nhabitat type mixed woods\n\n\nhab_orchard\ndouble\nhabitat type orchard\n\n\nhab_park\ndouble\nhabitat type park\n\n\nhab_water_fresh\ndouble\nhabitat type fresh water\n\n\nhab_water_salt\ndouble\nhabitat type salt water\n\n\nhab_residential\ndouble\nhabitat type residential\n\n\nhab_industrial\ndouble\nhabitat type industrial\n\n\nhab_agricultural\ndouble\nhabitat type agricultural\n\n\nhab_desert_scrub\ndouble\nhabitat type desert_scrub\n\n\nhab_young_woods\ndouble\nhabitat type young_woods\n\n\nhab_swamp\ndouble\nhabitat type swamp\n\n\nhab_marsh\ndouble\nhabitat type marsh\n\n\nevgr_trees_atleast\ndouble\nminimum number of trees or shrubs in the count area - evergreen trees\n\n\nevgr_shrbs_atleast\ndouble\nminimum number of trees or shrubs in the count area - evergreen shrubs\n\n\ndcid_trees_atleast\ndouble\nminimum number of trees or shrubs in the count area - deciduous trees\n\n\ndcid_shrbs_atleast\ndouble\nminimum number of trees or shrubs in the count area - deciduous srubs\n\n\nfru_trees_atleast\ndouble\nminimum number of trees or shrubs in the count area - fruit trees\n\n\ncacti_atleast\ndouble\nminimum number of trees or shrubs in the count area - cacti\n\n\nbrsh_piles_atleast\ndouble\nminimum number of brush piles located within the count area\n\n\nwater_srcs_atleast\ndouble\nminimum number of water sources located within the count area\n\n\nbird_baths_atleast\ndouble\nminimum number of bird baths located within the count area\n\n\nnearby_feeders\ndouble\npresence or absense of feeders\n\n\nsquirrels\ndouble\ndo squirrels take food from feeders at least 3 times per week?\n\n\ncats\ndouble\nare cats active within 30 m of the feeders for at least 30 minutes 3 days per week?\n\n\ndogs\ndouble\nare dogs active within 30 m of the feeders for at least 30 minutes 3 days per week?\n\n\nhumans\ndouble\nare humans active within 30 m of the feeders for at least 30 minutes 3 days per week?\n\n\nhousing_density\ndouble\nparticipant estimated housing density of neighborhood\n\n\nfed_yr_round\ndouble\nfed_yr_round\n\n\nfed_in_jan\ndouble\nfed_in_jan\n\n\nfed_in_feb\ndouble\nfed_in_feb\n\n\nfed_in_mar\ndouble\nfed_in_mar\n\n\nfed_in_apr\ndouble\nfed_in_apr\n\n\nfed_in_may\ndouble\nfed_in_may\n\n\nfed_in_jun\ndouble\nfed_in_jun\n\n\nfed_in_jul\ndouble\nfed_in_jul\n\n\nfed_in_aug\ndouble\nfed_in_aug\n\n\nfed_in_sep\ndouble\nfed_in_sep\n\n\nfed_in_oct\ndouble\nfed_in_oct\n\n\nfed_in_nov\ndouble\nfed_in_nov\n\n\nfed_in_dec\ndouble\nfed_in_dec\n\n\nnumfeeders_suet\ndouble\nnumfeeders suet\n\n\nnumfeeders_ground\ndouble\nnumfeeders ground\n\n\nnumfeeders_hanging\ndouble\nnumfeeders hanging\n\n\nnumfeeders_platfrm\ndouble\nnumfeeders platfrm\n\n\nnumfeeders_humming\ndouble\nnumfeeders hummingbird\n\n\nnumfeeders_water\ndouble\nnumfeeders water dispensers\n\n\nnumfeeders_thistle\ndouble\nnumfeeders thistle\n\n\nnumfeeders_fruit\ndouble\nnumfeeders fruit\n\n\nnumfeeders_hopper\ndouble\nnumfeeders hopper\n\n\nnumfeeders_tube\ndouble\nnumfeeders tube\n\n\nnumfeeders_other\ndouble\nnumfeeders other\n\n\npopulation_atleast\ndouble\nparticipant estimated population of city or town\n\n\ncount_area_size_sq_m_atleast\ndouble\nparticipant estimated area of survey site\n\n\n\n\nCleaning Script\n# Download the raw data.\n\nPFW_2021_public &lt;- readr::read_csv(\"https://clo-pfw-prod.s3.us-west-2.amazonaws.com/data/PFW_2021_public.csv\")\ndplyr::glimpse(PFW_2021_public)\n\n# There are almost three million rows! The file is too big for github, let's\n# subsample.\n\nset.seed(424242)\nPFW_2021_public_subset &lt;- dplyr::slice_sample(PFW_2021_public, n = 1e5)\n\nreadr::write_csv(PFW_2021_public_subset, here::here(\"data\", \"2023\", \"2023-01-10\", \"PFW_2021_public.csv\"))"
  },
  {
    "objectID": "data/2023/2023-01-24/readme.html",
    "href": "data/2023/2023-01-24/readme.html",
    "title": "Alone",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nAlone\nThe data this week comes from the Alone data package by Dan Oehm.\n\nThis dataset contains data from the TV series Alone collected and shared by Dan Oehm. As described in Oehm’s blog post](https://gradientdescending.com/alone-r-package-datasets-from-the-survival-tv-series/), in the survival TV series ‘Alone,’ 10 survivalists are dropped in an extremely remote area and must fend for themselves. They aim to last 100 days in the Artic winter, living off the land through their survival skills, endurance, and mental fortitude.\n\n\nThis package contains four datasets:\n\n\n\nsurvivalists.csv: A data frame of survivalists across all 9 seasons detailing name and demographics, location and profession, result, days lasted, reasons for tapping out (detailed and categorised), and page URL.\n\n\n\n\nloadouts.csv: The rules allow each survivalist to take 10 items with them. This dataset includes information on each survivalist’s loadout. It has detailed item descriptions and a simplified version for easier aggregation and analysis\n\n\n\n\nepisodes.csv: This dataset contains details of each episode including the title, number of viewers, beginning quote, and IMDb rating. New episodes are added at the end of future seasons.\n\n\n\n\nseasons.csv: The season summary dataset includes location, latitude and longitude, and other season-level information. It includes the date of drop-off where the information exists.\n\n\nAcknowledging the Alone dataset\n\nDan Oehm: * Alone data package: https://github.com/doehm/alone * Alone data package blog post: https://gradientdescending.com/alone-r-package-datasets-from-the-survival-tv-series/\n\nExamples of analyses are included in Dan Oehm’s blog post.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-01-24')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 4)\n\nalone &lt;- tuesdata$alone\n\n# Or read in the data manually\n\nsurvivalists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-24/survivalists.csv')\nloadouts &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-24/loadouts.csv')\nepisodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-24/episodes.csv')\nseasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-01-24/seasons.csv')\n\n\nData Dictionary\n\n\n\nsurvivalists.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nseason\ndouble\nThe season number\n\n\nname\ncharacter\nName of the survivalist\n\n\nage\ndouble\nAge of the survivalist\n\n\ngender\ncharacter\nGender\n\n\ncity\ncharacter\nCity\n\n\nstate\ncharacter\nState\n\n\ncountry\ncharacter\nCountry\n\n\nresult\ndouble\nPlace survivalist finished in the season\n\n\ndays_lasted\ndouble\nThe number of days lasted in the game before tapping out or winning\n\n\nmedically_evacuated\nlogical\nIf the survivalist was medically evacuated from the game\n\n\nreason_tapped_out\ncharacter\nThe reason the survivalist tapped out of the game. NA means they were the winner. Reason being that technically if they won they never tapped out.\n\n\nreason_category\ncharacter\nA simplified category of the reason for tapping out\n\n\nteam\ncharacter\nThe team they were associated with (only for season 4)\n\n\nday_linked_up\ndouble\nDay the team members linked up (only for season 4)\n\n\nprofession\ncharacter\nProfession\n\n\nurl\ncharacter\nURL of cast page on the history channel website. Prefix URL with https://www.history.com/shows/alone/cast\n\n\n\n\n\nloadouts.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nversion\ncharacter\nCountry code for the version of the show\n\n\nseason\ndouble\nThe season number\n\n\nname\ncharacter\nName of the survivalist\n\n\nitem_number\ndouble\nItem number\n\n\nitem_detailed\ncharacter\nDetailed loadout item description\n\n\nitem\ncharacter\nLoadout item. Simplified for aggregation\n\n\n\n\n\nepisodes.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nversion\ncharacter\nCountry code for the version of the show\n\n\nseason\ndouble\nThe season number\n\n\nepisode_number_overall\ndouble\nEpisode number across seasons\n\n\nepisode\ndouble\nEpisode\n\n\ntitle\ncharacter\nEpisode title\n\n\nair_date\ndouble\nDate the episode originally aired\n\n\nviewers\ndouble\nNumber of viewers in the US (millions)\n\n\nquote\ncharacter\nThe beginning quote\n\n\nauthor\ncharacter\nAuthor of the beginning quote\n\n\nimdb_rating\ndouble\nIMDb rating of the episode\n\n\nn_ratings\ndouble\nNumber of ratings given for the episode\n\n\n\n\n\nseasons.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nversion\ncharacter\nCountry code for the version of the show\n\n\nseason\ndouble\nThe season number\n\n\nlocation\ncharacter\nLocation\n\n\ncountry\ncharacter\nCountry\n\n\nn_survivors\ndouble\nNumber of survivalists in the season. In season 4 there were 7 teams of 2.\n\n\nlat\ndouble\nLatitude\n\n\nlon\ndouble\nLongitude\n\n\ndate_drop_off\ndouble\nThe date the survivalists were dropped off\n\n\n\n\nCleaning Script\nNo data cleaning"
  },
  {
    "objectID": "data/2023/2023-02-07/readme.html",
    "href": "data/2023/2023-02-07/readme.html",
    "title": "Big Tech Stock Prices",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nBig Tech Stock Prices\nThe data this week comes from Yahoo Finance via Kaggle (by Evan Gower).\n\nThis dataset consists of the daily stock prices and volume of 14 different tech companies, including Apple (AAPL), Amazon (AMZN), Alphabet (GOOGL), and Meta Platforms (META) and more!\n\nA number of articles have examined the collapse of “Big Tech” stock prices, including this article from morningstar.com.\nNote: All stock_symbols have 3271 prices, except META (2688) and TSLA (3148) because they were not publicly traded for part of the period examined.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-02-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 6)\n\nbig_tech_stock_prices &lt;- tuesdata$big_tech_stock_prices\nbig_tech_companies &lt;- tuesdata$big_tech_companies\n\n# Or read in the data manually\n\nbig_tech_stock_prices &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-07/big_tech_stock_prices.csv')\nbig_tech_companies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-07/big_tech_companies.csv')\n\n\nData Dictionary\n\n\n\nbig_tech_stock_prices.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstock_symbol\ncharacter\nstock_symbol\n\n\ndate\ndouble\ndate\n\n\nopen\ndouble\nThe price at market open.\n\n\nhigh\ndouble\nThe highest price for that day.\n\n\nlow\ndouble\nThe lowest price for that day.\n\n\nclose\ndouble\nThe price at market close, adjusted for splits.\n\n\nadj_close\ndouble\nThe closing price after adjustments for all applicable splits and dividend distributions. Data is adjusted using appropriate split and dividend multipliers, adhering to Center for Research in Security Prices (CRSP) standards.\n\n\nvolume\ndouble\nThe number of shares traded on that day.\n\n\n\n\n\nbig_tech_companies.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstock_symbol\ncharacter\nstock_symbol\n\n\ncompany\ncharacter\nFull name of the company.\n\n\n\n\nCleaning Script\nlibrary(fs)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\n# Source for datasets. The datasets were downloaded and extracted to an\n# \"archive\" folder within the working directory for processing, but they are not\n# included in this repo.\n\"https://www.kaggle.com/datasets/evangower/big-tech-stock-prices\"\n\n# This is mostly equivalent to fs::dir_map, but we need to keep the info from\n# the filename.\nbig_tech_stock_prices_list &lt;- purrr::map(\n  fs::dir_ls(\n    here::here(\"data\", \"2023\", \"2023-02-07\", \"archive\"),\n    glob = \"*.csv\"\n  ),\n  \\(path) {\n    ticker &lt;- fs::path_file(path) |&gt; fs::path_ext_remove()\n    readr::read_csv(\n      file = path,\n      col_types = cols(\n        Date = col_date(format = \"\"),\n        Open = col_double(),\n        High = col_double(),\n        Low = col_double(),\n        Close = col_double(),\n        `Adj Close` = col_double(),\n        Volume = col_double()\n      )\n    ) |&gt; \n      dplyr::mutate(stock_symbol = ticker, .before = 1)\n  }\n)\n\nbig_tech_stock_prices &lt;- purrr::list_rbind(big_tech_stock_prices_list) |&gt; \n  janitor::clean_names()\ndplyr::glimpse(big_tech_stock_prices)\n\nreadr::write_csv(\n  big_tech_stock_prices,\n  here::here(\n    \"data\", \"2023\", \"2023-02-07\",\n    \"big_tech_stock_prices.csv\"\n  )\n)\n\nbig_tech_stock_prices |&gt; \n  dplyr::count(stock_symbol, sort = TRUE)\n\n\n# Make a lookup for the symbols.\ntibble::tibble(\n  stock_symbol = c(\n    \"AAPL\",\n    \"ADBE\",\n    \"AMZN\",\n    \"CRM\",\n    \"CSCO\",\n    \"GOOGL\",\n    \"IBM\",\n    \"INTC\",\n    \"META\",\n    \"MSFT\",\n    \"NFLX\",\n    \"NVDA\",\n    \"ORCL\",\n    \"TSLA\"\n  ),\n  company = c(\n    \"Apple Inc.\",\n    \"Adobe Inc.\",\n    \"Amazon.com, Inc.\",\n    \"Salesforce, Inc.\",\n    \"Cisco Systems, Inc.\",\n    \"Alphabet Inc.\",\n    \"International Business Machines Corporation\",\n    \"Intel Corporation\",\n    \"Meta Platforms, Inc.\",\n    \"Microsoft Corporation\",\n    \"Netflix, Inc.\",\n    \"NVIDIA Corporation\",\n    \"Oracle Corporation\",\n    \"Tesla, Inc.\"\n  )\n) |&gt; \n  readr::write_csv(\n    here::here(\n      \"data\", \"2023\", \"2023-02-07\",\n      \"big_tech_companies.csv\"\n    )\n  )"
  },
  {
    "objectID": "data/2023/2023-02-21/readme.html",
    "href": "data/2023/2023-02-21/readme.html",
    "title": "Bob Ross Paintings",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nBob Ross Paintings\nThe data this week comes from Jared Wilber’s data on Bob Ross Paintings via @frankiethull Bob Ross Colors data package.\n\nThis is data from the paintings of Bob Ross featured in the TV Show ‘The Joy of Painting’.\n\n@frankiethull created an R data package {BobRossColors} with information on the palettes that leveraged imgpalr to mine divergent and qualitative colors from each painting image. In addition, unique Bob Ross named colors are in the package as well.\nIn the github repository of the dataset, there are also pngs of the paintings themselves!\nYou might also want to check out our previous Bob Ross dataset from 2019-08-06 to see if there are correlations between named objects and named colors!\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-02-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 8)\n\nbob_ross &lt;- tuesdata$bob_ross\n\n# Or read in the data manually\n\nbob_ross &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-02-21/bob_ross.csv')\n\n\nData Dictionary\n\n\n\nbob_ross.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npainting_index\ndouble\nPainting number as enumerated in collection.\n\n\nimg_src\ncharacter\nUrl path to image.\n\n\npainting_title\ncharacter\nTitle of the painting.\n\n\nseason\ndouble\nSeason of ‘The Joy of Painting’ in which the painting was featured.\n\n\nepisode\ndouble\nEpisode of ‘The Joy of Painting’ in which the painting was featured.\n\n\nnum_colors\ndouble\nNumber of unique colors used in the painting.\n\n\nyoutube_src\ncharacter\nYoutube video of episode featuring the painting.\n\n\ncolors\ncharacter\nList of colors used in the painting.\n\n\ncolor_hex\ncharacter\nList of colors (hexadecimal code) used in the painting.\n\n\nBlack_Gesso\nlogical\nBlack_Gesso used\n\n\nBright_Red\nlogical\nBright_Red used\n\n\nBurnt_Umber\nlogical\nBurnt_Umber used\n\n\nCadmium_Yellow\nlogical\nCadmium_Yellow used\n\n\nDark_Sienna\nlogical\nDark_Sienna used\n\n\nIndian_Red\nlogical\nIndian_Red used\n\n\nIndian_Yellow\nlogical\nIndian_Yellow used\n\n\nLiquid_Black\nlogical\nLiquid_Black used\n\n\nLiquid_Clear\nlogical\nLiquid_Clear used\n\n\nMidnight_Black\nlogical\nMidnight_Black used\n\n\nPhthalo_Blue\nlogical\nPhthalo_Blue used\n\n\nPhthalo_Green\nlogical\nPhthalo_Green used\n\n\nPrussian_Blue\nlogical\nPrussian_Blue used\n\n\nSap_Green\nlogical\nSap_Green used\n\n\nTitanium_White\nlogical\nTitanium_White used\n\n\nVan_Dyke_Brown\nlogical\nVan_Dyke_Brown used\n\n\nYellow_Ochre\nlogical\nYellow_Ochre used\n\n\nAlizarin_Crimson\nlogical\nAlizarin_Crimson used\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\n\n# Read in the data\nbob_ross &lt;- read_csv(\n  \"https://raw.githubusercontent.com/jwilber/Bob_Ross_Paintings/master/data/bob_ross_paintings.csv\",\n) \n\nglimpse(bob_ross)\n\n# The first column doesn't contain data that we need, so we can remove it\n\nbob_ross &lt;- select(bob_ross, -1)\n\n# Several columns refer to presence/absence of named colors.\n\nbob_ross &lt;- bob_ross |&gt; \n  mutate(\n    across(Black_Gesso:Alizarin_Crimson, as.logical)\n  )\n\n# Save the data.\nwrite_csv(\n  bob_ross,\n  here::here(\n    \"data\", \"2023\", \"2023-02-21\",\n    \"bob_ross.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-03-07/readme.html",
    "href": "data/2023/2023-03-07/readme.html",
    "title": "Numbats in Australia",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nNumbats in Australia\nThe data this week comes from the Atlas of Living Australia. Thanks to Di Cook for preparing this week’s dataset!\nThis Numbat page at the Atlas of Living Australia talks about these endangered species in greater detail.\nA csv file of numbat sightings is provided. The code to refresh the data is below.\nQuestions that would be interesting to answer are:\n\nWhere do you find numbats in Australia?\nWas the distribution more widespread historically? (You may need to exclude zoo reported observations.)\nWhat time of day do numbat sightings occur?\nAre they more frequent in the summer or winter?\nAre numbats seen more on sunny and warm days than cloudy, wet, cold days?\nDo sightings happen more on week days than weekends?\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-03-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 10)\n\nnumbats &lt;- tuesdata$numbats\n\n# Or read in the data manually\n\nnumbats &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-03-07/numbats.csv')\n\n\nData Dictionary\n\n\n\nnumbats.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndecimalLatitude\ndouble\ndecimalLatitude\n\n\ndecimalLongitude\ndouble\ndecimalLongitude\n\n\neventDate\ndatetime\neventDate\n\n\nscientificName\nfactor\nEither “Myrmecobius fasciatus” or “Myrmecobius fasciatus rufus”\n\n\ntaxonConceptID\nfactor\nThe URL for this (sub)species\n\n\nrecordID\ncharacter\nrecordID\n\n\ndataResourceName\nfactor\ndataResourceName\n\n\nyear\ninteger\nThe 4-digit year of the event (when available)\n\n\nmonth\nfactor\nThe 3-letter month abbreviation of the event (when available)\n\n\nwday\nfactor\nThe 3-letter weekday abbreviation of the event (when available)\n\n\nhour\ninteger\nThe hour of the event (when available)\n\n\nday\ndate\nThe date of the event (when available)\n\n\ndryandra\nlogical\nwhether the observation was in Dryandra Woodland\n\n\nprcp\ndouble\nPrecipitation on that day in Dryandra Woodland (when relevant), in millimeters\n\n\ntmax\ndouble\nMaximum temperature on that day in Dryandra Woodland (when relevant), in degrees Celsius\n\n\ntmin\ndouble\nMinimum temperature on that day in Dryandra Woodland (when relevant), in degrees Celsius\n\n\n\n\nCleaning Script\n# Cleaning script provided at\n# https://github.com/numbats/numbats-tidytuesday/blob/main/code/data.R Slightly\n# updated here.\n\nlibrary(galah) # API to ALA\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(rnoaa)\nlibrary(here)\n\n# Downloading data is free but you need to \n# create an account https://www.ala.org.au then\n# use this email address to pull data.\n# galah_config(email = YOUR_EMAIL_ADDRESS)\nid &lt;- galah_identify(\"numbat\")\n\nnumbats &lt;- atlas_occurrences(identify = id)\nnumbats &lt;- numbats %&gt;%\n  mutate(\n    year = year(eventDate),\n    month = month(eventDate, label=TRUE, abbr=TRUE),\n    wday = wday(eventDate, label=TRUE, abbr=TRUE, week_start = 1),\n    hour = hour(eventDate),\n    day = ymd(as.Date(eventDate))\n  )\n\nnarrogin &lt;- meteo_pull_monitors(\n  monitors = \"ASN00010614\",\n  var = c(\"PRCP\", \"TMAX\", \"TMIN\"),\n  date_min = \"2005-01-01\",\n  date_max = \"2023-02-23\")\n\nnarrogin %&gt;%\n  pivot_longer(cols = prcp:tmin, names_to = \"var\", values_to = \"value\") %&gt;%\n  mutate(day = lubridate::yday(date), year = lubridate::year(date)) %&gt;%\n  ggplot(aes(x = day, y= year, fill = is.na(value))) +\n  geom_tile() +\n  theme_minimal() +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(vars(var), ncol = 1) +\n  scale_fill_brewer(palette = \"Dark2\", name = \"missing\") +\n  xlab(\"Day of the year\")\n\nnarrogin_latlon &lt;- tibble(lon = 117.1782, lat = -32.9310)\n\nwithin_rad &lt;- function(x, y, lon, lat, km) {\n  deg &lt;- km/111\n  inside &lt;- sqrt((lon-x)^2 + (lat-y)^2) &lt; deg\n  return(inside)\n}\n\n# Only sites within 50km radius of Narrogin weather station\n# which is Dryandra Woodlands\nnumbats &lt;- numbats %&gt;%\n  mutate(\n    dryandra = within_rad(\n      decimalLongitude, decimalLatitude, \n      narrogin_latlon$lon, narrogin_latlon$lat,\n      50\n    )\n  )\n  \nnumbats &lt;- numbats %&gt;% \n  left_join(narrogin, by = join_by(day == date)) %&gt;%\n  mutate(\n    prcp = if_else(dryandra, prcp, NA, missing = NA),\n    tmax = if_else(dryandra, tmax, NA, missing = NA),\n    tmin = if_else(dryandra, tmin, NA, missing = NA)\n  ) %&gt;%\n  select(-id)\n\n# Things are only in this dataset if they were PRESENT.\nnumbats &lt;- numbats |&gt; \n  select(-occurrenceStatus)\n\n# Those last three values are in values to coerce them to integers, and might be\n# confusing. Translate them to doubles.\nnumbats &lt;- numbats |&gt; \n  mutate(\n    prcp = prcp/10,\n    tmax = tmax/10,\n    tmin = tmin/10\n  )\n\nwrite_csv(\n  numbats, \n  file = here::here(\n    \"data\",\n    \"2023\",\n    \"2023-03-07\",\n    \"numbats.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-03-21/readme.html",
    "href": "data/2023/2023-03-21/readme.html",
    "title": "Programming Languages",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nProgramming Languages\nThe data this week comes from the Programming Language DataBase. Thanks to Jesus M. Castagnetto for the suggestion!\nThe PLDB has a blog with numerous articles exploring the data, such as Does every programming language have line comments?.\nThe data is user-submitted, so you might want to confirm the accuracy of anything particularly surprising that you find before stating it with certainty!\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-03-21')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 12)\n\nlanguages &lt;- tuesdata$languages\n\n# Or read in the data manually\n\nlanguages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-03-21/languages.csv')\n\n\nData Dictionary\n\n\n\nlanguages.csv\nThe full data dictionary is available from PLDB.com.\n\n\n\nvariable\nclass\ndescription\n\n\n\n\npldb_id\ncharacter\nA standardized, uniquified version of the language name, used as an ID on the PLDB site.\n\n\ntitle\ncharacter\nThe official title of the language.\n\n\ndescription\ncharacter\nDescription of the repo on GitHub.\n\n\ntype\ncharacter\nWhich category in PLDB’s subjective ontology does this entity fit into.\n\n\nappeared\ndouble\nWhat year was the language publicly released and/or announced?\n\n\ncreators\ncharacter\nName(s) of the original creators of the language delimited by ” and ”\n\n\nwebsite\ncharacter\nURL of the official homepage for the language project.\n\n\ndomain_name\ncharacter\nIf the project website is on its own domain.\n\n\ndomain_name_registered\ndouble\nWhen was this domain first registered?\n\n\nreference\ncharacter\nA link to more info about this entity.\n\n\nisbndb\ndouble\nBooks about this language from ISBNdb.\n\n\nbook_count\ndouble\nComputed; the number of books found for this language at isbndb.com\n\n\nsemantic_scholar\ninteger\nPapers about this language from Semantic Scholar.\n\n\nlanguage_rank\ndouble\nComputed; A rank for the language, taking into account various online rankings. The computation for this column is not currently clear.\n\n\ngithub_repo\ncharacter\nURL of the official GitHub repo for the project if it hosted there.\n\n\ngithub_repo_stars\ndouble\nHow many stars of the repo?\n\n\ngithub_repo_forks\ndouble\nHow many forks of the repo?\n\n\ngithub_repo_updated\ndouble\nWhat year was the last commit made?\n\n\ngithub_repo_subscribers\ndouble\nHow many subscribers to the repo?\n\n\ngithub_repo_created\ndouble\nWhen was the Github repo for this entity created?\n\n\ngithub_repo_description\ncharacter\nDescription of the repo on GitHub.\n\n\ngithub_repo_issues\ndouble\nHow many isses on the repo?\n\n\ngithub_repo_first_commit\ndouble\nWhat year the first commit made in this git repo?\n\n\ngithub_language\ncharacter\nGitHub has a set of supported languages as defined here\n\n\ngithub_language_tm_scope\ncharacter\nThe TextMate scope that represents this programming language.\n\n\ngithub_language_type\ncharacter\nEither data, programming, markup, prose, or nil.\n\n\ngithub_language_ace_mode\ncharacter\nA String name of the Ace Mode used for highlighting whenever a file is edited. This must match one of the filenames in http://git.io/3XO_Cg. Use “text” if a mode does not exist.\n\n\ngithub_language_file_extensions\ncharacter\nAn Array of associated extensions (the first one is considered the primary extension, the others should be listed alphabetically).\n\n\ngithub_language_repos\ndouble\nHow many repos for this language does GitHub report?\n\n\nwikipedia\ncharacter\nURL of the entity on Wikipedia, if and only if it has a page dedicated to it.\n\n\nwikipedia_daily_page_views\ndouble\nHow many page views per day does this Wikipedia page get? Useful as a signal for rankings. Available via WP api.\n\n\nwikipedia_backlinks_count\ndouble\nHow many pages on WP link to this page?\n\n\nwikipedia_summary\ncharacter\nWhat is the text summary of the language from the Wikipedia page?\n\n\nwikipedia_page_id\ndouble\nWaht is the internal ID for this entity on WP?\n\n\nwikipedia_appeared\ndouble\nWhen does Wikipedia claim this entity first appeared?\n\n\nwikipedia_created\ndouble\nWhen was the Wikipedia page for this entity created?\n\n\nwikipedia_revision_count\ndouble\nHow many revisions does this page have?\n\n\nwikipedia_related\ncharacter\nWhat languages does Wikipedia have as related?\n\n\nfeatures_has_comments\nlogical\nDoes this language have a comment character?\n\n\nfeatures_has_semantic_indentation\nlogical\nDoes indentation have semantic meaning in this language?\n\n\nfeatures_has_line_comments\nlogical\nDoes this language support inline comments (as opposed to comments that must span an entire line)?\n\n\nline_comment_token\ncharacter\nDefined as a token that can be placed anywhere on a line and starts a comment that cannot be stopped except by a line break character or end of file.\n\n\nlast_activity\ndouble\nComputed; The most recent of any year field in the PLDB for this language.\n\n\nnumber_of_users\ndouble\nComputed; “Crude user estimate from a linear model.\n\n\nnumber_of_jobs\ndouble\nComputed; The estimated number of job openings for programmers in this language.\n\n\norigin_community\ncharacter\nIn what community(ies) did the language first originate?\n\n\ncentral_package_repository_count\ndouble\nNumber of packages in a central repository. If this value is not known, it is set to 0 (so “0” can mean “no repository exists”, “the repository exists but is empty” (unlikely), or “we do not know if a repository exists”. This value is definitely incorrect for R.\n\n\nfile_type\ncharacter\nWhat is the file encoding for programs in this language?\n\n\nis_open_source\nlogical\nIs it an open source project?\n\n\n\n\nCleaning Script\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\n\nlanguages_url &lt;- \"https://pldb.com/languages.csv\"\n\nlanguages_raw &lt;- read_csv(\n  languages_url, \n  # Some of the columns are very sparse, so have readr use everything for\n  # guessing.\n  guess_max = 4303\n) |&gt; \n  clean_names() |&gt;\n  # The semantic_scholar column is misformed for a handful of languages. It's ok\n  # to introduce NAs here.\n  mutate(\n    semantic_scholar = as.integer(semantic_scholar)\n  )\n  \n\n# Almost all columns are almost completely empty. Keep the columns that have\n# more than 10% coverage.\ngood_lang_cols &lt;- languages_raw |&gt; \n  summarize(\n    across(everything(), ~sum(!is.na(.x)))\n  ) |&gt;\n  tidyr::pivot_longer(\n    everything(),\n    names_to = \"column\",\n    values_to = \"non_empty\"\n  ) |&gt; \n  mutate(\n    coverage = non_empty/nrow(languages_raw)\n  ) |&gt; \n  filter(coverage &gt; 0.1) |&gt; \n  pull(column)\n\nlanguages &lt;- languages_raw |&gt; \n  select(!!!good_lang_cols) |&gt; \n  # This column references a site that doesn't want to be used in projects.\n  select(-hopl) |&gt; \n  # A couple columns are only relevant in the context of the mixed table with\n  # non-languages included.\n  select(-number_of_repos, -rank, -paper_count) |&gt; \n  # Some columns are internal metadata that is no longer true with this subset.\n  select(-fact_count, -example_count) |&gt; \n  # I looked at R specifically, and the \"country\" column was inaccurate. Let's\n  # not confuse people with known bad data.\n  select(-country) |&gt; \n  # Organize the columns.\n  select(\n    pldb_id,\n    title,\n    description,\n    type,\n    appeared,\n    creators,\n    website,\n    starts_with(\"domain_name\"),\n    reference,\n    isbndb,\n    book_count,\n    semantic_scholar,\n    language_rank,\n    starts_with(\"github_\"),\n    starts_with(\"wikipedia\"),\n    starts_with(\"features_\"),\n    line_comment_token,\n    everything()\n  )\n\nwrite_csv(\n  languages, \n  file = here::here(\n    \"data\",\n    \"2023\",\n    \"2023-03-21\",\n    \"languages.csv\"\n  )\n)\n\n# Use the online dictionary to help with the dictionary in the post.\ndictionary_url &lt;- \"https://pldb.com/columns.csv\"\ndictionary &lt;- read_csv(dictionary_url) |&gt; \n  clean_names() |&gt; \n  # I only need the column name and description for our dictionary.\n  select(column, description) |&gt; \n  # I cleaned the column names, so let's do the same here.\n  mutate(\n    column = make_clean_names(column)\n  ) |&gt; \n  # We don't need the extras.\n  filter(\n    column %in% colnames(languages)\n  )\n# Arrange dictionary to match the order of colnames(languages).\ndictionary &lt;- dictionary[match(colnames(languages), dictionary$column), ]\n\ndictionary |&gt; \n  mutate(\n    class = map_chr(languages, typeof)\n  ) |&gt; \n  select(\n    variable = column,\n    class,\n    description\n  ) |&gt; \n  knitr::kable()"
  },
  {
    "objectID": "data/2023/2023-04-04/readme.html",
    "href": "data/2023/2023-04-04/readme.html",
    "title": "Premier League Match Data 2021-2022",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nPremier League Match Data 2021-2022\nThe data this week comes from the Premier League Match Data 2021-2022 via Evan Gower on Kaggle.\nYou can explore match day statistics of every game and every team during the 2021-22 season of the English Premier League Data.\nData includes teams playing, date, referee, and stats for home and away side such as fouls, shots, cards, and more! Also included is a dataset of the weekly rankings for the season.\nThe data was collected from the official website of the Premier League. Evan then cleaned the data using google sheets.\nEvan did an analysis of Who wins the EPL if games end at half time? and there’s an article from the Athletic about fouls conceded per yellow card article.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-04-04')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 14)\n\nsoccer &lt;- tuesdata$soccer\n\n# Or read in the data manually\n\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-04-04/soccer21-22.csv')\n\n\nData Dictionary\n\n\n\nsoccer21-22.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nDate\ncharacter\nThe date when the match was played\n\n\nHomeTeam\ncharacter\nThe home team\n\n\nAwayTeam\ncharacter\nThe away team\n\n\nFTHG\ndouble\nFull time home goals\n\n\nFTAG\ndouble\nFull time away goals\n\n\nFTR\ncharacter\nFull time result\n\n\nHTHG\ndouble\nHalftime home goals\n\n\nHTAG\ndouble\nHalftime away goals\n\n\nHTR\ncharacter\nHalftime results\n\n\nReferee\ncharacter\nReferee of the match\n\n\nHS\ndouble\nNumber of shots taken by the home team\n\n\nAS\ndouble\nNumber of shots taken by the away team\n\n\nHST\ndouble\nNumber of shots on target by the home team\n\n\nAST\ndouble\nNumber of shots on target by the away team\n\n\nHF\ndouble\nNumber of fouls by the home team\n\n\nAF\ndouble\nNumber of fouls by the away team\n\n\nHC\ndouble\nNumber of corners taken by the home team\n\n\nAC\ndouble\nNumber of corners taken by the away team\n\n\nHY\ndouble\nNumber of yellow cards received by the home team\n\n\nAY\ndouble\nNumber of yellow cards received by the away team\n\n\nHR\ndouble\nNumber of red cards received by the home team\n\n\nAR\ndouble\nNumber of red cards received by the away team\n\n\n\n\nCleaning Script\nNo data cleaning"
  },
  {
    "objectID": "data/2023/2023-04-18/readme.html",
    "href": "data/2023/2023-04-18/readme.html",
    "title": "Neolithic Founder Crops",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nNeolithic Founder Crops\nThe data this week comes from The “Neolithic Founder Crops”” in Southwest Asia: Research Compendium. “Revisiting the concept of the ‘Neolithic Founder Crops’ in southwest Asia” is an open-access research paper that uses the data. Thank you for sharing your research, @joeroe!\nAccording to the social media thread about this dataset:\n\nEight ‘founder crops’ — emmer wheat, einkorn wheat, barley, lentil, pea, chickpea, bitter vetch, and flax — have long been thought to have been the bedrock of #Neolithic economies. … We found that Neolithic economies were much more diverse than previously thought, incorporating dozens of species of cereals, legumes, small-seeded grasses, brassicas, pseudocereals, sedges, flowering plants, trees, and shrubs. Free-threshing wheat, grass pea, faba bean, and ‘new’ glume wheat were especially widely cultivated.\n\nRead the thread for context about this data!\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-04-18')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 16)\n\nfounder_crops &lt;- tuesdata$founder_crops\n\n# Or read in the data manually\n\nfounder_crops &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-04-18/founder_crops.csv')\n\n\nData Dictionary\n\n\n\nfounder_crops.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsource\ncharacter\nthe source database\n\n\nsource_id\ncharacter\nid of this record in the source database\n\n\nsource_site_name\ncharacter\nname of the site in the source database\n\n\nsite_name\ncharacter\nstandardized site name\n\n\nlatitude\ndouble\nlatitude\n\n\nlongitude\ndouble\nlongitude\n\n\nphase\ncharacter\nphase\n\n\nphase_description\ncharacter\nphase_description\n\n\nphase_code\ncharacter\nphase_code\n\n\nage_start\ndouble\noldest date for the record, in years before 1950 CE (years BP)\n\n\nage_end\ndouble\nmost recent date for the record, in years before 1950 CE (years BP)\n\n\ntaxon_source\ncharacter\ntaxonomy as stated in the course database\n\n\nn\ndouble\nnumber of individuals in the sample\n\n\nprop\ndouble\nproportion of this sample that contains this crop\n\n\nreference\ncharacter\npapers describing this data\n\n\ntaxon_detail\ncharacter\ncanonical name for this taxonomic group\n\n\ntaxon\ncharacter\ntaxonomic details for this sample; this and the previous column may have been swapped in the source\n\n\ngenus\ncharacter\ngenus\n\n\nfamily\ncharacter\nfamily\n\n\ncategory\ncharacter\nbroad category for this sample\n\n\nfounder_crop\ncharacter\ntraditional founder crop to which this sample belongs\n\n\nedibility\ncharacter\nparts of the plant that are edible, if any\n\n\ngrass_type\ncharacter\nfor grasses, the category for this sample\n\n\nlegume_type\ncharacter\nfor legumes, the category for this sample\n\n\n\n\nCleaning Script\n# All packages used in this script:\nlibrary(tidyverse)\nlibrary(here)\n\nurl &lt;- \"https://raw.githubusercontent.com/joeroe/SWAsiaNeolithicFounderCrops/main/analysis/data/derived_data/swasia_neolithic_flora.tsv\"\nfounder_crops &lt;- readr::read_tsv(url) |&gt; \n  # de-duplicate the reference column\n  dplyr::mutate(\n    reference = purrr::map_chr(\n      reference,\n      \\(ref) {\n        if (is.na(ref)) {\n          return(NA)\n        }\n        ref |&gt; \n          stringr::str_split_1(\";\") |&gt; \n          unique() |&gt; \n          paste(collapse = \";\")\n      }\n    )\n  )\n\nreadr::write_csv(\n  founder_crops,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-04-18\",\n    \"founder_crops.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-05-02/readme.html",
    "href": "data/2023/2023-05-02/readme.html",
    "title": "The Portal Project",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nThe Portal Project\nThe data this week comes from the Portal Project. This is a long-term ecological research site studying the dynamics of desert rodents, plants, ants and weather in Arizona.\n\nThe Portal Project is a long-term ecological study being conducted near Portal, AZ. Since 1977, the site has been used to study the interactions among rodents, ants and plants and their respective responses to climate. To study the interactions among organisms, they experimentally manipulate access to 24 study plots. This study has produced over 100 scientific papers and is one of the longest running ecological studies in the U.S.\n\nThe Weecology research group monitors rodents, plants, ants, and weather. All data from the Portal Project are made openly available in near real-time so that they can provide the maximum benefit to scientific research and outreach. The core dataset is managed using an automated living data workflow run using GitHub and Continuous Analysis.\nThis dataset focuses on the rodent data. Full data is available through these resources:\n\nGitHub Data Repository\nLive Updating Zenodo Archive\nData Paper\nMethods Documentation\n\nThe Portal Project data can also be accessed through the Data Retriever, a package manager for data.\nData Retriever\nA teaching focused version of the dataset is also maintained with some of the complexities of the data removed to make it easy to use for computational training purposes. This dataset serves as the core dataset for the Data Carpentry Ecology material and has been downloaded almost 50,000 times.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-05-02')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 18)\n\nplots &lt;- tuesdata$plots\nspecies &lt;- tuesdata$species\nsurveys &lt;- tuesdata$surveys\n\n\n# Or read in the data manually\n\nplots &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-02/plots.csv')\nspecies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-02/species.csv')\nsurveys &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-02/surveys.csv')\n\n\n\nData Dictionary\n\n\n\nplots.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nplot\ndouble\nPlot number\n\n\ntreatment\ncharacter\nTreatment type\n\n\n\n\n\nspecies.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ncharacter\nSpecies\n\n\nscientificname\ncharacter\nScientific Name\n\n\ntaxa\ncharacter\nTaxa\n\n\ncommonname\ncharacter\nCommon Name\n\n\ncensustarget\ndouble\nTarget species (0 or 1)\n\n\nunidentified\ndouble\nUnidentified (0 or 1)\n\n\nrodent\ndouble\nRodent (0 or 1)\n\n\ngranivore\ndouble\nGranivore (0 or 1)\n\n\nminhfl\ndouble\nMinimum hindfoot length\n\n\nmeanhfl\ndouble\nMean hindfoot length\n\n\nmaxhfl\ndouble\nMaximum hindfoot length\n\n\nminwgt\ndouble\nMinimum weight\n\n\nmeanwgt\ndouble\nMean weight\n\n\nmaxwgt\ndouble\nMaximum weight\n\n\njuvwgt\ndouble\nJuvenile weight\n\n\n\n\n\nsurveys.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncensusdate\ndouble\nCensus date\n\n\nmonth\ndouble\nMonth\n\n\nday\ndouble\nDay\n\n\nyear\ndouble\nYear\n\n\ntreatment\ncharacter\nTreatment type\n\n\nplot\ndouble\nPlot number\n\n\nstake\ndouble\nStake number\n\n\nspecies\ncharacter\nSpecies code\n\n\nsex\ncharacter\nSex\n\n\nreprod\ncharacter\nReproductive condition\n\n\nage\ncharacter\nAge\n\n\ntestes\ncharacter\nTestes (Scrotal, Recent, or Minor)\n\n\nvagina\ncharacter\nVagina (Swollen, Plugged, or Both)\n\n\npregnant\ncharacter\nPregnant\n\n\nnipples\ncharacter\nNipples (Enlarged, Swollen, or Both)\n\n\nlactation\ncharacter\nLactating\n\n\nhfl\ndouble\nHindfoot length\n\n\nwgt\ndouble\nWeight\n\n\ntag\ncharacter\nPrimary individual identifier\n\n\nnote2\ncharacter\nNewly tagged individual for ‘tag’\n\n\nltag\ncharacter\nSecondary tag information when ear tags were used in both ears\n\n\nnote3\ncharacter\nNewly tagged individual for ‘ltag’\n\n\n\n\nCleaning Script\nThanks to @ethanwhite for the data cleaning script. This script downloads the data using the {portalr} package. It filters for the species and plot data, and years greater than 1977.\n# All packages used in this script:\nlibrary(portalr)\nlibrary(dplyr)\n\ndownload_observations(\".\")\ndata_tables &lt;- load_rodent_data()\n\nspecies_data &lt;- data_tables[[\"species_table\"]]\nplots_data &lt;- data_tables[[\"plots_table\"]]\n\nplot_treatments &lt;- plots_data %&gt;%\n  filter(year &gt; 1977) |&gt;\n  mutate(iso_date = as.Date(paste0(year, \"-\", month, \"-\", \"01\")), \n         plot = as.factor(plot)) %&gt;%\n  select(iso_date, plot, treatment)\n\nplots_data_longterm &lt;- plot_treatments |&gt;\n  group_by(plot) |&gt;\n  summarize(treatment = case_when(\n              all(treatment == \"control\") ~ \"control\",\n              all(treatment == \"exclosure\") ~ \"exclosure\")) |&gt;\n  filter(!is.na(treatment))\n\nspecies_data &lt;- species_data |&gt;\n  filter(censustarget == 1, unidentified == 0)\n\nsurvey_data &lt;- summarize_individual_rodents(\n  time = \"date\",\n  length = \"Longterm\") |&gt;\n  filter(year &gt; 1977) |&gt;\n  filter(species %in% unique(species_data$species))\n\nwrite.csv(survey_data, \"surveys.csv\", row.names = FALSE, na = \"\")\nwrite.csv(plots_data_longterm, \"plots.csv\", row.names = FALSE, na = \"\")\nwrite.csv(species_data, \"species.csv\", row.names = FALSE, na = \"\")"
  },
  {
    "objectID": "data/2023/2023-05-16/readme.html",
    "href": "data/2023/2023-05-16/readme.html",
    "title": "Tornados",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nTornados\nThe data this week comes from NOAA’s National Weather Service Storm Prediction Center Severe Weather Maps, Graphics, and Data Page. Thank you to Evan Gower for the suggestion!\nEvan investigated a version of this dataset on Kaggle.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-05-16')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 20)\n\ntornados &lt;- tornados\n\n# Or read in the data manually\n\ntornados &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-16/tornados.csv')\n\n\nData Dictionary\n\n\n\ntornados.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nom\ninteger\nTornado number. Effectively an ID for this tornado in this year.\n\n\nyr\ninteger\nYear, 1950-2022.\n\n\nmo\ninteger\nMonth, 1-12.\n\n\ndy\ninteger\nDay of the month, 1-31.\n\n\ndate\ndate\nDate.\n\n\ntime\ntime\nTime.\n\n\ntz\ncharacter\nCanonical tz database timezone.\n\n\ndatetime_utc\ndatetime\nDate and time normalized to UTC.\n\n\nst\ncharacter\nTwo-letter postal abbreviation for the state (DC = Washington, DC; PR = Puerto Rico; VI = Virgin Islands).\n\n\nstf\ninteger\nState FIPS (Federal Information Processing Standards) number.\n\n\nmag\ninteger\nMagnitude on the F scale (EF beginning in 2007). Some of these values are estimated (see fc).\n\n\ninj\ninteger\nNumber of injuries. When summing for state totals, use sn == 1 (see below).\n\n\nfat\ninteger\nNumber of fatalities. When summing for state totals, use sn == 1 (see below).\n\n\nloss\ndouble\nEstimated property loss information in dollars. Prior to 1996, values were grouped into ranges. The reported number for such years is the maximum of its range.\n\n\nslat\ndouble\nStarting latitude in decimal degrees.\n\n\nslon\ndouble\nStarting longitude in decimal degrees.\n\n\nelat\ndouble\nEnding latitude in decimal degrees.\n\n\nelon\ndouble\nEnding longitude in decimal degrees.\n\n\nlen\ndouble\nLength in miles.\n\n\nwid\ndouble\nWidth in yards.\n\n\nns\ninteger\nNumber of states affected by this tornado. 1, 2, or 3.\n\n\nsn\ninteger\nState number for this row. 1 means the row contains the entire track information for this state, 0 means there is at least one more entry for this state for this tornado (om + yr).\n\n\nf1\ninteger\nFIPS code for the 1st county.\n\n\nf2\ninteger\nFIPS code for the 2nd county.\n\n\nf3\ninteger\nFIPS code for the 3rd county.\n\n\nf4\ninteger\nFIPS code for the 4th county.\n\n\nfc\nlogical\nWas the mag column estimated?\n\n\n\n\nCleaning Script\n# All packages used in this script:\nlibrary(tidyverse)\nlibrary(here)\n\nurl &lt;- \"https://www.spc.noaa.gov/wcm/data/1950-2022_actual_tornadoes.csv\"\n\n# Some of the automatic column types are imperfect Get that spec and then\n# update it.\ntornados &lt;- read_csv(url)\nspec(tornados) # Copy/pasted into col_types below then edited.\ntornados &lt;- read_csv(\n  url,\n  col_types = cols(\n    om = col_integer(),\n    yr = col_integer(),\n    mo = col_integer(),\n    dy = col_integer(),\n    date = col_date(format = \"\"),\n    time = col_time(format = \"\"),\n    tz = col_integer(),\n    st = col_factor(),\n    stf = col_integer(),\n    stn = col_integer(),\n    mag = col_integer(),\n    inj = col_integer(),\n    fat = col_integer(),\n    loss = col_double(),\n    closs = col_double(),\n    slat = col_double(),\n    slon = col_double(),\n    elat = col_double(),\n    elon = col_double(),\n    len = col_double(),\n    wid = col_integer(),\n    ns = col_integer(),\n    sn = col_integer(),\n    sg = col_integer(),\n    f1 = col_integer(),\n    f2 = col_integer(),\n    f3 = col_integer(),\n    f4 = col_integer(),\n    fc = col_integer()\n  )\n)\n\nglimpse(tornados)\n\n# This table only contains one segment per tornado, so we can drop the sg\n# column.\ntornados$sg &lt;- NULL\n\n# The tz column is confusing in the provided dictionary\n# (https://www.spc.noaa.gov/wcm/data/SPC_severe_database_description.pdf).\n# Investigate it to make sense of the various values.\ntornados |&gt; \n  count(tz)\n\n# The doc says 3 == CST, and 9 == GMT. 0 appears to be NA. What is 6? \ntornados |&gt; \n  filter(tz == 6) |&gt;\n  count(st)\n\n# All tornados with tz == 6 are in Mountain Time states, so we'll make that\n# assumption. Update time encoding.\n\ntornados &lt;- tornados |&gt; \n  # We can't really judge even what day the recording was on for unknown tz, so\n  # drop those values.\n  filter(tz != 0) |&gt; \n  mutate(\n    # Make the remaining tz's more meaningful. We'll assume they meant Central\n    # (daylight or standard) for \"CST\", and likewise that they meant what we now\n    # call UTC for \"GMT\". \"GMT\" sometimes includes BST so we'll avoid using that\n    # name.\n    tz = case_match(\n      tz,\n      3 ~ \"America/Chicago\",\n      6 ~ \"America/Denver\",\n      9 ~ \"UTC\"\n    ),\n    # Add a datetime_utc column to normalize the times. ymd_hms only wants a\n    # single timezone (not a vector of them), so break it up with a case_match.\n    datetime_utc = case_match(\n      tz,\n      \"America/Chicago\" ~ lubridate::ymd_hms(\n        paste(date, time),\n        tz = \"America/Chicago\"\n      ),\n      \"America/Denver\" ~ lubridate::ymd_hms(\n        paste(date, time),\n        tz = \"America/Denver\"\n      ),\n      \"UTC\" ~ lubridate::ymd_hms(\n        paste(date, time),\n        tz = \"UTC\"\n      ) \n    ) |&gt; \n      lubridate::with_tz(\"UTC\"),\n    .after = tz\n  ) |&gt; \n  # Drop stn because it was discontinued and was inconsistent before being\n  # discontinued. closs (crop loss) has an unexplained discontinuity in 2016 and\n  # it isn't entirely clear what changed.\n  select(-\"stn\", -\"closs\") |&gt; \n  # Recode some more weird columns.\n  mutate(\n    # The mag column uses -9 for NA.\n    mag = na_if(mag, -9),\n    # The loss column is confusingly coded. Let's attempt to make it make sense.\n    # The documentation (last updated in 2010) explains that the coding changed in\n    # 1996. Observationally, it's clear that it changed again in 2016.\n    loss = case_when(\n      loss == 0 ~ NA,\n      yr &lt; 1996 & loss == 1 ~ 50,\n      yr &lt; 1996 & loss == 2 ~ 500,\n      yr &lt; 1996 & loss == 3 ~ 5000,\n      yr &lt; 1996 & loss == 4 ~ 50000,\n      yr &lt; 1996 & loss == 5 ~ 500000,\n      yr &lt; 1996 & loss == 6 ~ 5000000,\n      yr &lt; 1996 & loss == 7 ~ 50000000,\n      yr &lt; 1996 & loss == 8 ~ 500000000,\n      yr &lt; 1996 & loss == 9 ~ 5000000000,\n      yr &gt;= 1996 & yr &lt; 2016 ~ loss * 1e6,\n      TRUE ~ loss\n    ),\n    # The fc column is really a \"was mag estimated\" column\n    fc = as.logical(fc)\n  )\n\n# Some of the remaining columns are confusing, but we'll explain them in the\n# dictionary and see what people find!\n\nwrite_csv(\n  tornados,\n  here(\n    \"data\",\n    \"2023\",\n    \"2023-05-16\",\n    \"tornados.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-05-30/readme.html",
    "href": "data/2023/2023-05-30/readme.html",
    "title": "Verified Oldest People",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nVerified Oldest People\nThe data this week comes from the Wikipedia List of the verified oldest people via frankiethull on GitHub. Thank you for the submission, Frank!\n\nThese are lists of the 100 known verified oldest people sorted in descending order by age in years and days. The oldest person ever whose age has been independently verified is Jeanne Calment (1875–1997) of France, who lived to the age of 122 years and 164 days. The oldest verified man ever is Jiroemon Kimura (1897–2013) of Japan, who lived to the age of 116 years and 54 days. The oldest known living person is Maria Branyas of Spain, aged 116 years, 85 days. The oldest known living man is Juan Vicente Pérez of Venezuela, aged 114 years, 1 day. The 100 oldest women have, on average, lived several years longer than the 100 oldest men.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-05-30')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 22)\n\ncentenarians &lt;- tuesdata$centenarians\n\n# Or read in the data manually\n\ncentenarians &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-30/centenarians.csv')\n\n\nData Dictionary\n\n\n\ncentenarians.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nrank\ninteger\nThis person’s overall rank by age.\n\n\nname\ncharacter\nThe full name of this person.\n\n\nbirth_date\ndate\nThis person’s birth date.\n\n\ndeath_date\ndate\nThis person’s death date (or NA if still alive).\n\n\nage\ndouble\nThe person’s age, either on the day of their death or on the day when the dataset was extracted on 2023-05-25.\n\n\nplace_of_death_or_residence\ncharacter\nWhere the person lives now or where they were when they died.\n\n\ngender\ncharacter\nMost likely actually the sex assigned to the person at birth (the source article does not specify).\n\n\nstill_alive\ncharacter\nEither “alive” if the person was still alive at the time when the article as referenced, or “deceased” if the person was no longer alive.\n\n\n\n\nCleaning Script\nNo data cleaning. See the source GitHub repo for details."
  },
  {
    "objectID": "data/2023/2023-06-13/readme.html",
    "href": "data/2023/2023-06-13/readme.html",
    "title": "SAFI survey data",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nSAFI survey data\nThe data this week comes from the SAFI (Studying African Farmer-Led Irrigation) survey, a subset of the data used in the Data Carpentry Social Sciences workshop. So, if you’re looking how to learn how to work with this data, lessons are already available! Data is available through Figshare.\nCITATION: Woodhouse, Philip; Veldwisch, Gert Jan; Brockington, Daniel; Komakech, Hans C.; Manjichi, Angela; Venot, Jean-Philippe (2018): SAFI Survey Results. doi:10.6084/m9.figshare.6262019.v1\n\nSAFI (Studying African Farmer-Led Irrigation) is a currently running project which is looking at farming and irrigation methods. This is survey data relating to households and agriculture in Tanzania and Mozambique. The survey data was collected through interviews conducted between November 2016 and June 2017 using forms downloaded to Android Smartphones. The survey forms were created using the ODK (Open Data Kit) software via an Excel spreadsheet. The collected data is then sent back to a central server. The server can be used to download the collected data in both JSON and CSV formats. This is a teaching version of the collected data that we will be using. It is not the full dataset.\n\n\nThe survey covered such things as; household features (e.g. construction materials used, number of household members), agricultural practices (e.g. water usage), assets (e.g. number and types of livestock) and details about the household members.\n\n\nThe basic teaching dataset used in these lessons is a subset of the JSON dataset that has been converted into CSV format.\n\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-06-13')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 24)\n\nsafi_data.csv &lt;- tuesdata$`safi_data`\n\n# Or read in the data manually\n\nsafi_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-06-13/safi_data.csv')\n\n\nData Dictionary\n\n\n\nsafi_data.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nkey_ID\ninteger\nAdded to provide a unique Id for each observation. (The InstanceID field does this as well but it is not as convenient to use)\n\n\nvillage\ncharacter\nVillage name\n\n\ninterview_date\ncharacter\nDate of interview\n\n\nno_membrs\ninteger\nNumber of members in the household\n\n\nyears_liv\ninteger\nNumber of years living in this village or a neighboring village\n\n\nrespondent_wall_type\ncharacter\nType of walls the house has\n\n\nrooms\ninteger\nNumber of rooms in the main house used for sleeping\n\n\nmemb_assoc\ncharacter\nAre you a member of an irrigation association?\n\n\naffect_conflicts\ncharacter\nHave you been affected by conflicts with other irrigators in the area?\n\n\nliv_count\ninteger\nLivestock count\n\n\nitems_owned\ncharacter\nItems owned by the household\n\n\nno_meals\ninteger\nHow many meals do people in your household normally eat in a day?\n\n\nmonths_lack_food\ncharacter\nIndicate which months, In the last 12 months have you faced a situation when you did not have enough food to feed the household?\n\n\ninstanceID\ncharacter\nUnique identifier for the form data submission\n\n\n\n\nCleaning Script\nData was cleaned for the Data Carpentry Social Science lessons. Information available on their SAFI Teaching Dataset page."
  },
  {
    "objectID": "data/2023/2023-06-27/readme.html",
    "href": "data/2023/2023-06-27/readme.html",
    "title": "US Populated Places",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nUS Populated Places\nWhile we embark on a road trip for summer vacation, the data this week comes from the National Map Staged Products Directory from the US Board of Geographic Names.\nNote: Quite a lot of more data is available from the GNIS. See the cleaning script for clues for downloading the additional data.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-06-27')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 26)\n\nus_place_names &lt;- tuesdata$`us_place_names`\nus_place_history &lt;- tuesdata$`us_place_history`\n\n# Or read in the data manually\n\nus_place_names &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-06-27/us_place_names.csv')\nus_place_history &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-06-27/us_place_history.csv')\n\n\nData Dictionary\n\n\n\nus_place_names.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nfeature_id\ndouble\nPermanent, unique feature record identifier.\n\n\nfeature_name\ncharacter\nOfficial feature name.\n\n\nstate_name\ncharacter\nThe name of the state containing the primary coordinates.\n\n\ncounty_name\ncharacter\nThe name of the county containing the primary coordinates.\n\n\ncounty_numeric\ndouble\nThe 3-digit code for the county containing the primary coordinates.\n\n\ndate_created\ndate\nThe date the record was initially entered into the\n\n\nGeographic Names Information System.\n\n\n\n\ndate_edited\ndate\nThe date any attribute of an existing record was\n\n\nedited.\n\n\n\n\nprim_lat_dec\ndouble\nThe latitude of the official feature location. Note that some values are unknown.\n\n\nprim_long_dec\ndouble\nThe longitude of the official feature location. Note that some values are unknown.\n\n\n\n\n\nus_place_history.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nfeature_id\ndouble\nPermanent, unique feature record identifier.\n\n\ndescription\ncharacter\nCharacteristics or information about a feature or the feature data\n\n\nhistory\ncharacter\nRefers to the name origin, and/or cultural history of a feature.\n\n\n\n\nCleaning Script\nSee Jon Harmon’s cleaning and enriching scripts for most of the (extensive) cleaning.\nlibrary(tidyverse)\nlibrary(withr)\nlibrary(fs)\nlibrary(here)\n\nurl &lt;- \"https://prd-tnm.s3.amazonaws.com/StagedProducts/GeographicNames/DomesticNames/DomesticNames_National_Text.zip\"\n\npath_zip &lt;- local_tempfile(fileext = \".zip\")\ndownload.file(url, path_zip)\n\nus_place_names &lt;- read_delim(path_zip, \"|\") |&gt; \n  mutate(county_numeric = as.integer(county_numeric)) |&gt; \n  # We'll just use populated places, you might want to keep more!\n  filter(feature_class == \"Populated Place\") |&gt; \n  select(-feature_class, -starts_with(\"source_\")) |&gt; \n  # We also don't keep some redundant or less useful features.\n  select(\n    -state_numeric,\n    -map_name, \n    -starts_with(\"bgn\"), \n    -ends_with(\"_dms\")) |&gt; \n  mutate(\n    across(\n      ends_with(\"_dec\"),\n      ~ na_if(.x, 0)\n    ),\n    across(\n      starts_with(\"date_\"),\n      lubridate::mdy\n    )\n  ) |&gt; \n  distinct()\n\nglimpse(us_place_names)\n\nurl_history &lt;- \"https://prd-tnm.s3.amazonaws.com/StagedProducts/GeographicNames/Topical/FeatureDescriptionHistory_National_Text.zip\"\n\npath_history &lt;- local_tempfile(fileext = \".zip\")\ndownload.file(url_history, path_history)\n\nus_place_history &lt;- read_delim(path_history, \"|\") |&gt; \n  semi_join(us_place_names)\n\n# Data dictionary: \n# https://prd-tnm.s3.amazonaws.com/StagedProducts/GeographicNames/GNIS_file_format.pdf\n\nwrite_csv(\n  us_place_names,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-06-27\",\n    \"us_place_names.csv\"\n  )\n)\nwrite_csv(\n  us_place_history,\n  here::here(\n    \"data\",\n    \"2023\",\n    \"2023-06-27\",\n    \"us_place_history.csv\"\n  )\n)"
  },
  {
    "objectID": "data/2023/2023-07-11/readme.html",
    "href": "data/2023/2023-07-11/readme.html",
    "title": "Global surface temperatures",
    "section": "",
    "text": "Please add alt text to your posts\nPlease add alt text (alternative text) to all of your posted graphics for #TidyTuesday.\nTwitter provides guidelines for how to add alt text to your images.\nThe DataViz Society/Nightingale by way of Amy Cesal has an article on writing good alt text for plots/graphs.\n\nHere’s a simple formula for writing alt text for data visualization: ### Chart type It’s helpful for people with partial sight to know what chart type it is and gives context for understanding the rest of the visual. Example: Line graph ### Type of data What data is included in the chart? The x and y axis labels may help you figure this out. Example: number of bananas sold per day in the last year ### Reason for including the chart Think about why you’re including this visual. What does it show that’s meaningful. There should be a point to every visual and you should tell people what to look for. Example: the winter months have more banana sales ### Link to data or source Don’t include this in your alt text, but it should be included somewhere in the surrounding text. People should be able to click on a link to view the source data or dig further into the visual. This provides transparency about your source and lets people explore the data. Example: Data from the USDA\n\nPenn State has an article on writing alt text descriptions for charts and tables.\n\nCharts, graphs and maps use visuals to convey complex images to users. But since they are images, these media provide serious accessibility issues to colorblind users and users of screen readers. See the examples on this page for details on how to make charts more accessible.\n\nThe {rtweet} package includes the ability to post tweets with alt text programatically.\nNeed a reminder? There are extensions that force you to remember to add Alt Text to Tweets with media.\n\n\nGlobal surface temperatures\nThe data this week comes from the NASA GISS Surface Temperature Analysis (GISTEMP v4). This datasets are tables of global and hemispheric monthly means and zonal annual means. They combine land-surface, air and sea-surface water temperature anomalies (Land-Ocean Temperature Index, L-OTI). The values in the tables are deviations from the corresponding 1951-1980 means.\n\nThe GISS Surface Temperature Analysis version 4 (GISTEMP v4) is an estimate of global surface temperature change. Graphs and tables are updated around the middle of every month using current data files from NOAA GHCN v4 (meteorological stations) and ERSST v5 (ocean areas), combined as described in their publications Hansen et al. (2010) and Lenssen et al. (2019). These updated files incorporate reports for the previous month and also late reports and corrections for earlier months.\n\n\nWhen comparing seasonal temperatures, it is convenient to use “meteorological seasons” based on temperature and defined as groupings of whole months. Thus, Dec-Jan-Feb (DJF) is the Northern Hemisphere meteorological winter, Mar-Apr-May (MAM) is N.H. meteorological spring, Jun-Jul-Aug (JJA) is N.H. meteorological summer and Sep-Oct-Nov (SON) is N.H. meteorological autumn. String these four seasons together and you have the meteorological year that begins on Dec. 1 and ends on Nov. 30 (D-N). The full year is Jan to Dec (J-D). Brian Bartling\n\nAn analysis and more information on the data can be found in Lenssen, N., G. Schmidt, J. Hansen, M. Menne, A. Persin, R. Ruedy, and D. Zyss, 2019: Improvements in the GISTEMP uncertainty model. J. Geophys. Res. Atmos., 124, no. 12, 6307-6326, doi:10.1029/2018JD029522.\nThere’s also more detail and answers to commonly asked in questions in their FAQ.\nCitation: GISTEMP Team, 2023: GISS Surface Temperature Analysis (GISTEMP), version 4. NASA Goddard Institute for Space Studies. Dataset accessed 2023-07-09 at https://data.giss.nasa.gov/gistemp/.\n\nGet the data here\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-07-11')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 28)\n\nglobal_temps &lt;- tuesdata$global_temps\nnh_temps &lt;- tuesdata$nh_temps\nsh_temps &lt;- tuesdata$sh_temps\nzonann_temps &lt;- tuesdata$zonann_temps\n\n# Or read in the data manually\n\nglobal_temps &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-11/global_temps.csv')\nnh_temps &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-11/nh_temps.csv')\nsh_temps &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-11/sh_temps.csv')\nzonann_temps &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-11/zonann_temps.csv')\n\n\nData Dictionary\n\n\n\nglobal_temps.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndouble\nYear\n\n\nJan\ndouble\nJanuary\n\n\nFeb\ndouble\nFebruary\n\n\nMar\ndouble\nMarch\n\n\nApr\ndouble\nApril\n\n\nMay\ndouble\nMay\n\n\nJun\ndouble\nJune\n\n\nJul\ndouble\nJuly\n\n\nAug\ndouble\nAugust\n\n\nSep\ndouble\nSeptember\n\n\nOct\ndouble\nOctober\n\n\nNov\ndouble\nNovember\n\n\nDec\ndouble\nDecember\n\n\nJ-D\ndouble\nJanuary-December\n\n\nD-N\ndouble\nDecemeber-November\n\n\nDJF\ndouble\nDecember-January-February\n\n\nMAM\ndouble\nMarch-April-May\n\n\nJJA\ndouble\nJune-July-August\n\n\nSON\ndouble\nSeptember-October-November\n\n\n\n\n\nnh_temps.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndouble\nYear\n\n\nJan\ndouble\nJanuary\n\n\nFeb\ndouble\nFebruary\n\n\nMar\ndouble\nMarch\n\n\nApr\ndouble\nApril\n\n\nMay\ndouble\nMay\n\n\nJun\ndouble\nJune\n\n\nJul\ndouble\nJuly\n\n\nAug\ndouble\nAugust\n\n\nSep\ndouble\nSeptember\n\n\nOct\ndouble\nOctober\n\n\nNov\ndouble\nNovember\n\n\nDec\ndouble\nDecember\n\n\nJ-D\ndouble\nJanuary-December\n\n\nD-N\ndouble\nDecemeber-November\n\n\nDJF\ndouble\nDecember-January-February\n\n\nMAM\ndouble\nMarch-April-May\n\n\nJJA\ndouble\nJune-July-August\n\n\nSON\ndouble\nSeptember-October-November\n\n\n\n\n\nsh_temps.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndouble\nYear\n\n\nJan\ndouble\nJanuary\n\n\nFeb\ndouble\nFebruary\n\n\nMar\ndouble\nMarch\n\n\nApr\ndouble\nApril\n\n\nMay\ndouble\nMay\n\n\nJun\ndouble\nJune\n\n\nJul\ndouble\nJuly\n\n\nAug\ndouble\nAugust\n\n\nSep\ndouble\nSeptember\n\n\nOct\ndouble\nOctober\n\n\nNov\ndouble\nNovember\n\n\nDec\ndouble\nDecember\n\n\nJ-D\ndouble\nJanuary-December\n\n\nD-N\ndouble\nDecemeber-November\n\n\nDJF\ndouble\nDecember-January-February\n\n\nMAM\ndouble\nMarch-April-May\n\n\nJJA\ndouble\nJune-July-August\n\n\nSON\ndouble\nSeptember-October-November\n\n\n\n\n\nzonann_temps.csv\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear\ndouble\nYear\n\n\nGlob\ndouble\nGlobal\n\n\nNHem\ndouble\nNorthern Hemisphere\n\n\nSHem\ndouble\nSouthern Hemisphere\n\n\n24N-90N\ndouble\n24N-90N lattitude\n\n\n24S-24N\ndouble\n24S-24N lattitude\n\n\n90S-24S\ndouble\n90S-24S lattitude\n\n\n64N-90N\ndouble\n64N-90N lattitude\n\n\n44N-64N\ndouble\n44N-64N lattitude\n\n\n24N-44N\ndouble\n24N-44N lattitude\n\n\nEQU-24N\ndouble\nEQU-24N lattitude\n\n\n24S-EQU\ndouble\n24S-EQU lattitude\n\n\n44S-24S\ndouble\n44S-24S lattitude\n\n\n64S-44S\ndouble\n64S-44S lattitude\n\n\n90S-64S\ndouble\n90S-64S lattitude\n\n\n\n\nCleaning Script\nMissing data was indicated by ***. Replaced *** with an empty cell, so these would be NAs."
  },
  {
    "objectID": "data/2023/2023-07-25/readme.html",
    "href": "data/2023/2023-07-25/readme.html",
    "title": "Scurvy",
    "section": "",
    "text": "The data this week comes from the medicaldata R package. This is a data package from Peter Higgins, with 19 medical datasets for teaching Reproducible Medical Research with R.\nWe’re using the scurvy dataset.\n\nSource: This data set is from a study published in 1757 in A Treatise on the Scurvy in Three Parts, by James Lind.\n\n\nThis data set contains 12 participants with scurvy. In 1757, it was not known that scurvy is a manifestation of vitamin C deficiency. A variety of remedies had been anecdotally reported, but Lind was the first to test different regimens of acidic substances (including citrus fruits) against each other in a randomized, controlled trial. 6 distinct therapies were tested in 12 seamen with symptomatic scurvy, who were selected for similar severity. Six days of therapy were provided, and endpoints were reported in the text at the end of 6 days. These include rotting of the gums, skin sores, weakness of the knees, and lassitude, which are described in terms of severity. These have been translated into Likert scales from 0(none) to 3(severe). A dichotomous endpoint, fitness for duty, was also reported.\n\n\nScurvy was a common affliction of seamen on long voyages, leading to mouth sores, skin lesions, weakness of the knees, and lassitude. Scurvy could be fatal on long voyages. James Lind reported the treatment of 12 seamen with scurvy in 1757, in _A Treatise on the Scurvy in Three Parts). This 476 page bloviation can be found scanned to the Google Books website A Treatise on the Scurvy. Pages 149-153 are a rare gem among what can be generously described as 400+ pages of evidence-free blathering, and these 4 pages may represent the first report of a controlled clinical trial.\n\n\nLind was the ship’s surgeon on board the HMS Salisbury, and had a number of scurvy-affected seamen at his disposal. Many remedies had been described and advocated for, with no more than anecdotal evidence. On May 20, 1747, Lind decided to try the 6 therapies on the Salisbury in a comparative study in 12 affected seamen. He selected 12 with roughly similar severity, with notable skin and mouth sores, weakness of the knees, and significant lassitude, making them unfit for duty. They each received the standard shipboard diet of gruel and mutton broth, supplemented with occasional biscuits and puddings. Each treatment was a dietary supplement (including citrus fruits) or a medicinal.\n\n\nThis data frame was reconstructed from Lind’s account as recorded on these 4 pages, with his estimates of severity translated to a 4 point Likert scale (0-3) for each of the symptoms he described at his chosen endpoint on day 6. A somewhat fanciful study_id variable was added, along with detailed descriptions of the dosing schedule of each treatment. Of note, there is some dispute about whether this was truly the first clinical trial, or whether it actually happened, as there are no contemporaneous corroborating accounts. See link about the historical debate.\n\n\nLind reported that the seamen treated with 2 lemons and an orange daily did best, followed by those treated with cider. Those treated with elixir of vitriol only had improvement in mouth sores. One imagines that acidic substances (like dilute sulfuric acid, vinegar, cider, and citrus fruits) might have been rather painful on these mouth sores. Unfortunately, the burial of the 4 valuable pages of data in 476 pages of noise, a publication delay of 10 years, and Lind’s half-hearted conclusions (he was focused on acidity), meant that it took until 1795 before the British Navy mandated daily limes for seamen.\n\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-07-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 30)\n\nscurvy &lt;- tuesdata$scurvy\n\n# Option 2: Read directly from GitHub\n\nscurvy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-25/scurvy.csv')"
  },
  {
    "objectID": "data/2023/2023-07-25/readme.html#the-data",
    "href": "data/2023/2023-07-25/readme.html#the-data",
    "title": "Scurvy",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-07-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 30)\n\nscurvy &lt;- tuesdata$scurvy\n\n# Option 2: Read directly from GitHub\n\nscurvy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-07-25/scurvy.csv')"
  },
  {
    "objectID": "data/2023/2023-07-25/readme.html#how-to-participate",
    "href": "data/2023/2023-07-25/readme.html#how-to-participate",
    "title": "Scurvy",
    "section": "How to Participate",
    "text": "How to Participate\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-08-08/readme.html",
    "href": "data/2023/2023-08-08/readme.html",
    "title": "Hot Ones Episodes",
    "section": "",
    "text": "The data this week comes from Wikipedia articles: Hot Ones and List of Hot Ones episodes. Thank you to Carl Börstell for the suggestion and cleaning script!\n\nHot Ones is an American YouTube talk show, created by Chris Schonberger, hosted by Sean Evans and produced by First We Feast and Complex Media. Its basic premise involves celebrities being interviewed by Evans over a platter of increasingly spicy chicken wings.\n\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-08')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 32)\n\nepisodes &lt;- tuesdata$episodes\nsauces &lt;- tuesdata$sauces\nseasons &lt;- tuesdata$seasons\n\n# Option 2: Read directly from GitHub\n\nepisodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-08/episodes.csv')\nsauces &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-08/sauces.csv')\nseasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-08/seasons.csv')"
  },
  {
    "objectID": "data/2023/2023-08-08/readme.html#the-data",
    "href": "data/2023/2023-08-08/readme.html#the-data",
    "title": "Hot Ones Episodes",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-08')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 32)\n\nepisodes &lt;- tuesdata$episodes\nsauces &lt;- tuesdata$sauces\nseasons &lt;- tuesdata$seasons\n\n# Option 2: Read directly from GitHub\n\nepisodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-08/episodes.csv')\nsauces &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-08/sauces.csv')\nseasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-08/seasons.csv')"
  },
  {
    "objectID": "data/2023/2023-08-08/readme.html#how-to-participate",
    "href": "data/2023/2023-08-08/readme.html#how-to-participate",
    "title": "Hot Ones Episodes",
    "section": "How to Participate",
    "text": "How to Participate\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-08-22/readme.html",
    "href": "data/2023/2023-08-22/readme.html",
    "title": "Refugees",
    "section": "",
    "text": "The data this week comes from PopulationStatistics {refugees} R package.\n\n{refugees} is an R package designed to facilitate access to the United Nations High Commissioner for Refugees (UNHCR) Refugee Data Finder. It provides an easy-to-use interface to the database, which covers forcibly displaced populations, including refugees, asylum-seekers, internally displaced people, stateless people, and others over a span of more than 70 years.\n\nThis package provides data from three major sources:\n\nData from UNHCR’s annual statistical activities dating back to 1951.\nData from the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA), specifically for registered Palestine refugees under UNRWA’s mandate.\nData from the Internal Displacement Monitoring Centre (IDMC) on people displaced within their country due to conflict or violence.\n\nThe {refugees} package includes eight datasets. We’re working with population with data from 2010 to 2022.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-22')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 34)\n\npopulation &lt;- tuesdata$population\n\n# Option 2: Read directly from GitHub\n\npopulation &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-22/population.csv')"
  },
  {
    "objectID": "data/2023/2023-08-22/readme.html#the-data",
    "href": "data/2023/2023-08-22/readme.html#the-data",
    "title": "Refugees",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-22')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 34)\n\npopulation &lt;- tuesdata$population\n\n# Option 2: Read directly from GitHub\n\npopulation &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-08-22/population.csv')"
  },
  {
    "objectID": "data/2023/2023-08-22/readme.html#how-to-participate",
    "href": "data/2023/2023-08-22/readme.html#how-to-participate",
    "title": "Refugees",
    "section": "How to Participate",
    "text": "How to Participate\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-09-05/readme.html",
    "href": "data/2023/2023-09-05/readme.html",
    "title": "Union Membership in the United States",
    "section": "",
    "text": "Happy Labor Day*!\nThe data this week comes from the Union Membership, Coverage, and Earnings from the CPS by Barry Hirsch (Georgia State University), David Macpherson (Trinity University), and William Even (Miami University). They claim a copyright on the data, and state that “Use of data requires citation.”\n\nUnionstats.com provides annual measures of union, nonunion, and overall wages, beginning in 1973, compiled from the U.S. Current Population Surveys. Regression-based union wage gap estimates are presented economy-wide, for demographic groups, and sectors (private/public, industries). Union wage gaps are higher in the private than in the public sector, higher for men than women, roughly similar for black and white men, and much higher for Hispanic men than for Hispanic women. The database is updated annually.\n\nSee their open-access article “Five decades of CPS wages, methods, and union-nonunion wage gaps at Unionstats.com” for details about their methods and additional visualizations.\n\nThe first Monday in September was officially recognized as Labor Day by the state of Oregon in 1887 and became an official U.S. federal holiday in 1894, 10 years before May first was adopted as International Workers’ Day by the International Socialist Congress. May 1 was chosen in part to commemorate the Haymarket affair, a strike and incident of police violence that took place in Chicago in 1886. There’s no reason everyone can’t recognize both days!\n\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-09-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 36)\n\ndemographics &lt;- tuesdata$demographics\nwages &lt;- tuesdata$wages\nstates &lt;- tuesdata$states\n\n# Option 2: Read directly from GitHub\n\ndemographics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-05/demographics.csv')\nwages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-05/wages.csv')\nstates &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-05/states.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-09-05/readme.html#the-data",
    "href": "data/2023/2023-09-05/readme.html#the-data",
    "title": "Union Membership in the United States",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-09-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 36)\n\ndemographics &lt;- tuesdata$demographics\nwages &lt;- tuesdata$wages\nstates &lt;- tuesdata$states\n\n# Option 2: Read directly from GitHub\n\ndemographics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-05/demographics.csv')\nwages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-05/wages.csv')\nstates &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-05/states.csv')"
  },
  {
    "objectID": "data/2023/2023-09-05/readme.html#how-to-participate",
    "href": "data/2023/2023-09-05/readme.html#how-to-participate",
    "title": "Union Membership in the United States",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-09-19/readme.html",
    "href": "data/2023/2023-09-19/readme.html",
    "title": "CRAN Package Authors",
    "section": "",
    "text": "It’s time for posit::conf(2023L)! To celebrate, the data this week comes from the CRAN collaboration graph, a project by David Schoch.\n\nThe CRAN collaboration graph consists of R package developers who are connected if they appear together as authors of an R package in the DESCRIPTION file.\n\n\nThe “Hadley number” is defined as the distance of R developers to Hadley Wickham in the collaboration graph.\n\nSee the README of the GitHub project for a fun exploration of the data, and to see how it was gathered.\n\n\n# Scroll to the end of this code block to see how to recombine the data into a\n# graph!\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-09-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 38)\n\ncran_20230905 &lt;- tuesdata$cran_20230905\npackage_authors &lt;- tuesdata$package_authors\ncran_graph_nodes &lt;- tuesdata$cran_graph_nodes\ncran_graph_edges &lt;- tuesdata$cran_graph_edges\n\n# Option 2: Read directly from GitHub\n\ncran_20230905 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-19/cran_20230905.csv')\npackage_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-19/package_authors.csv')\ncran_graph_nodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-19/cran_graph_nodes.csv')\ncran_graph_edges &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-19/cran_graph_edges.csv')\n\n# Reconstruct a graph\nlibrary(tidygraph)\ncran_graph &lt;- tbl_graph(\n  nodes = cran_graph_nodes, edges = cran_graph_edges, directed = FALSE\n)\n\n# Fine a particular author in the graph.\ntarget_author &lt;- \"Jon Harmon\"\ntarget_author_index &lt;- which(igraph::V(cran_graph)$name == target_author)\n\n# Extract that author's Hadley number.\nigraph::V(cran_graph)$dist2HW[target_author_index]\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-09-19/readme.html#the-data",
    "href": "data/2023/2023-09-19/readme.html#the-data",
    "title": "CRAN Package Authors",
    "section": "",
    "text": "# Scroll to the end of this code block to see how to recombine the data into a\n# graph!\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-09-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 38)\n\ncran_20230905 &lt;- tuesdata$cran_20230905\npackage_authors &lt;- tuesdata$package_authors\ncran_graph_nodes &lt;- tuesdata$cran_graph_nodes\ncran_graph_edges &lt;- tuesdata$cran_graph_edges\n\n# Option 2: Read directly from GitHub\n\ncran_20230905 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-19/cran_20230905.csv')\npackage_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-19/package_authors.csv')\ncran_graph_nodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-19/cran_graph_nodes.csv')\ncran_graph_edges &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-09-19/cran_graph_edges.csv')\n\n# Reconstruct a graph\nlibrary(tidygraph)\ncran_graph &lt;- tbl_graph(\n  nodes = cran_graph_nodes, edges = cran_graph_edges, directed = FALSE\n)\n\n# Fine a particular author in the graph.\ntarget_author &lt;- \"Jon Harmon\"\ntarget_author_index &lt;- which(igraph::V(cran_graph)$name == target_author)\n\n# Extract that author's Hadley number.\nigraph::V(cran_graph)$dist2HW[target_author_index]"
  },
  {
    "objectID": "data/2023/2023-09-19/readme.html#how-to-participate",
    "href": "data/2023/2023-09-19/readme.html#how-to-participate",
    "title": "CRAN Package Authors",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-03/readme.html",
    "href": "data/2023/2023-10-03/readme.html",
    "title": "US Government Grant Opportunities",
    "section": "",
    "text": "The R4DS Online Learning Community is a community of learners at all skill levels working together to improve our data-science-related skills. We offer free data-related education through book clubs and free live question-answering on our Slack, and by curating a dataset every week here at TidyTuesday.\n(NOTE: Unfortunately, since this post, the Open Collective Foundation dissolved, so this is no longer true.)\nWe are now a fiscally sponsored project of Open Collective Foundation (https://opencollective.foundation), a 501(c)(3) public charity. That means donations to the R4DS Online Learning Community are now tax-deductible in the US! It also means that we are now eligible for a number of grants, including some of the grants listed on Grants.gov.\nWe have exported all grants past and present from that site, and we are making them available here for you to explore and visualize. We also scraped details for all posted grants. Please let us know if you find anything interesting!\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-03')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 40)\n\ngrants &lt;- tuesdata$grants\ngrant_opportunity_details &lt;- tuesdata$grant_opportunity_details\n\n# Option 2: Read directly from GitHub\n\ngrants &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-03/grants.csv')\ngrant_opportunity_details &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-03/grant_opportunity_details.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-03/readme.html#the-data",
    "href": "data/2023/2023-10-03/readme.html#the-data",
    "title": "US Government Grant Opportunities",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-03')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 40)\n\ngrants &lt;- tuesdata$grants\ngrant_opportunity_details &lt;- tuesdata$grant_opportunity_details\n\n# Option 2: Read directly from GitHub\n\ngrants &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-03/grants.csv')\ngrant_opportunity_details &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-03/grant_opportunity_details.csv')"
  },
  {
    "objectID": "data/2023/2023-10-03/readme.html#how-to-participate",
    "href": "data/2023/2023-10-03/readme.html#how-to-participate",
    "title": "US Government Grant Opportunities",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-17/readme.html",
    "href": "data/2023/2023-10-17/readme.html",
    "title": "Taylor Swift",
    "section": "",
    "text": "Since The Eras Tour Film was just released, this week we’re exploring Taylor Swift song data!\nAre you Ready for It?\nThe taylor R package from W. Jake Thompson is a curated data set of Taylor Swift songs, including lyrics and audio characteristics. The data comes from Genius and the Spotify API.\nThere are three main datasets.\n\nThe first is taylor_album_songs, which includes lyrics and audio features from the Spotify API for all songs on Taylor’s official studio albums. Notably this excludes singles released separately from an album (e.g., Only the Young, Christmas Tree Farm, etc.), and non-Taylor-owned albums that have a Taylor-owned alternative (e.g., Fearless is excluded in favor of Fearless (Taylor’s Version)). We stan artists owning their own songs.\n\n\nYou can access Taylor’s entire discography with taylor_all_songs. This includes all of the songs in taylor_album_songs plus EPs, individual singles, and the original versions of albums that have been re-released as Taylor’s Version.\n\n\nFinally, there is a small data set, taylor_albums, summarizing Taylor’s album release history.\n\nInformation on the audio features in the dataset from Spotify are included in their API documentation.\nFor your visualizations, the {taylor} package comes with it’s own class of color palettes, inspired by the work of Josiah Parry in the {cpcinema} package.\nYou might also be interested in the tayoRswift package by Alex Stephenson, a ggplot2 color palette based on Taylor Swift album covers. “For when your colors absolutely should not be excluded from the narrative.”\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-17')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 42)\n\ntaylor_album_songs &lt;- tuesdata$taylor_album_songs\ntaylor_all_songs &lt;- tuesdata$taylor_all_songs\ntaylor_albums &lt;- tuesdata$taylor_albums\n\n# Option 2: Read directly from GitHub\n\ntaylor_album_songs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-17/taylor_album_songs.csv')\ntaylor_all_songs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-17/taylor_all_songs.csv')\ntaylor_albums &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-17/taylor_albums.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-17/readme.html#the-data",
    "href": "data/2023/2023-10-17/readme.html#the-data",
    "title": "Taylor Swift",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-17')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 42)\n\ntaylor_album_songs &lt;- tuesdata$taylor_album_songs\ntaylor_all_songs &lt;- tuesdata$taylor_all_songs\ntaylor_albums &lt;- tuesdata$taylor_albums\n\n# Option 2: Read directly from GitHub\n\ntaylor_album_songs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-17/taylor_album_songs.csv')\ntaylor_all_songs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-17/taylor_all_songs.csv')\ntaylor_albums &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-17/taylor_albums.csv')"
  },
  {
    "objectID": "data/2023/2023-10-17/readme.html#how-to-participate",
    "href": "data/2023/2023-10-17/readme.html#how-to-participate",
    "title": "Taylor Swift",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-31/readme.html",
    "href": "data/2023/2023-10-31/readme.html",
    "title": "Horror Legends",
    "section": "",
    "text": "Happy Halloween! To celebrate, this week we’re exploring horror legends from Snopes.com!\n\nSince urban legends are often a means of expressing our fears about the dangers that ripple just beneath the surface of our seemingly calm and untroubled world, it should come as no surprise that horror legends are one of urban folklore’s richest veins. We worry about the terrible accidents we’re powerless to prevent, and we fear anonymous killers who choose victims at random. We cannot protect ourselves from the venomous animals who slither undetected into the places where we work, play, and shop, nor can we stop the onslaught of insects who invade our homes and our bodies. We’re repulsed by the contaminants that may lurk in our food. We’re afraid of foreigners and foreign places. We fear for our childrens’ safety in a world full of drugs, kidnappers, and poisons. We never know what gruesome discovery may be waiting around the next corner. And even if we somehow escape all of these horrors, our own vanities may do us in.\n\nYou might want to dig into the details of the articles this week – particularly if the rating is “mixture”. Each observation includes the URL to that article, which you can open directly from R with the utils::browseURL() function.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-31')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 44)\n\nhorror_articles &lt;- tuesdata$horror_articles\n\n# Option 2: Read directly from GitHub\n\nhorror_articles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-31/horror_articles.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-10-31/readme.html#the-data",
    "href": "data/2023/2023-10-31/readme.html#the-data",
    "title": "Horror Legends",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-10-31')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 44)\n\nhorror_articles &lt;- tuesdata$horror_articles\n\n# Option 2: Read directly from GitHub\n\nhorror_articles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-10-31/horror_articles.csv')"
  },
  {
    "objectID": "data/2023/2023-10-31/readme.html#how-to-participate",
    "href": "data/2023/2023-10-31/readme.html#how-to-participate",
    "title": "Horror Legends",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-11-14/readme.html",
    "href": "data/2023/2023-11-14/readme.html",
    "title": "Diwali Sales Data",
    "section": "",
    "text": "This week is Diwali, the festival of lights! The data this week comes from sales data for a retail store during the Diwali festival period in India. The data is shared on Kaggle by Saad Haroon.\nThis week we’re sharing Python data analysis examples! There’s a few out there, but these ones from Brushan Shelke or Vikas Vachheta (see the Diwali_Sales_Analysis.ipynb file for the code) are some data exploration analyses.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-11-14')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 46)\n\nhouse &lt;- tuesdata$diwali_sales_data\n\n# Option 2: Read directly from GitHub\n\nhouse &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-14/diwali_sales_data.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-11-14/readme.html#the-data",
    "href": "data/2023/2023-11-14/readme.html#the-data",
    "title": "Diwali Sales Data",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-11-14')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 46)\n\nhouse &lt;- tuesdata$diwali_sales_data\n\n# Option 2: Read directly from GitHub\n\nhouse &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-14/diwali_sales_data.csv')"
  },
  {
    "objectID": "data/2023/2023-11-14/readme.html#how-to-participate",
    "href": "data/2023/2023-11-14/readme.html#how-to-participate",
    "title": "Diwali Sales Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-11-28/readme.html",
    "href": "data/2023/2023-11-28/readme.html",
    "title": "Doctor Who Episodes",
    "section": "",
    "text": "Doctor Who is an extremely long-running British television program. The show was revived in 2005, and has proven very popular since then. To celebrate this year’s 60th anniversary of Doctor Who, we have three datasets.\nThe data this week comes from Wikipedia’s [List of Doctor Who episodes](https://en.wikipedia.org/wiki/List_of_Doctor_Who_episodes_(2005%E2%80%93present) via the {datardis} package by Jonathan Kitt. Thank you to Jonathan for compiling and sharing this data!\nAs of 2023-11-24, the data only includes episodes from the “revived” era. For an added challenge, consider submitting a pull request to the {datardis} package to update the data-extraction scripts to also fetch the “classic” era data!\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-11-28')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 48)\n\ndrwho_episodes &lt;- tuesdata$drwho_episodes\ndrwho_directors &lt;- tuesdata$drwho_directors\ndrwho_writers &lt;- tuesdata$drwho_writers\n\n# Option 2: Read directly from GitHub\n\ndrwho_episodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-28/drwho_episodes.csv')\ndrwho_directors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-28/drwho_directors.csv')\ndrwho_writers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-28/drwho_writers.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-11-28/readme.html#the-data",
    "href": "data/2023/2023-11-28/readme.html#the-data",
    "title": "Doctor Who Episodes",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-11-28')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 48)\n\ndrwho_episodes &lt;- tuesdata$drwho_episodes\ndrwho_directors &lt;- tuesdata$drwho_directors\ndrwho_writers &lt;- tuesdata$drwho_writers\n\n# Option 2: Read directly from GitHub\n\ndrwho_episodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-28/drwho_episodes.csv')\ndrwho_directors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-28/drwho_directors.csv')\ndrwho_writers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-11-28/drwho_writers.csv')"
  },
  {
    "objectID": "data/2023/2023-11-28/readme.html#how-to-participate",
    "href": "data/2023/2023-11-28/readme.html#how-to-participate",
    "title": "Doctor Who Episodes",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-12-12/readme.html",
    "href": "data/2023/2023-12-12/readme.html",
    "title": "Holiday Movies",
    "section": "",
    "text": "Happy holidays! This week we’re exploring “holiday” movies: movies with “holiday”, “Christmas”, “Hanukkah”, or “Kwanzaa” (or variants thereof) in their title!\nThe data this week comes from the Internet Movie Database. We don’t have an article using exactly this dataset, but you might get inspiration from this Christmas Movies blog post by Milán Janosov at Central European University.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 50)\n\nholiday_movies &lt;- tuesdata$holiday_movies\nholiday_movie_genres &lt;- tuesdata$holiday_movie_genres\n\n# Option 2: Read directly from GitHub\n\nholiday_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-12/holiday_movies.csv')\nholiday_movie_genres &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-12/holiday_movie_genres.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-12-12/readme.html#the-data",
    "href": "data/2023/2023-12-12/readme.html#the-data",
    "title": "Holiday Movies",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 50)\n\nholiday_movies &lt;- tuesdata$holiday_movies\nholiday_movie_genres &lt;- tuesdata$holiday_movie_genres\n\n# Option 2: Read directly from GitHub\n\nholiday_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-12/holiday_movies.csv')\nholiday_movie_genres &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-12/holiday_movie_genres.csv')"
  },
  {
    "objectID": "data/2023/2023-12-12/readme.html#how-to-participate",
    "href": "data/2023/2023-12-12/readme.html#how-to-participate",
    "title": "Holiday Movies",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-12-26/readme.html",
    "href": "data/2023/2023-12-26/readme.html",
    "title": "R Package Structure",
    "section": "",
    "text": "Happy Boxing Day! While you’re dealing with your physical packages, we’re looking into R packages!\nThe dataset this week comes from “Historical Trends in R Package Structure and Interdependency on CRAN” by Mark Padgham and Noam Ross. In that paper, they use the {pkgstats} R package to analyze the structure of R packages over time, using an archive of all packages on CRAN as of 2022-11-22. We’ve provided csv versions of two of the datasets from that paper.\nThe paper focuses on package characteristics over time. It might be interesting to look at the distribution of similar features (such as lines of code) across packages.\nIf you’re unfamiliar with some of the terminology in this dataset, you might find the R Packages book by Hadley Wickham and Jennifer Bryan helpful.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 52)\n\ncran_20221122 &lt;- tuesdata$cran_20221122\nexternal_calls &lt;- tuesdata$external_calls\ninternal_calls &lt;- tuesdata$internal_calls\n\n# Option 2: Read directly from GitHub\n\ncran_20221122 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-26/cran_20221122.csv')\nexternal_calls &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-26/external_calls.csv')\ninternal_calls &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-26/internal_calls.csv')\nIf you would like to dive deeper, you can download the larger dataset with this code:\ncran_all_20221122 &lt;- readr::read_rds(\"https://zenodo.org/records/7414296/files/pkgstats-CRAN-all.Rds?download=1\")\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2023/2023-12-26/readme.html#the-data",
    "href": "data/2023/2023-12-26/readme.html#the-data",
    "title": "R Package Structure",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 52)\n\ncran_20221122 &lt;- tuesdata$cran_20221122\nexternal_calls &lt;- tuesdata$external_calls\ninternal_calls &lt;- tuesdata$internal_calls\n\n# Option 2: Read directly from GitHub\n\ncran_20221122 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-26/cran_20221122.csv')\nexternal_calls &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-26/external_calls.csv')\ninternal_calls &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-12-26/internal_calls.csv')\nIf you would like to dive deeper, you can download the larger dataset with this code:\ncran_all_20221122 &lt;- readr::read_rds(\"https://zenodo.org/records/7414296/files/pkgstats-CRAN-all.Rds?download=1\")"
  },
  {
    "objectID": "data/2023/2023-12-26/readme.html#how-to-participate",
    "href": "data/2023/2023-12-26/readme.html#how-to-participate",
    "title": "R Package Structure",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-01-09/readme.html",
    "href": "data/2024/2024-01-09/readme.html",
    "title": "Canadian Hockey Player Birth Months",
    "section": "",
    "text": "If you’re a Canadian hockey player, happy birth month! That’s more likely to be correct this time of year than in the Fall!\nThe dataset this week comes from Statistics Canada, the NHL team list endpoint, and the NHL API. The dataset was inspired by the blog Are Birth Dates Still Destiny for Canadian NHL Players? by JLaw (via https://universeodon.com/@jlaw/111522860812359901)!\n\nIn the first chapter Malcolm Gladwell’s Outliers he discusses how in Canadian Junior Hockey there is a higher likelihood for players to be born in the first quarter of the year.\n\n\nBecause these kids are older within their year they make all the important teams at a young age which gets them better resources for skill development and so on.\n\n\nWhile it seems clear that more players are born in the first few months of the year, what isn’t explored is whether or not this would be expected. Maybe more people in Canada in general are born earlier in the year.\n\n\nI will explore whether Gladwell’s result is expected as well as whether this is still true in today’s NHL for Canadian-born players.\n\nCan you reproduce JLaw’s results? What else can you find in the NHL player data?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-01-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 2)\n\ncanada_births_1991_2022 &lt;- tuesdata$canada_births_1991_2022\nnhl_player_births &lt;- tuesdata$nhl_player_births\nnhl_rosters &lt;- tuesdata$nhl_rosters\nnhl_teams &lt;- tuesdata$nhl_teams\n\n# Option 2: Read directly from GitHub\n\ncanada_births_1991_2022 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/canada_births_1991_2022.csv')\nnhl_player_births &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/nhl_player_births.csv')\nnhl_rosters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/nhl_rosters.csv')\nnhl_teams &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/nhl_teams.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-01-09/readme.html#the-data",
    "href": "data/2024/2024-01-09/readme.html#the-data",
    "title": "Canadian Hockey Player Birth Months",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-01-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 2)\n\ncanada_births_1991_2022 &lt;- tuesdata$canada_births_1991_2022\nnhl_player_births &lt;- tuesdata$nhl_player_births\nnhl_rosters &lt;- tuesdata$nhl_rosters\nnhl_teams &lt;- tuesdata$nhl_teams\n\n# Option 2: Read directly from GitHub\n\ncanada_births_1991_2022 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/canada_births_1991_2022.csv')\nnhl_player_births &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/nhl_player_births.csv')\nnhl_rosters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/nhl_rosters.csv')\nnhl_teams &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-09/nhl_teams.csv')"
  },
  {
    "objectID": "data/2024/2024-01-09/readme.html#how-to-participate",
    "href": "data/2024/2024-01-09/readme.html#how-to-participate",
    "title": "Canadian Hockey Player Birth Months",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-01-23/readme.html",
    "href": "data/2024/2024-01-23/readme.html",
    "title": "Educational attainment of young people in English towns",
    "section": "",
    "text": "The dataset this week comes from The UK Office for National Statistics. It was explored in the July 2023 article “Why do children and young people in smaller towns do better academically than those in larger towns?”. Thank you Andrea Carpignani for the dataset suggestion.\nThe article this week contains several plots, one of which is interactive. Can you reproduce them? Can you find anything in the data that isn’t explored in the article?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-01-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 4)\n\nenglish_education &lt;- tuesdata$english_education\n\n# Option 2: Read directly from GitHub\n\nenglish_education &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-23/english_education.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-01-23/readme.html#the-data",
    "href": "data/2024/2024-01-23/readme.html#the-data",
    "title": "Educational attainment of young people in English towns",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-01-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 4)\n\nenglish_education &lt;- tuesdata$english_education\n\n# Option 2: Read directly from GitHub\n\nenglish_education &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-01-23/english_education.csv')"
  },
  {
    "objectID": "data/2024/2024-01-23/readme.html#how-to-participate",
    "href": "data/2024/2024-01-23/readme.html#how-to-participate",
    "title": "Educational attainment of young people in English towns",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-02-06/readme.html",
    "href": "data/2024/2024-02-06/readme.html",
    "title": "A few world heritage sites",
    "section": "",
    "text": "This week we’re exploring a very small subset of UNESCO World Heritage Sites. The 1 dataset, 100 visualizations project used this dataset to explore different ways of visualizing a simple dataset. This is your chance to try that out too! Try recreating some of their plots, or make your own. You can add to your data visualization code toolbox by creating types of visualizations you could use with other datasets, or getting inspiration from others. Share your favorite ones!\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-06')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 6)\n\nheritage &lt;- tuesdata$heritage\n\n# Option 2: Read directly from GitHub\n\nheritage &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-06/heritage.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-02-06/readme.html#the-data",
    "href": "data/2024/2024-02-06/readme.html#the-data",
    "title": "A few world heritage sites",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-06')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 6)\n\nheritage &lt;- tuesdata$heritage\n\n# Option 2: Read directly from GitHub\n\nheritage &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-06/heritage.csv')"
  },
  {
    "objectID": "data/2024/2024-02-06/readme.html#how-to-participate",
    "href": "data/2024/2024-02-06/readme.html#how-to-participate",
    "title": "A few world heritage sites",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-02-20/readme.html",
    "href": "data/2024/2024-02-20/readme.html",
    "title": "R Consortium ISC Grants",
    "section": "",
    "text": "The R Consortium Infrastructure Steering Committee (ISC) Grant Program will accept proposals again between March 1 and April 1, 2024 (and then again in the fall).\n\nThis initiative is a cornerstone of our commitment to bolstering and enhancing the R Ecosystem. We fund projects contributing to the R community’s technical and social infrastructures.\n\nLearn more in their blog post announcing this round of grants.\nThe R Consortium ISC has been awarding grants since 2016. This week’s data is an exploration of past grant recipients.\nAre there any keywords that stand out in the titles or summaries of awarded grants? Have the funded amounts changed over time?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-20')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 8)\n\nisc_grants &lt;- tuesdata$isc_grants\n\n# Option 2: Read directly from GitHub\n\nisc_grants &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-20/isc_grants.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-02-20/readme.html#the-data",
    "href": "data/2024/2024-02-20/readme.html#the-data",
    "title": "R Consortium ISC Grants",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-20')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 8)\n\nisc_grants &lt;- tuesdata$isc_grants\n\n# Option 2: Read directly from GitHub\n\nisc_grants &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-02-20/isc_grants.csv')"
  },
  {
    "objectID": "data/2024/2024-02-20/readme.html#how-to-participate",
    "href": "data/2024/2024-02-20/readme.html#how-to-participate",
    "title": "R Consortium ISC Grants",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-03-05/readme.html",
    "href": "data/2024/2024-03-05/readme.html",
    "title": "Trash Wheel Collection Data",
    "section": "",
    "text": "This week’s data is Trash Wheel Collection Data from the Mr. Trash Wheel Baltimore Healthy Harbor initiative.\n\nMr. Trash Wheel is a semi-autonomous trash interceptor that is placed at the end of a river, stream or other outfall. Far too lazy to chase trash around the ocean, Mr. Trash Wheel stays put and waits for the waste to flow to him. Sustainably powered and built to withstand the biggest storms, Mr. Trash Wheel uses a unique blend of solar and hydro power to pull hundreds of tons of trash out of the water each year.\n\nThe Healthy Harbor initiative has four Trash Wheels collecting trash. Mr. Trash Wheel was the first to start, and since then three more have joined the family. The Trash Wheel Family has collected more than 2,362 tons of trash. See more about how Mr. Trash Wheel works.\n\nData collection methodology\n\n\n\nWhen crew members are on the machine during the time when a dumpster is being filled, they will manually count the number of each of the item types listed on a single conveyor paddle. This process is repeated several times during the dumpster filling process. An average is then calculated for number of each item per paddle. The average is then multiplied by the paddle rate and then by the elapsed time to fill the dumpster.\n\n\n\nExample: * Paddle #1- 9 plastic bottles * Paddle #2- 14 plastic bottles * Paddle #3- 5 plastic bottles * Paddle #4- 12 plastic bottles * Average = 10 plastic bottles/paddle\n\n\nConveyor speed = 2.5 paddles per minute therefore an average of 25 plastic bottles are loaded each minute. If it takes 100 minutes to fill the dumpster, we estimate that there are 2,500 bottles in that dumpster.\n\n\n\nIf no crew is present during the loading, we will take random bushel size samples of the collected material and count items in these samples. A full dumpster contains approximately 325 bushels. Therefore, if an average bushel sample from a dumpster contains 3 polystyrene containers, we estimate that the dumpster contains 975 polystyrene containers.\n\n\n\n\nPeriodically “dumpster dives” are held where volunteers count everything in an entire dumpster. These events help validate our sampling methods and also look at what materials are dumpster. present that are not included in our sampling categories.\n\n\nWhat type of trash is collected the most? Do the different Trash Wheels collect different sets of trash? Are there times of the year when more or less trash is collected?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-03-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 10)\n\ntrashwheel &lt;- tuesdata$trashwheel\n\n\n# Option 2: Read directly from GitHub\n\ntrashwheel &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-05/trashwheel.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-03-05/readme.html#the-data",
    "href": "data/2024/2024-03-05/readme.html#the-data",
    "title": "Trash Wheel Collection Data",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-03-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 10)\n\ntrashwheel &lt;- tuesdata$trashwheel\n\n\n# Option 2: Read directly from GitHub\n\ntrashwheel &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-05/trashwheel.csv')"
  },
  {
    "objectID": "data/2024/2024-03-05/readme.html#how-to-participate",
    "href": "data/2024/2024-03-05/readme.html#how-to-participate",
    "title": "Trash Wheel Collection Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-03-19/readme.html",
    "href": "data/2024/2024-03-19/readme.html",
    "title": "X-Men Mutant Moneyball",
    "section": "",
    "text": "This week’s data is X-Men Mutant Moneyball from Rally’s Mutant moneyball: a data driven ultimate X-men by Anderson Evans.\nThis is the data used in Rally’s Mutant Moneyball article which visualizes X-Men value data, era by era, from the X-Men’s creation in 1963 up to 1993. The idea is that the the concepts in Michael Lewis’ book Moneyball, can be applied in a number of cultural industries, not just baseball like in the book and the movie.\n\nIt’s no accident that the name of this piece is Mutant Moneyball, as the overall point is an exercise in valuing each individual character on the X-Men team in a variety of data driven ways. But buying, reading, and loving comic books should be purely for fun and the raw expression of devotion toward characters, art, and story. True: It’s a lesson we fans have failed to learn in the past. The trend of buying comics based on value speculation crippled the entire comic book industry for over a decade in the late 1990’s and early 2000’s. And the point of this isn’t to repeat those mistakes, it’s to gamify open and available financial data, giving us special insight, once unattainable, regarding our magnificent mutants. Like mature adults.\n\n\nWhy are some characters sought after more than others, what stories did some mighty mutants convey that made them sought after, while others that eek out a continued existence are unable to resonate enough with a fan base to garner the same kind of love and appreciation in the secondary market? Are there answers that a close reading of value data can offer?\n\nDollar signs and percentage signs are in the data for some of the columns. Which columns are they in? Do you need to address that before you work with the data in those columns?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-03-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 12)\n\nmutant_moneyball &lt;- tuesdata$mutant_moneyball\n\n\n# Option 2: Read directly from GitHub\n\nmutant_moneyball &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-19/mutant_moneyball.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-03-19/readme.html#the-data",
    "href": "data/2024/2024-03-19/readme.html#the-data",
    "title": "X-Men Mutant Moneyball",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-03-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 12)\n\nmutant_moneyball &lt;- tuesdata$mutant_moneyball\n\n\n# Option 2: Read directly from GitHub\n\nmutant_moneyball &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-03-19/mutant_moneyball.csv')"
  },
  {
    "objectID": "data/2024/2024-03-19/readme.html#how-to-participate",
    "href": "data/2024/2024-03-19/readme.html#how-to-participate",
    "title": "X-Men Mutant Moneyball",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-02/readme.html",
    "href": "data/2024/2024-04-02/readme.html",
    "title": "Du Bois Visualization Challenge 2024",
    "section": "",
    "text": "This week we’re inviting you to participate in the 2024 Du Bois Visualization Challenge.\n\nThe goal of the challenge is to celebrate the data visualization legacy of W.E.B Du Bois by recreating the visualizations from the 1900 Paris Exposition using modern tools.\n\nThis year the challenge is organized around the colors of the Pan African Flag. For this final week, use a combination of the colors from the flag to reproduce plate 37, “A Series Of Statistical Charts Illustrating The Conditions Of Descendants Of Formal African Slaves Now Resident In The Unites States.” Visit the challenge GitHub repository for more details!\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-02')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 14)\n\ndubois_week10 &lt;- tuesdata$dubois_week10\n\n# Option 2: Read directly from GitHub\n\ndubois_week10 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-02/dubois_week10.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-02/readme.html#the-data",
    "href": "data/2024/2024-04-02/readme.html#the-data",
    "title": "Du Bois Visualization Challenge 2024",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-02')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 14)\n\ndubois_week10 &lt;- tuesdata$dubois_week10\n\n# Option 2: Read directly from GitHub\n\ndubois_week10 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-02/dubois_week10.csv')"
  },
  {
    "objectID": "data/2024/2024-04-02/readme.html#how-to-participate",
    "href": "data/2024/2024-04-02/readme.html#how-to-participate",
    "title": "Du Bois Visualization Challenge 2024",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-16/readme.html",
    "href": "data/2024/2024-04-16/readme.html",
    "title": "Shiny Packages",
    "section": "",
    "text": "It’s time for ShinyConf2024!\n\nShinyConf is more than just an event; it’s a global gathering of diverse Shiny community, creating a shared space for learning, networking, and collaborative exploration. Connect with like-minded enthusiasts from all corners of the world!\n\nYour TidyTuesday maintainers – Tracy Teal and Jon Harmon – are both speaking at ShinyConf on Thursday, 2024-04-18. We look forward to seeing you there!\nWhat is the most popular way in which packages are connected to Shiny? Can you create a Shiny app to explore this data?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 16)\n\nshiny_revdeps &lt;- tuesdata$shiny_revdeps\npackage_details &lt;- tuesdata$package_details\n\n# Option 2: Read directly from GitHub\n\nshiny_revdeps &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-16/shiny_revdeps.csv')\npackage_details &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-16/package_details.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-16/readme.html#the-data",
    "href": "data/2024/2024-04-16/readme.html#the-data",
    "title": "Shiny Packages",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 16)\n\nshiny_revdeps &lt;- tuesdata$shiny_revdeps\npackage_details &lt;- tuesdata$package_details\n\n# Option 2: Read directly from GitHub\n\nshiny_revdeps &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-16/shiny_revdeps.csv')\npackage_details &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-16/package_details.csv')"
  },
  {
    "objectID": "data/2024/2024-04-16/readme.html#how-to-participate",
    "href": "data/2024/2024-04-16/readme.html#how-to-participate",
    "title": "Shiny Packages",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-30/readme.html",
    "href": "data/2024/2024-04-30/readme.html",
    "title": "Worldwide Bureaucracy Indicators",
    "section": "",
    "text": "This week we’re looking at the Worldwide Bureaucracy Indicators (WWBI) dataset from the World Bank.\n\nThe Worldwide Bureaucracy Indicators (WWBI) database is a unique cross-national dataset on public sector employment and wages that aims to fill an information gap, thereby helping researchers, development practitioners, and policymakers gain a better understanding of the personnel dimensions of state capability, the footprint of the public sector within the overall labor market, and the fiscal implications of the public sector wage bill. The dataset is derived from administrative data and household surveys, thereby complementing existing, expert perception-based approaches.\n\nThe World Bank introduced the dataset with a series of four blogs:\n\nblog1\nblog2\nblog3\nblog4\n\nCan you replicate the figures in the blogs? Can you display any of the data more clearly than in the blogs?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 18)\n\nwwbi_data &lt;- tuesdata$wwbi_data\nwwbi_series &lt;- tuesdata$wwbi_series\nwwbi_country &lt;- tuesdata$wwbi_country\n\n\n# Option 2: Read directly from GitHub\n\nwwbi_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-30/wwbi_data.csv')\nwwbi_series &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-30/wwbi_series.csv')\nwwbi_country &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-30/wwbi_country.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-04-30/readme.html#the-data",
    "href": "data/2024/2024-04-30/readme.html#the-data",
    "title": "Worldwide Bureaucracy Indicators",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 18)\n\nwwbi_data &lt;- tuesdata$wwbi_data\nwwbi_series &lt;- tuesdata$wwbi_series\nwwbi_country &lt;- tuesdata$wwbi_country\n\n\n# Option 2: Read directly from GitHub\n\nwwbi_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-30/wwbi_data.csv')\nwwbi_series &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-30/wwbi_series.csv')\nwwbi_country &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-30/wwbi_country.csv')"
  },
  {
    "objectID": "data/2024/2024-04-30/readme.html#how-to-participate",
    "href": "data/2024/2024-04-30/readme.html#how-to-participate",
    "title": "Worldwide Bureaucracy Indicators",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-05-14/readme.html",
    "href": "data/2024/2024-05-14/readme.html",
    "title": "The Great American Coffee Taste Test",
    "section": "",
    "text": "In October 2023, “world champion barista” James Hoffmann and coffee company Cometeer held the “Great American Coffee Taste Test” on YouTube, during which viewers were asked to fill out a survey about 4 coffees they ordered from Cometeer for the tasting. Data blogger Robert McKeon Aloe analyzed the data the following month.\nDo you think participants in this survey are representative of Americans in general?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-14')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 20)\n\ncoffee_survey &lt;- tuesdata$coffee_survey\n\n\n# Option 2: Read directly from GitHub\n\ncoffee_survey &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-14/coffee_survey.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-05-14/readme.html#the-data",
    "href": "data/2024/2024-05-14/readme.html#the-data",
    "title": "The Great American Coffee Taste Test",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-14')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 20)\n\ncoffee_survey &lt;- tuesdata$coffee_survey\n\n\n# Option 2: Read directly from GitHub\n\ncoffee_survey &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-14/coffee_survey.csv')"
  },
  {
    "objectID": "data/2024/2024-05-14/readme.html#how-to-participate",
    "href": "data/2024/2024-05-14/readme.html#how-to-participate",
    "title": "The Great American Coffee Taste Test",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-05-28/readme.html",
    "href": "data/2024/2024-05-28/readme.html",
    "title": "Lisa’s Vegetable Garden Data",
    "section": "",
    "text": "We’re exploring Lisa Lendway’s vegetable garden from summer 2020 and summer 2021, from her {gardenR} package. Thank you to Lisa for suggesting this dataset back in 2021!\n\nThe gardenR package contains data collected by Lisa Lendway from her vegetable garden, starting in the summer of 2020. Data from the summer of 2021 was added 2022-01-29 (finally!). The data were used in her Introduction to Data Science course at Macalester College to introduce many concepts. For examples, see the tutorials for the course.\n\nLisa also has a YouTube video with a visual tour of the garden.\nWhat changed between 2020 and 2021?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-28')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 22)\n\nharvest_2020 &lt;- tuesdata$harvest_2020\nharvest_2021 &lt;- tuesdata$harvest_2021\nplanting_2020 &lt;- tuesdata$planting_2020\nplanting_2021 &lt;- tuesdata$planting_2021\nspending_2020 &lt;- tuesdata$spending_2020\nspending_2021 &lt;- tuesdata$spending_2021\n\n\n# Option 2: Read directly from GitHub\n\nharvest_2020 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/harvest_2020.csv')\nharvest_2021 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/harvest_2021.csv')\nplanting_2020 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/planting_2020.csv')\nplanting_2021 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/planting_2021.csv')\nspending_2020 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/spending_2020.csv')\nspending_2021 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/spending_2021.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-05-28/readme.html#the-data",
    "href": "data/2024/2024-05-28/readme.html#the-data",
    "title": "Lisa’s Vegetable Garden Data",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-28')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 22)\n\nharvest_2020 &lt;- tuesdata$harvest_2020\nharvest_2021 &lt;- tuesdata$harvest_2021\nplanting_2020 &lt;- tuesdata$planting_2020\nplanting_2021 &lt;- tuesdata$planting_2021\nspending_2020 &lt;- tuesdata$spending_2020\nspending_2021 &lt;- tuesdata$spending_2021\n\n\n# Option 2: Read directly from GitHub\n\nharvest_2020 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/harvest_2020.csv')\nharvest_2021 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/harvest_2021.csv')\nplanting_2020 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/planting_2020.csv')\nplanting_2021 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/planting_2021.csv')\nspending_2020 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/spending_2020.csv')\nspending_2021 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-05-28/spending_2021.csv')"
  },
  {
    "objectID": "data/2024/2024-05-28/readme.html#how-to-participate",
    "href": "data/2024/2024-05-28/readme.html#how-to-participate",
    "title": "Lisa’s Vegetable Garden Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-06-11/readme.html",
    "href": "data/2024/2024-06-11/readme.html",
    "title": "Campus Pride Index",
    "section": "",
    "text": "Happy Pride Month! Check out the {gglgbtq} package for LGBTQ-related themes and color palettes!\nThe data this week comes from the Campus Pride Index.\n\nSince 2007, the Campus Pride Index has been the premier LGBTQ national benchmarking tool for colleges and universities to create safer, more inclusive campus communities. The free online tool allows prospective students, families/parents and those interested in higher education to search a database of LGBTQ-friendly campuses who have come out to improve the academic experience and quality of campus life.\n\nThe website has additional information about each campus, including a more detailed breakdown of their Campus Pride Index rating.\nIs there a relationship between the size of a campus or the surrounding community and its rating? What about the state in which the campus is located?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-06-11')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 24)\n\npride_index &lt;- tuesdata$pride_index\npride_index_tags &lt;- tuesdata$pride_index_tags\n\n# Option 2: Read directly from GitHub\n\npride_index &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-11/pride_index.csv')\npride_index_tags &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-11/pride_index_tags.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-06-11/readme.html#the-data",
    "href": "data/2024/2024-06-11/readme.html#the-data",
    "title": "Campus Pride Index",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-06-11')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 24)\n\npride_index &lt;- tuesdata$pride_index\npride_index_tags &lt;- tuesdata$pride_index_tags\n\n# Option 2: Read directly from GitHub\n\npride_index &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-11/pride_index.csv')\npride_index_tags &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-11/pride_index_tags.csv')"
  },
  {
    "objectID": "data/2024/2024-06-11/readme.html#how-to-participate",
    "href": "data/2024/2024-06-11/readme.html#how-to-participate",
    "title": "Campus Pride Index",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-06-25/readme.html",
    "href": "data/2024/2024-06-25/readme.html",
    "title": "tidyRainbow Datasets",
    "section": "",
    "text": "Happy Pride Month! Check out the {gglgbtq} package for LGBTQ-related themes and color palettes!\nThe data this week comes from the tidyRainbow, “a data project for the LGBTQ+ community who use the R language ecosystem.”\n\nThe data sets in this repository focus on data pertaining to the LGBTQ+ community. We also look for data sets where LGBTQ+ folk are explicitly represented and where it is not assumed that gender is binary. Additionally, we include data sets that are relevant to LGBTQ+ folk because of the impact it has on the community.\n\nWe’re including their LGBTQ Movies database dataset curated by Cara Cuiule (She/Her), but we invite you to explore their other datasets, or to submit any LGBTQ+ related datatsets you know about!\nWhere do the most popular LGBTQ+ movies come from? Are more LGBTQ+ movies being released over time?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-06-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 26)\n\nlgbtq_movies &lt;- tuesdata$lgbtq_movies\n\n# Option 2: Read directly from GitHub\n\nlgbtq_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-25/lgbtq_movies.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-06-25/readme.html#the-data",
    "href": "data/2024/2024-06-25/readme.html#the-data",
    "title": "tidyRainbow Datasets",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-06-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 26)\n\nlgbtq_movies &lt;- tuesdata$lgbtq_movies\n\n# Option 2: Read directly from GitHub\n\nlgbtq_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-06-25/lgbtq_movies.csv')"
  },
  {
    "objectID": "data/2024/2024-06-25/readme.html#how-to-participate",
    "href": "data/2024/2024-06-25/readme.html#how-to-participate",
    "title": "tidyRainbow Datasets",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-09/readme.html",
    "href": "data/2024/2024-07-09/readme.html",
    "title": "David Robinson’s TidyTuesday Functions",
    "section": "",
    "text": "This week we’re seeing how David Robinson has explored TidyTuesday data in his YouTube screencasts! Thanks to Bryan Shalloway for the suggestion, the {funspotr} package, and the blog posts about how to use funspotr (including this one about making network graphs with the data).\n\nThe goal of funspotr (R function spotter) is to make it easy to identify which R functions and packages are used in files and projects. It was initially written to create reference tables of the functions and packages used in a few popular github repositories.\n\nWhat are David’s most-used functions? Can you find relationships between functions used and the variable types (using the {ttmeta} package)? How does his function graph compare to your own?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 28)\n\ndrob_funs &lt;- tuesdata$drob_funs\n\n# Option 2: Read directly from GitHub\n\ndrob_funs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-09/drob_funs.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-09/readme.html#the-data",
    "href": "data/2024/2024-07-09/readme.html#the-data",
    "title": "David Robinson’s TidyTuesday Functions",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 28)\n\ndrob_funs &lt;- tuesdata$drob_funs\n\n# Option 2: Read directly from GitHub\n\ndrob_funs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-09/drob_funs.csv')"
  },
  {
    "objectID": "data/2024/2024-07-09/readme.html#how-to-participate",
    "href": "data/2024/2024-07-09/readme.html#how-to-participate",
    "title": "David Robinson’s TidyTuesday Functions",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-23/readme.html",
    "href": "data/2024/2024-07-23/readme.html",
    "title": "American Idol data",
    "section": "",
    "text": "This week we’re exploring American Idol data! This is a comprehensive dataset put together by kkakey.\nThere’s so much data! What do you want to know about American Idol? Song choices, TV ratings, characteristics of winners?\n\nData in this dataset comes from Wikipedia. Data collected on seasons 1-18 of American Idol.\n\n\nThe Datasets * songs.csv - songs that contestants sang and competed with on American Idol from seasons 1-18 * auditions.csv - audition, cities, dates, and venues * elimination_chart.csv - eliminations by week. Data availability varies season-to-season based on season length and number of finalists competing * finalists.csv - information on top contestants, including birthday, hometown, and description * ratings.csv - episode ratings and views. * seasons.csv - season-level information, including season winner, runner-up, release dates, and judges\n\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 30)\n\nauditions &lt;- tuesdata$auditions\neliminations &lt;- tuesdata$eliminations\nfinalists &lt;- tuesdata$finalists\nratings &lt;- tuesdata$ratings\nseasons &lt;- tuesdata$seasons\nsongs &lt;- tuesdata$songs\n\n# Option 2: Read directly from GitHub\n\nauditions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/auditions.csv')\neliminations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/eliminations.csv')\nfinalists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/finalists.csv')\nratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/ratings.csv')\nseasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/seasons.csv')\nsongs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/songs.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-07-23/readme.html#the-data",
    "href": "data/2024/2024-07-23/readme.html#the-data",
    "title": "American Idol data",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 30)\n\nauditions &lt;- tuesdata$auditions\neliminations &lt;- tuesdata$eliminations\nfinalists &lt;- tuesdata$finalists\nratings &lt;- tuesdata$ratings\nseasons &lt;- tuesdata$seasons\nsongs &lt;- tuesdata$songs\n\n# Option 2: Read directly from GitHub\n\nauditions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/auditions.csv')\neliminations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/eliminations.csv')\nfinalists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/finalists.csv')\nratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/ratings.csv')\nseasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/seasons.csv')\nsongs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-07-23/songs.csv')"
  },
  {
    "objectID": "data/2024/2024-07-23/readme.html#how-to-participate",
    "href": "data/2024/2024-07-23/readme.html#how-to-participate",
    "title": "American Idol data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-08-06/readme.html",
    "href": "data/2024/2024-08-06/readme.html",
    "title": "Olympics athletes and medals",
    "section": "",
    "text": "This week we’re exploring Olympics data!\nThe data this week comes from the RGriffin Kaggle dataset: 120 years of Olympic history: athletes and results, basic bio data on athletes and medal results from Athens 1896 to Rio 2016.\nThis is an intentional repeat of TidyTuesday 2021-07-27, since it’s the Olympics!\n\nThis is a historical dataset on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2016. I scraped this data from www.sports-reference.com in May 2018.\nNote that the Winter and Summer Games were held in the same year up until 1992. After that, they staggered them such that Winter Games occur on a four year cycle starting with 1994, then Summer in 1996, then Winter in 1998, and so on. A common mistake people make when analyzing this data is to assume that the Summer and Winter Games have always been staggered.\n\n\nThe Olympic data on www.sports-reference.com is the result of an incredible amount of research by a group of Olympic history enthusiasts and self-proclaimed ‘statistorians’. Check out their blog for more information. All I did was consolidated their decades of work into a convenient format for data analysis.\n\nInformation from this year’s Olympics, along with past games, are on the Olympics results page.\nIf you were doing TidyTuesdays in 2021, what can you do differently now? Exploring for the first time? What are the patterns? Can you use this historical data to predict this year’s results, and how correct are you?\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-06')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 32)\n\nolympics &lt;- tuesdata$olympics\n\n\n# Option 2: Read directly from GitHub\n\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-06/olympics.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-08-06/readme.html#the-data",
    "href": "data/2024/2024-08-06/readme.html#the-data",
    "title": "Olympics athletes and medals",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-06')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 32)\n\nolympics &lt;- tuesdata$olympics\n\n\n# Option 2: Read directly from GitHub\n\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-06/olympics.csv')"
  },
  {
    "objectID": "data/2024/2024-08-06/readme.html#how-to-participate",
    "href": "data/2024/2024-08-06/readme.html#how-to-participate",
    "title": "Olympics athletes and medals",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag."
  },
  {
    "objectID": "data/2024/2024-08-20/readme.html",
    "href": "data/2024/2024-08-20/readme.html",
    "title": "English Monarchs and Marriages",
    "section": "",
    "text": "This week we are exploring English Monarchs and Marriages!\n\nthis dataset focuses on the names, ages, and marriages of various ‘kings’ and ‘consorts’. the data ranges all the way back to 850 where the details are a bit fuzzy, spanning all the way to current day. names contain special characters; ages & years can be a bit of a regex puzzle as well. additionally, the age of kings and consorts may show quite a bit of an age gap.\n\nThe data was scraped from Ian Visits by f. hull, who also curated this week’s post!\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-20')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 34)\n\nenglish_monarchs_marriages_df &lt;- tuesdata$english_monarchs_marriages_df\n\n# Option 2: Read directly from GitHub\n\nenglish_monarchs_marriages_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-20/english_monarchs_marriages_df.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-08-20/readme.html#the-data",
    "href": "data/2024/2024-08-20/readme.html#the-data",
    "title": "English Monarchs and Marriages",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-20')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 34)\n\nenglish_monarchs_marriages_df &lt;- tuesdata$english_monarchs_marriages_df\n\n# Option 2: Read directly from GitHub\n\nenglish_monarchs_marriages_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-08-20/english_monarchs_marriages_df.csv')"
  },
  {
    "objectID": "data/2024/2024-08-20/readme.html#how-to-participate",
    "href": "data/2024/2024-08-20/readme.html#how-to-participate",
    "title": "English Monarchs and Marriages",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-09-03/readme.html",
    "href": "data/2024/2024-09-03/readme.html",
    "title": "Stack Overflow Annual Developer Survey 2024",
    "section": "",
    "text": "This week’s dataset is derived from the 2024 Stack Overflow Annual Developer Survey. Conducted in May 2024, the survey gathered responses from over 65,000 developers across seven key sections:\n\nBasic information\nEducation, work, and career\nTech and tech culture\nStack Overflow community\nArtificial Intelligence (AI)\nProfessional Developer Series - Not part of the main survey\nThoughts on Survey\n\nThe dataset provided for this analysis focuses exclusively on the single-response questions from the main survey sections. Each categorical response in the survey has been integer-coded, with corresponding labels available in the crosswalk file.\nWhat can you see about developer demographics? How do developers engage with Stack Overflow? What do they think about AI?\nThank you to Havisha Khurana for curating this week’s dataset!\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-09-03')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 36)\n\nqname_levels_single_response_crosswalk &lt;- tuesdata$qname_levels_single_response_crosswalk\nstackoverflow_survey_questions &lt;- tuesdata$stackoverflow_survey_questions\nstackoverflow_survey_single_response &lt;- tuesdata$stackoverflow_survey_single_response\n\n# Option 2: Read directly from GitHub\n\nqname_levels_single_response_crosswalk &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-03/qname_levels_single_response_crosswalk.csv')\nstackoverflow_survey_questions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-03/stackoverflow_survey_questions.csv')\nstackoverflow_survey_single_response &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-03/stackoverflow_survey_single_response.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-09-03/readme.html#the-data",
    "href": "data/2024/2024-09-03/readme.html#the-data",
    "title": "Stack Overflow Annual Developer Survey 2024",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-09-03')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 36)\n\nqname_levels_single_response_crosswalk &lt;- tuesdata$qname_levels_single_response_crosswalk\nstackoverflow_survey_questions &lt;- tuesdata$stackoverflow_survey_questions\nstackoverflow_survey_single_response &lt;- tuesdata$stackoverflow_survey_single_response\n\n# Option 2: Read directly from GitHub\n\nqname_levels_single_response_crosswalk &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-03/qname_levels_single_response_crosswalk.csv')\nstackoverflow_survey_questions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-03/stackoverflow_survey_questions.csv')\nstackoverflow_survey_single_response &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-03/stackoverflow_survey_single_response.csv')"
  },
  {
    "objectID": "data/2024/2024-09-03/readme.html#how-to-participate",
    "href": "data/2024/2024-09-03/readme.html#how-to-participate",
    "title": "Stack Overflow Annual Developer Survey 2024",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-09-17/readme.html",
    "href": "data/2024/2024-09-17/readme.html",
    "title": "Shakespeare Dialogue",
    "section": "",
    "text": "This week we’re exploring dialogue in Shakespeare plays. The dataset this week comes from shakespeare.mit.edu (via github.com/nrennie/shakespeare) which is the Web’s first edition of the Complete Works of William Shakespeare. The site has offered Shakespeare’s plays and poetry to the internet community since 1993.\nDialogue from Hamlet, Macbeth, and Romeo and Juliet are provided for this week. Which play has the most stage directions compared to dialogue? Which play has the longest lines of dialogue? Which character speaks the most?\nThank you to Nicola Rennie for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-09-17')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 38)\n\nhamlet &lt;- tuesdata$hamlet\nmacbeth &lt;- tuesdata$macbeth\nromeo_juliet &lt;- tuesdata$romeo_juliet\n\n# Option 2: Read directly from GitHub\n\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/romeo_juliet.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-09-17/readme.html#the-data",
    "href": "data/2024/2024-09-17/readme.html#the-data",
    "title": "Shakespeare Dialogue",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-09-17')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 38)\n\nhamlet &lt;- tuesdata$hamlet\nmacbeth &lt;- tuesdata$macbeth\nromeo_juliet &lt;- tuesdata$romeo_juliet\n\n# Option 2: Read directly from GitHub\n\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/romeo_juliet.csv')"
  },
  {
    "objectID": "data/2024/2024-09-17/readme.html#how-to-participate",
    "href": "data/2024/2024-09-17/readme.html#how-to-participate",
    "title": "Shakespeare Dialogue",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-01/readme.html",
    "href": "data/2024/2024-10-01/readme.html",
    "title": "Chess Game Dataset (Lichess)",
    "section": "",
    "text": "The chess dataset this week comes from Lichess.org via Kaggle/Mitchell J.\n\nThis is a set of just over 20,000 games collected from a selection of users on the site Lichess.org.\n\nUse the data to explore -\n\nWhat the common opening moves? By ranks?\nHow many turns does a game last based on player ranking?\nWhat move patterns explain the game outcome?\n\nThank you to Havisha Khurana for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-01')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 40)\n\nchess &lt;- tuesdata$chess\n\n# Option 2: Read directly from GitHub\n\nchess &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-01/chess.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-01/readme.html#the-data",
    "href": "data/2024/2024-10-01/readme.html#the-data",
    "title": "Chess Game Dataset (Lichess)",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-01')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 40)\n\nchess &lt;- tuesdata$chess\n\n# Option 2: Read directly from GitHub\n\nchess &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-01/chess.csv')"
  },
  {
    "objectID": "data/2024/2024-10-01/readme.html#how-to-participate",
    "href": "data/2024/2024-10-01/readme.html#how-to-participate",
    "title": "Chess Game Dataset (Lichess)",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-15/readme.html",
    "href": "data/2024/2024-10-15/readme.html",
    "title": "Southern Resident Killer Whale Encounters",
    "section": "",
    "text": "The data this week comes from the Center for Whale Research (CWR), the leading organization monitoring and studying Southern Resident killer whales in their critical habitat: the Pacific Northwest’s Salish Sea. Each encounter is hosted on its own webpage at whaleresearch.com. Jadey Ryan scraped the encounter data from CWR’s website as a personal project to learn web scraping and presented the process at a Seattle R-Ladies meetup in 2023. The scraping functions and cleaning code for 2017 - 2024 encounters can be found in the {orcas} R package.\nThe dataset is mostly tidy but not completely clean. There are still missing values and typos, as evident from some encounters having a negative duration.\n\nAn Encounter refers to any time we observe killer whales (orcas), from one of CWR’s research boats or land, where at least one individual is identified and photographed. Typically, 2-4 staff are involved in an encounter. Once we come into contact with whales (ie. within distance of identifying individuals by sight) we have begun our encounter. During an encounter, our main goal is to photograph every individual present from both the left and right side.\n\nWhich pods or ecotypes have the longest duration encounters with CWR researchers? Are there trends in where orca encounters occur over time?\nThank you to Jadey Ryan for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-15')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 42)\n\norcas &lt;- tuesdata$orcas\n\n# Option 2: Read directly from GitHub\n\norcas &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-15/orcas.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-15/readme.html#the-data",
    "href": "data/2024/2024-10-15/readme.html#the-data",
    "title": "Southern Resident Killer Whale Encounters",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-15')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 42)\n\norcas &lt;- tuesdata$orcas\n\n# Option 2: Read directly from GitHub\n\norcas &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-15/orcas.csv')"
  },
  {
    "objectID": "data/2024/2024-10-15/readme.html#how-to-participate",
    "href": "data/2024/2024-10-15/readme.html#how-to-participate",
    "title": "Southern Resident Killer Whale Encounters",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-29/readme.html",
    "href": "data/2024/2024-10-29/readme.html",
    "title": "Monster Movies",
    "section": "",
    "text": "This week we’re exploring “monster” movies: movies with “monster” in their title!\nThe data this week comes from the Internet Movie Database. Check out “Why Do People Like Horror Films? A Statistical Analysis” for an exploration of “the unique appeal of scary movies”.\nWhat are the most common combinations of genres for “monster” movies? How do “monster” movies compare to “summer” movies or “holiday” movies? What words are joined with “monster” in popular “monster” movies?\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-29')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 44)\n\nmonster_movie_genres &lt;- tuesdata$monster_movie_genres\nmonster_movies &lt;- tuesdata$monster_movies\n\n# Option 2: Read directly from GitHub\n\nmonster_movie_genres &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-29/monster_movie_genres.csv')\nmonster_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-29/monster_movies.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-10-29/readme.html#the-data",
    "href": "data/2024/2024-10-29/readme.html#the-data",
    "title": "Monster Movies",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-10-29')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 44)\n\nmonster_movie_genres &lt;- tuesdata$monster_movie_genres\nmonster_movies &lt;- tuesdata$monster_movies\n\n# Option 2: Read directly from GitHub\n\nmonster_movie_genres &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-29/monster_movie_genres.csv')\nmonster_movies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-29/monster_movies.csv')"
  },
  {
    "objectID": "data/2024/2024-10-29/readme.html#how-to-participate",
    "href": "data/2024/2024-10-29/readme.html#how-to-participate",
    "title": "Monster Movies",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-11-12/readme.html",
    "href": "data/2024/2024-11-12/readme.html",
    "title": "ISO Country Codes",
    "section": "",
    "text": "We’ve referenced countries and country codes in many past datasets, but we’ve never looked closely at the ISO 3166 standard that defines these codes.\nWikipedia says:\n\nISO 3166 is a standard published by the International Organization for Standardization (ISO) that defines codes for the names of countries, dependent territories, special areas of geographical interest, and their principal subdivisions (e.g., provinces or states). The official name of the standard is Codes for the representation of names of countries and their subdivisions.\n\nThe dataset this week comes from the {ISOcodes} R package. It consists of three tables:\n\ncountries: Country codes from ISO 3166-1.\ncountry_subdivisions: Country subdivision code from ISO 3166-2.\nformer_countries: Code for formerly used names of countries from ISO 3166-3.\n\nTip: Try the quick_map() function in the {countries} package to produce maps colored by country.\nSome questions to consider:\n\nWhen did ISO 3166-3 begin to log the date withdrawn as a full date, rather than just a year?\nWhich countries have the most subdivisions identified by the International Organization for Standardization (ISO)?\nIs there a pattern to which countries have sub-subdivisions (subdivisions with a parent) and which don’t?\n\nYou can use this code to explore past datasets that have mentioned countries and/or country codes:\n# install.packages(\"pak\")\n# pak::pak(\"r4ds/ttmeta\")\nttmeta::tt_datasets_metadata |&gt; \n  dplyr::mutate(\n    has_country = purrr::map_lgl(variable_details, function(var_details) {\n      \"country_code\" %in% tolower(var_details$variable) ||\n        any(stringr::str_detect(tolower(var_details$variable), \"country\"))\n    })\n  ) |&gt; \n  dplyr::filter(has_country)\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-11-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 46)\n\ncountries &lt;- tuesdata$countries\ncountry_subdivisions &lt;- tuesdata$country_subdivisions\nformer_countries &lt;- tuesdata$former_countries\n\n# Option 2: Read directly from GitHub\n\ncountries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-12/countries.csv')\ncountry_subdivisions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-12/country_subdivisions.csv')\nformer_countries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-12/former_countries.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-11-12/readme.html#the-data",
    "href": "data/2024/2024-11-12/readme.html#the-data",
    "title": "ISO Country Codes",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-11-12')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 46)\n\ncountries &lt;- tuesdata$countries\ncountry_subdivisions &lt;- tuesdata$country_subdivisions\nformer_countries &lt;- tuesdata$former_countries\n\n# Option 2: Read directly from GitHub\n\ncountries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-12/countries.csv')\ncountry_subdivisions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-12/country_subdivisions.csv')\nformer_countries &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-12/former_countries.csv')"
  },
  {
    "objectID": "data/2024/2024-11-12/readme.html#how-to-participate",
    "href": "data/2024/2024-11-12/readme.html#how-to-participate",
    "title": "ISO Country Codes",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-11-26/readme.html",
    "href": "data/2024/2024-11-26/readme.html",
    "title": "U.S. Customs and Border Protection (CBP) Encounter Data",
    "section": "",
    "text": "This week we’re exploring U.S. Customs and Border Protection (CBP) encounter data:\n\nEncounter data includes U.S. Border Patrol Title 8 apprehensions, Office of Field Operations Title 8 inadmissibles, and all Title 42 expulsions for fiscal years 2020 to date. Data is available for the Northern Land Border, Southwest Land Border, and Nationwide (i.e., air, land, and sea modes of transportation) encounters. Data is extracted from live CBP systems and data sources. Statistical information is subject to change due to corrections, systems changes, change in data definition, additional information, or encounters pending final review. Final statistics are available at the conclusion of each fiscal year.\n\nThank you to Tony Galván for curating this dataset and providing a blog post that explores the data in more detail.\n\nHow has the implementation (and potential end) of Title 42 affected migration and enforcement trends compared to Title 8 actions?\nWhat are the key differences in migration patterns and enforcement activity between the Northern and Southwest Land Borders?\nAre there seasonal or year-over-year trends in encounters that can help predict future migration patterns?\n\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-11-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 48)\n\ncbp_resp &lt;- tuesdata$cbp_resp\ncbp_state &lt;- tuesdata$cbp_state\n\n# Option 2: Read directly from GitHub\n\ncbp_resp &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-26/cbp_resp.csv')\ncbp_state &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-26/cbp_state.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-11-26/readme.html#the-data",
    "href": "data/2024/2024-11-26/readme.html#the-data",
    "title": "U.S. Customs and Border Protection (CBP) Encounter Data",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-11-26')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 48)\n\ncbp_resp &lt;- tuesdata$cbp_resp\ncbp_state &lt;- tuesdata$cbp_state\n\n# Option 2: Read directly from GitHub\n\ncbp_resp &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-26/cbp_resp.csv')\ncbp_state &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-26/cbp_state.csv')"
  },
  {
    "objectID": "data/2024/2024-11-26/readme.html#how-to-participate",
    "href": "data/2024/2024-11-26/readme.html#how-to-participate",
    "title": "U.S. Customs and Border Protection (CBP) Encounter Data",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-10/readme.html",
    "href": "data/2024/2024-12-10/readme.html",
    "title": "The Scent of Data - Exploring the Parfumo Fragrance Dataset",
    "section": "",
    "text": "This week we’re diving into the fascinating world of fragrances with a dataset sourced from Parfumo, a vibrant community of perfume enthusiasts. Olga G. webscraped these data from the various fragrance sections on the Parfumo website. Here is a description from the author:\n\nThis dataset contains detailed information about perfumes sourced from Parfumo, obtained through web scraping. It includes data on perfume ratings, olfactory notes (top, middle, and base notes), perfumers, year of release and other relevant characteristics of the perfumes listed on the Parfumo website.\n\n\nThe data provides a comprehensive look at how various perfumes are rated, which families of scents they belong to, and detailed breakdowns of the key olfactory components that define their overall profile\n\nWe’ll explore how perfumes are rated, uncover the scent families they belong to, and delve into the minds of the perfumers behind them. From the year of release to the delicate composition of each scent, this dataset offers a rich olfactory experience for anyone curious about the magic behind their favorite perfumes.\nJoin us as we decode the stories within these perfumes, from the top notes that hit you first to the lasting base notes that linger in the air. Whether you’re a fragrance aficionado or just curious about the data behind the scents, this exploration will open your eyes (and nose) to the artistry of perfume crafting. Ready to sniff out some data?\n\nWhat factors most influence the rating of a perfume?\nAre there distinct scent families that dominate the market, and how are they perceived by users?\nHas the popularity of certain fragrance notes evolved over time?\n\nThank you to Nicolas Foss, Ed.D., MS | Bureau of Emergency Medical and Trauma Services &gt; Iowa HHS for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-10')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 50)\n\nparfumo_data_clean &lt;- tuesdata$parfumo_data_clean\n\n# Option 2: Read directly from GitHub\n\nparfumo_data_clean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-10/parfumo_data_clean.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-10/readme.html#the-data",
    "href": "data/2024/2024-12-10/readme.html#the-data",
    "title": "The Scent of Data - Exploring the Parfumo Fragrance Dataset",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-10')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 50)\n\nparfumo_data_clean &lt;- tuesdata$parfumo_data_clean\n\n# Option 2: Read directly from GitHub\n\nparfumo_data_clean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-10/parfumo_data_clean.csv')"
  },
  {
    "objectID": "data/2024/2024-12-10/readme.html#how-to-participate",
    "href": "data/2024/2024-12-10/readme.html#how-to-participate",
    "title": "The Scent of Data - Exploring the Parfumo Fragrance Dataset",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-24/readme.html",
    "href": "data/2024/2024-12-24/readme.html",
    "title": "Global Holidays and Travel",
    "section": "",
    "text": "This week we’re exploring how global holidays impact seasonal human mobility. We found the data via the article “Global holiday datasets for understanding seasonal human mobility and population dynamics” by Shengjie Lai (et al) (thank you to @lgibson7 for finding the dataset).\n\nPublic and school holidays have important impacts on population mobility and dynamics across multiple spatial and temporal scales, subsequently affecting the transmission dynamics of infectious diseases and many socioeconomic activities. However, worldwide data on public and school holidays for understanding their changes across regions and years have not been assembled into a single, open-source and multitemporal dataset. To address this gap, an open access archive of data on public and school holidays in 2010–2019 across the globe at daily, weekly, and monthly timescales was constructed. Airline passenger volumes across 90 countries from 2010 to 2018 were also assembled to illustrate the usage of the holiday data for understanding the changing spatiotemporal patterns of population movements.\n\nSources:\nLai S., Sorichetta A. and WorldPop (2020). Global Public and School Holidays 2010-2019. Mapping seasonal denominator dynamics in low- and middle-income settings, and Exploring the seasonality of COVID-19, funded by The Bill and Melinda Gates Foundation.\nLai S., Sorichetta A. and WorldPop (2020). Monthly volume of airline passengers in 90 countries 2010-2018. Mapping seasonal denominator dynamics in low- and middle-income settings, and Exploring the seasonality of COVID-19, funded by The Bill and Melinda Gates Foundation.\nThe source article contains a number of data visualizations.\n\nCan you reproduce them?\nCan you find better ways to show the same information?\n\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-24')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 52)\n\nglobal_holidays &lt;- tuesdata$global_holidays\nmonthly_passengers &lt;- tuesdata$monthly_passengers\n\n# Option 2: Read directly from GitHub\n\nglobal_holidays &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-24/global_holidays.csv')\nmonthly_passengers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-24/monthly_passengers.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/2024-12-24/readme.html#the-data",
    "href": "data/2024/2024-12-24/readme.html#the-data",
    "title": "Global Holidays and Travel",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-12-24')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 52)\n\nglobal_holidays &lt;- tuesdata$global_holidays\nmonthly_passengers &lt;- tuesdata$monthly_passengers\n\n# Option 2: Read directly from GitHub\n\nglobal_holidays &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-24/global_holidays.csv')\nmonthly_passengers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-24/monthly_passengers.csv')"
  },
  {
    "objectID": "data/2024/2024-12-24/readme.html#how-to-participate",
    "href": "data/2024/2024-12-24/readme.html#how-to-participate",
    "title": "Global Holidays and Travel",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2024/readme.html",
    "href": "data/2024/readme.html",
    "title": "2024 Data",
    "section": "",
    "text": "2024 Data\nArchive of datasets and articles from the 2024 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2024-01-02\nBring your own data from 2023!\nNA\nNA\n\n\n2\n2024-01-09\nCanadian NHL Player Birth Dates\nStatistics Canada, NHL team list endpoint, NHL API\nAre Birth Dates Still Destiny for Canadian NHL Players?\n\n\n3\n2024-01-16\nUS Polling Places 2012-2020\nCenter for Public Integrity\nNational data release sheds light on past polling place changes\n\n\n4\n2024-01-23\nEducational attainment of young people in English towns\nThe UK Office for National Statistics\nWhy do children and young people in smaller towns do better academically than those in larger towns?\n\n\n5\n2024-01-30\nGroundhog predictions\nGroundhog-day.com API\nGroundhog-day.com Predictions by Year\n\n\n6\n2024-02-06\nWorld heritage sites\nUNESCO World Heritage Sites\n1 dataset 100 visualizations\n\n\n7\n2024-02-13\nValentine’s Day consumer data\nValentine’s Days consumer survey data\nNational Retail Federation Valentine’s Day Data Center\n\n\n8\n2024-02-20\nR Consortium ISC Grants\nR Consortium ISC Funded Projects\nR Consortium ISC Call for Proposals 2024\n\n\n9\n2024-02-27\nLeap Day\nWikipedia: February 29\nWikipedia: February 29\n\n\n10\n2024-03-05\nTrash Wheel Collection Data\nHealthy Harbor Trash Wheel Collection Data\nMr. Trash Wheel\n\n\n11\n2024-03-12\nFiscal Sponsors\nFiscal Sponsor Directory\nFiscal Sponsor Directory facts\n\n\n12\n2024-03-19\nX-Men Mutant Moneyball\nMutant Moneyball Data\nMutant moneyball: a data-driven ultimate X-Men\n\n\n13\n2024-03-26\nNCAA Men’s March Madness\nMen’s March Madness Data\nBracketology: predicting March Madness\n\n\n14\n2024-04-02\nDu Bois Visualization Challenge 2024\nWeek 10 Data\nDu Bois Visualization Challenge: 2024\n\n\n15\n2024-04-09\n2023 & 2024 US Solar Eclipses\nNASA’s Scientific Visualization Studio cities-eclipse-2024.json, NASA’s Scientific Visualization Studio cities-eclipse-2023.json\nThe 2023 and 2024 Solar Eclipses: Map and Data\n\n\n16\n2024-04-16\nShiny Packages\nshiny on CRAN\nShinyConf2024: The Future is Shiny\n\n\n17\n2024-04-23\nObjects Launched into Space\nOur World in Data: Annual number of objects launched into space\nUN Office for Outer Space Affairs: Online index of objects launched into outer space\n\n\n18\n2024-04-30\nWorldwide Bureaucracy Indicators\nWorld Bank Data Catalog: Worldwide Bureaucracy Indicators\nIntroducing the Worldwide Bureaucracy Indicators\n\n\n19\n2024-05-07\nRolling Stone Album Rankings\nRolling Stone 500\nWhat Makes an Album the Greatest of All Time\n\n\n20\n2024-05-14\nThe Great American Coffee Taste Test\nJames Hoffmann and Cometeer Great American Coffee Taste Test Survey\nGreat American Coffee Taste Test Breakdown\n\n\n21\n2024-05-21\nCarbon Majors emissions data\nCarbonMajors dataset\nCarbonMajors\n\n\n22\n2024-05-28\nLisa’s Vegetable Garden Data\n{gardenR} package\nMacalester College COMP/STAT 112 Tutorials\n\n\n23\n2024-06-04\nCheese\ncheese.com\ncheese.com site and blog\n\n\n24\n2024-06-11\nCampus Pride Index\nCampus Pride Index search results\nCampus Pride Index\n\n\n25\n2024-06-18\nUS Federal Holidays\nWikipedia Federal holidays in the United States\nWikipedia Federal holidays in the United States\n\n\n26\n2024-06-25\ntidyRainbow Datasets\ntidyRainbow LGBTQ Movie Database\ntidyRainbow Datasets\n\n\n27\n2024-07-02\nTidyTuesday Datasets\nttmeta package\nttmeta package\n\n\n28\n2024-07-09\nDavid Robinson’s TidyTuesday Functions\nfunspotr examples\nNetwork Visualizations of Code Collections\n\n\n29\n2024-07-16\nEnglish Women’s Football\nThe English Women’s Football (EWF) Database, May 2024\nThe English Women’s Football (EWF) Database, May 2024\n\n\n30\n2024-07-23\nAmerican Idol data\nAmerican Idol data\nAmerican Idol data\n\n\n31\n2024-07-30\nSummer Movies\nIMDb non-commercial datasets\nIMDb’s 2023 Summer Movie Guide\n\n\n32\n2024-08-06\nOlympic Medals\nKaggle Olypmic history data\nFinancial Times Tokyo Olypmics analysis\n\n\n33\n2024-08-13\nWorld’s Fairs\nList of world expositions (Wikipedia)\nWorld’s fair (Wikipedia)\n\n\n34\n2024-08-20\nEnglish Monarchs and Marriages\nA list of Monarchs by marriage\nmonarchs and marriages\n\n\n35\n2024-08-27\nThe Power Rangers Franchise\nPower Rangers: Seasons and episodes data\nNational Power Rangers Day (August 28)\n\n\n36\n2024-09-03\nStack Overflow Annual Developer Survey 2024\nStack Overflow Annual Developer Survey 2024\nStack Overflow Annual Developer Survey Results\n\n\n37\n2024-09-10\nEconomic Diversity and Student Outcomes\nOpportunity Insights: College-Level Data for 139 Selective American Colleges\nEconomic diversity and student outcomes at the University of Texas at Dallas\n\n\n38\n2024-09-17\nShakespeare Dialogue\nThe Complete Works of William Shakespeare\nshakespeare\n\n\n39\n2024-09-24\nInternational Mathematical Olympiad (IMO) Data\nIMO Team and Individual Results\nGlobal Map of 2024 International Mathematical Olympiad Scores\n\n\n40\n2024-10-01\nChess Game Dataset (Lichess)\nChess Game Dataset (Lichess)\nBeginner’s Guide: Data Visualization with Python\n\n\n41\n2024-10-08\nNational Park Species\nNPSpecies - The National Park Service biodiversity database\nNPSpecies with Julia & Tidier\n\n\n42\n2024-10-15\nSouthern Resident Killer Whale Encounters\nCenter for Whale Research\nWeb Scraping & Mapping {orcas} Encounters\n\n\n43\n2024-10-22\nThe CIA World Factbook\nusdatasets R package\nThe World Factbook\n\n\n44\n2024-10-29\nMonster Movies\nIMDb non-commercial datasets\nWhy Do People Like Horror Films? A Statistical Analysis\n\n\n45\n2024-11-05\nDemocracy and Dictatorship\ndemocracyData R Package\nRegime types and regime change: A new dataset on democracy, coups, and political institutions\n\n\n46\n2024-11-12\nISO Country Codes\nISOcodes R Package\nISO 3166 on Wikipedia\n\n\n47\n2024-11-19\nBob’s Burgers Episodes\nbobsburgersR R Package\nBob’s Burgers Episode Fingerprints by Season\n\n\n48\n2024-11-26\nU.S. Customs and Border Protection (CBP) Encounter Data\nU.S. Customs and Border Protection\nU.S. Border Patrol Encounters\n\n\n49\n2024-12-03\nNational Highways Traffic Flow\nWebTRIS Traffic Flow API\nNational Highways\n\n\n50\n2024-12-10\nThe Scent of Data - Exploring the Parfumo Fragrance Dataset\nParfumo Fragrance Dataset\nPerfumes at parfumo\n\n\n51\n2024-12-17\nDungeons and Dragons Spells (2024)\nD&D Free Rules (2024), Spell Descriptions\nStart Playing Today with the 2024 D&D Free Rules\n\n\n52\n2024-12-24\nGlobal Holidays and Travel\nWorldPop Hub\nGlobal holiday datasets for understanding seasonal human mobility and population dynamics\n\n\n53\n2024-12-31\nJames Beard Awards\nJames Beard Foundation Awards Search\nJames Beard Foundation Awards",
    "crumbs": [
      "Datasets",
      "2024"
    ]
  },
  {
    "objectID": "data/2025/2025-01-21/readme.html",
    "href": "data/2025/2025-01-21/readme.html",
    "title": "The History of Himalayan Mountaineering Expeditions",
    "section": "",
    "text": "This week, we are exploring mountaineering data from the Himalayan Dataset!\nThe Himalayan Database is a comprehensive archive documenting mountaineering expeditions in the Nepal Himalaya. It continues the pioneering work of Elizabeth Hawley, a journalist who dedicated much of her life to cataloging climbing history in the region. Her meticulous records were initially compiled from a wide range of sources, including books, alpine journals, and direct correspondence with Himalayan climbers.\nOriginally published in 2004 by the American Alpine Club as a CD-ROM booklet, the Himalayan Database became a critical resource for the climbing community. In 2017, a non-profit organization named The Himalayan Database was formed to continue Hawley’s legacy. This marked the release of Version 2 of the database, now freely available for download via the internet.\nThese data are rich in historical value, detailing the peaks, expeditions, climbing statuses, and geographic information of numerous Himalayan summits. We will explore these data in two tidy tibbles, making it easier to analyze trends in mountaineering expeditions, including seasonality, success rates, and national participation over time. Participants this week will be able to use the peaks and expeditions datasets. To manage file size, the expeditions file was filtered down to 2020-2024.\n\nWhat is the distribution of climbing status (PSTATUS) across different mountain ranges (HIMAL_FACTOR)?\nWhich mountain range (HIMAL_FACTOR) has the highest average peak height (HEIGHTM)?\nWhat is the distribution of peak heights (HEIGHTM) for peaks that are open (OPEN) versus those that are not?\nWhich climbing routes (ROUTE1, ROUTE2, ROUTE3, ROUTE4) have the highest success rates (SUCCESS1, SUCCESS2, SUCCESS3, SUCCESS4) across all expeditions?\nHow does the use of supplemental oxygen (O2USED, O2NONE) affect summit success rates?\nHow often does bad weather (TERMREASON = 4) play a role in termination compared to technical difficulty (TERMREASON = 10)?\nAre expeditions with no hired personnel (NOHIRED) associated with higher or lower death rates?\n\nThank you to Nicolas Foss, Ed.D., MS for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-01-21')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 3)\n\nexped_tidy &lt;- tuesdata$exped_tidy\npeaks_tidy &lt;- tuesdata$peaks_tidy\n\n# Option 2: Read directly from GitHub\n\nexped_tidy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-21/exped_tidy.csv')\npeaks_tidy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-21/peaks_tidy.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-01-21/readme.html#the-data",
    "href": "data/2025/2025-01-21/readme.html#the-data",
    "title": "The History of Himalayan Mountaineering Expeditions",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-01-21')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 3)\n\nexped_tidy &lt;- tuesdata$exped_tidy\npeaks_tidy &lt;- tuesdata$peaks_tidy\n\n# Option 2: Read directly from GitHub\n\nexped_tidy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-21/exped_tidy.csv')\npeaks_tidy &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-21/peaks_tidy.csv')"
  },
  {
    "objectID": "data/2025/2025-01-21/readme.html#how-to-participate",
    "href": "data/2025/2025-01-21/readme.html#how-to-participate",
    "title": "The History of Himalayan Mountaineering Expeditions",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-02-04/readme.html",
    "href": "data/2025/2025-02-04/readme.html",
    "title": "Donuts, Data, and D’oh - A Deep Dive into The Simpsons",
    "section": "",
    "text": "This week, we are going to explore a Simpsons Dataset from Kaggle. Many thanks to Prashant Banerjee for making this dataset available to the public. The Simpsons Dataset is composed of four files that contain the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes. Please note that episodes and script lines have been filtered to only include episodes from 2010 to 2016 in the episodes data to keep file size within GitHub limits!\nHere is some history on the Simpsons Dataset from the author:\n\nOriginally, this dataset was scraped by [Todd W. Schneider] for his post The Simpsons by the Data, for which he made the scraper available on GitHub. Kaggle user William Cukierski used the scraper to upload the data set, which has been rehosted here.\n\n\nWhich character has the most spoken lines across all episodes, and how has their dialogue volume changed over the seasons?\nWhat are the most frequently used locations in the series, and do specific locations correspond to higher IMDb ratings for episodes?\nIs there a relationship between the number of U.S. viewers (in millions) and the IMDb ratings or votes for episodes?\nWhat are the most commonly used words or phrases in the dialogue across the series, and do they differ by character or location?\n\nThank you to Nicolas Foss, Ed.D., MS with Iowa HHS for curating this week’s dataset.\n\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-04')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 5)\n\nsimpsons_characters &lt;- tuesdata$simpsons_characters\nsimpsons_episodes &lt;- tuesdata$simpsons_episodes\nsimpsons_locations &lt;- tuesdata$simpsons_locations\nsimpsons_script_lines &lt;- tuesdata$simpsons_script_lines\n\n# Option 2: Read directly from GitHub\n\nsimpsons_characters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-04/simpsons_characters.csv')\nsimpsons_episodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-04/simpsons_episodes.csv')\nsimpsons_locations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-04/simpsons_locations.csv')\nsimpsons_script_lines &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-04/simpsons_script_lines.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-02-04/readme.html#the-data",
    "href": "data/2025/2025-02-04/readme.html#the-data",
    "title": "Donuts, Data, and D’oh - A Deep Dive into The Simpsons",
    "section": "",
    "text": "# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-04')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 5)\n\nsimpsons_characters &lt;- tuesdata$simpsons_characters\nsimpsons_episodes &lt;- tuesdata$simpsons_episodes\nsimpsons_locations &lt;- tuesdata$simpsons_locations\nsimpsons_script_lines &lt;- tuesdata$simpsons_script_lines\n\n# Option 2: Read directly from GitHub\n\nsimpsons_characters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-04/simpsons_characters.csv')\nsimpsons_episodes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-04/simpsons_episodes.csv')\nsimpsons_locations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-04/simpsons_locations.csv')\nsimpsons_script_lines &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-04/simpsons_script_lines.csv')"
  },
  {
    "objectID": "data/2025/2025-02-04/readme.html#how-to-participate",
    "href": "data/2025/2025-02-04/readme.html#how-to-participate",
    "title": "Donuts, Data, and D’oh - A Deep Dive into The Simpsons",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a shiny app, or some other piece of data-science-related output, using R or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-02-18/readme.html",
    "href": "data/2025/2025-02-18/readme.html",
    "title": "Agencies from the FBI Crime Data API",
    "section": "",
    "text": "This week we’re exploring data from the FBI Crime Data API! Specifically, we’re looking at agency-level data across all 50 states in the USA. This dataset provides details on law enforcement agencies that have submitted data to the FBI’s Uniform Crime Reporting (UCR) Program and are displayed on the Crime Data Explorer (CDE).\n\nCurrently, the FBI produces four annual publications from data provided by more than 18,000 federal, state, county, city, university and college, and tribal law enforcement agencies voluntarily participating in the UCR program.\nCrime data is dynamic. Offenses occur, arrests are made, and property is recovered every day. The FBI’s Crime Data Explorer, the digital front door for UCR data, is an attempt to reflect that fluidity in crime. The data presented there is updated regularly in a way that UCR publications previously could not be. Launched in 2017, the CDE’s content and features are updated and expanded continuously. CDE enables law enforcement agencies, researchers, journalists, and the public to more easily use and understand the massive amounts of UCR data using charts and graphs.\n\n\nHow do agency types vary? How are agencies distributed geographically within each state?\nWhat percentage of agencies in each state participate in NIBRS reporting? Are there any trends in NIBRS adoption?\n\nThank you to Ford Johnson for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package\n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-18')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 7)\n\nagencies &lt;- tuesdata$agencies\n\n# Option 2: Read directly from GitHub\n\nagencies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-18/agencies.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date(\"2025-02-18\")\n\n# Option 2: Read directly from GitHub and assign to an object\n\nagencies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-18/agencies.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-02-18/readme.html#the-data",
    "href": "data/2025/2025-02-18/readme.html#the-data",
    "title": "Agencies from the FBI Crime Data API",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package\n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-18')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 7)\n\nagencies &lt;- tuesdata$agencies\n\n# Option 2: Read directly from GitHub\n\nagencies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-18/agencies.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date(\"2025-02-18\")\n\n# Option 2: Read directly from GitHub and assign to an object\n\nagencies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-18/agencies.csv')"
  },
  {
    "objectID": "data/2025/2025-02-18/readme.html#how-to-participate",
    "href": "data/2025/2025-02-18/readme.html#how-to-participate",
    "title": "Agencies from the FBI Crime Data API",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-02-18/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "href": "data/2025/2025-02-18/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "title": "Agencies from the FBI Crime Data API",
    "section": "",
    "text": "Exploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-03-04/readme.html",
    "href": "data/2025/2025-03-04/readme.html",
    "title": "Long Beach Animal Shelter",
    "section": "",
    "text": "This week we’re exploring the Long Beach Animal Shelter Data!\nThe dataset comes from the City of Long Beach Animal Care Services via the {animalshelter} R package.\n\nThis dataset comprises of the intake and outcome record from Long Beach Animal Shelter.\n\n\nHow has the number of pet adoptions changed over the years?\nWhich type of pets are adopted most often?\n\nThank you to Lydia Gibson for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-03-04')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 9)\n\nlongbeach &lt;- tuesdata$longbeach\n\n# Option 2: Read directly from GitHub\n\nlongbeach &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-04/longbeach.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-03-04')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nlongbeach = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-04/longbeach.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-03-04/readme.html#the-data",
    "href": "data/2025/2025-03-04/readme.html#the-data",
    "title": "Long Beach Animal Shelter",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-03-04')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 9)\n\nlongbeach &lt;- tuesdata$longbeach\n\n# Option 2: Read directly from GitHub\n\nlongbeach &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-04/longbeach.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-03-04')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nlongbeach = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-04/longbeach.csv')"
  },
  {
    "objectID": "data/2025/2025-03-04/readme.html#how-to-participate",
    "href": "data/2025/2025-03-04/readme.html#how-to-participate",
    "title": "Long Beach Animal Shelter",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-03-04/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "href": "data/2025/2025-03-04/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "title": "Long Beach Animal Shelter",
    "section": "",
    "text": "Exploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-03-18/readme.html",
    "href": "data/2025/2025-03-18/readme.html",
    "title": "Palm Trees",
    "section": "",
    "text": "This week we’re exploring Palm Trees!\nThe dataset comes from the the PalmTraits 1.0 database via the palmtrees R package by Emil Hvitfeldt.\n\nPlant traits are critical to plant form and function —including growth, survival and reproduction— and therefore shape fundamental aspects of population and ecosystem dynamics as well as ecosystem services. Here, we present a global species-level compilation of key functional traits for palms (Arecaceae), a plant family with keystone importance in tropical and subtropical ecosystems.\n\n\nHow does the sizes of the different species of palms vary across sub families?\nWhich fruit colors occur most often?\n\nThank you to Lydia Gibson for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-03-18')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 11)\n\npalmtrees &lt;- tuesdata$palmtrees\n\n# Option 2: Read directly from GitHub\n\npalmtrees &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-18/palmtrees.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-03-18')\n\n# Option 2: Read directly from GitHub and assign to an object\n\npalmtrees = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-18/palmtrees.csv', encoding='windows-1252')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-03-18/readme.html#the-data",
    "href": "data/2025/2025-03-18/readme.html#the-data",
    "title": "Palm Trees",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-03-18')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 11)\n\npalmtrees &lt;- tuesdata$palmtrees\n\n# Option 2: Read directly from GitHub\n\npalmtrees &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-18/palmtrees.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-03-18')\n\n# Option 2: Read directly from GitHub and assign to an object\n\npalmtrees = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-03-18/palmtrees.csv', encoding='windows-1252')"
  },
  {
    "objectID": "data/2025/2025-03-18/readme.html#how-to-participate",
    "href": "data/2025/2025-03-18/readme.html#how-to-participate",
    "title": "Palm Trees",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-03-18/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "href": "data/2025/2025-03-18/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "title": "Palm Trees",
    "section": "",
    "text": "Exploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-04-01/readme.html",
    "href": "data/2025/2025-04-01/readme.html",
    "title": "Pokemon",
    "section": "",
    "text": "This week we are exploring Pokemon! This dataset is sourced from {pokemon} (CRAN | github), an R package which provides Pokemon information in both English and Brazilian Portuguese.\n\nThis package provides a dataset of Pokemon information in both English and Brazilian Portuguese. The dataset contains 949 rows and 22 columns, including information such as the Pokemon’s name, ID, height, weight, stats, type, and more.\n\nThank you to Frank Hull for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-01')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 13)\n\npokemon_df &lt;- tuesdata$pokemon_df\n\n# Option 2: Read directly from GitHub\n\npokemon_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-01/pokemon_df.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-01')\n\n# Option 2: Read directly from GitHub and assign to an object\n\npokemon_df = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-01/pokemon_df.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-04-01/readme.html#the-data",
    "href": "data/2025/2025-04-01/readme.html#the-data",
    "title": "Pokemon",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-01')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 13)\n\npokemon_df &lt;- tuesdata$pokemon_df\n\n# Option 2: Read directly from GitHub\n\npokemon_df &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-01/pokemon_df.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-01')\n\n# Option 2: Read directly from GitHub and assign to an object\n\npokemon_df = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-01/pokemon_df.csv')"
  },
  {
    "objectID": "data/2025/2025-04-01/readme.html#how-to-participate",
    "href": "data/2025/2025-04-01/readme.html#how-to-participate",
    "title": "Pokemon",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!"
  },
  {
    "objectID": "data/2025/2025-04-01/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "href": "data/2025/2025-04-01/readme.html#pydytuesday-a-posit-collaboration-with-tidytuesday",
    "title": "Pokemon",
    "section": "",
    "text": "Exploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-04-15/readme.html",
    "href": "data/2025/2025-04-15/readme.html",
    "title": "Base R Penguins",
    "section": "",
    "text": "This week we’re taking another look at penguins! The Palmer Penguins dataset first appeared in TidyTuesday back in July of 2020. We’re using the dataset again because, as of R 4.5.0 (released this past Friday), the datasets are available in the base R datasets package!\n\nThe Palmer Penguins data, contained in the palmerpenguins R package as the penguins and penguins_raw data frames, have become popular for data exploration and visualisation, particularly in an educational context. … The data was originally published in Gorman et al. (2014). Their inclusion in the datasets package included in the base R distribution was motivated by Horst et al. (2022).\n\nAlso check out the {basepenguins} R package to convert scripts that use {palmerpenguins} to use the base R versions of the datasets.\nQuestions:\n\nIf you participated in TidyTuesday in 2020, what have you learned since then that changes how you approach the data?\nSearch the internet for “palmerpenguins” to find examples of plots that use these datasets. Can you reproduce those plots? Can you improve them?\n\nThank you to Jon Harmon for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-15')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 15)\n\npenguins &lt;- tuesdata$penguins\npenguins_raw &lt;- tuesdata$penguins_raw\n\n# Option 2: Read directly from GitHub\n\npenguins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv')\npenguins_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins_raw.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-15')\n\n# Option 2: Read directly from GitHub and assign to an object\n\npenguins = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv')\npenguins_raw = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins_raw.csv')\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\nfactor\nPenguin species (Adelie, Gentoo, Chinstrap).\n\n\nisland\nfactor\nIsland where recorded (Biscoe, Dream, Torgersen).\n\n\nbill_len\ndouble\nBill length in millimeters (also known as culmen length).\n\n\nbill_dep\ndouble\nBill depth in millimeters (also known as culmen depth).\n\n\nflipper_len\ninteger\nFlipper length in mm.\n\n\nbody_mass\ninteger\nBody mass in grams.\n\n\nsex\nfactor\nSex of the animal (male, female, or NA if unknown).\n\n\nyear\ninteger\nYear recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstudyName\ncharacter\nStudy name.\n\n\nSample Number\ndouble\nSample id.\n\n\nSpecies\ncharacter\nSpecies of penguin.\n\n\nRegion\ncharacter\nRegion where recorded.\n\n\nIsland\ncharacter\nIsland where recorded.\n\n\nStage\ncharacter\nStage of egg.\n\n\nIndividual ID\ncharacter\nIndividual penguin ID.\n\n\nClutch Completion\ncharacter\nEgg clutch completion.\n\n\nDate Egg\ndate\nDate of egg.\n\n\nCulmen Length (mm)\ndouble\nculmen length in mm (beak length).\n\n\nCulmen Depth (mm)\ndouble\nculmen depth in mm (beak depth).\n\n\nFlipper Length (mm)\ndouble\nFlipper length in mm.\n\n\nBody Mass (g)\ndouble\nBody mass in g.\n\n\nSex\ncharacter\nSex of the penguin.\n\n\nDelta 15 N (o/oo)\ndouble\nBlood isotopic Nitrogen - used for dietary comparison.\n\n\nDelta 13 C (o/oo)\ndouble\nBlood isotopic Carbon - used for dietary comparison.\n\n\nComments\ncharacter\nMiscellaneous comments.\n\n\n\n\n\n\n\n# Clean data provided by the base R {datasets} package (version &gt;=4.5.0). No \n# cleaning was necessary.\npenguins &lt;- datasets::penguins\npenguins_raw &lt;- datasets::penguins_raw"
  },
  {
    "objectID": "data/2025/2025-04-15/readme.html#the-data",
    "href": "data/2025/2025-04-15/readme.html#the-data",
    "title": "Base R Penguins",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-15')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 15)\n\npenguins &lt;- tuesdata$penguins\npenguins_raw &lt;- tuesdata$penguins_raw\n\n# Option 2: Read directly from GitHub\n\npenguins &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv')\npenguins_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins_raw.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-15')\n\n# Option 2: Read directly from GitHub and assign to an object\n\npenguins = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv')\npenguins_raw = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins_raw.csv')"
  },
  {
    "objectID": "data/2025/2025-04-15/readme.html#how-to-participate",
    "href": "data/2025/2025-04-15/readme.html#how-to-participate",
    "title": "Base R Penguins",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-04-15/readme.html#data-dictionary",
    "href": "data/2025/2025-04-15/readme.html#data-dictionary",
    "title": "Base R Penguins",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nspecies\nfactor\nPenguin species (Adelie, Gentoo, Chinstrap).\n\n\nisland\nfactor\nIsland where recorded (Biscoe, Dream, Torgersen).\n\n\nbill_len\ndouble\nBill length in millimeters (also known as culmen length).\n\n\nbill_dep\ndouble\nBill depth in millimeters (also known as culmen depth).\n\n\nflipper_len\ninteger\nFlipper length in mm.\n\n\nbody_mass\ninteger\nBody mass in grams.\n\n\nsex\nfactor\nSex of the animal (male, female, or NA if unknown).\n\n\nyear\ninteger\nYear recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstudyName\ncharacter\nStudy name.\n\n\nSample Number\ndouble\nSample id.\n\n\nSpecies\ncharacter\nSpecies of penguin.\n\n\nRegion\ncharacter\nRegion where recorded.\n\n\nIsland\ncharacter\nIsland where recorded.\n\n\nStage\ncharacter\nStage of egg.\n\n\nIndividual ID\ncharacter\nIndividual penguin ID.\n\n\nClutch Completion\ncharacter\nEgg clutch completion.\n\n\nDate Egg\ndate\nDate of egg.\n\n\nCulmen Length (mm)\ndouble\nculmen length in mm (beak length).\n\n\nCulmen Depth (mm)\ndouble\nculmen depth in mm (beak depth).\n\n\nFlipper Length (mm)\ndouble\nFlipper length in mm.\n\n\nBody Mass (g)\ndouble\nBody mass in g.\n\n\nSex\ncharacter\nSex of the penguin.\n\n\nDelta 15 N (o/oo)\ndouble\nBlood isotopic Nitrogen - used for dietary comparison.\n\n\nDelta 13 C (o/oo)\ndouble\nBlood isotopic Carbon - used for dietary comparison.\n\n\nComments\ncharacter\nMiscellaneous comments."
  },
  {
    "objectID": "data/2025/2025-04-15/readme.html#cleaning-script",
    "href": "data/2025/2025-04-15/readme.html#cleaning-script",
    "title": "Base R Penguins",
    "section": "",
    "text": "# Clean data provided by the base R {datasets} package (version &gt;=4.5.0). No \n# cleaning was necessary.\npenguins &lt;- datasets::penguins\npenguins_raw &lt;- datasets::penguins_raw"
  },
  {
    "objectID": "data/2025/2025-04-29/readme.html",
    "href": "data/2025/2025-04-29/readme.html",
    "title": "useR! 2025 program",
    "section": "",
    "text": "This week we’re exploring the program for the useR! 2025 conference. useR! 2025 will be hosted at Duke University in Durham, NC, USA from August 8-10, 2025. The conference will feature keynote presentations from leading R developers and data scientists, technical talks and tutorials, interactive tutorials and training sessions, poster presentations, networking opportunities, and both in-person and virtual attendance options (with the virtual conference taking place on August 1, 2025). The event hashtag is #useR2025, so when sharing your TidyTuesday creations this week, please add this hashtag as well!\nFrom the useR! website:\n\nuseR! conferences are annual nonprofit gatherings organized by R community volunteers and supported by the R Foundation. These conferences have been the premier global venue for the R community since 2004, bringing together R developers, users, and enthusiasts from around the world.\n\nThe virtual conference program can be found at https://user2025.r-project.org/program/virtual and the in-person program at https://user2025.r-project.org/program/in-person. Use this week’s data to\n\nDiscover emerging themes at useR! 2025\nCreate an interactive conference program app\nBuild a data visualization that inspires folks to participate in useR! 2025\n\nor do whatever you think would be helpful to you or the R community to get the most out of useR! 2025, whether participating in person or virtually.\nThank you to Mine Çetinkaya-Rundel, Duke University + Posit PBC for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-29')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 17)\n\nuser2025 &lt;- tuesdata$user2025\n\n# Option 2: Read directly from GitHub\n\nuser2025 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-29')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nuser2025 = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-04-29')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nuser2025 = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nuser2025 = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ndouble\nSubmission ID from Indico, the conference management tool.\n\n\nsession\ncharacter\nName of session.\n\n\ndate\ndate\nDate of session.\n\n\ntime\ncharacter\nTime of session.\n\n\nroom\ncharacter\nRoom where session will take place.\n\n\ntitle\ncharacter\nTitle of talk, poster, or tutorial.\n\n\ncontent\ncharacter\nAbstract of talk, poster, or tutorial.\n\n\nvideo_recording\ncharacter\nWhether there will be a video recording available after the conference.\n\n\nkeywords\ncharacter\nKeywords of talk, poster, or tutorial.\n\n\nspeakers\ncharacter\nName(s) of speaker(s) and their affiliations.\n\n\nco_authors\ncharacter\nName(s) of co-author(s) and their affiliations.\n\n\n\n\n\n\n\n# Clean data provided by useR! 2025 program committee. No cleaning was necessary.\nuser2025 &lt;- readr::read_csv(\"program.csv\")"
  },
  {
    "objectID": "data/2025/2025-04-29/readme.html#the-data",
    "href": "data/2025/2025-04-29/readme.html#the-data",
    "title": "useR! 2025 program",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-29')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 17)\n\nuser2025 &lt;- tuesdata$user2025\n\n# Option 2: Read directly from GitHub\n\nuser2025 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-04-29')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nuser2025 = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-04-29')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nuser2025 = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nuser2025 = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-29/user2025.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-04-29/readme.html#how-to-participate",
    "href": "data/2025/2025-04-29/readme.html#how-to-participate",
    "title": "useR! 2025 program",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-04-29/readme.html#data-dictionary",
    "href": "data/2025/2025-04-29/readme.html#data-dictionary",
    "title": "useR! 2025 program",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nid\ndouble\nSubmission ID from Indico, the conference management tool.\n\n\nsession\ncharacter\nName of session.\n\n\ndate\ndate\nDate of session.\n\n\ntime\ncharacter\nTime of session.\n\n\nroom\ncharacter\nRoom where session will take place.\n\n\ntitle\ncharacter\nTitle of talk, poster, or tutorial.\n\n\ncontent\ncharacter\nAbstract of talk, poster, or tutorial.\n\n\nvideo_recording\ncharacter\nWhether there will be a video recording available after the conference.\n\n\nkeywords\ncharacter\nKeywords of talk, poster, or tutorial.\n\n\nspeakers\ncharacter\nName(s) of speaker(s) and their affiliations.\n\n\nco_authors\ncharacter\nName(s) of co-author(s) and their affiliations."
  },
  {
    "objectID": "data/2025/2025-04-29/readme.html#cleaning-script",
    "href": "data/2025/2025-04-29/readme.html#cleaning-script",
    "title": "useR! 2025 program",
    "section": "",
    "text": "# Clean data provided by useR! 2025 program committee. No cleaning was necessary.\nuser2025 &lt;- readr::read_csv(\"program.csv\")"
  },
  {
    "objectID": "data/2025/2025-05-13/readme.html",
    "href": "data/2025/2025-05-13/readme.html",
    "title": "Seismic Events at Mount Vesuvius",
    "section": "",
    "text": "The dataset this week explores seismic events detected at the famous Mount Vesuvius in Italy. It comes from the Italian Istituto Nazionale di Geofisica e Vulcanologia (INGV)’s Data Portal and can be explored along with other seismic areas on the GOSSIP website. The raw data was saved as individual CSV files from the GOSSIP website and some values were translated from Italian to English.\n\nThe Open Data Portal of Istituto Nazionale di Geofisica e Vulcanologia (INGV) gives public access to data resulting from institutional research activities in the fields of Seismology, Volcanology, and Environment.\n\nSome information about Mount Vesuvius from INGV:\n- Location: Campania, 40°49′18.01″N, 14°25’33.57” E - Maximum height: 1281 m above sea level - Total surface area: ≈115-150 km2 - Type of volcano: stratovolcano - Start of eruptive activity: &lt;39,000 years - Last eruption: 1944 (lasted about 10 days) - Activity status: quiescent (not active, but is still registering seismic activity)\nSome questions you might explore with this data set include: - How has the number and severity of seismic events changed over the last decade? - Is there a correlation between earthquake depth and magnitude at Vesuvius? - Do seismic events at Vesuvius follow any seasonal patterns or time-of-day patterns? - Has the average location of seismic events migrated at all over the course of the data collection period?\nThank you to Libby Heeren for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-05-13')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 19)\n\nvesuvius &lt;- tuesdata$vesuvius\n\n# Option 2: Read directly from GitHub\n\nvesuvius &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-05-13')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nvesuvius = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-05-13')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nvesuvius = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nvesuvius = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nevent_id\ninteger\nUnique identifier for each seismic event recorded.\n\n\ntime\ndatetime\nDate and time when the seismic event occurred, in UTC format.\n\n\nlatitude\ndouble\nGeographic latitude of the seismic event location in decimal degrees.\n\n\nlongitude\ndouble\nGeographic longitude of the seismic event location in decimal degrees.\n\n\ndepth_km\ndouble\nDepth of the seismic event epicenter in kilometers below the surface.\n\n\nduration_magnitude_md\ndouble\nDuration magnitude (Md) of the seismic event, a measure of its energy release. Md is often used for smaller magnitude events, and negative values can indicate very small events (microearthquakes).\n\n\nmd_error\ndouble\nEstimated error margin (“plus or minus”) for the duration magnitude measurement.\n\n\narea\ncharacter\nGeographic area where the seismic event was recorded. In this case, the Mt. Vesuvius area.\n\n\ntype\ncharacter\nClassification of the seismic event, such as “earthquake” or “eruption.”\n\n\nreview_level\ncharacter\nLevel of review the data has undergone. The data might be raw (preliminary) or revised (reviewed by someone).\n\n\nyear\ninteger\nCalendar year when the seismic event occurred.\n\n\n\n\n\n\n\n# This script was used to clean data saved as 13 csv files \n# from the Italian Istituto Nazionale di Geofisica e Vulcanologia found\n# here https://terremoti.ov.ingv.it/gossip/vesuvio/index.html\n\n# Load packages\nlibrary(tidyverse)\nlibrary(janitor)\n\n# List all vesuvius CSV files in the data_raw folder\nfile_list &lt;- list.files(path = \"data/data_raw\", pattern = \"^vesuvius_\\\\d{4}\\\\.csv$\", full.names = TRUE)\n\n# Read and row-bind all CSVs\nvesuvius_data &lt;- \n  file_list |&gt; \n  map_dfr(read_csv)\n\n# Clean column names and create year variable from time variable\nvesuvius &lt;- \n  vesuvius_data |&gt; \n  clean_names() |&gt; \n  mutate(\n    year = year(time)\n  )\n\n\n# Add unit and context to column names\nvesuvius &lt;- \n  vesuvius |&gt; \n  rename(duration_magnitude_md = md, \n         depth_km = depth,\n         event_id = number_event_id,\n         review_level = level)\n\n# Translate some values from Italian to English\n# Rivisto is revised (reviewed by humans)\n# Bollettino is \"bulletin\" or the original preliminary source of the data,\n# the Italian Seismic Bulletin (Il Bollettino Sismico Italiano)\nvesuvius &lt;-\n  vesuvius |&gt; \n  mutate(review_level = case_when(\n    review_level == \"Rivisto\" ~ \"revised\",\n    review_level == \"Bollettino\" ~ \"preliminary\",\n    .default = NA_character_\n  ))\n\n# Let's change vesuvio to Mount Vesuvius! (Same value for all rows, but still)\nvesuvius &lt;-\n  vesuvius |&gt; \n  mutate(area = case_when(\n    area == \"vesuvio\" ~ \"Mount Vesuvius\",\n    .default = NA_character_\n  ))\n\n# Make year and event_id into integers, but everything else is a decimal/dbl\nvesuvius &lt;-\n  vesuvius |&gt; \n  mutate(\n    dplyr::across(\n      c(\"event_id\", \"year\"),\n      as.integer\n    )\n  )"
  },
  {
    "objectID": "data/2025/2025-05-13/readme.html#the-data",
    "href": "data/2025/2025-05-13/readme.html#the-data",
    "title": "Seismic Events at Mount Vesuvius",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-05-13')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 19)\n\nvesuvius &lt;- tuesdata$vesuvius\n\n# Option 2: Read directly from GitHub\n\nvesuvius &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-05-13')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nvesuvius = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-05-13')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nvesuvius = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nvesuvius = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-13/vesuvius.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-05-13/readme.html#how-to-participate",
    "href": "data/2025/2025-05-13/readme.html#how-to-participate",
    "title": "Seismic Events at Mount Vesuvius",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-05-13/readme.html#data-dictionary",
    "href": "data/2025/2025-05-13/readme.html#data-dictionary",
    "title": "Seismic Events at Mount Vesuvius",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nevent_id\ninteger\nUnique identifier for each seismic event recorded.\n\n\ntime\ndatetime\nDate and time when the seismic event occurred, in UTC format.\n\n\nlatitude\ndouble\nGeographic latitude of the seismic event location in decimal degrees.\n\n\nlongitude\ndouble\nGeographic longitude of the seismic event location in decimal degrees.\n\n\ndepth_km\ndouble\nDepth of the seismic event epicenter in kilometers below the surface.\n\n\nduration_magnitude_md\ndouble\nDuration magnitude (Md) of the seismic event, a measure of its energy release. Md is often used for smaller magnitude events, and negative values can indicate very small events (microearthquakes).\n\n\nmd_error\ndouble\nEstimated error margin (“plus or minus”) for the duration magnitude measurement.\n\n\narea\ncharacter\nGeographic area where the seismic event was recorded. In this case, the Mt. Vesuvius area.\n\n\ntype\ncharacter\nClassification of the seismic event, such as “earthquake” or “eruption.”\n\n\nreview_level\ncharacter\nLevel of review the data has undergone. The data might be raw (preliminary) or revised (reviewed by someone).\n\n\nyear\ninteger\nCalendar year when the seismic event occurred."
  },
  {
    "objectID": "data/2025/2025-05-13/readme.html#cleaning-script",
    "href": "data/2025/2025-05-13/readme.html#cleaning-script",
    "title": "Seismic Events at Mount Vesuvius",
    "section": "",
    "text": "# This script was used to clean data saved as 13 csv files \n# from the Italian Istituto Nazionale di Geofisica e Vulcanologia found\n# here https://terremoti.ov.ingv.it/gossip/vesuvio/index.html\n\n# Load packages\nlibrary(tidyverse)\nlibrary(janitor)\n\n# List all vesuvius CSV files in the data_raw folder\nfile_list &lt;- list.files(path = \"data/data_raw\", pattern = \"^vesuvius_\\\\d{4}\\\\.csv$\", full.names = TRUE)\n\n# Read and row-bind all CSVs\nvesuvius_data &lt;- \n  file_list |&gt; \n  map_dfr(read_csv)\n\n# Clean column names and create year variable from time variable\nvesuvius &lt;- \n  vesuvius_data |&gt; \n  clean_names() |&gt; \n  mutate(\n    year = year(time)\n  )\n\n\n# Add unit and context to column names\nvesuvius &lt;- \n  vesuvius |&gt; \n  rename(duration_magnitude_md = md, \n         depth_km = depth,\n         event_id = number_event_id,\n         review_level = level)\n\n# Translate some values from Italian to English\n# Rivisto is revised (reviewed by humans)\n# Bollettino is \"bulletin\" or the original preliminary source of the data,\n# the Italian Seismic Bulletin (Il Bollettino Sismico Italiano)\nvesuvius &lt;-\n  vesuvius |&gt; \n  mutate(review_level = case_when(\n    review_level == \"Rivisto\" ~ \"revised\",\n    review_level == \"Bollettino\" ~ \"preliminary\",\n    .default = NA_character_\n  ))\n\n# Let's change vesuvio to Mount Vesuvius! (Same value for all rows, but still)\nvesuvius &lt;-\n  vesuvius |&gt; \n  mutate(area = case_when(\n    area == \"vesuvio\" ~ \"Mount Vesuvius\",\n    .default = NA_character_\n  ))\n\n# Make year and event_id into integers, but everything else is a decimal/dbl\nvesuvius &lt;-\n  vesuvius |&gt; \n  mutate(\n    dplyr::across(\n      c(\"event_id\", \"year\"),\n      as.integer\n    )\n  )"
  },
  {
    "objectID": "data/2025/2025-05-27/readme.html",
    "href": "data/2025/2025-05-27/readme.html",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "This week we’re exploring monsters from the Dungeons & Dragons System Reference Document! After the popularity of our Dungeons and Dragons Spells (2024), we thought it might be fun to explore the freely available monsters from the 2024 update.\n\nEvery monster is a font of adventure. In this bestiary of Dungeons & Dragons monsters, you’ll discover the weird, the whimsical, the majestic, and the macabre. Choose your favorites, and make them part of your D&D play.\n\n\nWhich types of monsters have the highest (or lowest) of each ability score?\nWhich monster types have a broad range of challenger ratings?\nWhich language allows your character to communicate with the most monsters?\nHow else can the dataset be processed to pull out common fields?\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-05-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 21)\n\nmonsters &lt;- tuesdata$monsters\n\n# Option 2: Read directly from GitHub\n\nmonsters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-05-27')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nmonsters = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-05-27')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nmonsters = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nmonsters = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nThe name of the monster.\n\n\ncategory\ncharacter\nThe category to which this monster belongs. Often the same as the name, but, for example, all “Animated Objects” share a category.\n\n\ncr\ndouble\nThe challenge rating of the monster.\n\n\nsize\ncharacter\nTiny, Small, Medium, Large, Huge or Gargantuan. If size options are presented, you choose the creature’s size from those options.\n\n\ntype\ncharacter\nEach monster has a tag that identifies the type of creature it is. Certain spells, magic items, class features, and other effects in the game interact in special ways with creatures of a particular type.\n\n\ndescriptive_tags\ncharacter\nOptional additional tags. Such tags provide additional categorization and have no rules of their own, but certain game effects might refer to them.\n\n\nalignment\ncharacter\nOne of Lawful Good, Neutral Good, Chaotic Good, Lawful Neutral, Neutral, Chaotic Neutral, Lawful Evil, Neutral Evil, Chaotic Evil, or Unaligned. The alignment specified in a monster’s stat block is a default suggestion of how to roleplay the monster, inspired by its traditional role in the game or real-world folklore. Change a monster’s alignment to suit your storytelling needs. The Neutral alignment, in particular, is an invitation for you to consider whether an individual leans toward one of the other alignments.\n\n\nac\ncharacter\nThe monster’s Armor Class (AC) includes its natural armor, Dexterity, gear, and other defenses.\n\n\ninitiative\ninteger\nThe monster’s Initiative modifier. Use the modifier when you roll to determine a monster’s Initiative. A monster’s Initiative modifier is typically equal to its Dexterity modifier, but some monsters have additional modifiers, such as Proficiency Bonus, applied to that number.\n\n\nhp\ncharacter\nThe monster’s Hit Points are presented as a number followed by parentheses, where the monster’s Hit Point Dice are provided, along with any contribution from its Constitution. Either use the number for the monster’s Hit Points (also available in “hp_number”) or roll the die expression in parentheses to determine the monster’s Hit Points randomly; don’t use both.\n\n\nhp_number\ninteger\nThe average Hit Points for the monster.\n\n\nspeed\ncharacter\nThe monster’s Speed. Some monsters have one or more of the following speeds: Burrow, Climb, Fly, Swim.\n\n\nspeed_base_number\ninteger\nThe first numeric speed for the monster, which is usually the walking speed.\n\n\nstr\ninteger\nThe monster’s strength score.\n\n\ndex\ninteger\nThe monster’s dexterity score.\n\n\ncon\ninteger\nThe monster’s constitution score.\n\n\nint\ninteger\nThe monster’s intelligence score.\n\n\nwis\ninteger\nThe monster’s wisdom score.\n\n\ncha\ninteger\nThe monster’s charisma score.\n\n\nstr_save\ninteger\nThe monster’s strength saving throw bonus.\n\n\ndex_save\ninteger\nThe monster’s dexterity saving throw bonus.\n\n\ncon_save\ninteger\nThe monster’s constitution saving throw bonus.\n\n\nint_save\ninteger\nThe monster’s intelligence saving throw bonus.\n\n\nwis_save\ninteger\nThe monster’s wisdom saving throw bonus.\n\n\ncha_save\ninteger\nThe monster’s charisma saving throw bonus.\n\n\nskills\ncharacter\nThe monster’s Skill proficiencies, if any. For example, a monster that is very perceptive and stealthy might have bonuses to Wisdom (Perception) and Dexterity (Stealth) checks. A skill bonus is the sum of a monster’s relevant ability modifier and its Proficiency Bonus. Other modifiers might apply.\n\n\nresistances\ncharacter\nThe monster’s Resistances, if any.\n\n\nvulnerabilities\ncharacter\nThe monster’s Vulnerabilities, if any.\n\n\nimmunities\ncharacter\nThe monster’s Immunities, if any. If the monster has damage and condition Immunities, the damage types are listed before the conditions.\n\n\ngear\ncharacter\nMonsters have proficiency with their equipment. If the monster has equipment that can be given away or retrieved, the items are listed in the Gear entry. The monster’s stat block might include special flourishes that happen when the monster uses an item, and the stat block might ignore the rules in “Equipment” for that item. When used by someone else, a retrievable item uses its “Equipment” rules, ignoring any special flourishes in the stat block. The Gear entry doesn’t necessarily list all of a monster’s equipment. For example, a monster that wears clothes is assumed to be dressed appropriately, and those clothes aren’t in this entry. Equipment mentioned outside the Gear entry is considered to be supernatural or highly specialized, and it is unusable when the monster is defeated.\n\n\nsenses\ncharacter\nThe monster’s Passive Perception score, as well as any special senses the monster possesses.\n\n\nlanguages\ncharacter\nLanguages that the monster can use to communicate. Sometimes the monster can understand a language but can’t communicate with it, which is noted in its entry. “None” indicates that the creature doesn’t comprehend any language. Telepathy is a magical ability that allows a creature to communicate mentally with another creature within a specified range.\n\n\nfull_text\ncharacter\nThe full text of the monster description, including the information above as well as monster Traits and Actions.\n\n\n\n\n\n\n\n# This work includes material from the System Reference Document 5.2.1 (“SRD\n# 5.2.1”) by Wizards of the Coast LLC, available at\n# https://www.dndbeyond.com/srd. The SRD 5.2.1 is licensed under the Creative\n# Commons Attribution 4.0 International License, available at\n# https://creativecommons.org/licenses/by/4.0/legalcode.\n\nlibrary(dplyr)\nlibrary(here)\nlibrary(pdftools)\nlibrary(purrr)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(withr)\n\nsrd_url &lt;- rvest::read_html(\"https://www.dndbeyond.com/srd\") |&gt;\n  rvest::html_nodes(\".download-link\") |&gt;\n  rvest::html_node(\"a\") |&gt;\n  rvest::html_attr(\"href\") |&gt;\n  _[[1]]\n\ntarget_path &lt;- withr::local_tempfile(fileext = \".pdf\")\ndownload.file(srd_url, target_path, mode = \"wb\")\n\n# Extract the raw text from the PDF.\nsrd_raw &lt;- pdftools::pdf_text(target_path)\n\n# Extract the index of monsters, to help us identify monster headers.\n#\n# The first page is weird, so handle it special.\nmonster_index1 &lt;- srd_raw[[2]] |&gt;\n  stringr::str_split(\"\\\\n\") |&gt;\n  _[[1]] |&gt;\n  _[46:86] |&gt;\n  stringr::str_extract(\"\\\\s{2,}([^.]+)\\\\.[. ]+(2|3)\\\\d{2}\", 1)\nmonster_index1 &lt;- monster_index1[!is.na(monster_index1)]\n\nmonster_index2 &lt;- srd_raw[3:4] |&gt;\n  stringr::str_split(\"\\\\n\") |&gt;\n  purrr::map(\n    \\(page) {\n      page &lt;- stringr::str_subset(page, \"\\\\.{3,}\")\n      stringr::str_squish(unlist(stringr::str_split(page, \"\\\\.[. ]+\\\\d{1,3}\")))\n    }\n  ) |&gt;\n  unlist()\nmonster_index2 &lt;- monster_index2[nchar(monster_index2) &gt; 0]\n\nmonster_index &lt;- sort(c(monster_index1, monster_index2))\n\n# Monster descriptions are on p258-364.\nmonsters_raw &lt;- srd_raw[258:364] |&gt;\n  stringr::str_split(\"\\\\n\") |&gt;\n  purrr::map(\n    \\(page) {\n      # If there's a second column, it will begin somewhere after column 60. It\n      # can be a largish gap if there's spacing in column 2, like if column 2\n      # just has the \"MOD SAVE\" headers. It \"begins\" when there's a non-space\n      # (\\S) after at least 3 spaces (\\s). There can be large spaces earlier\n      # that aren't columns, though, so it makes sense to do this by character\n      # count.\n      col2_start &lt;- stringr::str_locate(\n        substr(page, 60, nchar(page)),\n        \"\\\\s{3}\\\\S\"\n      )[,\"end\"] + 60 - 1\n\n      col1_end &lt;- col2_start - 1\n      col1_end &lt;- ifelse(is.na(col1_end), nchar(page), col1_end)\n\n      # Turn everything into a single vector of text for this page.\n      single_column &lt;- c(\n        stringr::str_trim(substr(page, 1, col1_end)),\n        stringr::str_trim(substr(page, col2_start, nchar(page)))\n      )\n      # Blank rows appear semi-randomly, so let's not count on them as dividers.\n      single_column[!is.na(single_column) & nchar(single_column) &gt; 0] |&gt;\n        stringr::str_subset(\"System Reference Document\", negate = TRUE)\n    }\n  ) |&gt;\n  # Pages don't tell us anything special.\n  unlist()\n\n# Use the index to find where monster entries begin. The name appears alone on a\n# line.\nmonster_start_lines &lt;- which(monsters_raw %in% monster_index)\n\n# The line before a monster name is sometimes the group to which that monster\n# belongs.\npotential_monster_category_lines &lt;- monster_start_lines - 1\n# It's only a category if it only contains letters (or ’-), and isn't all caps.\nmonster_category_lines &lt;- potential_monster_category_lines[\n  stringr::str_which(\n    monsters_raw[potential_monster_category_lines],\n    \"^[A-Z][a-z’-]+[A-Za-z ]*$\"\n  )\n]\n\n# Often the category is the same as the monster name, so remove categories from\n# the individual monster starts.\nmonster_start_lines &lt;- setdiff(monster_start_lines, monster_category_lines)\nall_start_lines &lt;- sort(c(monster_start_lines, monster_category_lines))\n\n# Some blocks are multi-line. This function helps extract those blocks.\nextract_block &lt;- function(this_monster_text, block_name) {\n  all_block_names &lt;- c(\n    \"Skills\",\n    \"Resistances\",\n    \"Vulnerabilities\",\n    \"Immunities\",\n    \"Gear\",\n    \"Senses\",\n    \"Languages\",\n    \"CR\"\n  )\n  block_start &lt;- which(stringr::str_starts(this_monster_text, block_name))\n  if (length(block_start)) {\n    block_ends &lt;- which(\n      stringr::str_starts(\n        this_monster_text,\n        paste(all_block_names, collapse = \"|\")\n      )\n    )\n    block_end &lt;- block_ends[block_ends &gt; block_start][[1]] - 1\n    block_text &lt;- this_monster_text[block_start[[1]]:block_end] |&gt;\n      stringr::str_remove(\n        paste0(\"^\", block_name, \"\\\\s*\")\n      ) |&gt;\n      paste(collapse = \" \") |&gt;\n      stringr::str_squish()\n    return(block_text)\n  }\n  return(NA_character_)\n}\n\nmonsters &lt;- purrr::map(\n  monster_start_lines,\n  \\(this_monster_start) {\n    this_monster &lt;- monsters_raw[[this_monster_start]]\n\n    # Categories appear before one or more monsters in a group, so this\n    # monster's category is whichever category is most-recently before this\n    # monster.\n    this_monster_category &lt;- monsters_raw[\n      monster_category_lines[\n        max(which(monster_category_lines &lt; this_monster_start))\n      ]\n    ]\n\n    # Everything except the last monster ends just before the next monster or\n    # category.\n    later_starts &lt;- which(all_start_lines &gt; this_monster_start)\n    if (length(later_starts)) {\n      this_monster_end &lt;- all_start_lines[later_starts][[1]] - 1\n    } else {\n      this_monster_end &lt;- length(monsters_raw)\n    }\n    this_monster_text &lt;- monsters_raw[this_monster_start:this_monster_end]\n\n    # The monster blocks have a mostly standard format, but there are\n    # protections in here to deal with corner cases where lines are out of\n    # order or lines that are normally a single line are split.\n    tibble::tibble(\n      name = this_monster,\n      category = this_monster_category,\n      cr = stringr::str_subset(this_monster_text, \"^CR\")[[1]] |&gt;\n        stringr::str_extract(\"CR ([^( ]+)\", 1) |&gt;\n        stringr::str_squish() |&gt;\n        # Deal with fractional challenge ratings.\n        stringr::str_replace_all(c(\n          \"1/2\" = \"0.5\",\n          \"1/4\" = \"0.25\",\n          \"1/8\" = \"0.125\"\n        )) |&gt;\n        as.double(),\n      general_details = this_monster_text[[2]],\n      ac = this_monster_text[[3]] |&gt;\n        stringr::str_extract(\"AC (\\\\d+)\", 1),\n      # Initiative values start with \"+\" or the minus sign, which is \\u2212.\n      initiative = stringr::str_subset(\n        this_monster_text,\n        \"Initiative\\\\s+(\\\\+|\\u2212)\"\n      )[[1]] |&gt;\n        stringr::str_extract(\"Initiative\\\\s+((\\\\+|\\u2212)\\\\d+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      hp = stringr::str_subset(this_monster_text, \"^HP\")[[1]] |&gt;\n        stringr::str_extract(\"HP (.+)\", 1),\n      hp_number = stringr::str_extract(.data$hp, \"^\\\\d+\") |&gt;\n        as.integer(),\n      speed = stringr::str_subset(this_monster_text, \"^Speed\\\\s+\\\\d+\")[[1]],\n      speed_base_number = stringr::str_extract(.data$speed, \"\\\\d+\") |&gt;\n        as.integer(),\n      # Ability scores are sometimes grouped, sometimes on their own lines.\n      # Protect against cases where the abbreviation and number smush together,\n      # but don't accidentally grab lines like \"Construct\".\n      str = stringr::str_subset(this_monster_text, \"Str[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Str\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      dex = stringr::str_subset(this_monster_text, \"Dex[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Dex\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      con = stringr::str_subset(this_monster_text, \"Con[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Con\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      int = stringr::str_subset(this_monster_text, \"Int[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Int\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      wis = stringr::str_subset(this_monster_text, \"Wis[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Wis\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      cha = stringr::str_subset(this_monster_text, \"Cha[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Cha\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      str_save = stringr::str_subset(this_monster_text, \"Str[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Str\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      dex_save = stringr::str_subset(this_monster_text, \"Dex[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Dex\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      con_save = stringr::str_subset(this_monster_text, \"Con[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Con\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      int_save = stringr::str_subset(this_monster_text, \"Int[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Int\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      wis_save = stringr::str_subset(this_monster_text, \"Wis[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Wis\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      cha_save = stringr::str_subset(this_monster_text, \"Cha[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Cha\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      skills = extract_block(this_monster_text, \"Skills\"),\n      resistances = extract_block(this_monster_text, \"Resistances\"),\n      vulnerabilities = extract_block(this_monster_text, \"Vulnerabilities\"),\n      immunities = extract_block(this_monster_text, \"Immunities\"),\n      gear = extract_block(this_monster_text, \"Gear\"),\n      senses = extract_block(this_monster_text, \"Senses\"),\n      languages = extract_block(this_monster_text, \"Languages\"),\n      # Also include the full text so users can correct any parsing errors and\n      # otherwise dig for additional information.\n      full_text = paste(this_monster_text, collapse = \"\\n\")\n    ) |&gt;\n      # I had trouble separating the general_details into its components purely\n      # via regex, so I did so in a few steps.\n      tidyr::separate_wider_delim(\n        \"general_details\",\n        delim = \", \",\n        names = c(\"size_type\", \"alignment\")\n      ) |&gt;\n      dplyr::mutate(\n        size = stringr::str_extract(\n          .data$size_type,\n          \"^(Tiny|Small|Medium|Large|Huge|Gargantuan)(( or )(Tiny|Small|Medium|Large|Huge|Gargantuan))?\"\n        ),\n        type = stringr::str_remove(.data$size_type, .data$size) |&gt;\n          stringr::str_squish(),\n        .keep = \"unused\",\n        .before = \"size_type\"\n      ) |&gt;\n      tidyr::separate_wider_regex(\n        \"type\",\n        patterns = c(\n          type = \"^[^(]+\",\n          \"\\\\(\",\n          descriptive_tags = \"[^)]+\",\n          \"\\\\)$\"\n        ),\n        too_few = \"align_start\"\n      ) |&gt;\n      dplyr::mutate(\n        dplyr::across(\n          c(\"type\", \"descriptive_tags\"),\n          stringr::str_squish\n        )\n      )\n  }\n) |&gt;\n  purrr::list_rbind()\n\ndplyr::glimpse(monsters)"
  },
  {
    "objectID": "data/2025/2025-05-27/readme.html#the-data",
    "href": "data/2025/2025-05-27/readme.html#the-data",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-05-27')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 21)\n\nmonsters &lt;- tuesdata$monsters\n\n# Option 2: Read directly from GitHub\n\nmonsters &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-05-27')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nmonsters = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-05-27')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nmonsters = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nmonsters = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-05-27/monsters.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-05-27/readme.html#how-to-participate",
    "href": "data/2025/2025-05-27/readme.html#how-to-participate",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-05-27/readme.html#data-dictionary",
    "href": "data/2025/2025-05-27/readme.html#data-dictionary",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nname\ncharacter\nThe name of the monster.\n\n\ncategory\ncharacter\nThe category to which this monster belongs. Often the same as the name, but, for example, all “Animated Objects” share a category.\n\n\ncr\ndouble\nThe challenge rating of the monster.\n\n\nsize\ncharacter\nTiny, Small, Medium, Large, Huge or Gargantuan. If size options are presented, you choose the creature’s size from those options.\n\n\ntype\ncharacter\nEach monster has a tag that identifies the type of creature it is. Certain spells, magic items, class features, and other effects in the game interact in special ways with creatures of a particular type.\n\n\ndescriptive_tags\ncharacter\nOptional additional tags. Such tags provide additional categorization and have no rules of their own, but certain game effects might refer to them.\n\n\nalignment\ncharacter\nOne of Lawful Good, Neutral Good, Chaotic Good, Lawful Neutral, Neutral, Chaotic Neutral, Lawful Evil, Neutral Evil, Chaotic Evil, or Unaligned. The alignment specified in a monster’s stat block is a default suggestion of how to roleplay the monster, inspired by its traditional role in the game or real-world folklore. Change a monster’s alignment to suit your storytelling needs. The Neutral alignment, in particular, is an invitation for you to consider whether an individual leans toward one of the other alignments.\n\n\nac\ncharacter\nThe monster’s Armor Class (AC) includes its natural armor, Dexterity, gear, and other defenses.\n\n\ninitiative\ninteger\nThe monster’s Initiative modifier. Use the modifier when you roll to determine a monster’s Initiative. A monster’s Initiative modifier is typically equal to its Dexterity modifier, but some monsters have additional modifiers, such as Proficiency Bonus, applied to that number.\n\n\nhp\ncharacter\nThe monster’s Hit Points are presented as a number followed by parentheses, where the monster’s Hit Point Dice are provided, along with any contribution from its Constitution. Either use the number for the monster’s Hit Points (also available in “hp_number”) or roll the die expression in parentheses to determine the monster’s Hit Points randomly; don’t use both.\n\n\nhp_number\ninteger\nThe average Hit Points for the monster.\n\n\nspeed\ncharacter\nThe monster’s Speed. Some monsters have one or more of the following speeds: Burrow, Climb, Fly, Swim.\n\n\nspeed_base_number\ninteger\nThe first numeric speed for the monster, which is usually the walking speed.\n\n\nstr\ninteger\nThe monster’s strength score.\n\n\ndex\ninteger\nThe monster’s dexterity score.\n\n\ncon\ninteger\nThe monster’s constitution score.\n\n\nint\ninteger\nThe monster’s intelligence score.\n\n\nwis\ninteger\nThe monster’s wisdom score.\n\n\ncha\ninteger\nThe monster’s charisma score.\n\n\nstr_save\ninteger\nThe monster’s strength saving throw bonus.\n\n\ndex_save\ninteger\nThe monster’s dexterity saving throw bonus.\n\n\ncon_save\ninteger\nThe monster’s constitution saving throw bonus.\n\n\nint_save\ninteger\nThe monster’s intelligence saving throw bonus.\n\n\nwis_save\ninteger\nThe monster’s wisdom saving throw bonus.\n\n\ncha_save\ninteger\nThe monster’s charisma saving throw bonus.\n\n\nskills\ncharacter\nThe monster’s Skill proficiencies, if any. For example, a monster that is very perceptive and stealthy might have bonuses to Wisdom (Perception) and Dexterity (Stealth) checks. A skill bonus is the sum of a monster’s relevant ability modifier and its Proficiency Bonus. Other modifiers might apply.\n\n\nresistances\ncharacter\nThe monster’s Resistances, if any.\n\n\nvulnerabilities\ncharacter\nThe monster’s Vulnerabilities, if any.\n\n\nimmunities\ncharacter\nThe monster’s Immunities, if any. If the monster has damage and condition Immunities, the damage types are listed before the conditions.\n\n\ngear\ncharacter\nMonsters have proficiency with their equipment. If the monster has equipment that can be given away or retrieved, the items are listed in the Gear entry. The monster’s stat block might include special flourishes that happen when the monster uses an item, and the stat block might ignore the rules in “Equipment” for that item. When used by someone else, a retrievable item uses its “Equipment” rules, ignoring any special flourishes in the stat block. The Gear entry doesn’t necessarily list all of a monster’s equipment. For example, a monster that wears clothes is assumed to be dressed appropriately, and those clothes aren’t in this entry. Equipment mentioned outside the Gear entry is considered to be supernatural or highly specialized, and it is unusable when the monster is defeated.\n\n\nsenses\ncharacter\nThe monster’s Passive Perception score, as well as any special senses the monster possesses.\n\n\nlanguages\ncharacter\nLanguages that the monster can use to communicate. Sometimes the monster can understand a language but can’t communicate with it, which is noted in its entry. “None” indicates that the creature doesn’t comprehend any language. Telepathy is a magical ability that allows a creature to communicate mentally with another creature within a specified range.\n\n\nfull_text\ncharacter\nThe full text of the monster description, including the information above as well as monster Traits and Actions."
  },
  {
    "objectID": "data/2025/2025-05-27/readme.html#cleaning-script",
    "href": "data/2025/2025-05-27/readme.html#cleaning-script",
    "title": "Dungeons and Dragons Monsters (2024)",
    "section": "",
    "text": "# This work includes material from the System Reference Document 5.2.1 (“SRD\n# 5.2.1”) by Wizards of the Coast LLC, available at\n# https://www.dndbeyond.com/srd. The SRD 5.2.1 is licensed under the Creative\n# Commons Attribution 4.0 International License, available at\n# https://creativecommons.org/licenses/by/4.0/legalcode.\n\nlibrary(dplyr)\nlibrary(here)\nlibrary(pdftools)\nlibrary(purrr)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(withr)\n\nsrd_url &lt;- rvest::read_html(\"https://www.dndbeyond.com/srd\") |&gt;\n  rvest::html_nodes(\".download-link\") |&gt;\n  rvest::html_node(\"a\") |&gt;\n  rvest::html_attr(\"href\") |&gt;\n  _[[1]]\n\ntarget_path &lt;- withr::local_tempfile(fileext = \".pdf\")\ndownload.file(srd_url, target_path, mode = \"wb\")\n\n# Extract the raw text from the PDF.\nsrd_raw &lt;- pdftools::pdf_text(target_path)\n\n# Extract the index of monsters, to help us identify monster headers.\n#\n# The first page is weird, so handle it special.\nmonster_index1 &lt;- srd_raw[[2]] |&gt;\n  stringr::str_split(\"\\\\n\") |&gt;\n  _[[1]] |&gt;\n  _[46:86] |&gt;\n  stringr::str_extract(\"\\\\s{2,}([^.]+)\\\\.[. ]+(2|3)\\\\d{2}\", 1)\nmonster_index1 &lt;- monster_index1[!is.na(monster_index1)]\n\nmonster_index2 &lt;- srd_raw[3:4] |&gt;\n  stringr::str_split(\"\\\\n\") |&gt;\n  purrr::map(\n    \\(page) {\n      page &lt;- stringr::str_subset(page, \"\\\\.{3,}\")\n      stringr::str_squish(unlist(stringr::str_split(page, \"\\\\.[. ]+\\\\d{1,3}\")))\n    }\n  ) |&gt;\n  unlist()\nmonster_index2 &lt;- monster_index2[nchar(monster_index2) &gt; 0]\n\nmonster_index &lt;- sort(c(monster_index1, monster_index2))\n\n# Monster descriptions are on p258-364.\nmonsters_raw &lt;- srd_raw[258:364] |&gt;\n  stringr::str_split(\"\\\\n\") |&gt;\n  purrr::map(\n    \\(page) {\n      # If there's a second column, it will begin somewhere after column 60. It\n      # can be a largish gap if there's spacing in column 2, like if column 2\n      # just has the \"MOD SAVE\" headers. It \"begins\" when there's a non-space\n      # (\\S) after at least 3 spaces (\\s). There can be large spaces earlier\n      # that aren't columns, though, so it makes sense to do this by character\n      # count.\n      col2_start &lt;- stringr::str_locate(\n        substr(page, 60, nchar(page)),\n        \"\\\\s{3}\\\\S\"\n      )[,\"end\"] + 60 - 1\n\n      col1_end &lt;- col2_start - 1\n      col1_end &lt;- ifelse(is.na(col1_end), nchar(page), col1_end)\n\n      # Turn everything into a single vector of text for this page.\n      single_column &lt;- c(\n        stringr::str_trim(substr(page, 1, col1_end)),\n        stringr::str_trim(substr(page, col2_start, nchar(page)))\n      )\n      # Blank rows appear semi-randomly, so let's not count on them as dividers.\n      single_column[!is.na(single_column) & nchar(single_column) &gt; 0] |&gt;\n        stringr::str_subset(\"System Reference Document\", negate = TRUE)\n    }\n  ) |&gt;\n  # Pages don't tell us anything special.\n  unlist()\n\n# Use the index to find where monster entries begin. The name appears alone on a\n# line.\nmonster_start_lines &lt;- which(monsters_raw %in% monster_index)\n\n# The line before a monster name is sometimes the group to which that monster\n# belongs.\npotential_monster_category_lines &lt;- monster_start_lines - 1\n# It's only a category if it only contains letters (or ’-), and isn't all caps.\nmonster_category_lines &lt;- potential_monster_category_lines[\n  stringr::str_which(\n    monsters_raw[potential_monster_category_lines],\n    \"^[A-Z][a-z’-]+[A-Za-z ]*$\"\n  )\n]\n\n# Often the category is the same as the monster name, so remove categories from\n# the individual monster starts.\nmonster_start_lines &lt;- setdiff(monster_start_lines, monster_category_lines)\nall_start_lines &lt;- sort(c(monster_start_lines, monster_category_lines))\n\n# Some blocks are multi-line. This function helps extract those blocks.\nextract_block &lt;- function(this_monster_text, block_name) {\n  all_block_names &lt;- c(\n    \"Skills\",\n    \"Resistances\",\n    \"Vulnerabilities\",\n    \"Immunities\",\n    \"Gear\",\n    \"Senses\",\n    \"Languages\",\n    \"CR\"\n  )\n  block_start &lt;- which(stringr::str_starts(this_monster_text, block_name))\n  if (length(block_start)) {\n    block_ends &lt;- which(\n      stringr::str_starts(\n        this_monster_text,\n        paste(all_block_names, collapse = \"|\")\n      )\n    )\n    block_end &lt;- block_ends[block_ends &gt; block_start][[1]] - 1\n    block_text &lt;- this_monster_text[block_start[[1]]:block_end] |&gt;\n      stringr::str_remove(\n        paste0(\"^\", block_name, \"\\\\s*\")\n      ) |&gt;\n      paste(collapse = \" \") |&gt;\n      stringr::str_squish()\n    return(block_text)\n  }\n  return(NA_character_)\n}\n\nmonsters &lt;- purrr::map(\n  monster_start_lines,\n  \\(this_monster_start) {\n    this_monster &lt;- monsters_raw[[this_monster_start]]\n\n    # Categories appear before one or more monsters in a group, so this\n    # monster's category is whichever category is most-recently before this\n    # monster.\n    this_monster_category &lt;- monsters_raw[\n      monster_category_lines[\n        max(which(monster_category_lines &lt; this_monster_start))\n      ]\n    ]\n\n    # Everything except the last monster ends just before the next monster or\n    # category.\n    later_starts &lt;- which(all_start_lines &gt; this_monster_start)\n    if (length(later_starts)) {\n      this_monster_end &lt;- all_start_lines[later_starts][[1]] - 1\n    } else {\n      this_monster_end &lt;- length(monsters_raw)\n    }\n    this_monster_text &lt;- monsters_raw[this_monster_start:this_monster_end]\n\n    # The monster blocks have a mostly standard format, but there are\n    # protections in here to deal with corner cases where lines are out of\n    # order or lines that are normally a single line are split.\n    tibble::tibble(\n      name = this_monster,\n      category = this_monster_category,\n      cr = stringr::str_subset(this_monster_text, \"^CR\")[[1]] |&gt;\n        stringr::str_extract(\"CR ([^( ]+)\", 1) |&gt;\n        stringr::str_squish() |&gt;\n        # Deal with fractional challenge ratings.\n        stringr::str_replace_all(c(\n          \"1/2\" = \"0.5\",\n          \"1/4\" = \"0.25\",\n          \"1/8\" = \"0.125\"\n        )) |&gt;\n        as.double(),\n      general_details = this_monster_text[[2]],\n      ac = this_monster_text[[3]] |&gt;\n        stringr::str_extract(\"AC (\\\\d+)\", 1),\n      # Initiative values start with \"+\" or the minus sign, which is \\u2212.\n      initiative = stringr::str_subset(\n        this_monster_text,\n        \"Initiative\\\\s+(\\\\+|\\u2212)\"\n      )[[1]] |&gt;\n        stringr::str_extract(\"Initiative\\\\s+((\\\\+|\\u2212)\\\\d+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      hp = stringr::str_subset(this_monster_text, \"^HP\")[[1]] |&gt;\n        stringr::str_extract(\"HP (.+)\", 1),\n      hp_number = stringr::str_extract(.data$hp, \"^\\\\d+\") |&gt;\n        as.integer(),\n      speed = stringr::str_subset(this_monster_text, \"^Speed\\\\s+\\\\d+\")[[1]],\n      speed_base_number = stringr::str_extract(.data$speed, \"\\\\d+\") |&gt;\n        as.integer(),\n      # Ability scores are sometimes grouped, sometimes on their own lines.\n      # Protect against cases where the abbreviation and number smush together,\n      # but don't accidentally grab lines like \"Construct\".\n      str = stringr::str_subset(this_monster_text, \"Str[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Str\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      dex = stringr::str_subset(this_monster_text, \"Dex[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Dex\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      con = stringr::str_subset(this_monster_text, \"Con[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Con\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      int = stringr::str_subset(this_monster_text, \"Int[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Int\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      wis = stringr::str_subset(this_monster_text, \"Wis[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Wis\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      cha = stringr::str_subset(this_monster_text, \"Cha[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Cha\\\\s*(\\\\d+)\", 1) |&gt;\n        as.integer(),\n      str_save = stringr::str_subset(this_monster_text, \"Str[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Str\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      dex_save = stringr::str_subset(this_monster_text, \"Dex[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Dex\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      con_save = stringr::str_subset(this_monster_text, \"Con[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Con\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      int_save = stringr::str_subset(this_monster_text, \"Int[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Int\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      wis_save = stringr::str_subset(this_monster_text, \"Wis[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Wis\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      cha_save = stringr::str_subset(this_monster_text, \"Cha[ 0-9]\")[[1]] |&gt;\n        stringr::str_extract(\"Cha\\\\s*\\\\d+\\\\s+\\\\S+\\\\s+(\\\\S+)\", 1) |&gt;\n        stringr::str_replace(\"\\u2212\", \"-\") |&gt;\n        as.integer(),\n      skills = extract_block(this_monster_text, \"Skills\"),\n      resistances = extract_block(this_monster_text, \"Resistances\"),\n      vulnerabilities = extract_block(this_monster_text, \"Vulnerabilities\"),\n      immunities = extract_block(this_monster_text, \"Immunities\"),\n      gear = extract_block(this_monster_text, \"Gear\"),\n      senses = extract_block(this_monster_text, \"Senses\"),\n      languages = extract_block(this_monster_text, \"Languages\"),\n      # Also include the full text so users can correct any parsing errors and\n      # otherwise dig for additional information.\n      full_text = paste(this_monster_text, collapse = \"\\n\")\n    ) |&gt;\n      # I had trouble separating the general_details into its components purely\n      # via regex, so I did so in a few steps.\n      tidyr::separate_wider_delim(\n        \"general_details\",\n        delim = \", \",\n        names = c(\"size_type\", \"alignment\")\n      ) |&gt;\n      dplyr::mutate(\n        size = stringr::str_extract(\n          .data$size_type,\n          \"^(Tiny|Small|Medium|Large|Huge|Gargantuan)(( or )(Tiny|Small|Medium|Large|Huge|Gargantuan))?\"\n        ),\n        type = stringr::str_remove(.data$size_type, .data$size) |&gt;\n          stringr::str_squish(),\n        .keep = \"unused\",\n        .before = \"size_type\"\n      ) |&gt;\n      tidyr::separate_wider_regex(\n        \"type\",\n        patterns = c(\n          type = \"^[^(]+\",\n          \"\\\\(\",\n          descriptive_tags = \"[^)]+\",\n          \"\\\\)$\"\n        ),\n        too_few = \"align_start\"\n      ) |&gt;\n      dplyr::mutate(\n        dplyr::across(\n          c(\"type\", \"descriptive_tags\"),\n          stringr::str_squish\n        )\n      )\n  }\n) |&gt;\n  purrr::list_rbind()\n\ndplyr::glimpse(monsters)"
  },
  {
    "objectID": "data/2025/2025-06-10/readme.html",
    "href": "data/2025/2025-06-10/readme.html",
    "title": "U.S. Judges and the historydata R package",
    "section": "",
    "text": "This week we’re exploring U. S. judge data from the {historydata} R package! This package is looking for a new maintainer. If you are interested in this dataset (and other datasets of historical information), please consider volunteering! Check out the rOpenSci Blog What Does It Mean to Maintain a Package? by Maëlle Salmon for more information.\nNote: The package help for this dataset links to a particular PDF with some information about judgeships, but the actual source data can be found at Biographical Directory of Article III Federal Judges: Export on the Federal Judicial Center website.\n\nThis dataset contains information about the appointments and careers of all federal judges in United States history since 1789. It includes judges who “judges presidentially appointed during good behavior who have served since 1789 on the U.S. District Courts, the U.S. Courts of Appeals, the Supreme Court of the United States, the former U.S. Circuit Courts, and the federal judiciary’s courts of special jurisdiction.” Some of the unnecessary information from the source has been excluded.\n\n\nHow many judges have Pacific Islander as part of their designated race?\nWhich Presidents appointed the most judges? The fewest?\nWhich political parties have appointed the most judges to courts of customs or internation trade? Due to some coding issues in the current version of the {historydata} package, you may need to combine some terms to find all such appointments.\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-06-10')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 23)\n\njudges_appointments &lt;- tuesdata$judges_appointments\njudges_people &lt;- tuesdata$judges_people\n\n# Option 2: Read directly from GitHub\n\njudges_appointments &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv')\njudges_people &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-06-10')\n\n# Option 2: Read directly from GitHub and assign to an object\n\njudges_appointments = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv')\njudges_people = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-06-10')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\njudges_appointments = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv\")\njudges_people = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\njudges_appointments = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv\", DataFrame)\njudges_people = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\njudge_id\ninteger\n“Judge Identification Number.”” This was used as a unique identifier for each judge, generated for purposes of the database, until July 2016. These numbers are no longer used and will not be generated for judges added to the database after July 2016, but will remain in the export as a courtesy to researchers who may have relied on them. This is one area that should be updated when the package gets a new maintainer.\n\n\ncourt_name\ncharacter\nThe name of the court to which the judge was appointed.\n\n\ncourt_type\ncharacter\nThe type of court to which the judge was appointed. “U. S. Court of Custo” and “U. S. Court of Inter” each have 1 entry due to coding issues.\n\n\npresident_name\ncharacter\nThe name of the President who appointed the judge. Some entries have “Assignment” or “Reassignment”, indicating the judge was assigned or reassigned to this appointment via statute rather than by a President.\n\n\npresident_party\ncharacter\nThe political party of the President who appointed the judge. Some entries have “Assignment” or “Reassignment”, indicating the judge was assigned or reassigned to this appointment via statute rather than by a President.\n\n\nnomination_date\ncharacter\nThe date on which the judge was nominated, in “MM/DD/YYYY” format.\n\n\npredecessor_last_name\ncharacter\nThe last name of the judge’s predecessor in this position. The word “new” is sometimes used to indicate that this is a new position (but also check “predecessor_first_name”).\n\n\npredecessor_first_name\ncharacter\nThe first name of the judge’s predecessor in this position. The word “new” is sometimes used to indicate that this is a new position (but also check “predecessor_last_name”).\n\n\nsenate_confirmation_date\ncharacter\nThe date on which the Senate confirmed this appointment, in “MM/DD/YYYY” format. Note that some judges were never confirmed (they were “recess appointments”), and some were put into the office by statute (“Assignment” or “Reassignment”).\n\n\ncommission_date\ncharacter\nThe date on which the judgeship officially began, in “MM/DD/YYYY” format. NA for all recess appointments, as well as for four judgeships with missing information.\n\n\nchief_judge_begin\ninteger\nYear in which the judge began temporary service as Chief Judge. Only non-NA for 2 judges.\n\n\nchief_judge_end\ninteger\nYear in which the judge ended temporary service as Chief Judge. Only non-NA for 2 judges.\n\n\nretirement_from_active_service\ncharacter\nThe date on which the judge retired from active service, in “MM/DD/YYYY” format.\n\n\ntermination_date\ncharacter\nThe date on which the judge’s service ended (when appropriate), in “MM/DD/YYYY” format.\n\n\ntermination_reason\ncharacter\nThe reason that the judge’s service ended (when appropriate). One of “Abolition of Court”, “Appointment to Another Judicial Position”, “Death”, “Impeachment & Conviction”, “Reassignment”, “Recess Appointment-Not Confirmed”, “Resignation”, “Retirement” or NA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\njudge_id\ninteger\n“Judge Identification Number.”” This was used as a unique identifier for each judge, generated for purposes of the database, until July 2016. These numbers are no longer used and will not be generated for judges added to the database after July 2016, but will remain in the export as a courtesy to researchers who may have relied on them. This is one area that should be updated when the package gets a new maintainer.\n\n\nname_first\ncharacter\nThe first name of the judge.\n\n\nname_middle\ncharacter\nThe middle name or initial of the judge, if available.\n\n\nname_last\ncharacter\nThe last name of the judge.\n\n\nname_suffix\ncharacter\nOne of “Jr.”, “Sr.”, “II”, “III”, or “IV”, when appropriate.\n\n\nbirth_date\ninteger\nThe year in which the judge was born, if known.\n\n\nbirthplace_city\ncharacter\nThe city in which the judge was born, if known.\n\n\nbirthplace_state\ncharacter\nThe state in which the judge was born, if known.\n\n\ndeath_date\ninteger\nThe year in which the judge died, if known and applicable.\n\n\ndeath_city\ncharacter\nThe city in which the judge died, if known and applicable.\n\n\ndeath_state\ncharacter\nThe state in which the judge died, if known and applicable.\n\n\ngender\ncharacter\nThe gender of the judge, as reported by the judiciary.\n\n\nrace\ncharacter\nThe race of the judge, as reported by the judiciary.\n\n\n\n\n\n\n\n# Clean data provided by the development version of the {historydata} R package.\n# No cleaning was necessary.\n\n# install.packages(\"pak\")\n# pak::pak(\"ropensci/historydata\")\nlibrary(historydata)\n\njudges_people &lt;- historydata::judges_people\njudges_appointments &lt;- historydata::judges_appointments"
  },
  {
    "objectID": "data/2025/2025-06-10/readme.html#the-data",
    "href": "data/2025/2025-06-10/readme.html#the-data",
    "title": "U.S. Judges and the historydata R package",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-06-10')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 23)\n\njudges_appointments &lt;- tuesdata$judges_appointments\njudges_people &lt;- tuesdata$judges_people\n\n# Option 2: Read directly from GitHub\n\njudges_appointments &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv')\njudges_people &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-06-10')\n\n# Option 2: Read directly from GitHub and assign to an object\n\njudges_appointments = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv')\njudges_people = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-06-10')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\njudges_appointments = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv\")\njudges_people = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\njudges_appointments = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_appointments.csv\", DataFrame)\njudges_people = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-10/judges_people.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-06-10/readme.html#how-to-participate",
    "href": "data/2025/2025-06-10/readme.html#how-to-participate",
    "title": "U.S. Judges and the historydata R package",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-06-10/readme.html#data-dictionary",
    "href": "data/2025/2025-06-10/readme.html#data-dictionary",
    "title": "U.S. Judges and the historydata R package",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\njudge_id\ninteger\n“Judge Identification Number.”” This was used as a unique identifier for each judge, generated for purposes of the database, until July 2016. These numbers are no longer used and will not be generated for judges added to the database after July 2016, but will remain in the export as a courtesy to researchers who may have relied on them. This is one area that should be updated when the package gets a new maintainer.\n\n\ncourt_name\ncharacter\nThe name of the court to which the judge was appointed.\n\n\ncourt_type\ncharacter\nThe type of court to which the judge was appointed. “U. S. Court of Custo” and “U. S. Court of Inter” each have 1 entry due to coding issues.\n\n\npresident_name\ncharacter\nThe name of the President who appointed the judge. Some entries have “Assignment” or “Reassignment”, indicating the judge was assigned or reassigned to this appointment via statute rather than by a President.\n\n\npresident_party\ncharacter\nThe political party of the President who appointed the judge. Some entries have “Assignment” or “Reassignment”, indicating the judge was assigned or reassigned to this appointment via statute rather than by a President.\n\n\nnomination_date\ncharacter\nThe date on which the judge was nominated, in “MM/DD/YYYY” format.\n\n\npredecessor_last_name\ncharacter\nThe last name of the judge’s predecessor in this position. The word “new” is sometimes used to indicate that this is a new position (but also check “predecessor_first_name”).\n\n\npredecessor_first_name\ncharacter\nThe first name of the judge’s predecessor in this position. The word “new” is sometimes used to indicate that this is a new position (but also check “predecessor_last_name”).\n\n\nsenate_confirmation_date\ncharacter\nThe date on which the Senate confirmed this appointment, in “MM/DD/YYYY” format. Note that some judges were never confirmed (they were “recess appointments”), and some were put into the office by statute (“Assignment” or “Reassignment”).\n\n\ncommission_date\ncharacter\nThe date on which the judgeship officially began, in “MM/DD/YYYY” format. NA for all recess appointments, as well as for four judgeships with missing information.\n\n\nchief_judge_begin\ninteger\nYear in which the judge began temporary service as Chief Judge. Only non-NA for 2 judges.\n\n\nchief_judge_end\ninteger\nYear in which the judge ended temporary service as Chief Judge. Only non-NA for 2 judges.\n\n\nretirement_from_active_service\ncharacter\nThe date on which the judge retired from active service, in “MM/DD/YYYY” format.\n\n\ntermination_date\ncharacter\nThe date on which the judge’s service ended (when appropriate), in “MM/DD/YYYY” format.\n\n\ntermination_reason\ncharacter\nThe reason that the judge’s service ended (when appropriate). One of “Abolition of Court”, “Appointment to Another Judicial Position”, “Death”, “Impeachment & Conviction”, “Reassignment”, “Recess Appointment-Not Confirmed”, “Resignation”, “Retirement” or NA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\njudge_id\ninteger\n“Judge Identification Number.”” This was used as a unique identifier for each judge, generated for purposes of the database, until July 2016. These numbers are no longer used and will not be generated for judges added to the database after July 2016, but will remain in the export as a courtesy to researchers who may have relied on them. This is one area that should be updated when the package gets a new maintainer.\n\n\nname_first\ncharacter\nThe first name of the judge.\n\n\nname_middle\ncharacter\nThe middle name or initial of the judge, if available.\n\n\nname_last\ncharacter\nThe last name of the judge.\n\n\nname_suffix\ncharacter\nOne of “Jr.”, “Sr.”, “II”, “III”, or “IV”, when appropriate.\n\n\nbirth_date\ninteger\nThe year in which the judge was born, if known.\n\n\nbirthplace_city\ncharacter\nThe city in which the judge was born, if known.\n\n\nbirthplace_state\ncharacter\nThe state in which the judge was born, if known.\n\n\ndeath_date\ninteger\nThe year in which the judge died, if known and applicable.\n\n\ndeath_city\ncharacter\nThe city in which the judge died, if known and applicable.\n\n\ndeath_state\ncharacter\nThe state in which the judge died, if known and applicable.\n\n\ngender\ncharacter\nThe gender of the judge, as reported by the judiciary.\n\n\nrace\ncharacter\nThe race of the judge, as reported by the judiciary."
  },
  {
    "objectID": "data/2025/2025-06-10/readme.html#cleaning-script",
    "href": "data/2025/2025-06-10/readme.html#cleaning-script",
    "title": "U.S. Judges and the historydata R package",
    "section": "",
    "text": "# Clean data provided by the development version of the {historydata} R package.\n# No cleaning was necessary.\n\n# install.packages(\"pak\")\n# pak::pak(\"ropensci/historydata\")\nlibrary(historydata)\n\njudges_people &lt;- historydata::judges_people\njudges_appointments &lt;- historydata::judges_appointments"
  },
  {
    "objectID": "data/2025/2025-06-24/readme.html",
    "href": "data/2025/2025-06-24/readme.html",
    "title": "Measles cases across the world",
    "section": "",
    "text": "This week we are exploring measles and rubella cases across the world. This data was downloaded from the World Health Organisation Provisional monthly measles and rubella data on 2025-06-12.\n\nPlease note that all data contained within is provisional. The number of cases of measles and rubella officially reported by a WHO Member State is only available by July of each year (through the joint WHO UNICEF annual data collection exercise). If any numbers from this provisional data are quoted, they should be properly sourced with a date (i.e. “provisional data based on monthly data reported to WHO (Geneva) as of June 2025”). For official data from 1980, please visit our website: https://immunizationdata.who.int/global/wiise-detail-page/measles-reported-cases-and-incidence\n\nThe measles outbreak in the USA has been the subject of much media coverage in the past few months, however, measles continues to be a threat across the world\n\nHow have global measles cases changed over time?\nWhich regions or countries consistently report the highest measles burden?\nAre there seasonal patterns in measles outbreaks across different regions?\nDoes the ratio of laboratory-confirmed cases to total cases reveal differences in healthcare capacity across countries?\n\nThank you to Jen Richmond (R-Ladies Sydney) for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-06-24')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 25)\n\ncases_month &lt;- tuesdata$cases_month\ncases_year &lt;- tuesdata$cases_year\n\n# Option 2: Read directly from GitHub\n\ncases_month &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv')\ncases_year &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-06-24')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncases_month = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv')\ncases_year = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-06-24')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncases_month = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv\")\ncases_year = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncases_month = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv\", DataFrame)\ncases_year = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nregion\ncharacter\nRegion name\n\n\ncountry\ncharacter\nCountry name\n\n\niso3\ncharacter\nThree letter country code\n\n\nyear\ndouble\nYear\n\n\nmonth\ndouble\nMonth\n\n\nmeasles_suspect\ndouble\nSuspected measles cases: A suspected case is one in which a patient with fever and maculopapular (non-vesicular) rash, or in whom a health-care worker suspects measles\n\n\nmeasles_clinical\ndouble\nClinically-compatible measles cases: A suspected case with fever and maculopapular (non-vesicular) rash and at least one of cough, coryza or conjunctivitis, but no adequate clinical specimen was taken and the case has not been linked epidemiologically to a laboratory-confirmed case of measles or other communicable disease\n\n\nmeasles_epi_linked\ndouble\nEpidemiologically-linked measles cases: A suspected case of measles that has not been confirmed by a laboratory, but was geographically and temporally related with dates of rash onset occurring 7–23 days apart from a laboratory-confirmed case or another epidemiologically linked measles case\n\n\nmeasles_lab_confirmed\ndouble\nLaboratory-confirmed measles cases: A suspected case of measles that has been confirmed positive by testing in a proficient laboratory, and vaccine-associated illness has been ruled out\n\n\nmeasles_total\ndouble\nTotal measles cases: the sum of clinically-compatible, epidemiologically linked and laboratory-confirmed cases\n\n\nrubella_clinical\ndouble\nClinically-compatible rubella cases\n\n\nrubella_epi_linked\ndouble\nEpidemiologically-linked rubella cases\n\n\nrubella_lab_confirmed\ndouble\nLaboratory-confirmed rubella cases\n\n\nrubella_total\ndouble\nTotal rubella cases\n\n\ndiscarded\ndouble\nDiscarded cases: A suspected case that has been investigated and discarded as a non-measles (and non-rubella)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nregion\ncharacter\nRegion name\n\n\ncountry\ncharacter\nCountry name\n\n\niso3\ncharacter\nThree letter country code\n\n\nyear\ncharacter\nYear\n\n\ntotal_population\ncharacter\nCountry population\n\n\nannualized_population_most_recent_year_only\ncharacter\nAnnualized population 2025\n\n\ntotal_suspected_measles_rubella_cases\ncharacter\nSuspected measles/rubella cases: A suspected case is one in which a patient with fever and maculopapular (non-vesicular) rash, or in whom a health-care worker suspects measles (or rubella)\n\n\nmeasles_total\ncharacter\nTotal measles cases: the sum of clinically-compatible, epidemiologically linked and laboratory-confirmed cases\n\n\nmeasles_lab_confirmed\ncharacter\nLaboratory-confirmed measles cases: A suspected case of measles that has been confirmed positive by testing in a proficient laboratory, and vaccine-associated illness has been ruled out\n\n\nmeasles_epi_linked\ncharacter\nEpidemiologically-linked measles cases: A suspected case of measles that has not been confirmed by a laboratory, but was geographically and temporally related with dates of rash onset occurring 7–23 days apart from a laboratory-confirmed case or another epidemiologically linked measles case\n\n\nmeasles_clinical\ncharacter\nClinically-compatible measles cases: A suspected case with fever and maculopapular (non-vesicular) rash and at least one of cough, coryza or conjunctivitis, but no adequate clinical specimen was taken and the case has not been linked epidemiologically to a laboratory-confirmed case of measles or other communicable disease\n\n\nmeasles_incidence_rate_per_1000000_total_population\ncharacter\nMeasles cases per million population\n\n\nrubella_total\ncharacter\nTotal rubella cases\n\n\nrubella_lab_confirmed\ncharacter\nLaboratory-confirmed rubella cases\n\n\nrubella_epi_linked\ncharacter\nEpidemiologically-linked rubella cases\n\n\nrubella_clinical\ncharacter\nClinically-compatible rubella cases\n\n\nrubella_incidence_rate_per_1000000_total_population\ncharacter\nRubella cases per million population\n\n\ndiscarded_cases\ncharacter\nDiscarded cases: A suspected case that has been investigated and discarded as a non-measles (and non-rubella)\n\n\ndiscarded_non_measles_rubella_cases_per_100000_total_population\ncharacter\nDiscarded cases per million population\n\n\n\n\n\n\n\n# Data provided by WHO Provisional measles and rubella data. Cleaning variable\n# names and fixing data types.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(readxl)\nlibrary(janitor)\n\ncases_month &lt;- read_xlsx(here(\"404-table-web-epi-curve-data.xlsx\"), 2) %&gt;%\n  janitor::clean_names() %&gt;%\n  dplyr::mutate(\n    dplyr::across(year:discarded, as.numeric)\n  )\n\ncases_year &lt;- read_xlsx(here(\"403-table-web-reporting-data.xlsx\"), 2) %&gt;%\n  row_to_names(1) %&gt;%\n  clean_names() %&gt;%\n  rename(\n    country = member_state,\n    iso3 = iso_country_code,\n    measles_total= number_of_measles_cases_by_confirmation_method,\n    measles_lab_confirmed = na,\n    measles_epi_linked = na_2,\n    measles_clinical = na_3,\n    rubella_total = number_of_rubella_cases_by_confirmation_method,\n    rubella_lab_confirmed = na_4,\n    rubella_epi_linked = na_5,\n    rubella_clinical = na_6\n  ) %&gt;%\n  slice(-1)"
  },
  {
    "objectID": "data/2025/2025-06-24/readme.html#the-data",
    "href": "data/2025/2025-06-24/readme.html#the-data",
    "title": "Measles cases across the world",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-06-24')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 25)\n\ncases_month &lt;- tuesdata$cases_month\ncases_year &lt;- tuesdata$cases_year\n\n# Option 2: Read directly from GitHub\n\ncases_month &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv')\ncases_year &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-06-24')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncases_month = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv')\ncases_year = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-06-24')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncases_month = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv\")\ncases_year = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncases_month = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_month.csv\", DataFrame)\ncases_year = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-06-24/cases_year.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-06-24/readme.html#how-to-participate",
    "href": "data/2025/2025-06-24/readme.html#how-to-participate",
    "title": "Measles cases across the world",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-06-24/readme.html#data-dictionary",
    "href": "data/2025/2025-06-24/readme.html#data-dictionary",
    "title": "Measles cases across the world",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nregion\ncharacter\nRegion name\n\n\ncountry\ncharacter\nCountry name\n\n\niso3\ncharacter\nThree letter country code\n\n\nyear\ndouble\nYear\n\n\nmonth\ndouble\nMonth\n\n\nmeasles_suspect\ndouble\nSuspected measles cases: A suspected case is one in which a patient with fever and maculopapular (non-vesicular) rash, or in whom a health-care worker suspects measles\n\n\nmeasles_clinical\ndouble\nClinically-compatible measles cases: A suspected case with fever and maculopapular (non-vesicular) rash and at least one of cough, coryza or conjunctivitis, but no adequate clinical specimen was taken and the case has not been linked epidemiologically to a laboratory-confirmed case of measles or other communicable disease\n\n\nmeasles_epi_linked\ndouble\nEpidemiologically-linked measles cases: A suspected case of measles that has not been confirmed by a laboratory, but was geographically and temporally related with dates of rash onset occurring 7–23 days apart from a laboratory-confirmed case or another epidemiologically linked measles case\n\n\nmeasles_lab_confirmed\ndouble\nLaboratory-confirmed measles cases: A suspected case of measles that has been confirmed positive by testing in a proficient laboratory, and vaccine-associated illness has been ruled out\n\n\nmeasles_total\ndouble\nTotal measles cases: the sum of clinically-compatible, epidemiologically linked and laboratory-confirmed cases\n\n\nrubella_clinical\ndouble\nClinically-compatible rubella cases\n\n\nrubella_epi_linked\ndouble\nEpidemiologically-linked rubella cases\n\n\nrubella_lab_confirmed\ndouble\nLaboratory-confirmed rubella cases\n\n\nrubella_total\ndouble\nTotal rubella cases\n\n\ndiscarded\ndouble\nDiscarded cases: A suspected case that has been investigated and discarded as a non-measles (and non-rubella)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nregion\ncharacter\nRegion name\n\n\ncountry\ncharacter\nCountry name\n\n\niso3\ncharacter\nThree letter country code\n\n\nyear\ncharacter\nYear\n\n\ntotal_population\ncharacter\nCountry population\n\n\nannualized_population_most_recent_year_only\ncharacter\nAnnualized population 2025\n\n\ntotal_suspected_measles_rubella_cases\ncharacter\nSuspected measles/rubella cases: A suspected case is one in which a patient with fever and maculopapular (non-vesicular) rash, or in whom a health-care worker suspects measles (or rubella)\n\n\nmeasles_total\ncharacter\nTotal measles cases: the sum of clinically-compatible, epidemiologically linked and laboratory-confirmed cases\n\n\nmeasles_lab_confirmed\ncharacter\nLaboratory-confirmed measles cases: A suspected case of measles that has been confirmed positive by testing in a proficient laboratory, and vaccine-associated illness has been ruled out\n\n\nmeasles_epi_linked\ncharacter\nEpidemiologically-linked measles cases: A suspected case of measles that has not been confirmed by a laboratory, but was geographically and temporally related with dates of rash onset occurring 7–23 days apart from a laboratory-confirmed case or another epidemiologically linked measles case\n\n\nmeasles_clinical\ncharacter\nClinically-compatible measles cases: A suspected case with fever and maculopapular (non-vesicular) rash and at least one of cough, coryza or conjunctivitis, but no adequate clinical specimen was taken and the case has not been linked epidemiologically to a laboratory-confirmed case of measles or other communicable disease\n\n\nmeasles_incidence_rate_per_1000000_total_population\ncharacter\nMeasles cases per million population\n\n\nrubella_total\ncharacter\nTotal rubella cases\n\n\nrubella_lab_confirmed\ncharacter\nLaboratory-confirmed rubella cases\n\n\nrubella_epi_linked\ncharacter\nEpidemiologically-linked rubella cases\n\n\nrubella_clinical\ncharacter\nClinically-compatible rubella cases\n\n\nrubella_incidence_rate_per_1000000_total_population\ncharacter\nRubella cases per million population\n\n\ndiscarded_cases\ncharacter\nDiscarded cases: A suspected case that has been investigated and discarded as a non-measles (and non-rubella)\n\n\ndiscarded_non_measles_rubella_cases_per_100000_total_population\ncharacter\nDiscarded cases per million population"
  },
  {
    "objectID": "data/2025/2025-06-24/readme.html#cleaning-script",
    "href": "data/2025/2025-06-24/readme.html#cleaning-script",
    "title": "Measles cases across the world",
    "section": "",
    "text": "# Data provided by WHO Provisional measles and rubella data. Cleaning variable\n# names and fixing data types.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(readxl)\nlibrary(janitor)\n\ncases_month &lt;- read_xlsx(here(\"404-table-web-epi-curve-data.xlsx\"), 2) %&gt;%\n  janitor::clean_names() %&gt;%\n  dplyr::mutate(\n    dplyr::across(year:discarded, as.numeric)\n  )\n\ncases_year &lt;- read_xlsx(here(\"403-table-web-reporting-data.xlsx\"), 2) %&gt;%\n  row_to_names(1) %&gt;%\n  clean_names() %&gt;%\n  rename(\n    country = member_state,\n    iso3 = iso_country_code,\n    measles_total= number_of_measles_cases_by_confirmation_method,\n    measles_lab_confirmed = na,\n    measles_epi_linked = na_2,\n    measles_clinical = na_3,\n    rubella_total = number_of_rubella_cases_by_confirmation_method,\n    rubella_lab_confirmed = na_4,\n    rubella_epi_linked = na_5,\n    rubella_clinical = na_6\n  ) %&gt;%\n  slice(-1)"
  },
  {
    "objectID": "data/2025/2025-07-08/readme.html",
    "href": "data/2025/2025-07-08/readme.html",
    "title": "The xkcd Color Survey Results",
    "section": "",
    "text": "In 2010, the xkcd Color Survey asked hundreds of thousands of people to name colors they saw, revealing the different ways in which people perceive and label colors.\n\nColor is a really fascinating topic, especially since we’re taught so many different and often contradictory ideas about rainbows, different primary colors, and frequencies of light.\n\n\nWhich types of users were most accurate in naming colors?\nWhich colors are mentioned most in the top 100 ranked color names?\nWhich types of users are least likely to be spam users?\n\nThank you to Nicola Rennie for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-08')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 27)\n\nanswers &lt;- tuesdata$answers\ncolor_ranks &lt;- tuesdata$color_ranks\nusers &lt;- tuesdata$users\n\n# Option 2: Read directly from GitHub\n\nanswers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv')\ncolor_ranks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv')\nusers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-08')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nanswers = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv')\ncolor_ranks = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv')\nusers = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-08')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nanswers = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv\")\ncolor_ranks = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv\")\nusers = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nanswers = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv\", DataFrame)\ncolor_ranks = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv\", DataFrame)\nusers = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nuser_id\ndouble\nThe id of the user who gave the answer.\n\n\nhex\ncharacter\nHex code of the color shown to a user.\n\n\nrank\ndouble\nThe rank of the color that the user gave as the name of the color they were shown (join with color_ranksto get the color name answer given by the user). Note that this table is a subset of the full answers data where the color_name_answer was one of the names of the 5 top ranked colors in the color_ranks data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncolor\ncharacter\nThe name of the color (for the 954 most common RGB monitor colors only).\n\n\nrank\ndouble\nThe rank of the color.\n\n\nhex\ncharacter\nThe hex code of the color.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nuser_id\ndouble\nThe id of the user.\n\n\nmonitor\ncharacter\nThe user’s monitor type.\n\n\ny_chromosome\ndouble\nWhether or not the user reported having a Y chromosome. The data was recorded in this way since chromosomal sex is related to colorblindness.\n\n\ncolorblind\ndouble\nWhether or not the user reported being colorblind.\n\n\nspam_prob\ndouble\nProbability of the user being a spam user.\n\n\n\n\n\n\n\n# Clean data provided by https://github.com/nrennie/xkcd-color-survey/. No further cleaning was necessary.\ncolor_ranks &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nrennie/xkcd-color-survey/main/data/clean/color_ranks.csv\")\nanswers &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nrennie/xkcd-color-survey/main/data/clean/answers.csv\")\nusers &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nrennie/xkcd-color-survey/main/data/clean/users.csv\")"
  },
  {
    "objectID": "data/2025/2025-07-08/readme.html#the-data",
    "href": "data/2025/2025-07-08/readme.html#the-data",
    "title": "The xkcd Color Survey Results",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-08')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 27)\n\nanswers &lt;- tuesdata$answers\ncolor_ranks &lt;- tuesdata$color_ranks\nusers &lt;- tuesdata$users\n\n# Option 2: Read directly from GitHub\n\nanswers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv')\ncolor_ranks &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv')\nusers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-08')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nanswers = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv')\ncolor_ranks = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv')\nusers = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-08')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nanswers = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv\")\ncolor_ranks = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv\")\nusers = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nanswers = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/answers.csv\", DataFrame)\ncolor_ranks = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/color_ranks.csv\", DataFrame)\nusers = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-08/users.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-07-08/readme.html#how-to-participate",
    "href": "data/2025/2025-07-08/readme.html#how-to-participate",
    "title": "The xkcd Color Survey Results",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-07-08/readme.html#data-dictionary",
    "href": "data/2025/2025-07-08/readme.html#data-dictionary",
    "title": "The xkcd Color Survey Results",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nuser_id\ndouble\nThe id of the user who gave the answer.\n\n\nhex\ncharacter\nHex code of the color shown to a user.\n\n\nrank\ndouble\nThe rank of the color that the user gave as the name of the color they were shown (join with color_ranksto get the color name answer given by the user). Note that this table is a subset of the full answers data where the color_name_answer was one of the names of the 5 top ranked colors in the color_ranks data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncolor\ncharacter\nThe name of the color (for the 954 most common RGB monitor colors only).\n\n\nrank\ndouble\nThe rank of the color.\n\n\nhex\ncharacter\nThe hex code of the color.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nuser_id\ndouble\nThe id of the user.\n\n\nmonitor\ncharacter\nThe user’s monitor type.\n\n\ny_chromosome\ndouble\nWhether or not the user reported having a Y chromosome. The data was recorded in this way since chromosomal sex is related to colorblindness.\n\n\ncolorblind\ndouble\nWhether or not the user reported being colorblind.\n\n\nspam_prob\ndouble\nProbability of the user being a spam user."
  },
  {
    "objectID": "data/2025/2025-07-08/readme.html#cleaning-script",
    "href": "data/2025/2025-07-08/readme.html#cleaning-script",
    "title": "The xkcd Color Survey Results",
    "section": "",
    "text": "# Clean data provided by https://github.com/nrennie/xkcd-color-survey/. No further cleaning was necessary.\ncolor_ranks &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nrennie/xkcd-color-survey/main/data/clean/color_ranks.csv\")\nanswers &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nrennie/xkcd-color-survey/main/data/clean/answers.csv\")\nusers &lt;- readr::read_csv(\"https://raw.githubusercontent.com/nrennie/xkcd-color-survey/main/data/clean/users.csv\")"
  },
  {
    "objectID": "data/2025/2025-07-22/readme.html",
    "href": "data/2025/2025-07-22/readme.html",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "This week we’re exploring the (New York) MTA Permanent Art Catalog! Thank you to Georgios Karamanis (GitHub | Bluesky | LinkedIn for suggesting this dataset. Submit your dataset ideas through our GitHub issue tracker!\nThe MTA has a dashboard to explore this dataset, but the dashboard is at least partially broken. Can you recreate it in Shiny for R or Python?\n\nThrough the Permanent Art Program, MTA Arts & Design (formerly Arts for Transit) commissions public art that is seen by millions of city-dwellers as well as national and international visitors who use the MTA’s subways and trains. Arts & Design works closely with the architects and engineers at MTA NYC Transit, Long Island Rail Road and Metro-North Railroad to determine the parameters and sites for the artwork that is to be incorporated into each station scheduled for renovation. A diversity of well-established, mid-career and emerging artists contribute to the growing collection of works created in the materials of the system -mosaic, ceramic, tile, bronze, steel and glass. Artists are chosen through a competitive process that uses selection panels comprised of visual arts professionals and community representatives which review and select artists. This data provides the branch or station and the artist and artwork information.\n\n\nCan you reproduce the station_lines dataset without looking at our Cleaning Script? How does your solution differ from ours?\nWhich agency has the most art? Which has the least?\nWhat are some common materials? How are details such as “hand forged” for bronze denoted in the data? Can you reliably take these details into account in your counts?\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-22')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 29)\n\nmta_art &lt;- tuesdata$mta_art\nstation_lines &lt;- tuesdata$station_lines\n\n# Option 2: Read directly from GitHub\n\nmta_art &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv')\nstation_lines &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-22')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nmta_art = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv')\nstation_lines = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-22')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nmta_art = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv\")\nstation_lines = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nmta_art = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv\", DataFrame)\nstation_lines = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nagency\ncharacter\nThis is the abbreviated code for an agency. Example: LIRR = Long Island Rail Road, MNR = Metro-North Railroad, NYCT = New York City Transit.\n\n\nstation_name\ncharacter\nThis is the railroad or subway station where the art is located.\n\n\nline\ncharacter\nThis is the railroad or subway line that is associated with the station.\n\n\nartist\ncharacter\nThe artist’s name(s).\n\n\nart_title\ncharacter\nThis is the name of the piece of art.\n\n\nart_date\ndouble\nThis is the year the art was first displayed at the station.\n\n\nart_material\ncharacter\nThis describes what materials were used in making the art.\n\n\nart_description\ncharacter\nThis is a description of the art along with other interesting facts.\n\n\nart_image_link\ncharacter\nThis is a link that can take you directly to the MTA website so you may see the available art.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nagency\ncharacter\nThis is the abbreviated code for an agency. Example: LIRR = Long Island Rail Road, MNR = Metro-North Railroad, NYCT = New York City Transit.\n\n\nstation_name\ncharacter\nThis is the railroad or subway station where the art is located.\n\n\nline\ncharacter\nThis is the railroad or subway line that is associated with the station.\n\n\n\n\n\n\n\n# Clean data provided by the State of New York's data.ny.gov. No cleaning was\n# necessary. We split off the station_lines dataset as a demonstration.\n\nlibrary(tidyverse)\nmta_art &lt;- readr::read_csv(\"https://data.ny.gov/resource/4y8j-9pkd.csv\")\n\nstation_lines &lt;- mta_art |&gt;\n  dplyr::select(\"agency\", \"station_name\", \"line\") |&gt;\n  dplyr::filter(!is.na(.data$line)) |&gt;\n  tidyr::separate_longer_delim(\"line\", \",\")"
  },
  {
    "objectID": "data/2025/2025-07-22/readme.html#the-data",
    "href": "data/2025/2025-07-22/readme.html#the-data",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-07-22')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 29)\n\nmta_art &lt;- tuesdata$mta_art\nstation_lines &lt;- tuesdata$station_lines\n\n# Option 2: Read directly from GitHub\n\nmta_art &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv')\nstation_lines &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-07-22')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nmta_art = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv')\nstation_lines = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-07-22')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nmta_art = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv\")\nstation_lines = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nmta_art = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/mta_art.csv\", DataFrame)\nstation_lines = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-22/station_lines.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-07-22/readme.html#how-to-participate",
    "href": "data/2025/2025-07-22/readme.html#how-to-participate",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-07-22/readme.html#data-dictionary",
    "href": "data/2025/2025-07-22/readme.html#data-dictionary",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nagency\ncharacter\nThis is the abbreviated code for an agency. Example: LIRR = Long Island Rail Road, MNR = Metro-North Railroad, NYCT = New York City Transit.\n\n\nstation_name\ncharacter\nThis is the railroad or subway station where the art is located.\n\n\nline\ncharacter\nThis is the railroad or subway line that is associated with the station.\n\n\nartist\ncharacter\nThe artist’s name(s).\n\n\nart_title\ncharacter\nThis is the name of the piece of art.\n\n\nart_date\ndouble\nThis is the year the art was first displayed at the station.\n\n\nart_material\ncharacter\nThis describes what materials were used in making the art.\n\n\nart_description\ncharacter\nThis is a description of the art along with other interesting facts.\n\n\nart_image_link\ncharacter\nThis is a link that can take you directly to the MTA website so you may see the available art.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nagency\ncharacter\nThis is the abbreviated code for an agency. Example: LIRR = Long Island Rail Road, MNR = Metro-North Railroad, NYCT = New York City Transit.\n\n\nstation_name\ncharacter\nThis is the railroad or subway station where the art is located.\n\n\nline\ncharacter\nThis is the railroad or subway line that is associated with the station."
  },
  {
    "objectID": "data/2025/2025-07-22/readme.html#cleaning-script",
    "href": "data/2025/2025-07-22/readme.html#cleaning-script",
    "title": "MTA Permanent Art Catalog",
    "section": "",
    "text": "# Clean data provided by the State of New York's data.ny.gov. No cleaning was\n# necessary. We split off the station_lines dataset as a demonstration.\n\nlibrary(tidyverse)\nmta_art &lt;- readr::read_csv(\"https://data.ny.gov/resource/4y8j-9pkd.csv\")\n\nstation_lines &lt;- mta_art |&gt;\n  dplyr::select(\"agency\", \"station_name\", \"line\") |&gt;\n  dplyr::filter(!is.na(.data$line)) |&gt;\n  tidyr::separate_longer_delim(\"line\", \",\")"
  },
  {
    "objectID": "data/2025/2025-08-05/readme.html",
    "href": "data/2025/2025-08-05/readme.html",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "This week we’re exploring Income Inequality Before and After Taxes, as processed and visualized by Joe Hasell at Our World in Data: “Income inequality before and after taxes: how much do countries redistribute income?”\nAll data was processed by [Our World in Data]](https://ourworldindata.org), using these sources: - Luxembourg Income Study (2025) - OECD (2024) - HYDE (2023) - Gapminder - Population v7 (2022) - UN, World Population Prospects (2024) - Gapminder - Systema Globalis (2022)\n\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Inequality is measured here in terms of income before and after taxes and benefits.\n\n\nWhich countries have the highest Gini coefficient before taxes?\nWhich countries have the highest Gini coefficient after taxes?\nWhich countries have the highest shifts in Gini coefficient?\nWhich countries have the lowest shifts in Gini coefficient?\nWhich countries have had the highest changes in redistribution in the available data?\n\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 31)\n\nincome_inequality_processed &lt;- tuesdata$income_inequality_processed\nincome_inequality_raw &lt;- tuesdata$income_inequality_raw\n\n# Option 2: Read directly from GitHub\n\nincome_inequality_processed &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv')\nincome_inequality_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-08-05')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nincome_inequality_processed = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv')\nincome_inequality_raw = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-08-05')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nincome_inequality_processed = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv\")\nincome_inequality_raw = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nincome_inequality_processed = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv\", DataFrame)\nincome_inequality_raw = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry (or other region) name.\n\n\nCode\ncharacter\nThree-digit code when available.\n\n\nYear\ninteger\nYear to which the data applies.\n\n\ngini_mi_eq\ndouble\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Income is “pre-tax” — measured before taxes have been paid and most government benefits have been received. Income has been equivalized – adjusted to account for the fact that people in the same household can share costs like rent and heating.\n\n\ngini_dhi_eq\ndouble\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Income is “post-tax” — measured after taxes have been paid and most government benefits have been received. Income has been equivalized – adjusted to account for the fact that people in the same household can share costs like rent and heating.\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry (or other region) name.\n\n\nCode\ncharacter\nThree-digit code when available. Some entities do not have codes, and some have special “OWID” codes, such as “OWID_AKD”.\n\n\nYear\ninteger\nYear to which the data applies.\n\n\ngini_disposable__age_total\ndouble\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Income is ‘post-tax’ — measured after taxes have been paid and most government benefits have been received. Income has been equivalized – adjusted to account for the fact that people in the same household can share costs like rent and heating. The entire population of each country is considered, and also the income definition is the newest from the OECD since 2012. For more information on the methodology, visit the OECD Income Distribution Database (IDD). Survey estimates for 2020 are subject to additional uncertainty and are to be treated with extra caution, as in most countries the survey fieldwork was affected by the Coronavirus (COVID-19) pandemic.\n\n\ngini_market__age_total\ndouble\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Income is ‘pre-tax’ — measured before taxes have been paid and most government benefits have been received. However, data for China, Hungary, Mexico, Turkey as well as part of the data for Greece refer to the income post taxes and before transfers. Income has been equivalized – adjusted to account for the fact that people in the same household can share costs like rent and heating. The entire population of each country is considered, and also the income definition is the newest from the OECD since 2012. For more information on the methodology, visit the OECD Income Distribution Database (IDD). Survey estimates for 2020 are subject to additional uncertainty and are to be treated with extra caution, as in most countries the survey fieldwork was affected by the Coronavirus (COVID-19) pandemic.\n\n\npopulation_historical\ndouble\nPopulation by country, available from 10,000 BCE to 2023, based on data and estimates from different sources.\n\n\nowid_region\ncharacter\nWorld regions according to Our World in Data.\n\n\n\n\n\n\n\n# Clean data *and* sample instructions for use in R provided by Our World in\n# Data via https://ourworldindata.org/income-inequality-before-and-after-taxes.\n# Minimal cleaning applied.\n\nlibrary(tidyverse)\nlibrary(jsonlite)\n\nincome_inequality_raw &lt;- readr::read_csv(\n  \"https://ourworldindata.org/grapher/inequality-of-incomes-before-and-after-taxes-and-transfers-scatter.csv?v=1&csvType=full&useColumnShortNames=true\"\n) |&gt;\n  dplyr::mutate(\n    Year = as.integer(Year)\n  )\n\nincome_inequality_processed &lt;- readr::read_csv(\n  \"https://ourworldindata.org/grapher/gini-coefficient-before-and-after-tax-lis.csv?v=1&csvType=full&useColumnShortNames=true\"\n) |&gt;\n  dplyr::mutate(\n    Year = as.integer(Year)\n  )\n\n# Download metadata for use in construction of data dictionaries.\nmetadata_raw &lt;- jsonlite::fromJSON(\n  \"https://ourworldindata.org/grapher/inequality-of-incomes-before-and-after-taxes-and-transfers-scatter.metadata.json?v=1&csvType=full&useColumnShortNames=true\"\n)\n\nmetadata_processed &lt;- jsonlite::fromJSON(\n  \"https://ourworldindata.org/grapher/gini-coefficient-before-and-after-tax-lis.metadata.json?v=1&csvType=full&useColumnShortNames=true\"\n)"
  },
  {
    "objectID": "data/2025/2025-08-05/readme.html#the-data",
    "href": "data/2025/2025-08-05/readme.html#the-data",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-05')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 31)\n\nincome_inequality_processed &lt;- tuesdata$income_inequality_processed\nincome_inequality_raw &lt;- tuesdata$income_inequality_raw\n\n# Option 2: Read directly from GitHub\n\nincome_inequality_processed &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv')\nincome_inequality_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-08-05')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nincome_inequality_processed = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv')\nincome_inequality_raw = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-08-05')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nincome_inequality_processed = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv\")\nincome_inequality_raw = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nincome_inequality_processed = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_processed.csv\", DataFrame)\nincome_inequality_raw = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-05/income_inequality_raw.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-08-05/readme.html#how-to-participate",
    "href": "data/2025/2025-08-05/readme.html#how-to-participate",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-08-05/readme.html#data-dictionary",
    "href": "data/2025/2025-08-05/readme.html#data-dictionary",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry (or other region) name.\n\n\nCode\ncharacter\nThree-digit code when available.\n\n\nYear\ninteger\nYear to which the data applies.\n\n\ngini_mi_eq\ndouble\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Income is “pre-tax” — measured before taxes have been paid and most government benefits have been received. Income has been equivalized – adjusted to account for the fact that people in the same household can share costs like rent and heating.\n\n\ngini_dhi_eq\ndouble\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Income is “post-tax” — measured after taxes have been paid and most government benefits have been received. Income has been equivalized – adjusted to account for the fact that people in the same household can share costs like rent and heating.\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nEntity\ncharacter\nCountry (or other region) name.\n\n\nCode\ncharacter\nThree-digit code when available. Some entities do not have codes, and some have special “OWID” codes, such as “OWID_AKD”.\n\n\nYear\ninteger\nYear to which the data applies.\n\n\ngini_disposable__age_total\ndouble\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Income is ‘post-tax’ — measured after taxes have been paid and most government benefits have been received. Income has been equivalized – adjusted to account for the fact that people in the same household can share costs like rent and heating. The entire population of each country is considered, and also the income definition is the newest from the OECD since 2012. For more information on the methodology, visit the OECD Income Distribution Database (IDD). Survey estimates for 2020 are subject to additional uncertainty and are to be treated with extra caution, as in most countries the survey fieldwork was affected by the Coronavirus (COVID-19) pandemic.\n\n\ngini_market__age_total\ndouble\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Income is ‘pre-tax’ — measured before taxes have been paid and most government benefits have been received. However, data for China, Hungary, Mexico, Turkey as well as part of the data for Greece refer to the income post taxes and before transfers. Income has been equivalized – adjusted to account for the fact that people in the same household can share costs like rent and heating. The entire population of each country is considered, and also the income definition is the newest from the OECD since 2012. For more information on the methodology, visit the OECD Income Distribution Database (IDD). Survey estimates for 2020 are subject to additional uncertainty and are to be treated with extra caution, as in most countries the survey fieldwork was affected by the Coronavirus (COVID-19) pandemic.\n\n\npopulation_historical\ndouble\nPopulation by country, available from 10,000 BCE to 2023, based on data and estimates from different sources.\n\n\nowid_region\ncharacter\nWorld regions according to Our World in Data."
  },
  {
    "objectID": "data/2025/2025-08-05/readme.html#cleaning-script",
    "href": "data/2025/2025-08-05/readme.html#cleaning-script",
    "title": "Income Inequality Before and After Taxes",
    "section": "",
    "text": "# Clean data *and* sample instructions for use in R provided by Our World in\n# Data via https://ourworldindata.org/income-inequality-before-and-after-taxes.\n# Minimal cleaning applied.\n\nlibrary(tidyverse)\nlibrary(jsonlite)\n\nincome_inequality_raw &lt;- readr::read_csv(\n  \"https://ourworldindata.org/grapher/inequality-of-incomes-before-and-after-taxes-and-transfers-scatter.csv?v=1&csvType=full&useColumnShortNames=true\"\n) |&gt;\n  dplyr::mutate(\n    Year = as.integer(Year)\n  )\n\nincome_inequality_processed &lt;- readr::read_csv(\n  \"https://ourworldindata.org/grapher/gini-coefficient-before-and-after-tax-lis.csv?v=1&csvType=full&useColumnShortNames=true\"\n) |&gt;\n  dplyr::mutate(\n    Year = as.integer(Year)\n  )\n\n# Download metadata for use in construction of data dictionaries.\nmetadata_raw &lt;- jsonlite::fromJSON(\n  \"https://ourworldindata.org/grapher/inequality-of-incomes-before-and-after-taxes-and-transfers-scatter.metadata.json?v=1&csvType=full&useColumnShortNames=true\"\n)\n\nmetadata_processed &lt;- jsonlite::fromJSON(\n  \"https://ourworldindata.org/grapher/gini-coefficient-before-and-after-tax-lis.metadata.json?v=1&csvType=full&useColumnShortNames=true\"\n)"
  },
  {
    "objectID": "data/2025/2025-08-19/readme.html",
    "href": "data/2025/2025-08-19/readme.html",
    "title": "Scottish Munros",
    "section": "",
    "text": "A Munro is a Scottish mountain with an elevation of over 3,000 feet, whereas a Munro Top is a subsidiary summit of a Munro that also exceeds 3,000 feet in height but is not considered a distinct mountain in its own right. The most famous Munro is Ben Nevis.\nIn 1891, Sir Hugh Munro produced the first list of these hills. However, unlike other classification schemes in Scotland which require a peak to have a prominence of at least 500 feet for inclusion, the Munros lack a rigid set of criteria for inclusion. And so, re-surveying can lead to changes in which peaks are included on the list.\n\nHow many peaks currently listed as Munros have always been included on the list?\nWhich year saw the largest number of changes to the classification?\nWhich Munro is the most remote?\n\nThe Database of British and Irish Hills is licensed under a Creative Commons Attribution 4.0 International Licence. Please reference The Database of British and Irish Hills v18.2 and link to www.hills-database.co.uk.\nThank you to Nicola Rennie for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 33)\n\nscottish_munros &lt;- tuesdata$scottish_munros\n\n# Option 2: Read directly from GitHub\n\nscottish_munros &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-08-19')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nscottish_munros = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-08-19')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nscottish_munros = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nscottish_munros = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nDoBIH_number\ncharacter\nID Number in the Database of British and Irish hills\n\n\nName\ncharacter\nName of the Munro.\n\n\nHeight_m\ndouble\nThe height of the Munro in metres.\n\n\nHeight_ft\ndouble\nThe height of the Munro in feet.\n\n\nxcoord\ndouble\nx-coordinate of Munro, using British National Grid (OSGB36) projection which uses easting/northing in metres.\n\n\nycoord\ndouble\ny-coordinate of Munro, using British National Grid (OSGB36) projection which uses easting/northing in metres.\n\n\n1891\ncharacter\nClassification of the Munro in 1891. Either Munro, Munro Top, or NA.\n\n\n1921\ncharacter\nClassification of the Munro in 1921. Either Munro, Munro Top, or NA.\n\n\n1933\ncharacter\nClassification of the Munro in 1933. Either Munro, Munro Top, or NA.\n\n\n1953\ncharacter\nClassification of the Munro in 1953. Either Munro, Munro Top, or NA.\n\n\n1969\ncharacter\nClassification of the Munro in 1969. Either Munro, Munro Top, or NA.\n\n\n1974\ncharacter\nClassification of the Munro in 1974. Either Munro, Munro Top, or NA.\n\n\n1981\ncharacter\nClassification of the Munro in 1981. Either Munro, Munro Top, or NA.\n\n\n1984\ncharacter\nClassification of the Munro in 1984. Either Munro, Munro Top, or NA.\n\n\n1990\ncharacter\nClassification of the Munro in 1990. Either Munro, Munro Top, or NA.\n\n\n1997\ncharacter\nClassification of the Munro in 1997. Either Munro, Munro Top, or NA.\n\n\n2021\ncharacter\nClassification of the Munro in 2021. Either Munro, Munro Top, or NA.\n\n\nComments\ncharacter\nFree text field describing any changes to the data over time.\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\nraw_data &lt;- read_csv(\"https://www.hills-database.co.uk/munrotab_v8.0.1.csv\")\n\nscottish_munros &lt;- raw_data |&gt;\n  select(\n    `DoBIH Number`, Name,\n    `Height (m)`, `Height\\n(ft)`, xcoord, ycoord,\n    `1891`:`2021`, Comments\n  ) |&gt;\n  drop_na(`DoBIH Number`) |&gt; \n  rename(\n    Height_m = `Height (m)`,\n    Height_ft = `Height\\n(ft)`,\n    DoBIH_number = `DoBIH Number`\n  ) |&gt; \n  mutate(\n    Comments = case_when(\n      Comments %in% c(\"See named worksheet\", \"See named worksheet for old mapping\") ~ NA_character_,\n      TRUE ~ Comments\n    )\n  ) |&gt; \n  mutate(\n    across(`1891`:`2021`, ~case_when(\n      .x == \"MUN\" ~ \"Munro\",\n      .x == \"TOP\" ~ \"Munro Top\"\n    ))\n  )"
  },
  {
    "objectID": "data/2025/2025-08-19/readme.html#the-data",
    "href": "data/2025/2025-08-19/readme.html#the-data",
    "title": "Scottish Munros",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-08-19')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 33)\n\nscottish_munros &lt;- tuesdata$scottish_munros\n\n# Option 2: Read directly from GitHub\n\nscottish_munros &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-08-19')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nscottish_munros = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-08-19')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nscottish_munros = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nscottish_munros = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-19/scottish_munros.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-08-19/readme.html#how-to-participate",
    "href": "data/2025/2025-08-19/readme.html#how-to-participate",
    "title": "Scottish Munros",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-08-19/readme.html#data-dictionary",
    "href": "data/2025/2025-08-19/readme.html#data-dictionary",
    "title": "Scottish Munros",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nDoBIH_number\ncharacter\nID Number in the Database of British and Irish hills\n\n\nName\ncharacter\nName of the Munro.\n\n\nHeight_m\ndouble\nThe height of the Munro in metres.\n\n\nHeight_ft\ndouble\nThe height of the Munro in feet.\n\n\nxcoord\ndouble\nx-coordinate of Munro, using British National Grid (OSGB36) projection which uses easting/northing in metres.\n\n\nycoord\ndouble\ny-coordinate of Munro, using British National Grid (OSGB36) projection which uses easting/northing in metres.\n\n\n1891\ncharacter\nClassification of the Munro in 1891. Either Munro, Munro Top, or NA.\n\n\n1921\ncharacter\nClassification of the Munro in 1921. Either Munro, Munro Top, or NA.\n\n\n1933\ncharacter\nClassification of the Munro in 1933. Either Munro, Munro Top, or NA.\n\n\n1953\ncharacter\nClassification of the Munro in 1953. Either Munro, Munro Top, or NA.\n\n\n1969\ncharacter\nClassification of the Munro in 1969. Either Munro, Munro Top, or NA.\n\n\n1974\ncharacter\nClassification of the Munro in 1974. Either Munro, Munro Top, or NA.\n\n\n1981\ncharacter\nClassification of the Munro in 1981. Either Munro, Munro Top, or NA.\n\n\n1984\ncharacter\nClassification of the Munro in 1984. Either Munro, Munro Top, or NA.\n\n\n1990\ncharacter\nClassification of the Munro in 1990. Either Munro, Munro Top, or NA.\n\n\n1997\ncharacter\nClassification of the Munro in 1997. Either Munro, Munro Top, or NA.\n\n\n2021\ncharacter\nClassification of the Munro in 2021. Either Munro, Munro Top, or NA.\n\n\nComments\ncharacter\nFree text field describing any changes to the data over time."
  },
  {
    "objectID": "data/2025/2025-08-19/readme.html#cleaning-script",
    "href": "data/2025/2025-08-19/readme.html#cleaning-script",
    "title": "Scottish Munros",
    "section": "",
    "text": "library(tidyverse)\n\nraw_data &lt;- read_csv(\"https://www.hills-database.co.uk/munrotab_v8.0.1.csv\")\n\nscottish_munros &lt;- raw_data |&gt;\n  select(\n    `DoBIH Number`, Name,\n    `Height (m)`, `Height\\n(ft)`, xcoord, ycoord,\n    `1891`:`2021`, Comments\n  ) |&gt;\n  drop_na(`DoBIH Number`) |&gt; \n  rename(\n    Height_m = `Height (m)`,\n    Height_ft = `Height\\n(ft)`,\n    DoBIH_number = `DoBIH Number`\n  ) |&gt; \n  mutate(\n    Comments = case_when(\n      Comments %in% c(\"See named worksheet\", \"See named worksheet for old mapping\") ~ NA_character_,\n      TRUE ~ Comments\n    )\n  ) |&gt; \n  mutate(\n    across(`1891`:`2021`, ~case_when(\n      .x == \"MUN\" ~ \"Munro\",\n      .x == \"TOP\" ~ \"Munro Top\"\n    ))\n  )"
  },
  {
    "objectID": "data/2025/2025-09-02/readme.html",
    "href": "data/2025/2025-09-02/readme.html",
    "title": "Australian Frogs",
    "section": "",
    "text": "This week we’re exploring 2023 data from the sixth annual release of FrogID data.\nFrogID is an Australian frog call identification initiative. The FrogID mobile app allows citizen scientists to record and submit frog calls for museum experts to identify. Since 2017, FrogID data has contributed to over 30 scientific papers exploring frog ecology, taxonomy, and conservation.\n\nAustralia is home to a unique and diverse array of frog species found almost nowhere else on Earth, with 257 native species distributed throughout the continent. But Australia’s frogs are in peril – almost one in five species are threatened with extinction due to threats such as climate change, urbanisation, disease, and the spread of invasive species.\n\nSome questions you might explore: - Are there species that are endemic to certain regions? - Do different frog species have distinct calling seasons? - Which species has the widest geographic range? Which is the rarest?\nPrimary citation for FrogID data: Rowley JJL, & Callaghan CT (2020) The FrogID dataset: expert-validated occurrence records of Australia’s frogs collected by citizen scientists. ZooKeys 912: 139-151\nOfficial frog name data: Australian Society of Herpetologists Official List of Australian Species. 2025. http://www.australiansocietyofherpetologists.org/ash-official-list-of-australian-species.\nThank you to Jessica Moore for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-02')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 35)\n\nfrogID_data &lt;- tuesdata$frogID_data\nfrog_names &lt;- tuesdata$frog_names\n\n# Option 2: Read directly from GitHub\n\nfrogID_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv')\nfrog_names &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-02')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nfrogID_data = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv')\nfrog_names = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-02')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nfrogID_data = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv\")\nfrog_names = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nfrogID_data = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv\", DataFrame)\nfrog_names = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\noccurrenceID\ndouble\nOccurrence ID.\n\n\neventID\ndouble\nEvent ID.\n\n\ndecimalLatitude\ndouble\nLatitude of frog recording.\n\n\ndecimalLongitude\ndouble\nLongitude of frog recording.\n\n\nscientificName\ncharacter\nScientific frog name.\n\n\neventDate\ndate\nDate of recording.\n\n\neventTime\ncharacter\nTime of recording in 24-hour format.\n\n\ntimezone\ncharacter\nTime zone of recording.\n\n\ncoordinateUncertaintyInMeters\ndouble\nUncertainty of coordinates. Exact locality is buffered for sensitive or endangered species.\n\n\nrecordedBy\ndouble\nUser ID.\n\n\nstateProvince\ncharacter\nState or territory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsubfamily\ncharacter\nName of frog subfamily.\n\n\ntribe\ncharacter\nName of frog tribe.\n\n\nscientificName\ncharacter\nScientific frog name.\n\n\ncommonName\ncharacter\nCommon frog name.\n\n\nsecondary_commonNames\ncharacter\nSecondary frog name/s.\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(here)\n\n# download and clean frogID data\nfrogID &lt;- here(\"frog_data.csv\")\n\ndownload.file(\"https://d2pifd398unoeq.cloudfront.net/FrogID6_final_dataset.csv\",\n              destfile = frogID, mode = \"wb\")\n\nfrogID_data &lt;- read_csv(\"frog_data.csv\") %&gt;%\n  # remove columns containing only one unique value and other unnecessary columns\n  select(-c(datasetName, basisOfRecord, dataGeneralizations,\n            sex, lifestage, behavior, samplingProtocol, country,\n            machineObservation, geoprivacy,\n            modified)) %&gt;%\n  # restrict to 2023 data only to reduce file size\n  filter(format(eventDate, \"%Y\") == \"2023\") %&gt;%\n  # split time column into time and timezone\n  separate_wider_regex(eventTime,\n    patterns = c(eventTime = \"^\\\\d{2}:\\\\d{2}:\\\\d{2}\", timezone   = \".*$\")) %&gt;%\n  mutate(timezone = ifelse(grepl(\"^[+-]\", timezone), paste0(\"GMT\", timezone), timezone))\n\n\n# read and tidy frog name and common name data\ndownload.file(\"https://raw.githubusercontent.com/jessjep/Frogs/main/frog_names.xlsx\",\n              destfile = \"frog_names.xlsx\", mode = \"wb\")\n\nfrog_names &lt;- readxl::read_xlsx(\"frog_names.xlsx\") %&gt;%\n  select(1:4) %&gt;%\n  separate_wider_delim(`GROUP, FAMILY, SUBFAMILY, TRIBE`, delim = \",\",\n                       names = c(\"family\", \"subfamily\", \"tribe\")) %&gt;%\n  rename(scientificName = `GENUS SPECIES SUBSPECIES`,\n         commonName = `COMMON NAME`,\n         secondary_commonNames = `SECONDARY COMMON NAMES`) %&gt;%\n  select(-1)\n\nfrog_names[frog_names == \"—\"] &lt;- NA"
  },
  {
    "objectID": "data/2025/2025-09-02/readme.html#the-data",
    "href": "data/2025/2025-09-02/readme.html#the-data",
    "title": "Australian Frogs",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-02')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 35)\n\nfrogID_data &lt;- tuesdata$frogID_data\nfrog_names &lt;- tuesdata$frog_names\n\n# Option 2: Read directly from GitHub\n\nfrogID_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv')\nfrog_names &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-02')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nfrogID_data = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv')\nfrog_names = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-02')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nfrogID_data = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv\")\nfrog_names = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nfrogID_data = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv\", DataFrame)\nfrog_names = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-09-02/readme.html#how-to-participate",
    "href": "data/2025/2025-09-02/readme.html#how-to-participate",
    "title": "Australian Frogs",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-09-02/readme.html#data-dictionary",
    "href": "data/2025/2025-09-02/readme.html#data-dictionary",
    "title": "Australian Frogs",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\noccurrenceID\ndouble\nOccurrence ID.\n\n\neventID\ndouble\nEvent ID.\n\n\ndecimalLatitude\ndouble\nLatitude of frog recording.\n\n\ndecimalLongitude\ndouble\nLongitude of frog recording.\n\n\nscientificName\ncharacter\nScientific frog name.\n\n\neventDate\ndate\nDate of recording.\n\n\neventTime\ncharacter\nTime of recording in 24-hour format.\n\n\ntimezone\ncharacter\nTime zone of recording.\n\n\ncoordinateUncertaintyInMeters\ndouble\nUncertainty of coordinates. Exact locality is buffered for sensitive or endangered species.\n\n\nrecordedBy\ndouble\nUser ID.\n\n\nstateProvince\ncharacter\nState or territory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nsubfamily\ncharacter\nName of frog subfamily.\n\n\ntribe\ncharacter\nName of frog tribe.\n\n\nscientificName\ncharacter\nScientific frog name.\n\n\ncommonName\ncharacter\nCommon frog name.\n\n\nsecondary_commonNames\ncharacter\nSecondary frog name/s."
  },
  {
    "objectID": "data/2025/2025-09-02/readme.html#cleaning-script",
    "href": "data/2025/2025-09-02/readme.html#cleaning-script",
    "title": "Australian Frogs",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\n\n# download and clean frogID data\nfrogID &lt;- here(\"frog_data.csv\")\n\ndownload.file(\"https://d2pifd398unoeq.cloudfront.net/FrogID6_final_dataset.csv\",\n              destfile = frogID, mode = \"wb\")\n\nfrogID_data &lt;- read_csv(\"frog_data.csv\") %&gt;%\n  # remove columns containing only one unique value and other unnecessary columns\n  select(-c(datasetName, basisOfRecord, dataGeneralizations,\n            sex, lifestage, behavior, samplingProtocol, country,\n            machineObservation, geoprivacy,\n            modified)) %&gt;%\n  # restrict to 2023 data only to reduce file size\n  filter(format(eventDate, \"%Y\") == \"2023\") %&gt;%\n  # split time column into time and timezone\n  separate_wider_regex(eventTime,\n    patterns = c(eventTime = \"^\\\\d{2}:\\\\d{2}:\\\\d{2}\", timezone   = \".*$\")) %&gt;%\n  mutate(timezone = ifelse(grepl(\"^[+-]\", timezone), paste0(\"GMT\", timezone), timezone))\n\n\n# read and tidy frog name and common name data\ndownload.file(\"https://raw.githubusercontent.com/jessjep/Frogs/main/frog_names.xlsx\",\n              destfile = \"frog_names.xlsx\", mode = \"wb\")\n\nfrog_names &lt;- readxl::read_xlsx(\"frog_names.xlsx\") %&gt;%\n  select(1:4) %&gt;%\n  separate_wider_delim(`GROUP, FAMILY, SUBFAMILY, TRIBE`, delim = \",\",\n                       names = c(\"family\", \"subfamily\", \"tribe\")) %&gt;%\n  rename(scientificName = `GENUS SPECIES SUBSPECIES`,\n         commonName = `COMMON NAME`,\n         secondary_commonNames = `SECONDARY COMMON NAMES`) %&gt;%\n  select(-1)\n\nfrog_names[frog_names == \"—\"] &lt;- NA"
  },
  {
    "objectID": "data/2025/2025-09-16/readme.html",
    "href": "data/2025/2025-09-16/readme.html",
    "title": "Allrecipes",
    "section": "",
    "text": "This week we’re exploring a curated collection of recipes collected from Allrecipes.com! The data this week comes from the tastyR package (a dataset assembled from Allrecipes.com) and was prepared for analysis in R. Fields have been cleaned and standardized where possible to make comparisons and visual exploration straightforward.\n\nA collection of recipe datasets scraped from https://www.allrecipes.com/, containing two complementary datasets: allrecipes with 14,426 general recipes, and cuisines with 2,218 recipes categorized by country of origin. Both datasets include comprehensive recipe information such as ingredients, nutritional facts (calories, fat, carbs, protein), cooking times (preparation and cooking), ratings, and review metadata. All data has been cleaned and standardized, ready for analysis.\n\n\nWhich authors are most successful: who is most prolific, who has the highest average ratings or popularity, and do top authors specialize by cuisine, ingredient, or recipe length?\nIs there a relationship between prep/cook time and average rating?\nWhich recipe categories or cuisines tend to have the highest average ratings and review counts?\nWhich recipes are the most “actionable” — high rating with low total time?\n\nThank you to Brian Mubia for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 37)\n\nall_recipes &lt;- tuesdata$all_recipes\ncuisines &lt;- tuesdata$cuisines\n\n# Option 2: Read directly from GitHub\n\nall_recipes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv')\ncuisines &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-16')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nall_recipes = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv')\ncuisines = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-16')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nall_recipes = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv\")\ncuisines = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nall_recipes = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv\", DataFrame)\ncuisines = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of the recipe.\n\n\nurl\ncharacter\nLink to the recipe.\n\n\nauthor\ncharacter\nAuthor of the recipe.\n\n\ndate_published\ndate\nWhen the recipe was published/updated.\n\n\ningredients\ncharacter\nThe ingredients of the recipe.\n\n\ncalories\ninteger\nCalories per serving.\n\n\nfat\ninteger\nFat per serving.\n\n\ncarbs\ninteger\nCarbs per serving.\n\n\nprotein\ninteger\nProteins per serving.\n\n\navg_rating\ndouble\nAverage rating out of 5 stars.\n\n\ntotal_ratings\ninteger\nNumber of ratings received. NOTE: These values are erroneously truncated to the thousands, unless there were less than 1000 ratings total.\n\n\nreviews\ninteger\nNumber of written reviews. NOTE: These values are erroneously truncated to the thousands, unless there were less than 1000 ratings total.\n\n\nprep_time\ninteger\nPreparation time in minutes.\n\n\ncook_time\ninteger\nCooking time in minutes.\n\n\ntotal_time\ninteger\nPrep + cook time in minutes. Note that this value may not always match the actual total effort required, as other time-related fields (such as refrigeration, marination, fry time or additional wait periods) have been excluded due to inconsistent availability across recipes.\n\n\nservings\ninteger\nNumber of servings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of the recipe.\n\n\ncountry\ncharacter\nThe country/region the cuisine is from.\n\n\nurl\ncharacter\nLink to the recipe.\n\n\nauthor\ncharacter\nAuthor of the recipe.\n\n\ndate_published\ndate\nWhen the recipe was published/updated.\n\n\ningredients\ncharacter\nThe ingredients of the recipe.\n\n\ncalories\ninteger\nCalories per serving.\n\n\nfat\ninteger\nFat per serving.\n\n\ncarbs\ninteger\nCarbs per serving.\n\n\nprotein\ninteger\nProteins per serving.\n\n\navg_rating\ndouble\nAverage rating out of 5 stars.\n\n\ntotal_ratings\ninteger\nNumber of ratings received. NOTE: These values are erroneously truncated to the thousands, unless there were less than 1000 ratings total.\n\n\nreviews\ninteger\nNumber of written reviews. NOTE: These values are erroneously truncated to the thousands, unless there were less than 1000 ratings total.\n\n\nprep_time\ninteger\nPreparation time in minutes.\n\n\ncook_time\ninteger\nCooking time in minutes.\n\n\ntotal_time\ninteger\nPrep + cook time in minutes. Note that this value may not always match the actual total effort required, as other time-related fields (such as refrigeration, marination, fry time, or additional wait periods) have been excluded due to inconsistent availability across recipes.\n\n\nservings\ninteger\nNumber of servings.\n\n\n\n\n\n\n\n# Clean data provided by owlzyseyes. No cleaning was necessary.\nall_recipes &lt;- readr::read_csv(\"https://raw.githubusercontent.com/owlzyseyes/tastyR/refs/heads/main/data-raw/allrecipes.csv\")\ncuisines &lt;- readr::read_csv(\"https://raw.githubusercontent.com/owlzyseyes/tastyR/refs/heads/main/data-raw/cuisines.csv\")"
  },
  {
    "objectID": "data/2025/2025-09-16/readme.html#the-data",
    "href": "data/2025/2025-09-16/readme.html#the-data",
    "title": "Allrecipes",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 37)\n\nall_recipes &lt;- tuesdata$all_recipes\ncuisines &lt;- tuesdata$cuisines\n\n# Option 2: Read directly from GitHub\n\nall_recipes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv')\ncuisines &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-16')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nall_recipes = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv')\ncuisines = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-16')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nall_recipes = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv\")\ncuisines = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nall_recipes = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv\", DataFrame)\ncuisines = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-09-16/readme.html#how-to-participate",
    "href": "data/2025/2025-09-16/readme.html#how-to-participate",
    "title": "Allrecipes",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-09-16/readme.html#data-dictionary",
    "href": "data/2025/2025-09-16/readme.html#data-dictionary",
    "title": "Allrecipes",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of the recipe.\n\n\nurl\ncharacter\nLink to the recipe.\n\n\nauthor\ncharacter\nAuthor of the recipe.\n\n\ndate_published\ndate\nWhen the recipe was published/updated.\n\n\ningredients\ncharacter\nThe ingredients of the recipe.\n\n\ncalories\ninteger\nCalories per serving.\n\n\nfat\ninteger\nFat per serving.\n\n\ncarbs\ninteger\nCarbs per serving.\n\n\nprotein\ninteger\nProteins per serving.\n\n\navg_rating\ndouble\nAverage rating out of 5 stars.\n\n\ntotal_ratings\ninteger\nNumber of ratings received. NOTE: These values are erroneously truncated to the thousands, unless there were less than 1000 ratings total.\n\n\nreviews\ninteger\nNumber of written reviews. NOTE: These values are erroneously truncated to the thousands, unless there were less than 1000 ratings total.\n\n\nprep_time\ninteger\nPreparation time in minutes.\n\n\ncook_time\ninteger\nCooking time in minutes.\n\n\ntotal_time\ninteger\nPrep + cook time in minutes. Note that this value may not always match the actual total effort required, as other time-related fields (such as refrigeration, marination, fry time or additional wait periods) have been excluded due to inconsistent availability across recipes.\n\n\nservings\ninteger\nNumber of servings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nName of the recipe.\n\n\ncountry\ncharacter\nThe country/region the cuisine is from.\n\n\nurl\ncharacter\nLink to the recipe.\n\n\nauthor\ncharacter\nAuthor of the recipe.\n\n\ndate_published\ndate\nWhen the recipe was published/updated.\n\n\ningredients\ncharacter\nThe ingredients of the recipe.\n\n\ncalories\ninteger\nCalories per serving.\n\n\nfat\ninteger\nFat per serving.\n\n\ncarbs\ninteger\nCarbs per serving.\n\n\nprotein\ninteger\nProteins per serving.\n\n\navg_rating\ndouble\nAverage rating out of 5 stars.\n\n\ntotal_ratings\ninteger\nNumber of ratings received. NOTE: These values are erroneously truncated to the thousands, unless there were less than 1000 ratings total.\n\n\nreviews\ninteger\nNumber of written reviews. NOTE: These values are erroneously truncated to the thousands, unless there were less than 1000 ratings total.\n\n\nprep_time\ninteger\nPreparation time in minutes.\n\n\ncook_time\ninteger\nCooking time in minutes.\n\n\ntotal_time\ninteger\nPrep + cook time in minutes. Note that this value may not always match the actual total effort required, as other time-related fields (such as refrigeration, marination, fry time, or additional wait periods) have been excluded due to inconsistent availability across recipes.\n\n\nservings\ninteger\nNumber of servings."
  },
  {
    "objectID": "data/2025/2025-09-16/readme.html#cleaning-script",
    "href": "data/2025/2025-09-16/readme.html#cleaning-script",
    "title": "Allrecipes",
    "section": "",
    "text": "# Clean data provided by owlzyseyes. No cleaning was necessary.\nall_recipes &lt;- readr::read_csv(\"https://raw.githubusercontent.com/owlzyseyes/tastyR/refs/heads/main/data-raw/allrecipes.csv\")\ncuisines &lt;- readr::read_csv(\"https://raw.githubusercontent.com/owlzyseyes/tastyR/refs/heads/main/data-raw/cuisines.csv\")"
  },
  {
    "objectID": "data/2025/2025-09-30/readme.html",
    "href": "data/2025/2025-09-30/readme.html",
    "title": "Crane Observations at Lake Hornborgasjön, Sweden (1994–2024)",
    "section": "",
    "text": "This week we are exploring crane observations at Lake Hornborgasjön in Sweden. For more than 30 years, cranes stopping at the Lake Hornborgasjön (‘Lake Hornborga’) in Västergötland, Sweden have been counted from the Hornborgasjön field station in the spring and the fall as they pass by during their yearly migration.\n\nThanks to crane counters from the Hornborgasjön field station, we know approximately how many cranes there are at Hornborgasjön during the spring. When there are the most cranes depends on whether spring is early or late. It also depends on when the winds from the south are suitable for crane flight.\n\n\nHas the crane population at Lake Hornborgasjön grown over the past 30 years?\nIf you wanted to see thousands of cranes, when is the best time of year to visit?\nIs it possible to predict the arrival of the cranes from weather patterns?\n\nThank you to Carl Borstell for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 39)\n\ncranes &lt;- tuesdata$cranes\n\n# Option 2: Read directly from GitHub\n\ncranes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-30')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncranes = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-30')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncranes = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncranes = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of counting observations\n\n\nobservations\ndouble\nNumber of observations\n\n\ncomment\ncharacter\nComments added by counting officials (only kept if comment indicated one of “Bad weather”, “Canceled/No count”, “First count of season”, “Last count of season”, “Record observation”, “Severe interference”, or “Uncertain count”; NA otherwise)\n\n\nweather_disruption\nlogical\nWhether original comments indicate a weather disruption by mentioning fog, rain, snow, thunder, or weather\n\n\n\n\n\n\n\n# Download the full CSV from the bottom of\n# https://www.hornborga.com/naturen/transtatistik/ as \"Transtatistik.csv\".\n#\n# Cleaning code extracts comments in English, adds a variable related to weather\n# disruption, selects relevant columns and filters the data to include only 30\n# years (1994-2024).\n\n# Read CSV (update path as needed)\ncsv_path &lt;- \"Transtatistik.csv\"\ncranes &lt;- readr::read_csv(csv_path)\n\n# Extract information from comments (relevant labels into English)\ncranes &lt;- cranes |&gt;\n  dplyr::mutate(\n    comment = dplyr::case_when(\n      stringr::str_detect(\n        Anteckning,\n        \"[Ii]nga siffror|[Ii]ngen siffra|[Ii]ngen räkn|[Ii]nst[.ä]|[Ii]nat\"\n        # no numbers, no number, no count, cancelled, innumerable\n      ) ~\n        \"Canceled/No count\",\n      stringr::str_detect(Anteckning, \"[Ss]vårräkn|[Oo]säk\") ~\n        \"Uncertain count\",\n      stringr::str_detect(Anteckning, \"[Rr]ekord\") ~ \"Record observation\",\n      stringr::str_detect(Anteckning, \"[Ff]örsta\") ~ \"First count of season\",\n      stringr::str_detect(Anteckning, \"[Ss]ista|[Aa]vslut\") ~\n        \"Last count of season\",\n      stringr::str_detect(Anteckning, \"[Dd]åligt [Vv]äder\") ~ \"Bad weather\",\n      stringr::str_detect(Anteckning, \"[Kk]raftig [Ss]törning\") ~\n        \"Severe interference\"\n    )\n  ) |&gt;\n  # Extract weather-related disruptions from comments into logical column\n  dplyr::mutate(\n    weather_disruption = dplyr::case_when(\n      stringr::str_detect(\n        Anteckning,\n        \"[Rr]egn|[Vv]äd|[Ss]nö|[Dd]imma|[Åå]ska\"\n        # rain, weather, snow, fog, thunder\n      ) ~\n        TRUE,\n      .default = FALSE\n    )\n  ) |&gt;\n  # Rename to English lowercase\n  dplyr::rename(date = Datum, observations = Antal) |&gt;\n  # Select relevant columns\n  dplyr::select(date, observations, comment, weather_disruption) |&gt;\n  # Filter to dates before 2025 (to get 30-year anniversary data, 1994—2024)\n  dplyr::filter(date &lt; \"2025-01-01\")"
  },
  {
    "objectID": "data/2025/2025-09-30/readme.html#the-data",
    "href": "data/2025/2025-09-30/readme.html#the-data",
    "title": "Crane Observations at Lake Hornborgasjön, Sweden (1994–2024)",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-09-30')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 39)\n\ncranes &lt;- tuesdata$cranes\n\n# Option 2: Read directly from GitHub\n\ncranes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-09-30')\n\n# Option 2: Read directly from GitHub and assign to an object\n\ncranes = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-09-30')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\ncranes = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\ncranes = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-30/cranes.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-09-30/readme.html#how-to-participate",
    "href": "data/2025/2025-09-30/readme.html#how-to-participate",
    "title": "Crane Observations at Lake Hornborgasjön, Sweden (1994–2024)",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-09-30/readme.html#data-dictionary",
    "href": "data/2025/2025-09-30/readme.html#data-dictionary",
    "title": "Crane Observations at Lake Hornborgasjön, Sweden (1994–2024)",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ndate\ndate\nDate of counting observations\n\n\nobservations\ndouble\nNumber of observations\n\n\ncomment\ncharacter\nComments added by counting officials (only kept if comment indicated one of “Bad weather”, “Canceled/No count”, “First count of season”, “Last count of season”, “Record observation”, “Severe interference”, or “Uncertain count”; NA otherwise)\n\n\nweather_disruption\nlogical\nWhether original comments indicate a weather disruption by mentioning fog, rain, snow, thunder, or weather"
  },
  {
    "objectID": "data/2025/2025-09-30/readme.html#cleaning-script",
    "href": "data/2025/2025-09-30/readme.html#cleaning-script",
    "title": "Crane Observations at Lake Hornborgasjön, Sweden (1994–2024)",
    "section": "",
    "text": "# Download the full CSV from the bottom of\n# https://www.hornborga.com/naturen/transtatistik/ as \"Transtatistik.csv\".\n#\n# Cleaning code extracts comments in English, adds a variable related to weather\n# disruption, selects relevant columns and filters the data to include only 30\n# years (1994-2024).\n\n# Read CSV (update path as needed)\ncsv_path &lt;- \"Transtatistik.csv\"\ncranes &lt;- readr::read_csv(csv_path)\n\n# Extract information from comments (relevant labels into English)\ncranes &lt;- cranes |&gt;\n  dplyr::mutate(\n    comment = dplyr::case_when(\n      stringr::str_detect(\n        Anteckning,\n        \"[Ii]nga siffror|[Ii]ngen siffra|[Ii]ngen räkn|[Ii]nst[.ä]|[Ii]nat\"\n        # no numbers, no number, no count, cancelled, innumerable\n      ) ~\n        \"Canceled/No count\",\n      stringr::str_detect(Anteckning, \"[Ss]vårräkn|[Oo]säk\") ~\n        \"Uncertain count\",\n      stringr::str_detect(Anteckning, \"[Rr]ekord\") ~ \"Record observation\",\n      stringr::str_detect(Anteckning, \"[Ff]örsta\") ~ \"First count of season\",\n      stringr::str_detect(Anteckning, \"[Ss]ista|[Aa]vslut\") ~\n        \"Last count of season\",\n      stringr::str_detect(Anteckning, \"[Dd]åligt [Vv]äder\") ~ \"Bad weather\",\n      stringr::str_detect(Anteckning, \"[Kk]raftig [Ss]törning\") ~\n        \"Severe interference\"\n    )\n  ) |&gt;\n  # Extract weather-related disruptions from comments into logical column\n  dplyr::mutate(\n    weather_disruption = dplyr::case_when(\n      stringr::str_detect(\n        Anteckning,\n        \"[Rr]egn|[Vv]äd|[Ss]nö|[Dd]imma|[Åå]ska\"\n        # rain, weather, snow, fog, thunder\n      ) ~\n        TRUE,\n      .default = FALSE\n    )\n  ) |&gt;\n  # Rename to English lowercase\n  dplyr::rename(date = Datum, observations = Antal) |&gt;\n  # Select relevant columns\n  dplyr::select(date, observations, comment, weather_disruption) |&gt;\n  # Filter to dates before 2025 (to get 30-year anniversary data, 1994—2024)\n  dplyr::filter(date &lt; \"2025-01-01\")"
  },
  {
    "objectID": "data/2025/2025-10-14/readme.html",
    "href": "data/2025/2025-10-14/readme.html",
    "title": "World Food Day",
    "section": "",
    "text": "This Thursday (2025-10-16) is World Food Day, which celebrates the foundation of The Food and Agriculture Organization of the United Nations (FAO), which turns 80 this week! To mark this occasion, this week we’re looking at the FAO’s Suite of Food Security Indicators.\n\nFollowing the recommendation of experts gathered in the Committee on World Food Security (CFS) Round Table on hunger measurement, hosted at FAO headquarters in September 2011, an initial set of indicators aiming to capture various aspects of food insecurity is presented here. The choice of the indicators has been informed by expert judgment and the availability of data with sufficient coverage to enable comparisons across regions and over time.\n\n\nWhich indicators tend to vary together?\nDo any indicators appear to be leading indicators of others?\nOf the indicators with confidence intervals, how have the confidence intervals changed over time or by region?\n\nThanks to Carl Börstell for suggesting this dataset!\nThank you to Jon Harmon, Data Science Learning Community for curating this week’s dataset.\nThanks to @ravichandrasekaran for reporting the major error in the dataset!\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-10-14')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 41)\n\nfood_security &lt;- tuesdata$food_security\n\n# Option 2: Read directly from GitHub\n\nfood_security &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-10-14')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nfood_security = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-10-14')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nfood_security = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nfood_security = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nYear_Start\ninteger\nFirst year of this observation.\n\n\nYear_End\ninteger\nFinal year of this observation.\n\n\nArea\ncharacter\nCountry or region in which the observation took place.\n\n\nItem\ncharacter\nThe specific indicator of food security.\n\n\nUnit\ncharacter\nUnit of the measurement. One of “g/cap/d” (Grams per capita per day), “Int\\(/cap\" (International Dollar per capita), \"kcal/cap/d\" (Kilocalories per capita per day), \"km\" (Kilometers), \"million No\" (million Number), \"No\" (Number), \"%\" (Percent), \"1000 Int\\)/cap” (thousand International Dollar per capita), or “index”.\n\n\nValue\ndouble\nThe numeric value of the measurement. Note: values of “0.09”, “0.49”, and “2.49” were “&lt;0.1”, “&lt;0.5”, and “&lt;2.5” (respectively) in the original dataset.\n\n\nCI_Lower\ndouble\nThe lower bound of the confidence interval of the measurement. Note: values of “0.09” and “0.49” were “&lt;0.1” and “&lt;0.5” (respectively) in the original dataset.\n\n\nCI_Upper\ndouble\nThe upper bound of the confidence interval of the measurement. Note: values of “0.09” and “0.49” were “&lt;0.1” and “&lt;0.5” (respectively) in the original dataset.\n\n\nFlag\ncharacter\nAdditional information about the measurement. One of “Estimated value”, “Figure from external organization”, “Missing value”, “Missing value; suppressed”, or “Official figure”\n\n\nNote\ncharacter\nAdditional details about this measurement (mostly NA).\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(withr)\n\nfood_security_url &lt;- \"https://bulks-faostat.fao.org/production/Food_Security_Data_E_All_Data_(Normalized).zip\"\ncsv_location &lt;- withr::local_tempfile(fileext = \".zip\")\ndownload.file(food_security_url, destfile = csv_location)\nfood_security &lt;- readr::read_csv(csv_location) |&gt;\n  # These fields are only useful if you're looking things up elsewhere\n  dplyr::select(-tidyselect::contains(\"Code\")) |&gt;\n  tidyr::pivot_wider(names_from = \"Element\", values_from = \"Value\") |&gt;\n  dplyr::rename(\n    CI_Lower = \"Confidence interval: Lower bound\",\n    CI_Upper = \"Confidence interval: Upper bound\"\n  ) |&gt;\n  dplyr::mutate(\n    Flag = dplyr::case_match(\n      .data$Flag,\n      \"E\" ~ \"Estimated value\",\n      \"X\" ~ \"Figure from external organization\",\n      \"O\" ~ \"Missing value\",\n      \"Q\" ~ \"Missing value; suppressed\",\n      \"A\" ~ \"Official figure\"\n    ),\n    # Remember to note this in dictionary!\n    dplyr::across(\n      c(tidyselect::starts_with(\"CI_\"), \"Value\"),\n      \\(CI) {\n        dplyr::case_match(\n          CI,\n          \"&lt;0.1\" ~ \"0.09\",\n          \"&lt;0.5\" ~ \"0.49\",\n          \"&lt;2.5\" ~ \"2.49\",\n          .default = CI\n        ) |&gt;\n          as.numeric()\n      }\n    ),\n    Year_Start = as.integer(stringr::str_extract(.data$Year, \"^(\\\\d{4})\")),\n    Year_End = as.integer(stringr::str_extract(.data$Year, \"(\\\\d{4})$\")),\n    Unit = dplyr::case_when(\n      is.na(.data$Unit) &\n        .data$Item ==\n        \"Political stability and absence of violence/terrorism (index)\" ~\n        \"index\",\n      .default = .data$Unit\n    )\n  ) |&gt;\n  dplyr::select(\n    tidyselect::starts_with(\"Year_\"),\n    \"Area\",\n    \"Item\",\n    \"Unit\",\n    \"Value\",\n    tidyselect::starts_with(\"CI\"),\n    \"Flag\",\n    \"Note\"\n  )\n\n# Filter so the resulting CSV will fit on GitHub.\nfood_security &lt;- food_security |&gt;\n  dplyr::filter(.data$Year_Start &gt;= 2005)\n\nrm(csv_location, food_security_url)"
  },
  {
    "objectID": "data/2025/2025-10-14/readme.html#the-data",
    "href": "data/2025/2025-10-14/readme.html#the-data",
    "title": "World Food Day",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-10-14')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 41)\n\nfood_security &lt;- tuesdata$food_security\n\n# Option 2: Read directly from GitHub\n\nfood_security &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-10-14')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nfood_security = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-10-14')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nfood_security = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nfood_security = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-14/food_security.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-10-14/readme.html#how-to-participate",
    "href": "data/2025/2025-10-14/readme.html#how-to-participate",
    "title": "World Food Day",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-10-14/readme.html#data-dictionary",
    "href": "data/2025/2025-10-14/readme.html#data-dictionary",
    "title": "World Food Day",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nYear_Start\ninteger\nFirst year of this observation.\n\n\nYear_End\ninteger\nFinal year of this observation.\n\n\nArea\ncharacter\nCountry or region in which the observation took place.\n\n\nItem\ncharacter\nThe specific indicator of food security.\n\n\nUnit\ncharacter\nUnit of the measurement. One of “g/cap/d” (Grams per capita per day), “Int\\(/cap\" (International Dollar per capita), \"kcal/cap/d\" (Kilocalories per capita per day), \"km\" (Kilometers), \"million No\" (million Number), \"No\" (Number), \"%\" (Percent), \"1000 Int\\)/cap” (thousand International Dollar per capita), or “index”.\n\n\nValue\ndouble\nThe numeric value of the measurement. Note: values of “0.09”, “0.49”, and “2.49” were “&lt;0.1”, “&lt;0.5”, and “&lt;2.5” (respectively) in the original dataset.\n\n\nCI_Lower\ndouble\nThe lower bound of the confidence interval of the measurement. Note: values of “0.09” and “0.49” were “&lt;0.1” and “&lt;0.5” (respectively) in the original dataset.\n\n\nCI_Upper\ndouble\nThe upper bound of the confidence interval of the measurement. Note: values of “0.09” and “0.49” were “&lt;0.1” and “&lt;0.5” (respectively) in the original dataset.\n\n\nFlag\ncharacter\nAdditional information about the measurement. One of “Estimated value”, “Figure from external organization”, “Missing value”, “Missing value; suppressed”, or “Official figure”\n\n\nNote\ncharacter\nAdditional details about this measurement (mostly NA)."
  },
  {
    "objectID": "data/2025/2025-10-14/readme.html#cleaning-script",
    "href": "data/2025/2025-10-14/readme.html#cleaning-script",
    "title": "World Food Day",
    "section": "",
    "text": "library(tidyverse)\nlibrary(withr)\n\nfood_security_url &lt;- \"https://bulks-faostat.fao.org/production/Food_Security_Data_E_All_Data_(Normalized).zip\"\ncsv_location &lt;- withr::local_tempfile(fileext = \".zip\")\ndownload.file(food_security_url, destfile = csv_location)\nfood_security &lt;- readr::read_csv(csv_location) |&gt;\n  # These fields are only useful if you're looking things up elsewhere\n  dplyr::select(-tidyselect::contains(\"Code\")) |&gt;\n  tidyr::pivot_wider(names_from = \"Element\", values_from = \"Value\") |&gt;\n  dplyr::rename(\n    CI_Lower = \"Confidence interval: Lower bound\",\n    CI_Upper = \"Confidence interval: Upper bound\"\n  ) |&gt;\n  dplyr::mutate(\n    Flag = dplyr::case_match(\n      .data$Flag,\n      \"E\" ~ \"Estimated value\",\n      \"X\" ~ \"Figure from external organization\",\n      \"O\" ~ \"Missing value\",\n      \"Q\" ~ \"Missing value; suppressed\",\n      \"A\" ~ \"Official figure\"\n    ),\n    # Remember to note this in dictionary!\n    dplyr::across(\n      c(tidyselect::starts_with(\"CI_\"), \"Value\"),\n      \\(CI) {\n        dplyr::case_match(\n          CI,\n          \"&lt;0.1\" ~ \"0.09\",\n          \"&lt;0.5\" ~ \"0.49\",\n          \"&lt;2.5\" ~ \"2.49\",\n          .default = CI\n        ) |&gt;\n          as.numeric()\n      }\n    ),\n    Year_Start = as.integer(stringr::str_extract(.data$Year, \"^(\\\\d{4})\")),\n    Year_End = as.integer(stringr::str_extract(.data$Year, \"(\\\\d{4})$\")),\n    Unit = dplyr::case_when(\n      is.na(.data$Unit) &\n        .data$Item ==\n        \"Political stability and absence of violence/terrorism (index)\" ~\n        \"index\",\n      .default = .data$Unit\n    )\n  ) |&gt;\n  dplyr::select(\n    tidyselect::starts_with(\"Year_\"),\n    \"Area\",\n    \"Item\",\n    \"Unit\",\n    \"Value\",\n    tidyselect::starts_with(\"CI\"),\n    \"Flag\",\n    \"Note\"\n  )\n\n# Filter so the resulting CSV will fit on GitHub.\nfood_security &lt;- food_security |&gt;\n  dplyr::filter(.data$Year_Start &gt;= 2005)\n\nrm(csv_location, food_security_url)"
  },
  {
    "objectID": "data/2025/2025-10-28/readme.html",
    "href": "data/2025/2025-10-28/readme.html",
    "title": "Selected British Literary Prizes (1990-2022)",
    "section": "",
    "text": "This week we are exploring data related to the Selected British Literary Prizes (1990-2022) dataset which comes from the Post45 Data Collective.\n\n“This dataset contains primary categories of information on individual authors comprising gender, sexuality, UK residency, ethnicity, geography and details of educational background, including institutions where the authors acquired their degrees and their fields of study. Along with other similar projects, we aim to provide information to assess the cultural, social and political factors determining literary prestige. Our goal is to contribute to greater transparency in discussions around diversity and equity in literary prize cultures.”\n\nAdditional metadata discussion relating to the ethnicity, gender and sexuality, and educational classification variables is available on the Post45 site. Follow them on BlueSky at @post45data.bsky.social, and here on GitHub at @Post45-Data-Collective.\nThank you to Georgios Karamanis for the dataset suggestion!\nIn relation to ethical considerations, the authors note that…\n\n“All of the information in this dataset is publicly available. Information about a writer’s location, gender identity, race, ethnicity, or education from scholarly and public sources can be sensitive. The data provided here enables the study of broad patterns and is not intended as definitive.”\n\n\nIn which genres are women, Black, Asian and ethnically diverse writers most likely to be shortlisted and/or awarded?\nHave prizes improved their record on gender and/or ethnic representation in shortlists and awardees?\nIs there a connection between specific educational credentials and/or educational institutions and writers’ chances of being shortlisted or winning?\n\nThank you to Jen Richmond for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-10-28')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 43)\n\nprizes &lt;- tuesdata$prizes\n\n# Option 2: Read directly from GitHub\n\nprizes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-10-28')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nprizes = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-10-28')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nprizes = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nprizes = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nprize_id\ninteger\nUnique prize identifier used in the SBLP dataset.\n\n\nprize_alias\ncharacter\nName of the prize awarded, regularized to the most current name.\n\n\nprize_name\ncharacter\nName of the prize awarded, at the time of award.\n\n\nprize_institution\ncharacter\nInstitution that sponsored the prize.\n\n\nprize_year\ninteger\nYear the prize was awarded.\n\n\nprize_genre\ncharacter\nGenre category of book that the prize was awarded to.\n\n\nperson_id\ncharacter\nUnique author identifier used in the SBLP dataset, assigned in order of entity entry to the dataset.\n\n\nperson_role\ncharacter\nWhether author was shortlisted or won the prize.\n\n\nlast_name\ncharacter\nFamily name of author.\n\n\nfirst_name\ncharacter\nGiven name of author.\n\n\nname\ncharacter\nFull name author in family name, given name format.\n\n\ngender\ncharacter\nAuthor’s gender, as self-declared/publicly available.\n\n\nsexuality\ncharacter\nAuthor’s sexuality, as self-declared/publicly available.\n\n\nuk_residence\nlogical\nWhether the author holds residence status in the UK at the time of data gathering.\n\n\nethnicity_macro\ncharacter\nEthnicity macro category, as created for this dataset.\n\n\nethnicity\ncharacter\nEthnicity as self-declared/publicly available.\n\n\nhighest_degree\ncharacter\nHighest level of post-secondary education.\n\n\ndegree_institution\ncharacter\nInstitution from which the highest degree was attained.\n\n\ndegree_field_category\ncharacter\nDegree macro category, as created for this dataset.\n\n\ndegree_field\ncharacter\nField of study, as self-declared/publicly available.\n\n\nviaf\ncharacter\nVirtual internet authority file code.\n\n\nbook_id\ncharacter\nUnique book identifier used in the SBLP dataset.\n\n\nbook_title\ncharacter\nTitle of the awarded or shortlisted book.\n\n\n\n\n\n\n\n# Data obtained from Post45 Data Collective Github, no cleaning necessary\n\nprizes &lt;- readr::read_csv(\"https://raw.githubusercontent.com/Post45-Data-Collective/data/refs/heads/main/british_literary_prizes/british_literary_prizes-1990-2022.csv\")"
  },
  {
    "objectID": "data/2025/2025-10-28/readme.html#the-data",
    "href": "data/2025/2025-10-28/readme.html#the-data",
    "title": "Selected British Literary Prizes (1990-2022)",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-10-28')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 43)\n\nprizes &lt;- tuesdata$prizes\n\n# Option 2: Read directly from GitHub\n\nprizes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-10-28')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nprizes = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-10-28')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nprizes = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nprizes = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-10-28/prizes.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-10-28/readme.html#how-to-participate",
    "href": "data/2025/2025-10-28/readme.html#how-to-participate",
    "title": "Selected British Literary Prizes (1990-2022)",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-10-28/readme.html#data-dictionary",
    "href": "data/2025/2025-10-28/readme.html#data-dictionary",
    "title": "Selected British Literary Prizes (1990-2022)",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nprize_id\ninteger\nUnique prize identifier used in the SBLP dataset.\n\n\nprize_alias\ncharacter\nName of the prize awarded, regularized to the most current name.\n\n\nprize_name\ncharacter\nName of the prize awarded, at the time of award.\n\n\nprize_institution\ncharacter\nInstitution that sponsored the prize.\n\n\nprize_year\ninteger\nYear the prize was awarded.\n\n\nprize_genre\ncharacter\nGenre category of book that the prize was awarded to.\n\n\nperson_id\ncharacter\nUnique author identifier used in the SBLP dataset, assigned in order of entity entry to the dataset.\n\n\nperson_role\ncharacter\nWhether author was shortlisted or won the prize.\n\n\nlast_name\ncharacter\nFamily name of author.\n\n\nfirst_name\ncharacter\nGiven name of author.\n\n\nname\ncharacter\nFull name author in family name, given name format.\n\n\ngender\ncharacter\nAuthor’s gender, as self-declared/publicly available.\n\n\nsexuality\ncharacter\nAuthor’s sexuality, as self-declared/publicly available.\n\n\nuk_residence\nlogical\nWhether the author holds residence status in the UK at the time of data gathering.\n\n\nethnicity_macro\ncharacter\nEthnicity macro category, as created for this dataset.\n\n\nethnicity\ncharacter\nEthnicity as self-declared/publicly available.\n\n\nhighest_degree\ncharacter\nHighest level of post-secondary education.\n\n\ndegree_institution\ncharacter\nInstitution from which the highest degree was attained.\n\n\ndegree_field_category\ncharacter\nDegree macro category, as created for this dataset.\n\n\ndegree_field\ncharacter\nField of study, as self-declared/publicly available.\n\n\nviaf\ncharacter\nVirtual internet authority file code.\n\n\nbook_id\ncharacter\nUnique book identifier used in the SBLP dataset.\n\n\nbook_title\ncharacter\nTitle of the awarded or shortlisted book."
  },
  {
    "objectID": "data/2025/2025-10-28/readme.html#cleaning-script",
    "href": "data/2025/2025-10-28/readme.html#cleaning-script",
    "title": "Selected British Literary Prizes (1990-2022)",
    "section": "",
    "text": "# Data obtained from Post45 Data Collective Github, no cleaning necessary\n\nprizes &lt;- readr::read_csv(\"https://raw.githubusercontent.com/Post45-Data-Collective/data/refs/heads/main/british_literary_prizes/british_literary_prizes-1990-2022.csv\")"
  },
  {
    "objectID": "data/2025/2025-11-11/readme.html",
    "href": "data/2025/2025-11-11/readme.html",
    "title": "WHO TB Burden Data: Incidence, Mortality, and Population",
    "section": "",
    "text": "This week, we explore global tuberculosis (TB) burden estimates from the World Health Organization, using data curated via the getTBinR R package by Sam Abbott. The dataset includes country-level indicators such as TB incidence, mortality, case detection rates, and population estimates across multiple years. These metrics help researchers, public health professionals, and learners understand the scale and distribution of TB worldwide.\n\nTuberculosis remains one of the world’s deadliest infectious diseases. WHO estimates that 10.6 million people fell ill with TB in 2021, and 1.6 million died from the disease. Monitoring TB burden is essential to guide national responses and global strategies.\n\n\nAre there any years where global TB metrics show unusual spikes or drops?\nHow does TB mortality differ between HIV-positive and HIV-negative populations?\nWhich regions show consistent high TB burden across multiple years?\n\nThank you to Darakhshan Nehal for curating this week’s dataset.\n(Note: We removed the original dataset that was slated to run this week after being informed about the history of that dataset. See Case Study of Pima Indian Diabetes Data: Intersection of Big Data & History by Dr. Joanna Radin, Associate Professor of History of Medicine and History at Yale University, for a detailed exploration of the issues inherint in that dataset and many like it, and Diabetes — and Privacy — Meet ‘Big Data’ for a summary on the Duke Research Blog by Maya Iskandarani. If you recognize issues with any TidyTuesday dataset, we greatly appreciate an issue or pull request letting us know!)\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-11')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 45)\n\nwho_tb_data &lt;- tuesdata$who_tb_data\n\n# Option 2: Read directly from GitHub\n\nwho_tb_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-11-11')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nwho_tb_data = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-11-11')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nwho_tb_data = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nwho_tb_data = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry or territory name\n\n\ng_whoregion\ncharacter\nWHO region\n\n\niso_numeric\ninteger\nISO numeric country/territory code\n\n\niso2\ncharacter\nISO 2-character country/territory code. Note that Namibia’s code (“‘NA’”) includes single quotes to avoid being encoded as missing\n\n\niso3\ncharacter\nISO 3-character country/territory code\n\n\nyear\ninteger\nYear of observation\n\n\nc_cdr\ndouble\nCase detection rate (all forms) [also known as TB treatment coverage], percent\n\n\nc_newinc_100k\ndouble\nCase notification rate, which is the total of new and relapse cases and cases with unknown previous TB treatment history per 100 000 population (calculated)\n\n\ncfr\ndouble\nEstimated TB case fatality ratio\n\n\ne_inc_100k\ndouble\nEstimated incidence (all forms) per 100 000 population\n\n\ne_inc_num\ninteger\nEstimated number of incident cases (all forms)\n\n\ne_mort_100k\ndouble\nEstimated mortality of TB cases (all forms) per 100 000 population\n\n\ne_mort_exc_tbhiv_100k\ndouble\nEstimated mortality of TB cases (all forms, excluding HIV) per 100 000 population\n\n\ne_mort_exc_tbhiv_num\ninteger\nEstimated number of deaths from TB (all forms, excluding HIV)\n\n\ne_mort_num\ninteger\nEstimated number of deaths from TB (all forms)\n\n\ne_mort_tbhiv_100k\ndouble\nEstimated mortality of TB cases who are HIV-positive, per 100 000 population\n\n\ne_mort_tbhiv_num\ninteger\nEstimated number of deaths from TB in people who are HIV-positive\n\n\ne_pop_num\ninteger\nEstimated total population number\n\n\n\n\n\n\n\n# This data is a subset of WHO TB data via the getTBinR package (Sam Abbott)\n\n# Import libraries\nlibrary(tidyverse)\nlibrary(devtools)\n\n# Install getTBinR package\n#devtools::install_github(\"seabbs/getTBinR\")\nlibrary(getTBinR)\n\n# Load WHO TB burden data\ntb_burden &lt;- get_tb_burden()\n\n# Create a vector of variable of interest\nvars_of_interest &lt;- c(\n  \"country\",\n  \"g_whoregion\",\n  \"iso_numeric\",\n  \"iso2\",\n  \"iso3\",\n  \"year\",\n  \"c_cdr\",\n  \"c_newinc_100k\",\n  \"cfr\",\n  \"e_inc_100k\",\n  \"e_inc_num\",\n  \"e_mort_100k\",\n  \"e_mort_exc_tbhiv_100k\",\n  \"e_mort_exc_tbhiv_num\",\n  \"e_mort_num\",\n  \"e_mort_tbhiv_100k\",\n  \"e_mort_tbhiv_num\",\n  \"e_pop_num\"\n)\n\n# Subset the dataset \nwho_tb_data &lt;- tb_burden %&gt;%\n  select(all_of(vars_of_interest))\n\n# No data cleaning needed"
  },
  {
    "objectID": "data/2025/2025-11-11/readme.html#the-data",
    "href": "data/2025/2025-11-11/readme.html#the-data",
    "title": "WHO TB Burden Data: Incidence, Mortality, and Population",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-11')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 45)\n\nwho_tb_data &lt;- tuesdata$who_tb_data\n\n# Option 2: Read directly from GitHub\n\nwho_tb_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-11-11')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nwho_tb_data = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-11-11')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nwho_tb_data = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nwho_tb_data = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-11/who_tb_data.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-11-11/readme.html#how-to-participate",
    "href": "data/2025/2025-11-11/readme.html#how-to-participate",
    "title": "WHO TB Burden Data: Incidence, Mortality, and Population",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-11-11/readme.html#data-dictionary",
    "href": "data/2025/2025-11-11/readme.html#data-dictionary",
    "title": "WHO TB Burden Data: Incidence, Mortality, and Population",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ncountry\ncharacter\nCountry or territory name\n\n\ng_whoregion\ncharacter\nWHO region\n\n\niso_numeric\ninteger\nISO numeric country/territory code\n\n\niso2\ncharacter\nISO 2-character country/territory code. Note that Namibia’s code (“‘NA’”) includes single quotes to avoid being encoded as missing\n\n\niso3\ncharacter\nISO 3-character country/territory code\n\n\nyear\ninteger\nYear of observation\n\n\nc_cdr\ndouble\nCase detection rate (all forms) [also known as TB treatment coverage], percent\n\n\nc_newinc_100k\ndouble\nCase notification rate, which is the total of new and relapse cases and cases with unknown previous TB treatment history per 100 000 population (calculated)\n\n\ncfr\ndouble\nEstimated TB case fatality ratio\n\n\ne_inc_100k\ndouble\nEstimated incidence (all forms) per 100 000 population\n\n\ne_inc_num\ninteger\nEstimated number of incident cases (all forms)\n\n\ne_mort_100k\ndouble\nEstimated mortality of TB cases (all forms) per 100 000 population\n\n\ne_mort_exc_tbhiv_100k\ndouble\nEstimated mortality of TB cases (all forms, excluding HIV) per 100 000 population\n\n\ne_mort_exc_tbhiv_num\ninteger\nEstimated number of deaths from TB (all forms, excluding HIV)\n\n\ne_mort_num\ninteger\nEstimated number of deaths from TB (all forms)\n\n\ne_mort_tbhiv_100k\ndouble\nEstimated mortality of TB cases who are HIV-positive, per 100 000 population\n\n\ne_mort_tbhiv_num\ninteger\nEstimated number of deaths from TB in people who are HIV-positive\n\n\ne_pop_num\ninteger\nEstimated total population number"
  },
  {
    "objectID": "data/2025/2025-11-11/readme.html#cleaning-script",
    "href": "data/2025/2025-11-11/readme.html#cleaning-script",
    "title": "WHO TB Burden Data: Incidence, Mortality, and Population",
    "section": "",
    "text": "# This data is a subset of WHO TB data via the getTBinR package (Sam Abbott)\n\n# Import libraries\nlibrary(tidyverse)\nlibrary(devtools)\n\n# Install getTBinR package\n#devtools::install_github(\"seabbs/getTBinR\")\nlibrary(getTBinR)\n\n# Load WHO TB burden data\ntb_burden &lt;- get_tb_burden()\n\n# Create a vector of variable of interest\nvars_of_interest &lt;- c(\n  \"country\",\n  \"g_whoregion\",\n  \"iso_numeric\",\n  \"iso2\",\n  \"iso3\",\n  \"year\",\n  \"c_cdr\",\n  \"c_newinc_100k\",\n  \"cfr\",\n  \"e_inc_100k\",\n  \"e_inc_num\",\n  \"e_mort_100k\",\n  \"e_mort_exc_tbhiv_100k\",\n  \"e_mort_exc_tbhiv_num\",\n  \"e_mort_num\",\n  \"e_mort_tbhiv_100k\",\n  \"e_mort_tbhiv_num\",\n  \"e_pop_num\"\n)\n\n# Subset the dataset \nwho_tb_data &lt;- tb_burden %&gt;%\n  select(all_of(vars_of_interest))\n\n# No data cleaning needed"
  },
  {
    "objectID": "data/2025/2025-11-25/readme.html",
    "href": "data/2025/2025-11-25/readme.html",
    "title": "Statistical Performance Indicators",
    "section": "",
    "text": "The World Bank has developed Statistical Performance Indicators (SPI) to monitor the statistical performance of countries. The SPI focuses on five key dimensions of a country’s statistical performance: (i) data use, (ii) data services, (iii) data products, (iv) data sources, and (v) data infrastructure. This set of countries covers 99 percent of the world population. The data extend from 2016-2023, with some indicators going back to 2004.\n\nThe purpose of the SPI is to help countries assess and improve the performance of their statistical systems.\n\nIn relation to these indicators, it should be noted that:\n\nSmall differences between countries should not be highlighted since they can reflect imprecision arising from the currently available indicators rather than meaningful differences in performance.\n\n\nHow has the statistical performance of a country changed over time?\nIs statistical performance related to a country’s income level or population?\nWhich pillar do countries score lowest in?\n\nThank you to Nicola Rennie for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 47)\n\nspi_indicators &lt;- tuesdata$spi_indicators\n\n# Option 2: Read directly from GitHub\n\nspi_indicators &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-11-25')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nspi_indicators = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-11-25')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nspi_indicators = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nspi_indicators = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\niso3c\ncharacter\nISO3 country code.\n\n\ncountry\ncharacter\nCountry name.\n\n\nregion\ncharacter\nRegion name.\n\n\nincome\ncharacter\nIncome level of country.\n\n\nyear\ninteger\nYear.\n\n\npopulation\ndouble\nPopulation of the country.\n\n\noverall_score\ndouble\nOverall statistical performance score.\n\n\ndata_use_score\ndouble\nScore relating to Pillar 1 - Data use.\n\n\ndata_services_score\ndouble\nScore relating to Pillar 2 - Data services.\n\n\ndata_products_score\ndouble\nScore relating to Pillar 3 - Data products.\n\n\ndata_sources_score\ndouble\nScore relating to Pillar 4 - Data sources.\n\n\ndata_infrastructure_score\ndouble\nScore relating to Pillar 5 - Data infrastructure.\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\nraw_data &lt;- read_csv(\"https://datacatalogfiles.worldbank.org/ddh-published/0037996/8/DR0046108/SPI_index.csv\")\n\nspi_indicators &lt;- raw_data |&gt;\n  select(\n    iso3c, country, region, income, date, population,\n    SPI.INDEX, starts_with(\"SPI.INDEX.\")\n  ) |&gt;\n  rename(\n    year = date,\n    overall_score = SPI.INDEX,\n    data_use_score = SPI.INDEX.PIL1,\n    data_services_score = SPI.INDEX.PIL2,\n    data_products_score = SPI.INDEX.PIL3,\n    data_sources_score = SPI.INDEX.PIL4,\n    data_infrastructure_score = SPI.INDEX.PIL5\n  )"
  },
  {
    "objectID": "data/2025/2025-11-25/readme.html#the-data",
    "href": "data/2025/2025-11-25/readme.html#the-data",
    "title": "Statistical Performance Indicators",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-11-25')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 47)\n\nspi_indicators &lt;- tuesdata$spi_indicators\n\n# Option 2: Read directly from GitHub\n\nspi_indicators &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-11-25')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nspi_indicators = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-11-25')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nspi_indicators = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nspi_indicators = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-25/spi_indicators.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-11-25/readme.html#how-to-participate",
    "href": "data/2025/2025-11-25/readme.html#how-to-participate",
    "title": "Statistical Performance Indicators",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-11-25/readme.html#data-dictionary",
    "href": "data/2025/2025-11-25/readme.html#data-dictionary",
    "title": "Statistical Performance Indicators",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\niso3c\ncharacter\nISO3 country code.\n\n\ncountry\ncharacter\nCountry name.\n\n\nregion\ncharacter\nRegion name.\n\n\nincome\ncharacter\nIncome level of country.\n\n\nyear\ninteger\nYear.\n\n\npopulation\ndouble\nPopulation of the country.\n\n\noverall_score\ndouble\nOverall statistical performance score.\n\n\ndata_use_score\ndouble\nScore relating to Pillar 1 - Data use.\n\n\ndata_services_score\ndouble\nScore relating to Pillar 2 - Data services.\n\n\ndata_products_score\ndouble\nScore relating to Pillar 3 - Data products.\n\n\ndata_sources_score\ndouble\nScore relating to Pillar 4 - Data sources.\n\n\ndata_infrastructure_score\ndouble\nScore relating to Pillar 5 - Data infrastructure."
  },
  {
    "objectID": "data/2025/2025-11-25/readme.html#cleaning-script",
    "href": "data/2025/2025-11-25/readme.html#cleaning-script",
    "title": "Statistical Performance Indicators",
    "section": "",
    "text": "library(tidyverse)\n\nraw_data &lt;- read_csv(\"https://datacatalogfiles.worldbank.org/ddh-published/0037996/8/DR0046108/SPI_index.csv\")\n\nspi_indicators &lt;- raw_data |&gt;\n  select(\n    iso3c, country, region, income, date, population,\n    SPI.INDEX, starts_with(\"SPI.INDEX.\")\n  ) |&gt;\n  rename(\n    year = date,\n    overall_score = SPI.INDEX,\n    data_use_score = SPI.INDEX.PIL1,\n    data_services_score = SPI.INDEX.PIL2,\n    data_products_score = SPI.INDEX.PIL3,\n    data_sources_score = SPI.INDEX.PIL4,\n    data_infrastructure_score = SPI.INDEX.PIL5\n  )"
  },
  {
    "objectID": "data/2025/2025-12-02/readme.html",
    "href": "data/2025/2025-12-02/readme.html",
    "title": "Can an exploding snowman predict the summer season?",
    "section": "",
    "text": "This week we’re exploring the weather prediction of Zurich’s infamous exploding snowman!\n\nThe Boeoegg is a snowman effigy made of cotton wool and stuffed with fireworks, created every year for Zurich’s “Sechselaeuten” spring festival. The saying goes that the quicker the Boeoeg’s head explodes, the finer the summer will be.\n\n\nCheck the burn duration of our snowman against the average summer temperature. Does folk science stand its ground against hard science?\nCan you find a number of successive years so that our snowman’s predictions seem more accurate?\nDoes our snowman’s forecasting ability improve if you choose climate variables other than temperature?\nWhat happened in the years for which there was no duration recorded? You can check the Wikipedia entry for “Sechselaeuten” for some funny anecdotes!\n\nThank you to Matt for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-02')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 48)\n\nsechselaeuten &lt;- tuesdata$sechselaeuten\n\n# Option 2: Read directly from GitHub\n\nsechselaeuten &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-02')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nsechselaeuten = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-02')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nsechselaeuten = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nsechselaeuten = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear of Sechselauten festival.\n\n\nduration\ndouble\nTime elapsed from ignition of Boeoeg effigy until explosion, in minutes.\n\n\ntre200m0\ndouble\nAverage air temperature 2 m above ground in degrees Celsius (monthly mean).\n\n\ntre200mn\ndouble\nMinimum air temperature 2 m above ground in degrees Celsius (absolute monthly minimum).\n\n\ntre200mx\ndouble\nMaximum air temperature 2 m above ground in degrees Celsius (absolute monthly maximum).\n\n\nsre000m0\ndouble\nTotal sunshine duration in hours (monthly total).\n\n\nsremaxmv\ndouble\nTotal sunshine duration as a percentage of the possible maximum.\n\n\nrre150m0\ndouble\nTotal precipitation in mm (monthly total).\n\n\nrecord\nlogical\nYears with average summer temperature above 19 degrees Celsius.\n\n\n\n\n\n\n\nlibary(tidyverse)\n\n## burn duration ----\n# https://github.com/philshem/Sechselaeuten-data\nburn_duration &lt;- readr::read_csv(\n  file = \"https://raw.githubusercontent.com/philshem/Sechselaeuten-data/refs/heads/master/boeoegg_burn_duration.csv\"\n) |&gt;\n  dplyr::mutate(duration = round(burn_duration_seconds / 60, digits = 2)) |&gt;\n  dplyr::select(year, duration)\n\n## variable selection ----\nvariable_selection &lt;- c(\n  \"tre200m0\",\n  \"tre200mn\",\n  \"tre200mx\",\n  \"sre000m0\",\n  \"sremaxmv\",\n  \"rre150m0\"\n)\n\n## climate data ----\nclimate_data &lt;- readr::read_delim(\n  file = \"https://data.geo.admin.ch/ch.meteoschweiz.ogd-smn/sma/ogd-smn_sma_m.csv\",\n  delim = \";\"\n) |&gt;\n  dplyr::select(\n    date = reference_timestamp,\n    dplyr::any_of(variable_selection)\n  ) |&gt;\n  dplyr::mutate(\n    date = lubridate::dmy_hm(date),\n    year = lubridate::year(date),\n    month = lubridate::month(date)\n  ) |&gt;\n  dplyr::filter(month %in% 6:8) |&gt;\n  dplyr::group_by(year) |&gt;\n  dplyr::summarise(dplyr::across(.cols = -c(date, month), .fns = \\(x) {\n    mean(x, na.rm = TRUE)\n  })) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::mutate(sre000m0 = sre000m0 / 60) |&gt;\n  dplyr::mutate(dplyr::across(.cols = -c(year), .fns = \\(x) {\n    round(x, digits = 2)\n  })) |&gt;\n  dplyr::mutate(dplyr::across(.cols = -c(year), .fns = \\(x) {\n    ifelse(is.nan(x), NA, x)\n  }))\n\n## combine datasets ----\nsechselaeuten &lt;- dplyr::left_join(\n  x = burn_duration,\n  y = climate_data,\n  by = dplyr::join_by(year)\n) |&gt;\n  dplyr::mutate(record = ifelse(tre200m0 &gt;= 19, TRUE, FALSE))"
  },
  {
    "objectID": "data/2025/2025-12-02/readme.html#the-data",
    "href": "data/2025/2025-12-02/readme.html#the-data",
    "title": "Can an exploding snowman predict the summer season?",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-02')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 48)\n\nsechselaeuten &lt;- tuesdata$sechselaeuten\n\n# Option 2: Read directly from GitHub\n\nsechselaeuten &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-02')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nsechselaeuten = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-02')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nsechselaeuten = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nsechselaeuten = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-02/sechselaeuten.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-12-02/readme.html#how-to-participate",
    "href": "data/2025/2025-12-02/readme.html#how-to-participate",
    "title": "Can an exploding snowman predict the summer season?",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-12-02/readme.html#data-dictionary",
    "href": "data/2025/2025-12-02/readme.html#data-dictionary",
    "title": "Can an exploding snowman predict the summer season?",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nyear\ndouble\nYear of Sechselauten festival.\n\n\nduration\ndouble\nTime elapsed from ignition of Boeoeg effigy until explosion, in minutes.\n\n\ntre200m0\ndouble\nAverage air temperature 2 m above ground in degrees Celsius (monthly mean).\n\n\ntre200mn\ndouble\nMinimum air temperature 2 m above ground in degrees Celsius (absolute monthly minimum).\n\n\ntre200mx\ndouble\nMaximum air temperature 2 m above ground in degrees Celsius (absolute monthly maximum).\n\n\nsre000m0\ndouble\nTotal sunshine duration in hours (monthly total).\n\n\nsremaxmv\ndouble\nTotal sunshine duration as a percentage of the possible maximum.\n\n\nrre150m0\ndouble\nTotal precipitation in mm (monthly total).\n\n\nrecord\nlogical\nYears with average summer temperature above 19 degrees Celsius."
  },
  {
    "objectID": "data/2025/2025-12-02/readme.html#cleaning-script",
    "href": "data/2025/2025-12-02/readme.html#cleaning-script",
    "title": "Can an exploding snowman predict the summer season?",
    "section": "",
    "text": "libary(tidyverse)\n\n## burn duration ----\n# https://github.com/philshem/Sechselaeuten-data\nburn_duration &lt;- readr::read_csv(\n  file = \"https://raw.githubusercontent.com/philshem/Sechselaeuten-data/refs/heads/master/boeoegg_burn_duration.csv\"\n) |&gt;\n  dplyr::mutate(duration = round(burn_duration_seconds / 60, digits = 2)) |&gt;\n  dplyr::select(year, duration)\n\n## variable selection ----\nvariable_selection &lt;- c(\n  \"tre200m0\",\n  \"tre200mn\",\n  \"tre200mx\",\n  \"sre000m0\",\n  \"sremaxmv\",\n  \"rre150m0\"\n)\n\n## climate data ----\nclimate_data &lt;- readr::read_delim(\n  file = \"https://data.geo.admin.ch/ch.meteoschweiz.ogd-smn/sma/ogd-smn_sma_m.csv\",\n  delim = \";\"\n) |&gt;\n  dplyr::select(\n    date = reference_timestamp,\n    dplyr::any_of(variable_selection)\n  ) |&gt;\n  dplyr::mutate(\n    date = lubridate::dmy_hm(date),\n    year = lubridate::year(date),\n    month = lubridate::month(date)\n  ) |&gt;\n  dplyr::filter(month %in% 6:8) |&gt;\n  dplyr::group_by(year) |&gt;\n  dplyr::summarise(dplyr::across(.cols = -c(date, month), .fns = \\(x) {\n    mean(x, na.rm = TRUE)\n  })) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::mutate(sre000m0 = sre000m0 / 60) |&gt;\n  dplyr::mutate(dplyr::across(.cols = -c(year), .fns = \\(x) {\n    round(x, digits = 2)\n  })) |&gt;\n  dplyr::mutate(dplyr::across(.cols = -c(year), .fns = \\(x) {\n    ifelse(is.nan(x), NA, x)\n  }))\n\n## combine datasets ----\nsechselaeuten &lt;- dplyr::left_join(\n  x = burn_duration,\n  y = climate_data,\n  by = dplyr::join_by(year)\n) |&gt;\n  dplyr::mutate(record = ifelse(tre200m0 &gt;= 19, TRUE, FALSE))"
  },
  {
    "objectID": "data/2025/2025-12-09/intro.html",
    "href": "data/2025/2025-12-09/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "This week we are exploring data about cars in Qatar!\nOne of the most common example datasets in R is mtcars, which contains data on a bunch of cars from 1974 (!). Some of the car companies in there don’t even exist anymore, like Datsun. The mpg dataset that comes with {ggplot2} was designed to be an improvement on mtcars and includes vehicles from 1999 and 2008. However, both mpg and mtcars are highly US-centric—most people in the world don’t think in gallons and miles and feet and inches—and neither dataset includes details about electric cars, which are increasingly common today.\nQatar Cars (also available as the {qatarcars} R package) provides a more internationally focused, modern-cars-based demonstration dataset. It mirrors many of the columns in mtcars, but uses (1) non-US-centric makes and models, (2) 2025 prices, and (3) metric measurements, making it more appropriate for use as an example dataset outside the United States.\nPaul Musgrave and students in his international politics course at Georgetown University in Qatar collected this data in early 2025 with the goal of creating a new toy dataset that does not suffer from “U.S. defaultism”:\nThe price column is stored as Qatari Riyals (QAR). At the time of data collection in January 2025, the exchange rates between QAR and US Dollars and Euros were:\nThere are many possible questions to explore!"
  },
  {
    "objectID": "data/2025/2025-12-09/intro.html#footnotes",
    "href": "data/2025/2025-12-09/intro.html#footnotes",
    "title": "TidyTuesday",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPaul Musgrave, “Defaulting to Inclusion: Producing Sample Datasets for the Global Data Science Classroom,” Journal of Political Science Education, 2025, 1–11, https://doi.org/10.1080/15512169.2025.2572320.↩︎"
  },
  {
    "objectID": "data/2025/2025-12-09/readme.html",
    "href": "data/2025/2025-12-09/readme.html",
    "title": "Cars in Qatar",
    "section": "",
    "text": "This week we are exploring data about cars in Qatar!\nOne of the most common example datasets in R is mtcars, which contains data on a bunch of cars from 1974 (!). Some of the car companies in there don’t even exist anymore, like Datsun. The mpg dataset that comes with {ggplot2} was designed to be an improvement on mtcars and includes vehicles from 1999 and 2008. However, both mpg and mtcars are highly US-centric—most people in the world don’t think in gallons and miles and feet and inches—and neither dataset includes details about electric cars, which are increasingly common today.\nQatar Cars (also available as the {qatarcars} R package) provides a more internationally focused, modern-cars-based demonstration dataset. It mirrors many of the columns in mtcars, but uses (1) non-US-centric makes and models, (2) 2025 prices, and (3) metric measurements, making it more appropriate for use as an example dataset outside the United States.\nPaul Musgrave and students in his international politics course at Georgetown University in Qatar collected this data in early 2025 with the goal of creating a new toy dataset that does not suffer from “U.S. defaultism”:\n\n“U.S. defaultism”—the assumption that American contexts, units, and perspectives are universal—manifests in many ways in political science. In this article, I describe how toy datasets commonly employed in quantitative methods courses exemplify this problem. Using customary units, for instance, is unsuitable for an internationalized higher education system. To address these limitations, I introduce the Qatar Cars dataset, a freely available alternative toy dataset that uses International System (SI) units, reflects current global automotive market trends (such as the rise of Chinese manufacturers and electric vehicles), and avoids ethnocentric classifications such as labeling the non-U.S. world “foreign.” Created through collaborative data collection with students, the Qatar Cars dataset maintains the pedagogical advantages of earlier datasets, improves statistical instruction by removing barriers for international audiences, and provides opportunities to discuss data-generating processes and research ethics.1\n\nThe price column is stored as Qatari Riyals (QAR). At the time of data collection in January 2025, the exchange rates between QAR and US Dollars and Euros were:\n\n1 USD = 3.64 QAR\n1 EUR = 4.15 QAR\n\n\nThere are many possible questions to explore!\n\nWhat’s the distribution of price? (there are some really expensive cars here!)\nWhat’s the relationship between (logged) price and performance?\nAre there patterns across cars from different countries? Do some countries make more expensive cars? More electic cars?\nWhat’s the relationship between car dimensions and seating or trunk volume?\n\nThank you to Andrew Heiss, Georgia State University for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 49)\n\nqatarcars &lt;- tuesdata$qatarcars\n\n# Option 2: Read directly from GitHub\n\nqatarcars &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-09/qatarcars.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-09')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nqatarcars = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-09/qatarcars.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-09')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nqatarcars = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-09/qatarcars.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nqatarcars = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-09/qatarcars.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\norigin\ncharacter\nThe country associated with the car brand.\n\n\nmake\ncharacter\nThe brand of the car, such as Toyota or Land Rover.\n\n\nmodel\ncharacter\nThe specific type of car, such as Land Cruiser or Defender.\n\n\nlength\ndouble\nLength of the car (in meters).\n\n\nwidth\ndouble\nWidth of the car (in meters).\n\n\nheight\ndouble\nHeight of the car (in meters).\n\n\nseating\ndouble\nNumber of seats in the car.\n\n\ntrunk\ndouble\nCapacity or volume of the trunk (in liters).\n\n\neconomy\ndouble\nFuel economy of the car (in liters per 100 km).\n\n\nhorsepower\ndouble\nCar horsepower.\n\n\nprice\ndouble\nPrice of the car in 2025 Qatari riyals.\n\n\nmass\ndouble\nMass of the car (in kg).\n\n\nperformance\ndouble\nTime to accelerate from 0 to 100 km/h (in seconds).\n\n\ntype\ncharacter\nThe type of the car, such as coupe, sedan, or SUV.\n\n\nenginetype\ncharacter\nThe type of engine: electric, hybrid, or petrol.\n\n\n\n\n\n\n\n# Clean data provided by {qatarcars}. No cleaning was necessary.\nqatarcars &lt;- readr::read_csv(\"https://raw.githubusercontent.com/profmusgrave/qatarcars/refs/heads/main/inst/extdata/qatarcars.csv\")\n\n# Or alternatively, using the {qatarcars} package:\n#\n# install.packages(\"qatarcars\")\n# library(qatarcars)\n# data(qatarcars, package = \"qatarcars\")"
  },
  {
    "objectID": "data/2025/2025-12-09/readme.html#the-data",
    "href": "data/2025/2025-12-09/readme.html#the-data",
    "title": "Cars in Qatar",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-09')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 49)\n\nqatarcars &lt;- tuesdata$qatarcars\n\n# Option 2: Read directly from GitHub\n\nqatarcars &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-09/qatarcars.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-09')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nqatarcars = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-09/qatarcars.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-09')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nqatarcars = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-09/qatarcars.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nqatarcars = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-09/qatarcars.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-12-09/readme.html#how-to-participate",
    "href": "data/2025/2025-12-09/readme.html#how-to-participate",
    "title": "Cars in Qatar",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-12-09/readme.html#data-dictionary",
    "href": "data/2025/2025-12-09/readme.html#data-dictionary",
    "title": "Cars in Qatar",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\norigin\ncharacter\nThe country associated with the car brand.\n\n\nmake\ncharacter\nThe brand of the car, such as Toyota or Land Rover.\n\n\nmodel\ncharacter\nThe specific type of car, such as Land Cruiser or Defender.\n\n\nlength\ndouble\nLength of the car (in meters).\n\n\nwidth\ndouble\nWidth of the car (in meters).\n\n\nheight\ndouble\nHeight of the car (in meters).\n\n\nseating\ndouble\nNumber of seats in the car.\n\n\ntrunk\ndouble\nCapacity or volume of the trunk (in liters).\n\n\neconomy\ndouble\nFuel economy of the car (in liters per 100 km).\n\n\nhorsepower\ndouble\nCar horsepower.\n\n\nprice\ndouble\nPrice of the car in 2025 Qatari riyals.\n\n\nmass\ndouble\nMass of the car (in kg).\n\n\nperformance\ndouble\nTime to accelerate from 0 to 100 km/h (in seconds).\n\n\ntype\ncharacter\nThe type of the car, such as coupe, sedan, or SUV.\n\n\nenginetype\ncharacter\nThe type of engine: electric, hybrid, or petrol."
  },
  {
    "objectID": "data/2025/2025-12-09/readme.html#cleaning-script",
    "href": "data/2025/2025-12-09/readme.html#cleaning-script",
    "title": "Cars in Qatar",
    "section": "",
    "text": "# Clean data provided by {qatarcars}. No cleaning was necessary.\nqatarcars &lt;- readr::read_csv(\"https://raw.githubusercontent.com/profmusgrave/qatarcars/refs/heads/main/inst/extdata/qatarcars.csv\")\n\n# Or alternatively, using the {qatarcars} package:\n#\n# install.packages(\"qatarcars\")\n# library(qatarcars)\n# data(qatarcars, package = \"qatarcars\")"
  },
  {
    "objectID": "data/2025/2025-12-09/readme.html#footnotes",
    "href": "data/2025/2025-12-09/readme.html#footnotes",
    "title": "Cars in Qatar",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPaul Musgrave, “Defaulting to Inclusion: Producing Sample Datasets for the Global Data Science Classroom,” Journal of Political Science Education, 2025, 1–11, https://doi.org/10.1080/15512169.2025.2572320.↩︎"
  },
  {
    "objectID": "data/2025/2025-12-16/readme.html",
    "href": "data/2025/2025-12-16/readme.html",
    "title": "Roundabouts across the world",
    "section": "",
    "text": "This week we are exploring data from the {roundabouts} package by Emil Hvitfeldt. The roundabouts package provides an R friendly way to access the roundabouts database which is compiled by Kittelson & Associates and contains information about the location, configuration, and construction of roundabout intersections around the world.\n\n“It started with an inventory of U.S. roundabouts that identified 150 sites,” Lee says. “One thing led to another, and now we’re at over 20,000 records in the database.”\n\n\nHow has roundabout construction evolved over time? Are certain regions adopting them faster than others?\nWhat types of intersections are most commonly converted to roundabouts?\nWhere are the roundabouts with the most unusual configurations (highest number of approaches/driveways)?\n\nThank you to Jen Richmond for curating this week’s dataset.\nNote: After this dataset was posted, Georgios Karamanis discovered that the latitude and longitude columns were reversed in the cleaning script. If you re-scrape the data, change the order in the tidyr::separate() function accordingly as noted in the cleaning script below!\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 50)\n\nroundabouts_clean &lt;- tuesdata$roundabouts_clean\n\n# Option 2: Read directly from GitHub\n\nroundabouts_clean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-16/roundabouts_clean.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-16')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nroundabouts_clean = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-16/roundabouts_clean.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-16')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nroundabouts_clean = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-16/roundabouts_clean.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nroundabouts_clean = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-16/roundabouts_clean.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nname\ncharacter\nRoundabout name\n\n\naddress\ncharacter\nRoundabout address\n\n\ntown_city\ncharacter\nTown/City\n\n\ncounty_area\ncharacter\nCounty/Area\n\n\nstate_region\ncharacter\nState/Region\n\n\ncountry\ncharacter\nCountry\n\n\nlat\ndouble\nLatitude\n\n\nlong\ndouble\nLongitude\n\n\ntype\ncharacter\nType, one of “roundabout”, “traffic calming circle”, “signalized roundabout/circle”, “rotary”, “other”, “unknown”\n\n\nstatus\ncharacter\nStatus, one of “existing”, “removed”, “unknown”\n\n\nyear_completed\ninteger\nYear construction completed\n\n\napproaches\ninteger\nNumber of approaches\n\n\ndriveways\ninteger\nNumber of driveways\n\n\nlane_type\ncharacter\nLane type\n\n\nfunctional_class\ncharacter\nFunctional Class\n\n\ncontrol_type\ncharacter\nControl Type\n\n\nother_control_type\ncharacter\nOther control type\n\n\nprevious_control_type\ncharacter\nPrevious control type\n\n\n\n\n\n\n\n# Data read in from roundabouts package, cleaning removes strange tags, separates country, town, county, and state columns. \n# Separates lat and long columns and fixes data types\n\nroundabouts &lt;- roundabouts::roundabouts\n\nroundabouts_clean &lt;- roundabouts |&gt;\n  dplyr::mutate(address = stringr::str_remove(address, stringr::fixed(\"&lt;![CDATA[\"))) |&gt;  # remove strange tags from address field\n  dplyr::mutate(address = stringr::str_remove(address, stringr::fixed(\"]]&gt;\"))) |&gt;\n  dplyr::mutate(country = stringr::str_extract(address, \"\\\\([^)]+\\\\)$\"), # extract string within last bracket\n                country = stringr::str_remove_all(country, \"[\\\\(\\\\)]\"),  # remove brackets\n                address2 = stringr::str_remove(address, \"\\\\s*\\\\([^)]+\\\\)$\")) |&gt;  # remove last () from original address\n  tidyr::separate(address2, into = c(\"town_city\", \"county_area\", \"state_region\"), sep = \",\") |&gt; # separate address by comma\n  # Note: It was discovered after this dataset was shared that latitude and \n  # longitude were reversed here. If you re-scrape the data, change this to `c(\"long\", \"lat\")`.\n  tidyr::separate(coordinates, into = c(\"lat\", \"long\"), sep = \",\") |&gt; # separate latitude longitude by comma\n  dplyr::select(name, address, town_city, county_area, state_region, country, lat, long, everything()) |&gt; # reorder variables\n  dplyr::mutate(lat = as.numeric(lat), long = as.numeric(long), # fix data types\n                year_completed = as.integer(year_completed), \n                approaches = as.integer( approaches), \n                driveways = as.integer(driveways))"
  },
  {
    "objectID": "data/2025/2025-12-16/readme.html#the-data",
    "href": "data/2025/2025-12-16/readme.html#the-data",
    "title": "Roundabouts across the world",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-16')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 50)\n\nroundabouts_clean &lt;- tuesdata$roundabouts_clean\n\n# Option 2: Read directly from GitHub\n\nroundabouts_clean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-16/roundabouts_clean.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-16')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nroundabouts_clean = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-16/roundabouts_clean.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-16')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nroundabouts_clean = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-16/roundabouts_clean.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nroundabouts_clean = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-16/roundabouts_clean.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-12-16/readme.html#how-to-participate",
    "href": "data/2025/2025-12-16/readme.html#how-to-participate",
    "title": "Roundabouts across the world",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-12-16/readme.html#data-dictionary",
    "href": "data/2025/2025-12-16/readme.html#data-dictionary",
    "title": "Roundabouts across the world",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nname\ncharacter\nRoundabout name\n\n\naddress\ncharacter\nRoundabout address\n\n\ntown_city\ncharacter\nTown/City\n\n\ncounty_area\ncharacter\nCounty/Area\n\n\nstate_region\ncharacter\nState/Region\n\n\ncountry\ncharacter\nCountry\n\n\nlat\ndouble\nLatitude\n\n\nlong\ndouble\nLongitude\n\n\ntype\ncharacter\nType, one of “roundabout”, “traffic calming circle”, “signalized roundabout/circle”, “rotary”, “other”, “unknown”\n\n\nstatus\ncharacter\nStatus, one of “existing”, “removed”, “unknown”\n\n\nyear_completed\ninteger\nYear construction completed\n\n\napproaches\ninteger\nNumber of approaches\n\n\ndriveways\ninteger\nNumber of driveways\n\n\nlane_type\ncharacter\nLane type\n\n\nfunctional_class\ncharacter\nFunctional Class\n\n\ncontrol_type\ncharacter\nControl Type\n\n\nother_control_type\ncharacter\nOther control type\n\n\nprevious_control_type\ncharacter\nPrevious control type"
  },
  {
    "objectID": "data/2025/2025-12-16/readme.html#cleaning-script",
    "href": "data/2025/2025-12-16/readme.html#cleaning-script",
    "title": "Roundabouts across the world",
    "section": "",
    "text": "# Data read in from roundabouts package, cleaning removes strange tags, separates country, town, county, and state columns. \n# Separates lat and long columns and fixes data types\n\nroundabouts &lt;- roundabouts::roundabouts\n\nroundabouts_clean &lt;- roundabouts |&gt;\n  dplyr::mutate(address = stringr::str_remove(address, stringr::fixed(\"&lt;![CDATA[\"))) |&gt;  # remove strange tags from address field\n  dplyr::mutate(address = stringr::str_remove(address, stringr::fixed(\"]]&gt;\"))) |&gt;\n  dplyr::mutate(country = stringr::str_extract(address, \"\\\\([^)]+\\\\)$\"), # extract string within last bracket\n                country = stringr::str_remove_all(country, \"[\\\\(\\\\)]\"),  # remove brackets\n                address2 = stringr::str_remove(address, \"\\\\s*\\\\([^)]+\\\\)$\")) |&gt;  # remove last () from original address\n  tidyr::separate(address2, into = c(\"town_city\", \"county_area\", \"state_region\"), sep = \",\") |&gt; # separate address by comma\n  # Note: It was discovered after this dataset was shared that latitude and \n  # longitude were reversed here. If you re-scrape the data, change this to `c(\"long\", \"lat\")`.\n  tidyr::separate(coordinates, into = c(\"lat\", \"long\"), sep = \",\") |&gt; # separate latitude longitude by comma\n  dplyr::select(name, address, town_city, county_area, state_region, country, lat, long, everything()) |&gt; # reorder variables\n  dplyr::mutate(lat = as.numeric(lat), long = as.numeric(long), # fix data types\n                year_completed = as.integer(year_completed), \n                approaches = as.integer( approaches), \n                driveways = as.integer(driveways))"
  },
  {
    "objectID": "data/2025/2025-12-23/endangered_status.html",
    "href": "data/2025/2025-12-23/endangered_status.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "Variable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language\n\n\nstatus_code\ncharacter\nCode of the agglomerated endangerment status (1–6)\n\n\nstatus_label\ncharacter\nDescriptive label of endangerment category"
  },
  {
    "objectID": "data/2025/2025-12-23/intro.html",
    "href": "data/2025/2025-12-23/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "This week we’re exploring The Languages of the World, curated from Glottolog 5.2.1, an open-access database in linguistics, maintained by the Max Planck Institute for Evolutionary Anthropology.\n\nGlottolog is the most comprehensive language database in linguistics, and contains information (names, genealogy, geographical information, endangerment status, etc.) of over 8,000 languages of the world.\n\n\nWhich macroareas have the highest concentration of endangered languages?\nAre language isolates more likely to be endangered?\nWhich language families span the widest geographic range?\nWhat geographic patterns emerge when mapping endangered languages?"
  },
  {
    "objectID": "data/2025/2025-12-23/readme.html",
    "href": "data/2025/2025-12-23/readme.html",
    "title": "The Languages of the World",
    "section": "",
    "text": "This week we’re exploring The Languages of the World, curated from Glottolog 5.2.1, an open-access database in linguistics, maintained by the Max Planck Institute for Evolutionary Anthropology.\n\nGlottolog is the most comprehensive language database in linguistics, and contains information (names, genealogy, geographical information, endangerment status, etc.) of over 8,000 languages of the world.\n\n\nWhich macroareas have the highest concentration of endangered languages?\nAre language isolates more likely to be endangered?\nWhich language families span the widest geographic range?\nWhat geographic patterns emerge when mapping endangered languages?\n\nThank you to Darakhshan Nehal for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 51)\n\nendangered_status &lt;- tuesdata$endangered_status\nfamilies &lt;- tuesdata$families\nlanguages &lt;- tuesdata$languages\n\n# Option 2: Read directly from GitHub\n\nendangered_status &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/endangered_status.csv')\nfamilies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/families.csv')\nlanguages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/languages.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-23')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nendangered_status = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/endangered_status.csv')\nfamilies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/families.csv')\nlanguages = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/languages.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-23')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nendangered_status = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/endangered_status.csv\")\nfamilies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/families.csv\")\nlanguages = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/languages.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nendangered_status = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/endangered_status.csv\", DataFrame)\nfamilies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/families.csv\", DataFrame)\nlanguages = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/languages.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language\n\n\nstatus_code\ncharacter\nCode of the agglomerated endangerment status (1–6)\n\n\nstatus_label\ncharacter\nDescriptive label of endangerment category\n\n\n\n\n\n\n\n\n\nVariable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language family\n\n\nname\ncharacter\nLanguage family name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language\n\n\nname\ncharacter\nLanguage name\n\n\nmacroarea\ncharacter\nGeneral geographic area in which the language is found\n\n\nlatitude\ndouble\nLatitude of language location (as point)\n\n\nlongitude\ndouble\nLongitude of language location (as point)\n\n\niso639p3code\ncharacter\nISO 639-3 identifier of language (if available)\n\n\ncountries\ncharacter\nCountries in which language is used (separated by “;”)\n\n\nis_isolate\nlogical\nWhether language is an isolate (i.e. has no known relatives)\n\n\nfamily_id\ncharacter\nUnique identifier of family that the language is part of (if not isolate)\n\n\n\n\n\n\n\n# Imports\nlibrary(tidyverse)\n\n# Download raw data and filter to endangered status\nendangered_status &lt;- \n  readr::read_csv(\"https://raw.githubusercontent.com/glottolog/glottolog-cldf/refs/heads/master/cldf/values.csv\") |&gt; \n  dplyr::filter(Parameter_ID == \"aes\") |&gt; \n  dplyr::select(Language_ID, Value, Code_ID) |&gt; \n  dplyr::rename(id = Language_ID,\n                status_code = Value,\n                status_label = Code_ID) |&gt; \n  dplyr::mutate(status_label = stringr::str_replace(stringr::str_remove(status_label, \"^aes-\"), \"_\", \" \"))\n\n# Download language and family data\nfam_lgs &lt;- \n  readr::read_csv(\"https://raw.githubusercontent.com/glottolog/glottolog-cldf/refs/heads/master/cldf/languages.csv\")\n\n# Filter and clean language family data\nfamilies &lt;- \n  fam_lgs |&gt; \n  dplyr::filter(Level == \"family\") |&gt; \n  dplyr::select(ID, Name) |&gt; \n  dplyr::rename(Family = Name) |&gt; \n  dplyr::rename_with(stringr::str_to_lower, dplyr::everything())\n\n# Filter and clean language data\nlanguages &lt;- \n  fam_lgs |&gt; \n  dplyr::filter(Level == \"language\") |&gt; \n  dplyr::select(ID, Name, Macroarea, Latitude, Longitude, ISO639P3code, Countries, Is_Isolate, Family_ID) |&gt; \n  dplyr::rename_with(stringr::str_to_lower, dplyr::everything())"
  },
  {
    "objectID": "data/2025/2025-12-23/readme.html#the-data",
    "href": "data/2025/2025-12-23/readme.html#the-data",
    "title": "The Languages of the World",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-23')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 51)\n\nendangered_status &lt;- tuesdata$endangered_status\nfamilies &lt;- tuesdata$families\nlanguages &lt;- tuesdata$languages\n\n# Option 2: Read directly from GitHub\n\nendangered_status &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/endangered_status.csv')\nfamilies &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/families.csv')\nlanguages &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/languages.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2025-12-23')\n\n# Option 2: Read directly from GitHub and assign to an object\n\nendangered_status = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/endangered_status.csv')\nfamilies = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/families.csv')\nlanguages = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/languages.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2025-12-23')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\nendangered_status = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/endangered_status.csv\")\nfamilies = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/families.csv\")\nlanguages = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/languages.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\nendangered_status = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/endangered_status.csv\", DataFrame)\nfamilies = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/families.csv\", DataFrame)\nlanguages = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-12-23/languages.csv\", DataFrame)"
  },
  {
    "objectID": "data/2025/2025-12-23/readme.html#how-to-participate",
    "href": "data/2025/2025-12-23/readme.html#how-to-participate",
    "title": "The Languages of the World",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2025/2025-12-23/readme.html#data-dictionary",
    "href": "data/2025/2025-12-23/readme.html#data-dictionary",
    "title": "The Languages of the World",
    "section": "",
    "text": "Variable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language\n\n\nstatus_code\ncharacter\nCode of the agglomerated endangerment status (1–6)\n\n\nstatus_label\ncharacter\nDescriptive label of endangerment category\n\n\n\n\n\n\n\n\n\nVariable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language family\n\n\nname\ncharacter\nLanguage family name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nClass\nDescription\n\n\n\n\nid\ncharacter\nUnique identifier for language\n\n\nname\ncharacter\nLanguage name\n\n\nmacroarea\ncharacter\nGeneral geographic area in which the language is found\n\n\nlatitude\ndouble\nLatitude of language location (as point)\n\n\nlongitude\ndouble\nLongitude of language location (as point)\n\n\niso639p3code\ncharacter\nISO 639-3 identifier of language (if available)\n\n\ncountries\ncharacter\nCountries in which language is used (separated by “;”)\n\n\nis_isolate\nlogical\nWhether language is an isolate (i.e. has no known relatives)\n\n\nfamily_id\ncharacter\nUnique identifier of family that the language is part of (if not isolate)"
  },
  {
    "objectID": "data/2025/2025-12-23/readme.html#cleaning-script",
    "href": "data/2025/2025-12-23/readme.html#cleaning-script",
    "title": "The Languages of the World",
    "section": "",
    "text": "# Imports\nlibrary(tidyverse)\n\n# Download raw data and filter to endangered status\nendangered_status &lt;- \n  readr::read_csv(\"https://raw.githubusercontent.com/glottolog/glottolog-cldf/refs/heads/master/cldf/values.csv\") |&gt; \n  dplyr::filter(Parameter_ID == \"aes\") |&gt; \n  dplyr::select(Language_ID, Value, Code_ID) |&gt; \n  dplyr::rename(id = Language_ID,\n                status_code = Value,\n                status_label = Code_ID) |&gt; \n  dplyr::mutate(status_label = stringr::str_replace(stringr::str_remove(status_label, \"^aes-\"), \"_\", \" \"))\n\n# Download language and family data\nfam_lgs &lt;- \n  readr::read_csv(\"https://raw.githubusercontent.com/glottolog/glottolog-cldf/refs/heads/master/cldf/languages.csv\")\n\n# Filter and clean language family data\nfamilies &lt;- \n  fam_lgs |&gt; \n  dplyr::filter(Level == \"family\") |&gt; \n  dplyr::select(ID, Name) |&gt; \n  dplyr::rename(Family = Name) |&gt; \n  dplyr::rename_with(stringr::str_to_lower, dplyr::everything())\n\n# Filter and clean language data\nlanguages &lt;- \n  fam_lgs |&gt; \n  dplyr::filter(Level == \"language\") |&gt; \n  dplyr::select(ID, Name, Macroarea, Latitude, Longitude, ISO639P3code, Countries, Is_Isolate, Family_ID) |&gt; \n  dplyr::rename_with(stringr::str_to_lower, dplyr::everything())"
  },
  {
    "objectID": "data/2025/2025-12-30/christmas_novel_text.html",
    "href": "data/2025/2025-12-30/christmas_novel_text.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ngutenberg_id\ninteger\nNumeric ID, used to retrieve works from Project Gutenberg\n\n\ntext\ncharacter\nA line of text from the work (NA indicates an empty line)"
  },
  {
    "objectID": "data/2025/2025-12-30/intro.html",
    "href": "data/2025/2025-12-30/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "This week we’re exploring “Christmas” novels from Project Gutenberg via the {gutenbergr} R package! I originally curated this dataset to serve as an “ad” of sorts for a new maintainer of that package, but Jordan Bradford has already assumed that role. Thank you for taking over stewardship of the package, Jordan! He could still use help, so, if you enjoy working with text data and R, consider stepping up to help maintain this useful package!\nYou might find Text Mining with R helpful for analyzing this data.\n\nWhich is mentioned more often in these novels: “spirit” or “santa”?\nWhat is the overall sentiment of each novel?\nHow does the text sentiment change over the course of each novel?"
  },
  {
    "objectID": "data/2025/readme.html",
    "href": "data/2025/readme.html",
    "title": "2025 Data",
    "section": "",
    "text": "2025 Data\nArchive of datasets and articles from the 2025 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2025-01-07\nBring your own data from 2024!\nNA\nNA\n\n\n2\n2025-01-14\nposit::conf talks\nposit::conf attendee portal 2023, posit::conf attendee portal 2024\nposit::conf(2025) in-person registration is now open!\n\n\n3\n2025-01-21\nThe History of Himalayan Mountaineering Expeditions\nThe Himalayan Database\nThe Expedition Archives of Elizabeth Hawley\n\n\n4\n2025-01-28\nWater Insecurity\nUS Census Data from tidycensus\nMapping water insecurity in R with tidycensus\n\n\n5\n2025-02-04\nDonuts, Data, and D’oh - A Deep Dive into The Simpsons\nThe Simpsons Dataset\nThe Simpsons by the Data\n\n\n6\n2025-02-11\nCDC Datasets\nCDC datasets uploaded before January 28th, 2025\nTrump administration purges websites across federal health agencies\n\n\n7\n2025-02-18\nAgencies from the FBI Crime Data API\nFBI Crime Data API\nUniform Crime Reporting Program: Still Vital After 90 Years\n\n\n8\n2025-02-25\nAcademic Literature on Racial and Ethnic Disparities in Reproductive Medicine in the US\nRacial and ethnic disparities in reproductive medicine in the United States: a narrative review of contemporary high-quality evidence\nThis Art is HARD\n\n\n9\n2025-03-04\nLong Beach Animal Shelter\nCity of Long Beach Animal Care Services\nLong Beach Animal Care Services Hits Highest Adoption Rate Ever, Surpasses 2024 Strategic Plan Goal\n\n\n10\n2025-03-11\nPixar Films\npixarfilms R package\nPixar Film Ratings\n\n\n11\n2025-03-18\nPalm Trees\n{palmtrees} R package\nPalmTraits 1.0, a species-level functional trait database of palms worldwide\n\n\n12\n2025-03-25\nText Data From Amazon’s Annual Reports\nAmazon’s Annual Reports\nExploring The Use of TidyText and LLMs to Understand Amazon’s Annual Reports\n\n\n13\n2025-04-01\nPokemon\npokemon R package\nPokemon Data Visualization and Analysis with R\n\n\n14\n2025-04-08\nTimely and Effective Care by US State\nCenters for Medicare & Medicaid Services\nMapped: Emergency Room Visit Times by State\n\n\n15\n2025-04-15\nBase R Penguins\nThe R Datasets Package\nPreparing the Palmer Penguins Data for the datasets Package in R\n\n\n16\n2025-04-22\nFatal Car Crashes on 4/20\n420 (data-raw)\nThe Annual Cannabis Holiday and Fatal Traffic Crashes\n\n\n17\n2025-04-29\nuseR! 2025 program\nProgram for the useR! 2025 conference\nProgram for the useR! 2025 conference\n\n\n18\n2025-05-06\nNational Science Foundation Grant Terminations under the Trump Administration\nGrant Watch\nNational Science Foundation Terminates Hundreds of Active Research Awards\n\n\n19\n2025-05-13\nSeismic Events at Mount Vesuvius\nItalian Istituto Nazionale di Geofisica e Vulcanologia (INGV)\nSomma Vesuvio\n\n\n20\n2025-05-20\nWater Quality at Sydney Beaches\nBeachwatchNSW, Open-Meteo\nSydney beachgoers urged not to swim in waterways, amid pollution warnings and bull shark activity\n\n\n21\n2025-05-27\nDungeons and Dragons Monsters (2024)\nSystem Reference Document v5.2.1\nYou Can Now Publish Your Own Creations Using the New Core Rules\n\n\n22\n2025-06-03\nProject Gutenberg\nThe R gutenbergr package\n50 years of eBooks: 1971-2021\n\n\n23\n2025-06-10\nU.S. Judges and the historydata R package\nWeb site of the Federal Judicial Center, via the historydata R package\nWhat Does It Mean to Maintain a Package?\n\n\n24\n2025-06-17\nAPI Specs\nAPIs.guru\nWeb APIs with R: Introduction\n\n\n25\n2025-06-24\nMeasles cases across the world\nWorld Health Organisation Provisional monthly measles and rubella data\nMeasles cases reach 1,046 in US as infections confirmed in 30 states: CDC\n\n\n26\n2025-07-01\nWeekly US Gas Prices\nU.S. Gasoline and Diesel Retail Prices\nWeekly U.S. Gasoline and Diesel Retail Prices\n\n\n27\n2025-07-08\nThe xkcd Color Survey Results\nxkcd Color Survey SQLite database\nColor Survey Results\n\n\n28\n2025-07-15\nBritish Library Funding\nBL Funding Over Time\nBritish Library funding breakdown & trends\n\n\n29\n2025-07-22\nMTA Permanent Art Catalog\nMTA Permanent Art Catalog: Beginning 1980\nPermanent Art\n\n\n30\n2025-07-29\nWhat have we been watching on Netflix?\nNetflix Engagement Report 2023b, Netflix Engagement Report 2024a, Netflix Engagement Report 2024b, Netflix Engagement Report 2025a\nWhat We Watched the First Half of 2025\n\n\n31\n2025-08-05\nIncome Inequality Before and After Taxes\nOur World in Data\nIncome inequality before and after taxes: how much do countries redistribute income?\n\n\n32\n2025-08-12\nExtreme Weather Attribution Studies\nCarbon Brief\nMapped: How climate change affects extreme weather around the word.\n\n\n33\n2025-08-19\nScottish Munros\nDatabase of British and Irish hills\nThe Accuracy of The Munro Society Heighting Surveys\n\n\n34\n2025-08-26\nBillboard Hot 100 Number Ones\nBillboard Hot 100 Number Ones\nUncharted Territory from a Dataviz Perspective\n\n\n35\n2025-09-02\nAustralian Frogs\nFrogID dataset 6.0\nThe FrogID Dataset 6.0 – nearly one million frog records now published open-access and online!\n\n\n36\n2025-09-09\nHenley Passport Index Data\nHenley Passport Index Data\nUS slips again in passport power rankings\n\n\n37\n2025-09-16\nAllrecipes\nThe tastyR package\nI scraped 14K recipes, so you won’t have to.\n\n\n38\n2025-09-23\nFIDE Chess Player Ratings\nFIDE Player Database\nFIDE September 2025 rating list: Vincent Keymer debuts in top 10 Open\n\n\n39\n2025-09-30\nCrane Observations at Lake Hornborgasjön, Sweden (1994–2024)\nTranstatistik (‘crane statistics’), Naturum, Hornborgasjön, Västra Götaland County Administrative Board\nThe dancing cranes of Lake Hornborga\n\n\n40\n2025-10-07\nEuroLeague Basketball\nEuroleagueBasketball R package (curated from Wikipedia and EuroLeague records)\nEuroLeague on Wikipedia\n\n\n41\n2025-10-14\nWorld Food Day\nSuite of Food Security Indicators\nWorld Food Day (Wikipedia)\n\n\n42\n2025-10-21\nHistoric UK Meteorological & Climate Data\nHistorical monthly data for meteorological stations\nMet Office: Historic station data\n\n\n43\n2025-10-28\nSelected British Literary Prizes (1990-2022)\nPost45 Data Collective\nWhy we still need a women’s prize for fiction\n\n\n44\n2025-11-04\nLead concentration in Flint water samples in 2015\nUsing Flint, Michigan, lead data in introductory statistics\nThe Murky Tale of Flint’s Deceptive Water Data\n\n\n45\n2025-11-11\nWHO TB Burden Data: Incidence, Mortality, and Population\ngetTBinR package\ngetTBinR: Access and Summarise World Health Organization Tuberculosis Data\n\n\n46\n2025-11-18\nThe Complete Sherlock Holmes\nThe Complete Sherlock Holmes via {sherlock} R Package\nThe Complete Sherlock Holmes\n\n\n47\n2025-11-25\nStatistical Performance Indicators\nWorld Bank Statistical Performance Indicators\nMeasuring the Statistical Performance of Countries\n\n\n48\n2025-12-02\nCan an exploding snowman predict the summer season?\nOpenData for Zurich’s Sechselaeuten\nBoeoegg prediction\n\n\n49\n2025-12-09\nCars in Qatar\n{qatarcars}\nIntroducing the Qatar Cars Dataset\n\n\n50\n2025-12-16\nRoundabouts across the world\nroundabouts R package\nHow Many Roundabouts Are in the United States?\n\n\n51\n2025-12-23\nThe Languages of the World\nGlottolog_5.2\nGlottolog: A Free, Online, Comprehensive Bibliography of the World’s Languages\n\n\n52\n2025-12-30\nChristmas Novels\nProject Gutenberg\nNew (gutenbergr) Maintainer Wanted",
    "crumbs": [
      "Datasets",
      "2025"
    ]
  },
  {
    "objectID": "data/2026/2026-01-13/intro.html",
    "href": "data/2026/2026-01-13/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "This week we’re exploring data about popular languages spoken on the African continent. The dataset this week comes from the Languages of Africa page on Wikipedia.\n\nThe number of languages natively spoken in Africa is variously estimated (depending on the delineation of language vs. dialect) at between 1,250 and 2,100 and by some counts at over 3,000.\n\nThe dataset is rich with information on the number of languages spoken across the continent. Some of the questions that could be thought of include:\n\nWhich country in Africa has the largest number of spoken languages?\nWhich family of languages has the highest density of speakers?\nAre there any languages that cut across multiple countries?\n\nCan’t wait to see the kind of visualisations that can be created!"
  },
  {
    "objectID": "data/2026/2026-01-20/apod.html",
    "href": "data/2026/2026-01-20/apod.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ncopyright\ncharacter\nThe name of the copyright holder.\n\n\ndate\nDate\nDate of image.\n\n\nexplanation\ncharacter\nThe supplied text explanation of the image.\n\n\nmedia_type\ncharacter\nThe type of media (data) returned. May either be ‘image’ or ‘video’ depending on content.\n\n\ntitle\ncharacter\nThe title of the image.\n\n\nurl\ncharacter\nThe URL of the APOD image or video of the day.\n\n\nhdurl\ncharacter\nThe URL for any high-resolution image for that day. Will be omitted in the response IF it does not exist originally at APOD."
  },
  {
    "objectID": "data/2026/2026-01-20/readme.html",
    "href": "data/2026/2026-01-20/readme.html",
    "title": "Astronomy Picture of the Day (APOD) Archive",
    "section": "",
    "text": "This week we’re exploring the Astronomy Picture of the Day (APOD) archive. APOD is a popular NASA website featuring daily astronomy related images with a scientific explanation.\nEach day a different image or photograph of our universe is featured, along with a brief explanation. This APOD archive contains image information from the 2007 - 2025, pulled together into the {astropic} R package.\n\nWhat types of objects are most common in the archive?\nAre any images posted more than once?\n\nThank you to Erin Grand for curating this week’s dataset.\n\n\n# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2026-01-20')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 3)\n\napod &lt;- tuesdata$apod\n\n# Option 2: Read directly from GitHub\n\napod &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-20/apod.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2026-01-20')\n\n# Option 2: Read directly from GitHub and assign to an object\n\napod = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-20/apod.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2026-01-20')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\napod = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-20/apod.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\napod = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-20/apod.csv\", DataFrame)\n\n\n\n\nExplore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ncopyright\ncharacter\nThe name of the copyright holder.\n\n\ndate\nDate\nDate of image.\n\n\nexplanation\ncharacter\nThe supplied text explanation of the image.\n\n\nmedia_type\ncharacter\nThe type of media (data) returned. May either be ‘image’ or ‘video’ depending on content.\n\n\ntitle\ncharacter\nThe title of the image.\n\n\nurl\ncharacter\nThe URL of the APOD image or video of the day.\n\n\nhdurl\ncharacter\nThe URL for any high-resolution image for that day. Will be omitted in the response IF it does not exist originally at APOD.\n\n\n\n\n\n\n\nremotes::install_github(\"eringrand/astropic\")\n\n# Dataset inside the {{astropic}} R package on GitHub.\nlibrary(astropic)\nlibrary(dplyr)\ndata(\"hist_apod\")\n\n# Remove one column with constant values\napod &lt;- hist_apod |&gt; \n  select(-service_version)"
  },
  {
    "objectID": "data/2026/2026-01-20/readme.html#the-data",
    "href": "data/2026/2026-01-20/readme.html#the-data",
    "title": "Astronomy Picture of the Day (APOD) Archive",
    "section": "",
    "text": "# Using R\n# Option 1: tidytuesdayR R package \n## install.packages(\"tidytuesdayR\")\n\ntuesdata &lt;- tidytuesdayR::tt_load('2026-01-20')\n## OR\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 3)\n\napod &lt;- tuesdata$apod\n\n# Option 2: Read directly from GitHub\n\napod &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-20/apod.csv')\n# Using Python\n# Option 1: pydytuesday python library\n## pip install pydytuesday\n\nimport pydytuesday\n\n# Download files from the week, which you can then read in locally\npydytuesday.get_date('2026-01-20')\n\n# Option 2: Read directly from GitHub and assign to an object\n\napod = pandas.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-20/apod.csv')\n# Using Julia\n# Option 1: TidierTuesday.jl library\n## Pkg.add(url=\"https://github.com/TidierOrg/TidierTuesday.jl\")\n\nusing TidierTuesday\n\n# Download files from the week, which you can then read in locally\ndownload_dataset('2026-01-20')\n\n# Option 2: Read directly from GitHub and assign to an object with TidierFiles\n\napod = read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-20/apod.csv\")\n\n# Option 3: Read directly from Github and assign without Tidier dependencies\napod = CSV.read(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2026/2026-01-20/apod.csv\", DataFrame)"
  },
  {
    "objectID": "data/2026/2026-01-20/readme.html#how-to-participate",
    "href": "data/2026/2026-01-20/readme.html#how-to-participate",
    "title": "Astronomy Picture of the Day (APOD) Archive",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "data/2026/2026-01-20/readme.html#data-dictionary",
    "href": "data/2026/2026-01-20/readme.html#data-dictionary",
    "title": "Astronomy Picture of the Day (APOD) Archive",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\ncopyright\ncharacter\nThe name of the copyright holder.\n\n\ndate\nDate\nDate of image.\n\n\nexplanation\ncharacter\nThe supplied text explanation of the image.\n\n\nmedia_type\ncharacter\nThe type of media (data) returned. May either be ‘image’ or ‘video’ depending on content.\n\n\ntitle\ncharacter\nThe title of the image.\n\n\nurl\ncharacter\nThe URL of the APOD image or video of the day.\n\n\nhdurl\ncharacter\nThe URL for any high-resolution image for that day. Will be omitted in the response IF it does not exist originally at APOD."
  },
  {
    "objectID": "data/2026/2026-01-20/readme.html#cleaning-script",
    "href": "data/2026/2026-01-20/readme.html#cleaning-script",
    "title": "Astronomy Picture of the Day (APOD) Archive",
    "section": "",
    "text": "remotes::install_github(\"eringrand/astropic\")\n\n# Dataset inside the {{astropic}} R package on GitHub.\nlibrary(astropic)\nlibrary(dplyr)\ndata(\"hist_apod\")\n\n# Remove one column with constant values\napod &lt;- hist_apod |&gt; \n  select(-service_version)"
  },
  {
    "objectID": "data/2026/2026-01-27/intro.html",
    "href": "data/2026/2026-01-27/intro.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "This week we’re exploring Brazilian Companies, curated from Brazil’s open CNPJ (Cadastro Nacional da Pessoa Jurídica) records published by the Brazilian Ministry of Finance / Receita Federal on the national open-data portal (dados.gov.br).\n\nThe CNPJ open data is a large-scale public registry of Brazilian legal entities. For this dataset, the raw company records were cleaned and enriched with lookup tables (legal nature, owner qualification, and company size), then filtered to retain firms above a share-capital threshold so the analysis focuses on meaningful variation in capital stock.\n\n\nWhich legal nature categories concentrate the highest total and average capital stock?\nHow does company size relate to capital stock (and how skewed is it)?\nDo specific owner qualification groups dominate high-capital companies?\nWhat patterns emerge when comparing the top capital-stock tail across categories (legal nature, size, qualification)?"
  },
  {
    "objectID": "data/2026/2026-01-27/qualifications.html",
    "href": "data/2026/2026-01-27/qualifications.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nid\ninteger\nOwner qualification code (source registry code).\n\n\nowner_qualification\ncharacter\nOwner qualification label corresponding to id."
  },
  {
    "objectID": "data/2026/2026-01-27/size.html",
    "href": "data/2026/2026-01-27/size.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "variable\nclass\ndescription\n\n\n\n\nid\ninteger\nCompany size code (source registry code).\n\n\ncompany_size\ncharacter\nCompany size label corresponding to id (e.g., micro-enterprise, small-enterprise)."
  },
  {
    "objectID": "data/curated/template/instructions.html",
    "href": "data/curated/template/instructions.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "These instructions are for preparing a dataset using the R programming language. We hope to provide instructions for other programming languages eventually.\nIf you have not yet set up your computer for submitting a dataset, please see the full instructions at https://github.com/rfordatascience/tidytuesday/blob/main/pr_instructions.md.\n\ncleaning.R: Modify the cleaning.R file to get and clean the data.\n\nWrite the code to download and clean the data in cleaning.R.\nIf you’re getting the data from a github repo, remember to use the ‘raw’ version of the URL.\nThis script should result in one or more data.frames, with descriptive variable names (eg players and teams, not df1 and df2).\n\nsaving.R: Usesaving.R to save your datasets. This process creates both the .csv file(s) and the data dictionary template file(s) for your datasets. Don’t save the CSV files using a separate process because we also need the data dictionaries.\n\nRun the first line of saving.R to create the functions we’ll use to save your dataset.\nProvide the name of your directory as dir_name.\nUse ttsave() for each dataset you created in cleaning.R, substituting the name for the dataset for YOUR_DATASET_DF.\n\n{dataset}.md: Edit the {dataset}.md files to describe your datasets (where {dataset} is the name of the dataset). These files are created by saving.R. There should be one file for each of your datasets. You most likely only need to edit the “description” column to provide a description of each variable.\nintro.md: Edit the intro.md file to describe your dataset. You don’t need to add a # Title at the top; this is just a paragraph or two to introduce the week.\nFind at least one image for your dataset. These often come from the article about your dataset. If you can’t find an image, create an example data visualization, and save the images in your folder as png files.\nmeta.yaml: Edit meta.yaml to provide information about your dataset and how we can credit you. You can delete lines from the credit block that do not apply to you.\n\n\n\n\nCommit the changes with this folder to your branch. In RStudio, you can do this on the “Git” tab (the “Commit” button).\nSubmit a pull request to https://github.com/rfordatascience/tidytuesday. In R, you can do this with usethis::pr_push(), and then follow the instructions in your browser."
  },
  {
    "objectID": "data/curated/template/instructions.html#prepare-the-dataset",
    "href": "data/curated/template/instructions.html#prepare-the-dataset",
    "title": "TidyTuesday",
    "section": "",
    "text": "These instructions are for preparing a dataset using the R programming language. We hope to provide instructions for other programming languages eventually.\nIf you have not yet set up your computer for submitting a dataset, please see the full instructions at https://github.com/rfordatascience/tidytuesday/blob/main/pr_instructions.md.\n\ncleaning.R: Modify the cleaning.R file to get and clean the data.\n\nWrite the code to download and clean the data in cleaning.R.\nIf you’re getting the data from a github repo, remember to use the ‘raw’ version of the URL.\nThis script should result in one or more data.frames, with descriptive variable names (eg players and teams, not df1 and df2).\n\nsaving.R: Usesaving.R to save your datasets. This process creates both the .csv file(s) and the data dictionary template file(s) for your datasets. Don’t save the CSV files using a separate process because we also need the data dictionaries.\n\nRun the first line of saving.R to create the functions we’ll use to save your dataset.\nProvide the name of your directory as dir_name.\nUse ttsave() for each dataset you created in cleaning.R, substituting the name for the dataset for YOUR_DATASET_DF.\n\n{dataset}.md: Edit the {dataset}.md files to describe your datasets (where {dataset} is the name of the dataset). These files are created by saving.R. There should be one file for each of your datasets. You most likely only need to edit the “description” column to provide a description of each variable.\nintro.md: Edit the intro.md file to describe your dataset. You don’t need to add a # Title at the top; this is just a paragraph or two to introduce the week.\nFind at least one image for your dataset. These often come from the article about your dataset. If you can’t find an image, create an example data visualization, and save the images in your folder as png files.\nmeta.yaml: Edit meta.yaml to provide information about your dataset and how we can credit you. You can delete lines from the credit block that do not apply to you.\n\n\n\n\nCommit the changes with this folder to your branch. In RStudio, you can do this on the “Git” tab (the “Commit” button).\nSubmit a pull request to https://github.com/rfordatascience/tidytuesday. In R, you can do this with usethis::pr_push(), and then follow the instructions in your browser."
  },
  {
    "objectID": "dataset_announcements.html",
    "href": "dataset_announcements.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "TidyTuesday is a social data project, which means we rely on social media. We do our best to foster a safe, diverse, welcoming community where possible!\nWe post datasets to social media every Monday morning. Data is currently posted at these locations:\n\nBluesky\nMastodon\nLinkedIn\nGitHub\nDSLC.io Slack\n\nWatch the #TidyTuesday, #DataViz, #RStats, and #PyData hashtags on your favorite social media platform! As additional APIs become available, we will likely add additional networks, provided those networks support a safe, diverse, welcoming community."
  },
  {
    "objectID": "dataset_announcements.html#weekly-dataset-announcements",
    "href": "dataset_announcements.html#weekly-dataset-announcements",
    "title": "TidyTuesday",
    "section": "",
    "text": "TidyTuesday is a social data project, which means we rely on social media. We do our best to foster a safe, diverse, welcoming community where possible!\nWe post datasets to social media every Monday morning. Data is currently posted at these locations:\n\nBluesky\nMastodon\nLinkedIn\nGitHub\nDSLC.io Slack\n\nWatch the #TidyTuesday, #DataViz, #RStats, and #PyData hashtags on your favorite social media platform! As additional APIs become available, we will likely add additional networks, provided those networks support a safe, diverse, welcoming community."
  },
  {
    "objectID": "dataset_review.html",
    "href": "dataset_review.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "We identify, curate, and post a new dataset every week, 51 weeks per year. To make that possible, we welcome dataset idea submissions. Once submitted, we need to verify that each dataset meets our standards.\nYou can help us identify usable datasets! Read through our open issues tagged with the dataset label, and leave a comment after checking for each of these things. Copy-paste this checklist as-is into your comment, then either add “x” between the [ ], or save your issue and then click the checkboxes. If anything isn’t provided in the original issue, but you can provide it, please do so!\n- [ ] I can download the dataset from the link provided.\n- [ ] The dataset will (probably) be less than 50MB when saved as a tidy CSV.\n- [ ] There is a link to an article that has something to do with the dataset.\n- [ ] I can imagine a data visualization related to this dataset.\n- [ ] This dataset has not already been used in TidyTuesday.\n- [ ] ALT text is provided for all (both) images.\n- [ ] There is a data dictionary describing the columns of the dataset.\n- [ ] The TidyTuesday maintainers are unlikely to get sued for using the dataset.\nSearches to help you identify issues that might need to be checked:\n\nI can download the dataset from the link provided.\nThe dataset will (probably) be less than 50MB when saved as a tidy CSV.\nThere is a link to an article that has something to do with the dataset.\nI can imagine a data visualization related to this dataset.\nThis dataset has not already been used in TidyTuesday.\nThere are links to two images related to the dataset (or images are attached to the issue).\nALT text is provided for all (both) images.\nThere is a data dictionary describing the columns of the dataset.\nThe TidyTuesday maintainers are unlikely to get sued for using the dataset.\n\nIf enough users show interest in helping us out, we will try to add a Shiny app to streamline the process!"
  },
  {
    "objectID": "dataset_review.html#dataset-review",
    "href": "dataset_review.html#dataset-review",
    "title": "TidyTuesday",
    "section": "",
    "text": "We identify, curate, and post a new dataset every week, 51 weeks per year. To make that possible, we welcome dataset idea submissions. Once submitted, we need to verify that each dataset meets our standards.\nYou can help us identify usable datasets! Read through our open issues tagged with the dataset label, and leave a comment after checking for each of these things. Copy-paste this checklist as-is into your comment, then either add “x” between the [ ], or save your issue and then click the checkboxes. If anything isn’t provided in the original issue, but you can provide it, please do so!\n- [ ] I can download the dataset from the link provided.\n- [ ] The dataset will (probably) be less than 50MB when saved as a tidy CSV.\n- [ ] There is a link to an article that has something to do with the dataset.\n- [ ] I can imagine a data visualization related to this dataset.\n- [ ] This dataset has not already been used in TidyTuesday.\n- [ ] ALT text is provided for all (both) images.\n- [ ] There is a data dictionary describing the columns of the dataset.\n- [ ] The TidyTuesday maintainers are unlikely to get sued for using the dataset.\nSearches to help you identify issues that might need to be checked:\n\nI can download the dataset from the link provided.\nThe dataset will (probably) be less than 50MB when saved as a tidy CSV.\nThere is a link to an article that has something to do with the dataset.\nI can imagine a data visualization related to this dataset.\nThis dataset has not already been used in TidyTuesday.\nThere are links to two images related to the dataset (or images are attached to the issue).\nALT text is provided for all (both) images.\nThere is a data dictionary describing the columns of the dataset.\nThe TidyTuesday maintainers are unlikely to get sued for using the dataset.\n\nIf enough users show interest in helping us out, we will try to add a Shiny app to streamline the process!"
  },
  {
    "objectID": "pr_instructions.html",
    "href": "pr_instructions.html",
    "title": "How to Submit a Dataset",
    "section": "",
    "text": "Thank you for helping us help learners!\n\n[!NOTE] This article talks about submitting datasets “from scratch,” but there’s an easier way! The {tidytuesdayR} R package has a set of functions for curating TidyTuesday datasets in R! The functions currently only work in Rstudio, but we hope to also support Positron soon!\n\nThere are 4 main steps to submit a dataset:\n\nFind a dataset.\nPrepare your repositry.\nCreate a branch.\nPrepare the dataset.\n\n\n\nFind a dataset that would be good for TidyTuesday: either one that is already ready for analysis, or one that you can clean so that it meets the criteria. These are the requirements for a dataset:\n\nData can be saved as one or more CSV files.\nThe whole dataset (all files) is less than 20MB.\nYou can describe each variable (either using an existing data dictionary or by creating your own dictionary).\nThe data is publicly available and free for reuse, either with or without attribution.\n\nYou will also need:\n\nThe source of the dataset\nAn article about the dataset or that uses the dataset\nAt least one image related to or using the dataset\n\n\n\n\nYou’ll need to perform this step the first time you submit a pull request to this repository. A “pull request” is a submission of code to a git repository. If you have never worked with git before, that’s fine! We’ll help you get set up.\n\nSet up git, github, and your IDE (such as RStudio). We have step-by-step instructions for setting up things to work with the Data Science Learning Community.\nFork the tidytuesday repository. In R, you can use usethis::create_from_github(\"rfordatascience/tidytuesday\") to create your personal fork on GitHub and copy it to your computer. Note: This requires about 8 GB of space on disk.\n\n\n\n\nWe use a fork/branch approach to pull requests, meaning you’ll create a version of the repo specifically for your changes, and then ask us to merge those changes into the main tidytuesday repository.\n\nIf you are on anything other than the main branch of your local repository, switch back to main. In R, you can use usethis::pr_pause() (if your previous submission is still pending), or usethis::pr_finish() (if we’ve accepted your submission).\nPull the latest version of the repository to your computer. In R, use usethis::pr_merge_main()\nCreate a new branch, with something similar to the name of the dataset you’re submitting. In R, you can create this branch using usethis::pr_init(BRANCHNAME). For instance if it’s a dataset on American baseball, something like “american-baseball” using usethis::pr_init(\"american-baseball\").\nNavigate to the data/curated folder in your branch of the repository.\nMake a copy of the template folder for your dataset, inside the curated folder. Name it something descriptive – the same name as your branch would work, so “american-baseball” not “my_dataset”.\nInside the folder you just created is where you’re going to do your work.\n\n\n\n\nA copy the following instructions is also available in the folder you’ve created, as instructions.md. These instructions are for preparing a dataset using the R programming language, but we hope to provide instructions for other programming languages eventually.\n\ncleaning.R: Modify the cleaning.R file to get and clean the data.\n\nWrite the code to download and clean the data in cleaning.R.\nIf you’re getting the data from a github repo, remember to use the ‘raw’ version of the URL.\nThis script should result in one or more data.frames, with descriptive variable names (eg players and teams, not df1 and df2).\n\nsaving.R: Usesaving.R to save your datasets. This process creates both the .csv file(s) and the data dictionary template file(s) for your datasets. Don’t save the CSV files using a separate process because we also need the data dictionaries.\n\nRun the first line of saving.R to create the functions we’ll use to save your dataset.\nProvide the name of your directory as dir_name.\nUse ttsave() for each dataset you created in cleaning.R, substituting the name for the dataset for YOUR_DATASET_DF.\n\n{dataset}.md: Edit the {dataset}.md files to describe your datasets (where {dataset} is the name of the dataset). These files are created by saving.R. There should be one file for each of your datasets. You most likely only need to edit the “description” column to provide a description of each variable.\nintro.md: Edit the intro.md file to describe your dataset. You don’t need to add a # Title at the top; this is just a paragraph or two to introduce the week.\nFind at least one image for your dataset. These often come from the article about your dataset. If you can’t find an image, create an example data visualization, and save the images in your folder as png files.\nmeta.yaml: Edit meta.yaml to provide information about your dataset and how we can credit you. You can delete lines from the credit block that do not apply to you.\n\n\n\n\nCommit the changes with this folder to your branch. In RStudio, you can do this on the “Git” tab (the “Commit” button).\nSubmit a pull request to https://github.com/rfordatascience/tidytuesday. In R, you can do this with usethis::pr_push(), and then follow the instructions in your browser."
  },
  {
    "objectID": "pr_instructions.html#find-a-dataset",
    "href": "pr_instructions.html#find-a-dataset",
    "title": "How to Submit a Dataset",
    "section": "",
    "text": "Find a dataset that would be good for TidyTuesday: either one that is already ready for analysis, or one that you can clean so that it meets the criteria. These are the requirements for a dataset:\n\nData can be saved as one or more CSV files.\nThe whole dataset (all files) is less than 20MB.\nYou can describe each variable (either using an existing data dictionary or by creating your own dictionary).\nThe data is publicly available and free for reuse, either with or without attribution.\n\nYou will also need:\n\nThe source of the dataset\nAn article about the dataset or that uses the dataset\nAt least one image related to or using the dataset"
  },
  {
    "objectID": "pr_instructions.html#prepare-your-repository",
    "href": "pr_instructions.html#prepare-your-repository",
    "title": "How to Submit a Dataset",
    "section": "",
    "text": "You’ll need to perform this step the first time you submit a pull request to this repository. A “pull request” is a submission of code to a git repository. If you have never worked with git before, that’s fine! We’ll help you get set up.\n\nSet up git, github, and your IDE (such as RStudio). We have step-by-step instructions for setting up things to work with the Data Science Learning Community.\nFork the tidytuesday repository. In R, you can use usethis::create_from_github(\"rfordatascience/tidytuesday\") to create your personal fork on GitHub and copy it to your computer. Note: This requires about 8 GB of space on disk."
  },
  {
    "objectID": "pr_instructions.html#create-a-branch",
    "href": "pr_instructions.html#create-a-branch",
    "title": "How to Submit a Dataset",
    "section": "",
    "text": "We use a fork/branch approach to pull requests, meaning you’ll create a version of the repo specifically for your changes, and then ask us to merge those changes into the main tidytuesday repository.\n\nIf you are on anything other than the main branch of your local repository, switch back to main. In R, you can use usethis::pr_pause() (if your previous submission is still pending), or usethis::pr_finish() (if we’ve accepted your submission).\nPull the latest version of the repository to your computer. In R, use usethis::pr_merge_main()\nCreate a new branch, with something similar to the name of the dataset you’re submitting. In R, you can create this branch using usethis::pr_init(BRANCHNAME). For instance if it’s a dataset on American baseball, something like “american-baseball” using usethis::pr_init(\"american-baseball\").\nNavigate to the data/curated folder in your branch of the repository.\nMake a copy of the template folder for your dataset, inside the curated folder. Name it something descriptive – the same name as your branch would work, so “american-baseball” not “my_dataset”.\nInside the folder you just created is where you’re going to do your work."
  },
  {
    "objectID": "pr_instructions.html#prepare-the-dataset",
    "href": "pr_instructions.html#prepare-the-dataset",
    "title": "How to Submit a Dataset",
    "section": "",
    "text": "A copy the following instructions is also available in the folder you’ve created, as instructions.md. These instructions are for preparing a dataset using the R programming language, but we hope to provide instructions for other programming languages eventually.\n\ncleaning.R: Modify the cleaning.R file to get and clean the data.\n\nWrite the code to download and clean the data in cleaning.R.\nIf you’re getting the data from a github repo, remember to use the ‘raw’ version of the URL.\nThis script should result in one or more data.frames, with descriptive variable names (eg players and teams, not df1 and df2).\n\nsaving.R: Usesaving.R to save your datasets. This process creates both the .csv file(s) and the data dictionary template file(s) for your datasets. Don’t save the CSV files using a separate process because we also need the data dictionaries.\n\nRun the first line of saving.R to create the functions we’ll use to save your dataset.\nProvide the name of your directory as dir_name.\nUse ttsave() for each dataset you created in cleaning.R, substituting the name for the dataset for YOUR_DATASET_DF.\n\n{dataset}.md: Edit the {dataset}.md files to describe your datasets (where {dataset} is the name of the dataset). These files are created by saving.R. There should be one file for each of your datasets. You most likely only need to edit the “description” column to provide a description of each variable.\nintro.md: Edit the intro.md file to describe your dataset. You don’t need to add a # Title at the top; this is just a paragraph or two to introduce the week.\nFind at least one image for your dataset. These often come from the article about your dataset. If you can’t find an image, create an example data visualization, and save the images in your folder as png files.\nmeta.yaml: Edit meta.yaml to provide information about your dataset and how we can credit you. You can delete lines from the credit block that do not apply to you.\n\n\n\n\nCommit the changes with this folder to your branch. In RStudio, you can do this on the “Git” tab (the “Commit” button).\nSubmit a pull request to https://github.com/rfordatascience/tidytuesday. In R, you can do this with usethis::pr_push(), and then follow the instructions in your browser."
  },
  {
    "objectID": "static/templates/how_to_participate.html",
    "href": "static/templates/how_to_participate.html",
    "title": "TidyTuesday",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  },
  {
    "objectID": "static/templates/how_to_participate.html#how-to-participate",
    "href": "static/templates/how_to_participate.html#how-to-participate",
    "title": "TidyTuesday",
    "section": "",
    "text": "Explore the data, watching out for interesting relationships. We would like to emphasize that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\nCreate a visualization, a model, a Quarto report, a shiny app, or some other piece of data-science-related output, using R, Python, or another programming language.\nShare your output and the code used to generate it on social media with the #TidyTuesday hashtag.\nSubmit your own dataset!\n\n\n\n\nExploring the TidyTuesday data in Python? Posit has some extra resources for you! Have you tried making a Quarto dashboard? Find videos and other resources in Posit’s PydyTuesday repo.\nShare your work with the world using the hashtags #TidyTuesday and #PydyTuesday so that Posit has the chance to highlight your work, too!\nDeploy or share your work however you want! If you’d like a super easy way to publish your work, give Connect Cloud a try."
  }
]